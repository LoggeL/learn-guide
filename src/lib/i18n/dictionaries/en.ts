export const en = {
  // Common UI
  common: {
    learnAi: 'Learn AI',
    interactiveGuide: 'Interactive Guide',
    topics: 'Topics',
    search: 'Search...',
    searchTopics: 'Search topics...',
    startTyping: 'Start typing to search topics...',
    trySearching: 'Try "temperature" or "attention"',
    noResults: 'No results found for',
    pressEsc: 'Press ESC to close',
    enterToSelect: 'Enter to select',
    previous: 'Previous',
    next: 'Next',
    projectBy: 'A project by',
    proTip: 'Pro tip: Press',
    toSearchTopics: 'to search topics',
    interactiveAiLearning: 'Interactive AI Learning',
    guidesDescription: 'Interactive guides to understand AI concepts',
  },

  // Home page
  home: {
    heroTitle1: 'Master AI Concepts',
    heroTitle2: 'Through Experience',
    heroDescription: 'Explore artificial intelligence and large language models through beautiful, interactive demonstrations. Learn by doing, not just reading.',
    startLearning: 'Start Learning',
    browseTopics: 'Browse Topics',
    exploreTopics: 'Explore Topics',
    diveIntoLessons: 'Dive into interactive lessons',
  },

  // Features
  features: {
    interactiveDemos: 'Interactive Demos',
    interactiveDemosDesc: 'Hands-on explorations that make abstract concepts tangible and intuitive.',
    visualLearning: 'Visual Learning',
    visualLearningDesc: 'Beautiful visualizations that reveal how AI systems actually work under the hood.',
    buildIntuition: 'Build Intuition',
    buildIntuitionDesc: 'Go beyond memorization—develop deep understanding through experimentation.',
  },

  // Topic categories
  categories: {
    ai: 'Artificial Intelligence',
    agents: 'AI Agents',
    llm: 'Large Language Models',
  },

  // Topic names
  topicNames: {
    'agent-loop': 'The Agent Loop',
    'agent-context': 'Context Anatomy',
    'agent-problems': 'Agent Problems',
    'agent-security': 'Agent Security',
    'agentic-patterns': 'Agentic Patterns',
    'mcp': 'MCP (Model Context Protocol)',
    'context-rot': 'Context Rot',
    'temperature': 'Temperature',
    'attention': 'Attention Mechanism',
    'vision': 'Vision & Images',
    'visual-challenges': 'Visual Challenges',
  },

  // Temperature page
  temperature: {
    title: 'Temperature',
    description: 'Understanding how a single parameter controls the balance between predictable logic and creative randomness in AI outputs.',
    whatIs: 'What is Temperature?',
    whatIsDesc: 'In LLMs, Temperature is a hyperparameter that scales the "logits" (raw scores) of the next token predictions before they are converted into probabilities. It essentially controls how much the model favors the most likely options versus exploring less likely ones.',
    lowTemp: 'Low Temperature',
    lowTempDesc: 'Focuses on the top results. Reliable, consistent, and factual. Great for code, math, and structured data.',
    highTemp: 'High Temperature',
    highTempDesc: 'Spreads probability to more tokens. Diverse, creative, and surprising. Great for stories, brainstorming, and poetry.',
    interactiveDistribution: 'Interactive Distribution',
    adjustSlider: 'Adjust the temperature to see its effect',
    adjustDesc: 'Adjust the temperature slider to see how it reshapes the probability distribution for the next token. Watch how "the" (the most likely choice) dominates at low temperatures and loses its lead as the temperature rises.',
    howItWorks: 'How it Works Mathematically',
    mathDesc: 'The model generates a score for every possible token. To get probabilities, we use the Softmax function, modified by temperature:',
    whenLow: 'When T → 0',
    low: 'Low',
    whenLowDesc: 'Dividing by a small T amplifies differences between scores. The highest logit dominates exponentially.',
    whenHigh: 'When T → ∞',
    high: 'High',
    whenHighDesc: 'Dividing by a large T compresses all scores toward zero, making them nearly equal after exponentiation.',
    practicalGuidelines: 'Practical Guidelines',
    useCase: 'Use Case',
    tempLabel: 'Temperature',
    why: 'Why?',
    codingMath: 'Coding & Math',
    codingMathWhy: 'Errors in logic are costly; you want the most likely correct path.',
    factRetrieval: 'Fact Retrieval',
    factRetrievalWhy: 'Reduces "hallucinations" by sticking to the most probable data points.',
    generalChat: 'General Chat',
    generalChatWhy: 'The "sweet spot" for most models to sound natural and helpful.',
    creativeWriting: 'Creative Writing',
    creativeWritingWhy: 'Encourages the model to use more interesting, varied vocabulary.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generates wild, unconventional ideas that might spark inspiration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Temperature 0 is deterministic ("Greedy Search") — always picks the top token',
    takeaway2: 'Higher temperature increases variety and creativity but decreases coherence',
    takeaway3: 'Too high temperature (> 1.5) often results in gibberish',
    takeaway4: "Always match your temperature to the task's requirement for precision vs. creativity",
  },

  // Context Rot page
  contextRot: {
    title: 'Context Rot',
    description: 'Understanding how information degrades over long conversations and why LLMs struggle with extended contexts.',
    whatIs: 'What is Context Rot?',
    whatIsDesc: 'Context Rot refers to the gradual degradation of an LLM\'s ability to accurately recall and use information from earlier parts of a long conversation or document. As context grows, the model\'s attention becomes diluted.',
    whyHappens: 'Why Does It Happen?',
    whyHappensDesc: 'LLMs have finite context windows and use attention mechanisms that must distribute focus across all tokens. As conversations grow longer, earlier information competes with newer content for the model\'s limited attention capacity.',
    symptoms: 'Common Symptoms',
    symptom1: 'Forgetting instructions given at the start of a conversation',
    symptom2: 'Contradicting earlier statements or decisions',
    symptom3: 'Losing track of complex multi-step tasks',
    symptom4: 'Mixing up details from different parts of the context',
    mitigation: 'Mitigation Strategies',
    mitigation1: 'Summarize important context periodically',
    mitigation2: 'Place critical instructions at both start and end',
    mitigation3: 'Use structured formats to highlight key information',
    mitigation4: 'Break long tasks into smaller, focused conversations',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'See how memory fades as context length increases',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context rot is an inherent limitation of current LLM architectures',
    takeaway2: 'The "lost in the middle" effect means information at the start and end is recalled better',
    takeaway3: 'Strategic information placement can significantly improve recall',
    takeaway4: 'Regular summarization helps maintain important context over long conversations',
  },

  // Attention page
  attention: {
    title: 'Attention Mechanism',
    description: 'Explore how transformers focus on relevant parts of input through the powerful attention mechanism.',
    whatIs: 'What is Attention?',
    whatIsDesc: 'Attention is the core mechanism that allows transformers to weigh the importance of different parts of the input when generating each output token. It enables the model to "focus" on relevant context.',
    howWorks: 'How It Works',
    howWorksDesc: 'For each position, the model computes Query, Key, and Value vectors. Attention scores are calculated by comparing queries to keys, then used to create a weighted sum of values.',
    selfAttention: 'Self-Attention',
    selfAttentionDesc: 'Allows each token to attend to all other tokens in the sequence, capturing relationships regardless of distance.',
    multiHead: 'Multi-Head Attention',
    multiHeadDesc: 'Multiple attention heads allow the model to focus on different types of relationships simultaneously.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Attention enables transformers to capture long-range dependencies',
    takeaway2: 'The quadratic complexity of attention limits context window size',
    takeaway3: 'Different attention heads learn to focus on different linguistic patterns',
    takeaway4: 'Attention visualization can help interpret model behavior',
  },

  // Vision page
  vision: {
    title: 'Vision & Images',
    description: 'How modern LLMs process and understand visual information alongside text.',
    whatIs: 'How LLMs See Images',
    whatIsDesc: 'Vision-enabled LLMs convert images into sequences of tokens that can be processed alongside text. This typically involves dividing images into patches and encoding them with a vision transformer.',
    patchEncoding: 'Patch Encoding',
    patchEncodingDesc: 'Images are divided into fixed-size patches (e.g., 14x14 pixels), each converted into an embedding vector similar to text tokens.',
    multimodal: 'Multimodal Understanding',
    multimodalDesc: 'The model learns to align visual and textual representations, enabling tasks like image captioning, visual QA, and document understanding.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Images consume many more tokens than equivalent text descriptions',
    takeaway2: 'Resolution and patch size affect detail recognition',
    takeaway3: 'Visual understanding is approximate—models can miss fine details',
    takeaway4: 'Combining vision and language enables powerful new applications',
  },

  // Visual Challenges page
  visualChallenges: {
    title: 'Visual Challenges',
    description: 'Common challenges and limitations when working with vision-enabled AI models.',
    overview: 'Common Visual Challenges',
    overviewDesc: 'While vision models are impressive, they face several systematic challenges that are important to understand when building applications.',
    challenge1: 'Counting Objects',
    challenge1Desc: 'Models often struggle to accurately count objects in images, especially when there are many similar items.',
    challenge2: 'Spatial Reasoning',
    challenge2Desc: 'Understanding precise spatial relationships between objects (left/right, above/below) can be unreliable.',
    challenge3: 'Small Text Recognition',
    challenge3Desc: 'Fine text in images may be misread or missed entirely, especially at low resolutions.',
    challenge4: 'Hallucination',
    challenge4Desc: 'Models may describe objects or details that aren\'t actually present in the image.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Always verify critical visual information through other means',
    takeaway2: 'Higher resolution images generally improve accuracy',
    takeaway3: 'Break complex visual tasks into simpler sub-questions',
    takeaway4: 'Be explicit about what aspects of an image you need analyzed',
  },

  // Agent Loop page
  agentLoop: {
    title: 'The Agent Loop',
    description: 'Understanding the core cycle that powers autonomous AI agents: observe, think, act, repeat.',
    whatIs: 'What is the Agent Loop?',
    whatIsDesc: 'The agent loop is the fundamental cycle that enables AI agents to interact with their environment autonomously. It consists of observation, reasoning, action, and feedback phases that repeat continuously.',
    phases: 'The Four Phases',
    observe: 'Observe',
    observeDesc: 'Gather information from the environment, tools, and user input.',
    think: 'Think',
    thinkDesc: 'Reason about the current state and decide on the next action.',
    act: 'Act',
    actDesc: 'Execute the chosen action using available tools.',
    learn: 'Learn',
    learnDesc: 'Process feedback and update understanding for the next iteration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'The loop continues until the task is complete or terminated',
    takeaway2: 'Each iteration builds on previous observations and actions',
    takeaway3: 'Error handling and recovery are crucial for robust agents',
    takeaway4: 'The quality of tools directly impacts agent capabilities',
  },

  // Agent Context page
  agentContext: {
    title: 'Context Anatomy',
    description: 'Breaking down the structure of context windows and how agents manage information.',
    whatIs: 'Understanding Agent Context',
    whatIsDesc: 'Agent context includes the system prompt, conversation history, tool definitions, and retrieved information. Managing this context efficiently is crucial for agent performance.',
    components: 'Context Components',
    systemPrompt: 'System Prompt',
    systemPromptDesc: 'Defines the agent\'s role, capabilities, and behavioral guidelines.',
    toolDefs: 'Tool Definitions',
    toolDefsDesc: 'Descriptions of available tools and how to use them.',
    history: 'Conversation History',
    historyDesc: 'Previous messages, tool calls, and their results.',
    retrieved: 'Retrieved Information',
    retrievedDesc: 'External knowledge fetched during the conversation.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context management is key to agent reliability',
    takeaway2: 'Prioritize recent and relevant information',
    takeaway3: 'Tool definitions should be clear and unambiguous',
    takeaway4: 'Summarization helps maintain context over long sessions',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agent Problems',
    description: 'Common failure modes and challenges faced by AI agents in real-world applications.',
    overview: 'Common Agent Failure Modes',
    overviewDesc: 'Understanding typical agent failures helps in building more robust systems and setting appropriate expectations.',
    problem1: 'Tool Misuse',
    problem1Desc: 'Agents may call tools incorrectly, with wrong parameters, or at inappropriate times.',
    problem2: 'Infinite Loops',
    problem2Desc: 'Agents can get stuck repeating the same actions without making progress.',
    problem3: 'Goal Drift',
    problem3Desc: 'Agents may gradually shift focus away from the original task objective.',
    problem4: 'Over-confidence',
    problem4Desc: 'Agents may proceed with actions despite uncertainty or incomplete information.',
    
    // Expanded Content
    hallucination: 'Tool Hallucination',
    hallucinationDesc: 'Agents sometimes "invent" tool parameters or even entire tools that don\'t exist. This usually happens when the tool definition is ambiguous or when the model tries to force a solution.',
    hallucinationExample: 'Example: Calling `get_weather(location="Tokyo", date="tomorrow")` when the function only accepts `location`.',
    
    loops: 'Looping Issues',
    loopsDesc: 'Agents can get trapped in repetitive cycles where they perform the same action, receive the same error, and try again without changing strategy.',
    loopsMitigation: 'Mitigation: Implement loop detection logic that stops execution if the same tool call sequence occurs multiple times.',
    
    costLatency: 'Cost & Latency',
    costLatencyDesc: 'Every step in the agent loop requires a full LLM inference call. Multi-step tasks can quickly become expensive and slow.',
    costFactor: 'The Cost Factor',
    costFactorDesc: 'A simple task requiring 5 steps means 5x the cost and 5x the latency of a standard chat response.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Implement safeguards like iteration limits and cost controls',
    takeaway2: 'Add human-in-the-loop checkpoints for critical actions',
    takeaway3: 'Monitor agent behavior and log all actions for debugging',
    takeaway4: 'Design clear success and failure criteria',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agent Security',
    description: 'Security considerations and best practices when deploying autonomous AI agents.',
    overview: 'Security Considerations',
    overviewDesc: 'Autonomous agents with tool access present unique security challenges that must be carefully addressed.',
    threat1: 'Prompt Injection',
    threat1Desc: 'Malicious inputs can manipulate agent behavior through crafted prompts.',
    threat2: 'Tool Abuse',
    threat2Desc: 'Agents might be tricked into using tools in harmful ways.',
    threat3: 'Data Exfiltration',
    threat3Desc: 'Agents with network access could leak sensitive information.',
    threat4: 'Privilege Escalation',
    threat4Desc: 'Agents might gain access to resources beyond their intended scope.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Apply principle of least privilege to agent capabilities',
    takeaway2: 'Validate and sanitize all inputs before tool execution',
    takeaway3: 'Implement rate limiting and usage monitoring',
    takeaway4: 'Use sandboxing for potentially dangerous operations',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentic Patterns',
    description: 'Design patterns and architectures for building effective AI agent systems.',
    overview: 'Common Agentic Patterns',
    overviewDesc: 'Several architectural patterns have emerged as effective approaches for building AI agent systems.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Interleave reasoning traces with actions for better transparency and control.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Create a high-level plan first, then execute steps sequentially.',
    pattern3: 'Multi-Agent Systems',
    pattern3Desc: 'Multiple specialized agents collaborate to solve complex tasks.',
    pattern4: 'Reflection',
    pattern4Desc: 'Agents review their own outputs and iteratively improve them.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Choose patterns based on task complexity and reliability needs',
    takeaway2: 'ReAct is great for transparency but may be slower',
    takeaway3: 'Multi-agent systems add complexity but enable specialization',
    takeaway4: 'Reflection patterns can significantly improve output quality',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'Understanding MCP: when external tool servers make sense, and when they are overkill.',
    whatIs: 'What is MCP?',
    whatIsDesc: 'The Model Context Protocol (MCP) is a standardized way to connect AI agents to external tools and data sources via dedicated server processes. Instead of defining tools inline in your agent code, MCP runs a separate server that exposes tools over a structured protocol.',
    vsToolCalls: 'MCP vs. Regular Tool Calls',
    vsToolCallsDesc: 'Regular tool calls are functions defined directly in your agent\'s codebase. The agent calls them, they execute, and results return in the same process. MCP separates this: tools live in external servers that the agent communicates with over a protocol.',
    
    // Comparison
    regularTools: 'Regular Tool Calls',
    regularToolsDesc: 'Tools defined inline in your agent code. Simple, fast, and sufficient for most use cases.',
    mcpTools: 'MCP Servers',
    mcpToolsDesc: 'Tools exposed by external server processes. Adds network overhead but enables cross-language tooling and shared tool ecosystems.',
    
    // When to use
    whenToUse: 'When MCP Makes Sense',
    whenToUseDesc: 'MCP shines in specific scenarios where its additional complexity pays off.',
    useCase1: 'Multi-Language Teams',
    useCase1Desc: 'Your tools are written in Python but your agent is in TypeScript, or vice versa.',
    useCase2: 'Shared Tool Ecosystem',
    useCase2Desc: 'Multiple agents across different projects need to access the same tools.',
    useCase3: 'Enterprise Integration',
    useCase3Desc: 'You need to expose existing internal services as agent tools without modifying them.',
    useCase4: 'Tool Marketplace',
    useCase4Desc: 'You want to use community-maintained tools without copying code into your project.',
    
    // When it's overkill
    overkill: 'When MCP is Overkill',
    overkillDesc: 'For many use cases, MCP adds unnecessary complexity.',
    overkillCase1: 'Single-Language Projects',
    overkillCase1Desc: 'If your tools and agent are in the same language, inline functions are simpler and faster.',
    overkillCase2: 'Simple Agents',
    overkillCase2Desc: 'A chatbot with a few tools doesn\'t need the overhead of running separate server processes.',
    overkillCase3: 'Rapid Prototyping',
    overkillCase3Desc: 'When iterating quickly, the indirection of MCP slows down development.',
    overkillCase4: 'Latency-Sensitive Apps',
    overkillCase4Desc: 'Network calls to tool servers add latency that inline functions don\'t have.',
    
    // Architecture
    architecture: 'How MCP Works',
    architectureDesc: 'MCP defines a client-server architecture where the agent is the client and tools are exposed by servers.',
    step1: 'Discovery',
    step1Desc: 'The agent connects to an MCP server and receives a list of available tools with their schemas.',
    step2: 'Invocation',
    step2Desc: 'When the LLM decides to use a tool, the agent sends a request to the MCP server.',
    step3: 'Execution',
    step3Desc: 'The MCP server runs the tool and returns results in a standardized format.',
    step4: 'Integration',
    step4Desc: 'Results flow back to the agent and into the LLM context, just like regular tool results.',
    
    // Practical advice
    practicalAdvice: 'Practical Advice',
    adviceDesc: 'Guidelines for deciding whether to use MCP in your project.',
    advice1: 'Start simple: use inline tool definitions until you hit a specific limitation.',
    advice2: 'Consider MCP when you find yourself copy-pasting tool code between projects.',
    advice3: 'The overhead of running MCP servers only makes sense at scale or in enterprise settings.',
    advice4: 'Community MCP servers can accelerate development but add dependency risks.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'MCP is a protocol for exposing tools via external servers, not a replacement for regular tool calls',
    takeaway2: 'For most single-project agents, inline tools are simpler and have lower latency',
    takeaway3: 'MCP shines in polyglot environments and shared tool ecosystems',
    takeaway4: 'Don\'t reach for MCP by default—it\'s a solution for specific scaling and interoperability challenges',
  },

  // Metadata
  metadata: {
    title: 'Learn AI Concepts | Interactive Guide',
    description: 'Master artificial intelligence and large language model concepts through beautiful, interactive demonstrations.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Control Panel',
    adjustTemperature: 'Adjust the temperature',
    temperature: 'Temperature',
    samplePrompt: 'Sample Prompt',
    onceUponATime: '"Once upon a time, there was..."',
    liveCompletion: 'Live Completion',
    regenerate: 'Regenerate',
    deterministic: 'Deterministic',
    balanced: 'Balanced',
    creative: 'Creative',
    chaotic: 'Chaotic',
    frozen: 'Frozen',
    focused: 'Focused',
    wild: 'Wild',
    greedyMode: 'Greedy mode: Always picks the single most probable token.',
    lowTemp: 'Low temperature: Focused on high-probability continuations.',
    balancedTemp: 'Balanced: Natural mix of predictability and variety.',
    highTemp: 'High temperature: Exploring creative, less common word choices.',
    veryHighTemp: 'Very high: Probability distribution is nearly uniform—expect chaos!',
    
    // Context Rot Simulator
    setInstruction: 'Set Your System Instruction',
    persistInstruction: 'This should persist throughout the conversation',
    systemPrompt: 'System Prompt',
    quickExamples: 'Quick Examples',
    startSimulation: 'Start Simulation',
    contextOverflow: 'Context Overflow!',
    conversation: 'Conversation',
    messagesPushed: 'messages pushed out of window',
    messages: 'messages',
    overflowIt: 'Overflow It!',
    reset: 'Reset',
    typeMessage: 'Type a message...',
    systemInstructionLost: 'System Instruction Lost!',
    systemLostDesc: 'Your system instruction has been pushed completely out of the context window. The model can no longer see it at all—it\'s as if you never gave the instruction. This is the worst case of context rot: total amnesia.',
    contextFilling: 'Context Filling Up',
    contextFillingDesc: 'Your system instruction is losing influence as newer messages take priority. Notice how it\'s fading visually—this represents the model\'s waning attention to it.',
    exampleFrench: 'Always respond in French.',
    examplePirate: 'You are a pirate. Say \'Arrr\' a lot.',
    exampleHaiku: 'End every response with a haiku.',
    labelFrench: 'Speak French',
    labelPirate: 'Be a Pirate',
    labelHaiku: 'End with Haiku',

    // Attention Visualizer
    hoverToSee: 'Hover to see attention weights',
    token: 'Token',
    attentionScore: 'Attention Score',
    strongConnection: 'Strong Connection',
    weakConnection: 'Weak Connection',

    // Patch Grid Visualizer
    originalImage: 'Original Image',
    patchGrid: 'Patch Grid',
    flattenedPatches: 'Flattened Patches',
    transformerInput: 'Transformer Input',
    processDesc: 'The image is split into a fixed grid of patches (e.g., 16x16 pixels). Each patch is then flattened into a vector and linearly projected into an embedding space.',

    // Agent Loop Visualizer
    startLoop: 'Start Loop',
    step: 'Step',
    context: 'Context',
    llmResponse: 'LLM Response',
    toolExecution: 'Tool Execution',
    finalAnswer: 'Final Answer',
    system: 'System',
    user: 'User',
    assistant: 'Assistant',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Plan & Execute',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflection',
    patternDesc: 'Select a pattern to see how it structures the agent\'s workflow.',
  },
}

// Create a recursive string type for the dictionary
type DeepStringify<T> = {
  [K in keyof T]: T[K] extends Record<string, unknown> ? DeepStringify<T[K]> : string
}

export type Dictionary = DeepStringify<typeof en>
