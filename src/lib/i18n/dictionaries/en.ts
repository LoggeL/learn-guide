export const en = {
  // Common UI
  common: {
    learnAi: 'Learn AI',
    interactiveGuide: 'Interactive Guide',
    topics: 'Topics',
    search: 'Search...',
    searchTopics: 'Search topics...',
    startTyping: 'Start typing to search topics...',
    trySearching: 'Try "temperature" or "attention"',
    noResults: 'No results found for',
    pressEsc: 'Press ESC to close',
    enterToSelect: 'Enter to select',
    previous: 'Previous',
    next: 'Next',
    projectBy: 'A project by',
    blog: 'Blog',
    proTip: 'Pro tip: Press',
    toSearchTopics: 'to search topics',
    interactiveAiLearning: 'Interactive AI Learning',
    helpMakeThisBetter: 'Help Make This Better',
    helpMakeThisBetterDesc: 'This guide is open source. Got an idea for a new topic? Found a bug? Want to improve an explanation? Every contribution helps.',
    requestATopic: 'Request a Topic',
    reportABug: 'Report a Bug',
    starOnGithub: 'Star on GitHub',
    guidesDescription: 'Interactive guides to understand AI concepts',
    backToAllTopics: 'Back to all topics',
    lastUpdated: 'Last updated',
    recentlyUpdated: 'Recently Updated',
    whatsNew: "What's New",
  },

  // 404 page
  notFound: {
    badge: '404: Plot Twist',
    title: 'This page went rogue.',
    description: 'Either the link is broken, or this page escaped into another branch.',
    joke1: 'I checked three agents. Two blamed caching, one blamed mercury retrograde.',
    joke2: 'Tried adding more context. The page still said: "Nah."',
    joke3: 'Good news: your wallet is safe here. No tokens burned on this URL.',
    goHome: 'Back to home',
    exploreFavorite: "Open Logge's Favourite Model",
    exploreTemperature: 'Open Temperature',
    hint: 'If this was a real link, it probably moved during a refactor.',
  },

  // Home page
  home: {
    heroTitle1: 'Master AI Concepts',
    heroTitle2: 'Through Experience',
    heroDescription: 'Explore artificial intelligence and large language models through beautiful, interactive demonstrations. Learn by doing, not just reading.',
    startLearning: 'Start Learning',
    browseTopics: 'Browse Topics',
    exploreTopics: 'Explore Topics',
    diveIntoLessons: 'Dive into interactive lessons',
  },

  // Features
  features: {
    interactiveDemos: 'Interactive Demos',
    interactiveDemosDesc: 'Hands-on explorations that make abstract concepts tangible and intuitive.',
    visualLearning: 'Visual Learning',
    visualLearningDesc: 'Beautiful visualizations that reveal how AI systems actually work under the hood.',
    buildIntuition: 'Build Intuition',
    buildIntuitionDesc: 'Go beyond memorization—develop deep understanding through experimentation.',
  },

  // Topic categories
  categories: {
    ai: 'Artificial Intelligence',
    agents: 'AI Agents',
    llm: 'Large Language Models',
    mlFundamentals: 'ML Fundamentals',
    prompting: 'Prompting',
    safety: 'AI Safety & Ethics',
    industry: 'AI Industry',
    llmInference: 'LLM Inference',
  },

  // Category descriptions (for category landing pages)
  categoryDescriptions: {
    agents: 'Learn how to build autonomous AI systems that can reason, plan, and take actions using tools and memory.',
    llm: 'Understand the inner workings of large language models, from tokenization to attention mechanisms.',
    'ml-fundamentals': 'Master the foundational concepts of machine learning that power modern AI systems.',
    prompting: 'Discover techniques to communicate effectively with AI models and get better results.',
    safety: 'Explore the ethical considerations and best practices for building responsible AI systems.',
    industry: 'Explore the companies and trends shaping the AI industry landscape.',
    'llm-inference': 'Understand how large language models generate text efficiently — from KV caching to batching strategies and serving infrastructure.',
  },

  // Topic names
  topicNames: {
    // Getting Started
    'hands-on': 'Hands-On',
    'getting-started': 'Getting Started',
    'getting-started-section': 'Start Here',
    // Agent subcategories
    'agents-core': 'Core Concepts',
    'agents-building': 'Building Blocks',
    'agents-patterns': 'Patterns',
    'agents-quality': 'Quality & Security',
    // Agent topics
    'agent-loop': 'The Agent Loop',
    'agent-context': 'Context Anatomy',
    'agent-problems': 'Agent Problems',
    'agent-security': 'Agent Security',
    'agentic-patterns': 'Agentic Patterns',
    'mcp': 'MCP (Model Context Protocol)',
    'tool-design': 'Tool Design',
    'memory': 'Memory Systems',
    'orchestration': 'Orchestration',
    'evaluation': 'Evaluation',
    'skills': 'Agent Skills',
    // LLM subcategories
    'llm-fundamentals': 'Fundamentals',
    'llm-behavior': 'Behavior',
    'llm-capabilities': 'Capabilities',
    'llm-architecture': 'Architecture',
    // LLM topics
    'tokenization': 'Tokenization',
    'embeddings': 'Embeddings',
    'rag': 'RAG',
    'context-rot': 'Context Rot',
    'temperature': 'Temperature',
    'attention': 'Attention Mechanism',
    'vision': 'Vision & Images',
    'visual-challenges': 'Visual Challenges',
    'agentic-vision': 'Agentic Vision',
    'multimodality': 'Multimodality',
    'transformer-architecture': 'Transformer Architecture',
    'llm-training': 'LLM Training',
    'moe': 'Mixture of Experts',
    'quantization': 'Quantization',
    'nested-learning': 'Nested Learning',
    'distillation': 'Distillation',
    'lora': 'Fine-Tuning & LoRA',
    'speculative-decoding': 'Speculative Decoding',
    // LLM Inference
    'llm-inference': 'LLM Inference',
    'kv-cache': 'KV Cache',
    'prompt-caching': 'Prompt Caching',
    'batching': 'Batching & Throughput',
    'local-inference': 'Running Models Locally',
    // ML Fundamentals
    'ml-fundamentals': 'ML Fundamentals',
    'neural-networks': 'Neural Networks',
    'gradient-descent': 'Gradient Descent',
    'training': 'Training Process',
    'world-models': 'World Models',
    // Prompting
    'prompt-basics': 'Prompt Basics',
    'advanced-prompting': 'Advanced Techniques',
    'system-prompts': 'System Prompts',
    // Safety
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Responsible AI',
    // Industry
    'european-ai': 'AI Made in Europe',
    'open-source': 'Open Source Advantages',
    'logges-favourite-model': "Logge's Favourite Models",
  },

  // Topic descriptions (short, for cards and search results)
  topicDescriptions: {
    'getting-started': 'Make your first LLM API call in 10 minutes — for free',
    'agent-loop': 'The observe-think-act cycle that powers autonomous agents',
    'agent-context': 'How agents structure and manage their context window',
    'tool-design': 'Principles for building tools that agents can use reliably',
    'memory': 'How agents remember across conversations and sessions',
    'skills': 'Reusable capabilities that extend what agents can do',
    'mcp': 'A standard protocol for connecting agents to external tools',
    'agentic-patterns': 'Architectural patterns like ReAct, reflection, and multi-agent',
    'orchestration': 'Coordinating multiple agents and tools for complex tasks',
    'agent-problems': 'Common failure modes: loops, hallucinations, and goal drift',
    'agent-security': 'Defending agents against prompt injection and data exfiltration',
    'evaluation': 'Measuring agent performance with benchmarks and metrics',
    'tokenization': 'How text gets broken into pieces the model understands',
    'embeddings': 'Turning words into numbers that capture meaning',
    'attention': 'How transformers decide which tokens matter for each prediction',
    'temperature': 'Control randomness and creativity in LLM outputs',
    'context-rot': 'Why models forget instructions in long conversations',
    'rag': 'Augment LLM answers with retrieved external knowledge',
    'vision': 'How LLMs process and understand images alongside text',
    'visual-challenges': 'Where vision models fail: counting, spatial reasoning, OCR',
    'agentic-vision': 'Active visual investigation through zoom, crop, and code execution',
    'multimodality': 'Processing text, images, audio, and video in one model',
    'transformer-architecture': 'The foundational architecture behind GPT, BERT, and all modern LLMs',
    'llm-training': 'From pretraining on raw text to fine-tuning with human feedback',
    'moe': 'Run only a fraction of model parameters per token',
    'quantization': 'Shrink model size by reducing numerical precision',
    'nested-learning': 'Learning algorithms that operate at multiple levels',
    'distillation': 'Transfer knowledge from a large model to a smaller one',
    'lora': 'Adapt large models efficiently by training only tiny low-rank matrices',
    'speculative-decoding': 'Speed up inference by drafting tokens with a smaller model',
    'kv-cache': 'Store computed keys and values to avoid redundant work',
    'prompt-caching': 'Reuse computed KV caches across API requests to save cost and latency',
    'batching': 'Process multiple requests simultaneously for higher throughput',
    'local-inference': 'Run LLMs on your own hardware for privacy, speed, and zero API costs',
    'neural-networks': 'Layers of connected neurons that learn patterns from data',
    'gradient-descent': 'The optimization algorithm that trains neural networks',
    'training': 'How models learn from data through forward and backward passes',
    'prompt-basics': 'Fundamental techniques for getting good results from LLMs',
    'advanced-prompting': 'Chain-of-thought, few-shot, and other power techniques',
    'system-prompts': 'Set the persona, rules, and behavior of an LLM',
    'bias': 'How training data biases surface in model outputs',
    'responsible-ai': 'Building AI systems that are fair, transparent, and safe',
    'european-ai': 'The growing European AI ecosystem and its unique strengths',
    'open-source': 'Why open weights matter for innovation and transparency',
    'logges-favourite-model': "Curated picks of the best models for different tasks",
    'world-models': 'Internal simulators that let AI predict and plan in virtual worlds',
  },

  // Temperature page
  temperature: {
    title: 'Temperature',
    description: 'Understanding how a single parameter controls the balance between predictable logic and creative randomness in AI outputs.',
    whatIs: 'What is Temperature?',
    whatIsDesc: 'In LLMs, Temperature is a hyperparameter that scales the "logits" (raw scores) of the next token predictions before they are converted into probabilities. It essentially controls how much the model favors the most likely options versus exploring less likely ones.',
    lowTemp: 'Low Temperature',
    lowTempDesc: 'Focuses on the top results. Reliable, consistent, and factual. Great for code, math, and structured data.',
    highTemp: 'High Temperature',
    highTempDesc: 'Spreads probability to more tokens. Diverse, creative, and surprising. Great for stories, brainstorming, and poetry.',
    interactiveDistribution: 'Interactive Distribution',
    adjustSlider: 'Adjust the temperature to see its effect',
    adjustDesc: 'Adjust the temperature slider to see how it reshapes the probability distribution for the next token. Watch how "the" (the most likely choice) dominates at low temperatures and loses its lead as the temperature rises.',
    howItWorks: 'How it Works Mathematically',
    mathDesc: 'The model generates a score for every possible token. To get probabilities, we use the Softmax function, modified by temperature:',
    whenLow: 'When T → 0',
    low: 'Low',
    whenLowDesc: 'Dividing by a small T amplifies differences between scores. The highest logit dominates exponentially.',
    whenHigh: 'When T → ∞',
    high: 'High',
    whenHighDesc: 'Dividing by a large T compresses all scores toward zero, making them nearly equal after exponentiation.',
    practicalGuidelines: 'Practical Guidelines',
    useCase: 'Use Case',
    tempLabel: 'Temperature',
    why: 'Why?',
    codingMath: 'Coding & Math',
    codingMathWhy: 'Errors in logic are costly; you want the most likely correct path.',
    factRetrieval: 'Fact Retrieval',
    factRetrievalWhy: 'Reduces "hallucinations" by sticking to the most probable data points.',
    generalChat: 'General Chat',
    generalChatWhy: 'The "sweet spot" for most models to sound natural and helpful.',
    creativeWriting: 'Creative Writing',
    creativeWritingWhy: 'Encourages the model to use more interesting, varied vocabulary.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generates wild, unconventional ideas that might spark inspiration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Temperature 0 is deterministic ("Greedy Search") — always picks the top token',
    takeaway2: 'Higher temperature increases variety and creativity but decreases coherence',
    takeaway3: 'Too high temperature (> 1.5) often results in gibberish',
    takeaway4: "Always match your temperature to the task's requirement for precision vs. creativity",
  },

  // Context Rot page
  contextRot: {
    title: 'Context Rot',
    description: 'Understanding how information degrades over long conversations and why LLMs struggle with extended contexts.',
    whatIs: 'What is Context Rot?',
    whatIsDesc: 'refers to the gradual degradation of an LLM\'s ability to accurately recall and use information from earlier parts of a long conversation or document. As context grows, the model\'s attention becomes diluted.',
    whyHappens: 'Why Does It Happen?',
    whyHappensDesc: 'LLMs have finite context windows and use attention mechanisms that must distribute focus across all tokens. As conversations grow longer, earlier information competes with newer content for the model\'s limited attention capacity.',
    // Reasons
    reason1Title: 'Finite Context Windows',
    reason1Desc: 'LLMs have finite context windows and use attention mechanisms that must distribute focus across all tokens. As conversations grow longer, earlier information competes with newer content for the model\'s limited attention capacity.',
    reason2Title: 'Attention Dilution',
    reason2Desc: 'The model\'s attention mechanism spreads across all tokens. More content means each token gets proportionally less attention.',
    reason3Title: 'Recency Bias',
    reason3Desc: 'Transformers tend to weight recent tokens more heavily. Instructions at the start naturally become less influential.',
    symptoms: 'Common Symptoms',
    symptom1: 'Forgetting instructions given at the start of a conversation',
    symptom2: 'Contradicting earlier statements or decisions',
    symptom3: 'Losing track of complex multi-step tasks',
    symptom4: 'Mixing up details from different parts of the context',
    mitigation: 'Mitigation Strategies',
    mitigation1Title: 'Periodic Instruction Reinforcement',
    mitigation1: 'Summarize important context periodically',
    mitigation2Title: 'Conversation Summarization',
    mitigation2: 'Place critical instructions at both start and end',
    mitigation3Title: 'Hierarchical Memory',
    mitigation3: 'Use external memory systems to store and retrieve relevant context on-demand.',
    mitigation4Title: 'Instruction Anchoring',
    mitigation4: 'Place critical instructions at both the beginning and the end of your prompt to reinforce them.',
    mitigation5Title: 'Shorter Task Chains',
    mitigation5: 'Break long tasks into smaller, focused conversations.',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'See how memory fades as context length increases',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context rot is an inherent limitation of current LLM architectures',
    takeaway2: 'The "lost in the middle" effect means information at the start and end is recalled better',
    takeaway3: 'Strategic information placement can significantly improve recall',
    takeaway4: 'Regular summarization helps maintain important context over long conversations',
    takeaway5: '2025 research confirms consistent U-shaped attention patterns across 18+ SOTA models',
    takeaway6: 'Position-aware prompting strategies can recover 10-20% of lost accuracy',
    // 2025 Research Findings
    researchTitle: '2025 Research Findings',
    researchDesc: 'Recent studies have systematically quantified context degradation across state-of-the-art models, revealing consistent patterns in how LLMs process long contexts.',
    // Needle in Haystack
    needleTitle: 'Needle in a Haystack Benchmark',
    needleDesc: 'A standard evaluation method where a specific piece of information (the "needle") is placed at various positions within a large context (the "haystack"). The model is then asked to retrieve this information.',
    needleMethod: 'How It Works',
    needleMethodDesc: 'Researchers insert a random fact (e.g., "The special magic number is 42") at different depths (10%, 25%, 50%, 75%, 90%) within documents of varying lengths. The model must accurately recall this fact when queried.',
    needleFindings: 'Key Finding',
    needleFindingsDesc: 'Performance varies significantly based on needle position and context length. Most models show degraded accuracy when the needle is placed in the middle of very long contexts.',
    // Lost in the Middle
    lostMiddleTitle: 'Lost in the Middle Effect',
    lostMiddleDesc: '2025 research confirms that LLMs exhibit a U-shaped attention pattern: they attend better to information at the beginning and end of their context window, while middle content receives significantly less attention.',
    lostMiddlePattern: 'The U-Shaped Pattern',
    lostMiddlePatternDesc: 'When tested with multi-document question answering, models show highest accuracy when relevant information appears in the first or last few documents. Accuracy drops by 10-20% when critical information is in the middle third of the context.',
    lostMiddleImplication: 'Practical Implication',
    lostMiddleImplicationDesc: 'For prompts with multiple pieces of information, place the most critical content at the very beginning or end. Avoid burying important instructions in the middle of long system prompts.',
    // Quantitative Findings
    quantTitle: 'Quantitative Findings from SOTA Models',
    quantDesc: 'Comprehensive studies tested 18 state-of-the-art models including GPT-4, Claude, Gemini, and Llama variants, revealing consistent degradation patterns across architectures.',
    quantFinding1Title: 'Consistent U-Curve',
    quantFinding1Desc: 'All 18 models tested showed the U-shaped retrieval pattern, though magnitude varied. Closed-source models (GPT-4, Claude) showed smaller drops than open-source alternatives.',
    quantFinding2Title: 'Context Length Impact',
    quantFinding2Desc: 'Performance degradation increases with context length. At 4K tokens, middle-position accuracy drops ~10%. At 32K+ tokens, drops can exceed 30% for some models.',
    quantFinding3Title: 'Task Dependency',
    quantFinding3Desc: 'Retrieval tasks show the strongest position effects. Reasoning and summarization tasks are less affected but still exhibit degradation patterns.',
    quantFinding4Title: 'Position Sensitivity',
    quantFinding4Desc: 'The "primacy" effect (favoring early content) is often stronger than the "recency" effect, though this varies by model architecture.',
    // Position-Aware Strategies
    positionTitle: 'Position-Aware Strategies',
    positionDesc: 'Based on 2025 research findings, these evidence-based strategies can improve model performance on long-context tasks.',
    position1Title: 'Front-Load Critical Information',
    position1Desc: 'Place your most important instructions, constraints, and context at the very beginning of your prompt. This leverages the primacy effect observed across all tested models.',
    position2Title: 'Mirror Key Instructions',
    position2Desc: 'Repeat critical instructions at both the start and end of long prompts. This "sandwich" technique ensures at least one copy falls in a high-attention zone.',
    position3Title: 'Summarize Middle Content',
    position3Desc: 'For long documents, create summaries of middle sections and place these summaries at the beginning. The full content can remain for reference, but key points should be extracted.',
    position4Title: 'Chunk and Query',
    position4Desc: 'For very long contexts, break content into smaller chunks and process sequentially. Aggregate results rather than relying on single-pass long-context processing.',
  },

  // Attention page
  attention: {
    title: 'Attention Mechanism',
    description: 'Explore how transformers focus on relevant parts of input through the powerful attention mechanism.',
    whatIs: 'What is Attention?',
    whatIsDesc: 'Attention is the core mechanism that allows transformers to weigh the importance of different parts of the input when generating each output token. It enables the model to "focus" on relevant context.',
    howWorks: 'How It Works',
    howWorksDesc: 'For each position, the model computes Query, Key, and Value vectors. Attention scores are calculated by comparing queries to keys, then used to create a weighted sum of values.',
    selfAttention: 'Self-Attention',
    selfAttentionDesc: 'Allows each token to attend to all other tokens in the sequence, capturing relationships regardless of distance.',
    multiHead: 'Multi-Head Attention',
    multiHeadDesc: 'Multiple attention heads allow the model to focus on different types of relationships simultaneously.',
    // Interactive section
    interactiveTitle: 'Interactive Attention Map',
    interactiveDesc: 'Hover to explore attention patterns',
    interactiveExplain: 'Hover over different words in the sentences below. The highlighting shows where the model is "looking" to understand that specific word.',
    // QKV section
    qkvTitle: 'The Three Keys: Query, Key, and Value',
    queryTitle: 'Query',
    queryDesc: '"What am I looking for?" - Represents the current word seeking context.',
    keyTitle: 'Key',
    keyDesc: '"What do I contain?" - A label for every word in the sequence to check against the query.',
    valueTitle: 'Value',
    valueDesc: '"What information do I offer?" - The actual content that gets passed forward if the Query and Key match.',
    qkvExplain: 'The model calculates a score by multiplying Q and K. This score determines how much of V to keep.',
    // Benefits section
    benefitsTitle: 'Why it Changed Everything',
    benefit1Title: 'Parallel Processing',
    benefit1Desc: 'Unlike older models (RNNs), Transformers can process all words in a sentence at the same time, making training much faster.',
    benefit2Title: 'Long-Range Dependencies',
    benefit2Desc: 'Attention can link two words even if they are thousands of tokens apart, as long as they are within the same context window.',
    benefit3Title: 'Dynamic Context',
    benefit3Desc: 'The model doesn\'t just look at words; it learns which words are important *for each other* based on the specific sentence.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Attention enables transformers to capture long-range dependencies',
    takeaway2: 'The quadratic complexity of attention limits context window size',
    takeaway3: 'Different attention heads learn to focus on different linguistic patterns',
    takeaway4: 'Attention visualization can help interpret model behavior',
    // Quadratic problem
    quadraticTitle: 'The Quadratic Problem',
    quadraticDesc: 'Standard attention computes scores between every pair of tokens, resulting in O(n²) complexity. Doubling the context length quadruples memory usage and compute. This is why extending context windows is so challenging.',
    // Optimizations
    optimizationsTitle: 'Attention Optimizations',
    optimizationsDesc: 'Several techniques have been developed to make attention more efficient, enabling longer contexts and faster inference.',
    flashAttentionDesc: 'Rewrites the attention algorithm to be IO-aware, computing attention in blocks that fit in GPU fast memory (SRAM) rather than constantly reading/writing to slow HBM. The math is identical—just smarter memory access patterns.',
    mqaDesc: 'Instead of separate Key and Value heads for each Query head, all Query heads share a single K and V. Reduces the KV cache size dramatically, speeding up inference at the cost of some quality.',
    gqaDesc: 'A middle ground between standard Multi-Head Attention and MQA. Groups of Query heads share K/V heads. Maintains most of the quality while still reducing memory.',
    slidingWindowDesc: 'Each token only attends to a fixed window of nearby tokens (e.g., 4096) rather than the full context. Information propagates through layers, so distant tokens can still influence each other indirectly.',
    ringAttentionDesc: 'Distributes the sequence across multiple devices in a ring topology. Each device computes attention for its chunk while passing KV states around the ring, enabling context lengths in the millions.',
  },

  // Transformer Architecture page
  transformerArchitecture: {
    title: 'Transformer Architecture',
    description: 'The foundational architecture behind GPT, BERT, and all modern large language models.',
    whatIs: 'What is the Transformer?',
    whatIsDesc: 'The Transformer is a neural network architecture introduced in the landmark 2017 paper "Attention is All You Need" by Vaswani et al. It replaced recurrent and convolutional approaches with a purely attention-based mechanism, enabling massive parallelization during training and capturing long-range dependencies far more effectively. Nearly every modern large language model -- GPT, BERT, LLaMA, Claude -- is built on the Transformer.',
    paperCitation: '-- Vaswani et al., "Attention Is All You Need" (2017, Google Brain)',

    // Bycroft link
    bycroftTitle: 'LLM Visualization by Brendan Bycroft',
    bycroftDesc: 'The best interactive 3D visualization of transformer internals available. Explore how GPT-style models process tokens through embedding, attention, and feed-forward layers -- step by step, parameter by parameter. Highly recommended.',

    // Layer explorer
    layersHeading: 'Transformer Layer Stack',
    layersIntro: 'A Transformer is built from a stack of identical layers. Click through each component to understand what it does and how data flows through the architecture.',
    layersTitle: 'Layer-by-Layer Explorer',
    layersDesc: 'Click each layer to see its role in the architecture',
    layersNx: 'Layers 3-6 repeat N times',
    layer_input_embedding: 'Input Embedding',
    layer_input_embedding_desc: 'Converts each input token (an integer ID) into a dense vector of dimension d_model (e.g. 768 or 4096). This learned lookup table is the model\'s vocabulary representation -- each word gets a unique high-dimensional vector.',
    layer_input_embedding_in: 'Token IDs',
    layer_input_embedding_out: 'Vectors',
    layer_positional_encoding: 'Positional Encoding',
    layer_positional_encoding_desc: 'Adds position information to each embedding so the model knows token order. Without this, "The cat sat on the mat" and "mat the on sat cat the" would be identical. Uses sinusoidal functions or learned position embeddings.',
    layer_positional_encoding_in: 'Embeddings',
    layer_positional_encoding_out: '+ Position',
    layer_multi_head_attention: 'Multi-Head Attention',
    layer_multi_head_attention_desc: 'The core mechanism. Each head computes Query, Key, and Value projections, then calculates attention scores to determine which tokens should attend to which others. Multiple heads let the model capture different relationship types (syntax, semantics, coreference) simultaneously.',
    layer_multi_head_attention_in: 'Vectors',
    layer_multi_head_attention_out: 'Attended',
    layer_add_norm_1: 'Add & Norm (Post-Attention)',
    layer_add_norm_1_desc: 'A residual connection adds the attention output back to its input, then layer normalization stabilizes the values. This "skip connection" is critical -- it lets gradients flow directly through the network and enables training of very deep models (50+ layers).',
    layer_add_norm_1_in: 'Input + Attn',
    layer_add_norm_1_out: 'Normalized',
    layer_feed_forward: 'Feed-Forward Network',
    layer_feed_forward_desc: 'A two-layer MLP applied independently to each position: first expands to a larger dimension (typically 4x d_model), applies a non-linearity (ReLU or GeLU), then projects back down. This is where much of the model\'s "knowledge" is stored as learned weight patterns.',
    layer_feed_forward_in: 'Normalized',
    layer_feed_forward_out: 'Transformed',
    layer_add_norm_2: 'Add & Norm (Post-FFN)',
    layer_add_norm_2_desc: 'Another residual connection and layer normalization after the feed-forward network. The pattern of [sublayer -> residual add -> normalize] repeats throughout the Transformer and is key to training stability.',
    layer_add_norm_2_in: 'Input + FFN',
    layer_add_norm_2_out: 'Layer Output',
    layer_output: 'Output Projection',
    layer_output_desc: 'After all N transformer layers, a final linear projection maps the d_model-dimensional vectors to vocabulary-sized logits. A softmax converts these to probabilities over the next token. The token with the highest probability (or a sampled one) becomes the output.',
    layer_output_in: 'Final Hidden',
    layer_output_out: 'Probabilities',
    layers_send_token: 'Send a Token Through',
    layers_token_flying: 'Processing...',
    layer_detail_attention_matrix: 'Attention weight matrix (which tokens attend to which)',
    layer_detail_concat_heads: 'Concat all heads → Linear projection',
    layer_detail_ffn_input: 'd_model input',
    layer_detail_ffn_expand: 'Expand to 4× d_model + GeLU activation',
    layer_detail_ffn_contract: 'Contract back to d_model',
    layer_detail_norm_before: 'Before (varied)',
    layer_detail_norm_after: 'After (stable)',
    layer_detail_norm_desc: 'Values are centered and scaled to stabilize training',
    layer_detail_residual_desc: 'Skip connection adds input directly to sublayer output',
    layer_detail_embed_desc: 'Each token ID maps to a learned dense vector',
    layer_detail_pos_desc: 'Sinusoidal waves encode position into each embedding',
    layer_detail_output_desc: 'Softmax over vocabulary → next token probabilities',

    // Encoder-decoder
    archHeading: 'Architecture Variants',
    archIntro: 'The original Transformer used both an encoder and decoder. Modern models often use just one. Toggle between the three main variants to see which components each uses.',
    archTitle: 'Encoder-Decoder Variants',
    archDesc: 'Toggle to compare architectures',
    arch_encoder_decoder: 'Encoder-Decoder',
    arch_encoder_only: 'Encoder-Only',
    arch_decoder_only: 'Decoder-Only',
    arch_encoder_decoder_desc: 'The original architecture. The encoder processes the full input bidirectionally, then the decoder generates output tokens one at a time, attending to the encoder\'s representations via cross-attention. Used for translation, summarization, and sequence-to-sequence tasks.',
    arch_encoder_only_desc: 'Uses only the encoder stack with bidirectional attention -- each token can attend to all other tokens. Excellent for understanding tasks like classification, named entity recognition, and sentence similarity. Not designed for text generation.',
    arch_decoder_only_desc: 'Uses only the decoder stack with causal (left-to-right) masked attention -- each token can only attend to previous tokens. The dominant architecture for modern LLMs because it naturally models text generation and scales exceptionally well.',
    encoder: 'Encoder',
    decoder: 'Decoder',
    crossAttention: 'Cross-Attention',
    notUsed: 'Not used',
    archModels: 'Example models',

    // Dataflow
    dataflowHeading: 'Token Dataflow',
    dataflowIntro: 'Follow a single token as it flows through the entire Transformer pipeline, from raw text to output probabilities. Watch how the tensor shape changes at each step.',
    dataflowTitle: 'Step-by-Step Dataflow',
    dataflowDesc: 'Play or scrub through the processing pipeline',
    dataflowPlay: 'Play Animation',
    dataflowPlaying: 'Playing...',
    flow_tokenize: 'Tokenize',
    flow_tokenize_desc: 'Raw text is split into token IDs using BPE or similar. Each token maps to an integer.',
    flow_embed: 'Embed',
    flow_embed_desc: 'Each token ID is looked up in the embedding table to produce a d_model-dimensional vector.',
    flow_pos_encode: 'Add Position',
    flow_pos_encode_desc: 'Positional encodings are added element-wise so the model knows token order.',
    flow_attention: 'Compute Attention',
    flow_attention_desc: 'Q, K, V matrices are computed. Attention scores form a [seq_len x seq_len] matrix per head.',
    flow_attn_output: 'Attention Output',
    flow_attn_output_desc: 'Weighted values are concatenated across heads and projected back to d_model dimensions.',
    flow_ffn: 'Feed-Forward',
    flow_ffn_desc: 'Each position passes through a two-layer MLP, expanding to d_ff (typically 4x d_model) then back.',
    flow_ffn_output: 'FFN Output',
    flow_ffn_output_desc: 'After residual connection and layer norm, the output is ready for the next transformer layer.',
    flow_logits: 'Output Logits',
    flow_logits_desc: 'Final projection to vocabulary size produces a probability distribution over all possible next tokens.',

    // Key concepts
    conceptsTitle: 'Key Concepts',
    concept1Title: 'Residual Connections',
    concept1Desc: 'Skip connections that add the input of each sublayer directly to its output. They solve the vanishing gradient problem and allow training of very deep networks. Without them, transformers with 50+ layers would be impossible to train.',
    concept2Title: 'Layer Normalization',
    concept2Desc: 'Normalizes activations across the feature dimension to stabilize training. Applied after each residual addition. Pre-norm (normalize before sublayer) has become more common than post-norm in modern architectures.',
    concept3Title: 'Positional Encoding',
    concept3Desc: 'Since attention has no inherent notion of order, position must be explicitly injected. The original paper used fixed sinusoidal functions; modern models typically use learned position embeddings or relative position encodings like RoPE.',

    // Why it matters
    whyMattersTitle: 'Why It Matters',
    whyMattersDesc: 'The Transformer architecture is arguably the most impactful innovation in AI of the past decade. It unlocked the scaling laws that make modern LLMs possible.',
    takeaway1: 'The Transformer replaced RNNs and LSTMs by enabling full parallelization during training, reducing training time from weeks to days',
    takeaway2: 'Its attention mechanism captures long-range dependencies that sequential models struggled with, enabling understanding of entire documents',
    takeaway3: 'The architecture scales remarkably well -- from 100M parameter BERT to 1.8T parameter GPT-4, performance improves predictably with scale',
    takeaway4: 'Every major LLM today (GPT, Claude, Gemini, LLaMA, Mistral) is built on the Transformer, making it the foundation of modern AI',
  },

  // Vision page
  vision: {
    title: 'Vision & Images',
    description: 'How modern LLMs process and understand visual information alongside text.',
    whatIs: 'How LLMs See Images',
    whatIsDesc: 'Vision-enabled LLMs convert images into sequences of tokens that can be processed alongside text. This typically involves dividing images into patches and encoding them with a vision transformer.',
    patchEncoding: 'Patch Encoding',
    patchEncodingDesc: 'Images are divided into fixed-size patches (e.g., 14x14 pixels), each converted into an embedding vector similar to text tokens.',
    // Vision Transformer section
    vitTitle: 'The Vision Transformer (ViT)',
    vitDesc: 'The Vision Transformer architecture adapts the transformer model for image processing. Instead of processing words, it processes image patches.',
    vitStep1: 'Divide into Patches',
    vitStep1Desc: 'The image is split into a grid of fixed-size patches (typically 14x14 or 16x16 pixels).',
    vitStep2: 'Flatten & Project',
    vitStep2Desc: 'Each patch is flattened into a vector and linearly projected into an embedding space.',
    vitStep3: 'Add Position Info',
    vitStep3Desc: 'Positional embeddings are added so the model knows where each patch came from.',
    vitStep4: 'Process with Transformer',
    vitStep4Desc: 'The sequence of patch embeddings is processed by standard transformer layers.',
    // Token costs
    tokenCosts: 'Token Costs',
    tokenCostsDesc: 'Images are expensive in terms of tokens. Understanding this helps you optimize your applications.',
    tokenExample1: 'A 512x512 image with 16x16 patches',
    tokenExample1Value: '~1,024 tokens',
    tokenExample2: 'A 1024x1024 high-res image',
    tokenExample2Value: '~4,096 tokens',
    tokenExample3: 'Equivalent text description',
    tokenExample3Value: '~50-100 tokens',
    tokenTip: 'Always consider whether a text description might be more efficient than passing the actual image.',
    // Use cases
    useCases: 'Common Use Cases',
    useCasesDesc: 'Vision-enabled LLMs unlock many practical applications.',
    useCase1: 'Document Analysis',
    useCase1Desc: 'Extract information from PDFs, receipts, forms, and handwritten notes.',
    useCase2: 'Visual Q&A',
    useCase2Desc: 'Answer questions about image contents, charts, and diagrams.',
    useCase3: 'Image Captioning',
    useCase3Desc: 'Generate detailed descriptions of images for accessibility or indexing.',
    useCase4: 'UI Understanding',
    useCase4Desc: 'Analyze screenshots, wireframes, and user interfaces.',
    multimodal: 'Multimodal Understanding',
    multimodalDesc: 'The model learns to align visual and textual representations, enabling tasks like image captioning, visual QA, and document understanding.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Images consume many more tokens than equivalent text descriptions',
    takeaway2: 'Resolution and patch size affect detail recognition',
    takeaway3: 'Visual understanding is approximate—models can miss fine details',
    takeaway4: 'Combining vision and language enables powerful new applications',
  },

  // Visual Challenges page
  visualChallenges: {
    title: 'Visual Challenges',
    description: 'Common challenges and limitations when working with vision-enabled AI models.',
    overview: 'Common Visual Challenges',
    overviewDesc: 'While vision models are impressive, they face several systematic challenges that are important to understand when building applications. These limitations stem from how vision models process images—through patches, embeddings, and attention—rather than the way humans perceive visual information.',

    // Challenge 1: Counting
    challenge1: 'Counting Objects',
    challenge1Desc: 'Models often struggle to accurately count objects in images, especially when there are many similar items.',
    challenge1Why: 'Why This Happens',
    challenge1WhyDesc: 'Vision models process images as patches (typically 14x14 or 16x16 pixels), not as discrete objects. They lack the built-in concept of "object permanence" and struggle to maintain accurate counts across overlapping or dense arrangements.',
    challenge1Examples: 'Common Failures',
    challenge1Example1: 'Counting people in a crowd (often off by 20-50%)',
    challenge1Example2: 'Counting items in a grid or array',
    challenge1Example3: 'Distinguishing between "few" and "many" when items overlap',
    challenge1Mitigation: 'Workarounds',
    challenge1MitigationDesc: 'For critical counting tasks, consider using specialized object detection models (YOLO, Faster R-CNN) or asking the model to identify and describe each item individually rather than providing a total count.',

    // Challenge 2: Spatial Reasoning
    challenge2: 'Spatial Reasoning',
    challenge2Desc: 'Understanding precise spatial relationships between objects (left/right, above/below) can be unreliable.',
    challenge2Why: 'Why This Happens',
    challenge2WhyDesc: 'Positional information is encoded through patch position embeddings, but these don\'t provide pixel-level precision. The model learns statistical correlations between positions rather than explicit spatial reasoning.',
    challenge2Examples: 'Common Failures',
    challenge2Example1: 'Confusing left/right relationships in mirrored or symmetric images',
    challenge2Example2: 'Misjudging relative distances ("closer to" or "farther from")',
    challenge2Example3: 'Difficulty with rotated or unusual orientations',
    challenge2Mitigation: 'Workarounds',
    challenge2MitigationDesc: 'Be explicit in your prompts about which reference frame to use. Consider annotating images with visual markers or grids for critical spatial tasks.',

    // Challenge 3: Small Text Recognition
    challenge3: 'Small Text Recognition',
    challenge3Desc: 'Fine text in images may be misread or missed entirely, especially at low resolutions.',
    challenge3Why: 'Why This Happens',
    challenge3WhyDesc: 'Text smaller than the patch size (14-16 pixels) gets compressed into a single embedding, losing character-level detail. OCR is not built into vision LLMs—they learn text recognition as a byproduct of training, not as a dedicated capability.',
    challenge3Examples: 'Common Failures',
    challenge3Example1: 'Misreading license plates, street signs, or small labels',
    challenge3Example2: 'Confusing similar characters (0/O, 1/l/I, 5/S)',
    challenge3Example3: 'Missing text in busy or low-contrast backgrounds',
    challenge3Mitigation: 'Workarounds',
    challenge3MitigationDesc: 'Use high-resolution images and zoom in on text regions. For critical OCR tasks, use dedicated OCR tools (Tesseract, Google Vision API, Amazon Textract) alongside or instead of vision LLMs.',

    // Challenge 4: Hallucination
    challenge4: 'Visual Hallucination',
    challenge4Desc: 'Models may describe objects or details that aren\'t actually present in the image.',
    challenge4Why: 'Why This Happens',
    challenge4WhyDesc: 'Vision LLMs are trained to generate plausible descriptions. When image features are ambiguous, the model fills in gaps with statistically likely content—even if that content isn\'t in the image. This is the same mechanism that causes text hallucination.',
    challenge4Examples: 'Common Failures',
    challenge4Example1: 'Adding objects that "should" be in a scene (a keyboard near a monitor)',
    challenge4Example2: 'Describing brand names or text that isn\'t visible',
    challenge4Example3: 'Inventing details when asked about unclear regions',
    challenge4Mitigation: 'Workarounds',
    challenge4MitigationDesc: 'Ask the model to express uncertainty. Use prompts like "describe only what you can clearly see" or "if you cannot determine X, say so." Cross-reference critical details.',

    // Challenge 5: Fine Detail Recognition
    challenge5: 'Fine Detail Recognition',
    challenge5Desc: 'Subtle details, textures, or small distinguishing features are often missed or misidentified.',
    challenge5Why: 'Why This Happens',
    challenge5WhyDesc: 'The patch-based architecture averages information within each patch, losing fine-grained detail. High-frequency visual information (edges, textures, small features) is compressed.',
    challenge5Examples: 'Common Failures',
    challenge5Example1: 'Distinguishing between similar objects (dog breeds, car models)',
    challenge5Example2: 'Reading gauges, meters, or instrument displays',
    challenge5Example3: 'Identifying subtle damage or defects in inspection tasks',
    challenge5Mitigation: 'Workarounds',
    challenge5MitigationDesc: 'Use the highest resolution available. Crop and focus on specific regions of interest. For specialized tasks, consider fine-tuned models trained on domain-specific data.',

    // Challenge 6: Multi-Image Reasoning
    challenge6: 'Multi-Image Reasoning',
    challenge6Desc: 'Comparing or reasoning across multiple images is significantly harder than single-image tasks.',
    challenge6Why: 'Why This Happens',
    challenge6WhyDesc: 'Each image is encoded separately into token sequences. Cross-image attention must happen through the language model\'s context window, which is less efficient than dedicated multi-image architectures.',
    challenge6Examples: 'Common Failures',
    challenge6Example1: 'Finding differences between two similar images ("spot the difference")',
    challenge6Example2: 'Tracking object identity across frames',
    challenge6Example3: 'Comparing fine details between product images',
    challenge6Mitigation: 'Workarounds',
    challenge6MitigationDesc: 'Describe each image separately first, then ask for comparison. Consider combining images into a single composite for direct comparison.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Vision LLMs process images as patches—detail below patch resolution is lost',
    takeaway2: 'Counting and spatial reasoning are fundamental weaknesses, not edge cases',
    takeaway3: 'Visual hallucination follows the same pattern as text hallucination—plausible fabrication',
    takeaway4: 'Use higher resolution, cropped regions, and explicit prompts to improve accuracy',
    takeaway5: 'For critical tasks, combine vision LLMs with specialized tools (OCR, object detection)',
    takeaway6: 'Always verify important visual information through other means',
  },

  // Agentic Vision page
  agenticVision: {
    title: 'Agentic Vision',
    description: 'How AI models transform passive image viewing into active visual investigation through code execution and iterative reasoning.',
    whatIs: 'What is Agentic Vision?',
    whatIsDesc: 'Agentic Vision transforms image understanding from a static, one-shot process into an active investigation. Instead of simply describing what it sees, the model formulates plans to zoom in, inspect, manipulate, and analyze images step-by-step—grounding answers in visual evidence gathered through code execution.',

    // The Loop
    loopTitle: 'The Think-Act-Observe Loop',
    loopDesc: 'At the core of agentic vision is a rigorous iterative process that mirrors how humans investigate complex visual information.',
    thinkTitle: 'Think',
    thinkDesc: 'The model analyzes the user\'s request and the initial image, then formulates a multi-step plan for how to extract the needed information.',
    actTitle: 'Act',
    actDesc: 'The model generates and executes Python code to manipulate or analyze the image—cropping regions of interest, running calculations, counting objects, or drawing annotations.',
    observeTitle: 'Observe',
    observeDesc: 'The transformed image is appended back into the model\'s context window, allowing it to inspect the results before deciding on the next action or producing a final answer.',

    // Capabilities
    capabilitiesTitle: 'Key Capabilities',
    capabilitiesDesc: 'Agentic vision enables several powerful capabilities that passive vision models cannot match.',
    capability1Title: 'Zoom & Inspect',
    capability1Desc: 'The model detects when details are too small to read (like a distant gauge or serial number) and writes code to crop and re-examine the area at higher resolution.',
    capability2Title: 'Visual Math',
    capability2Desc: 'Run multi-step calculations using code—summing line items on a receipt, measuring angles in a diagram, or generating charts from extracted data.',
    capability3Title: 'Image Annotation',
    capability3Desc: 'Draw arrows, bounding boxes, or other annotations directly onto images to answer spatial questions like "Where should this item go?"',
    capability4Title: 'Iterative Refinement',
    capability4Desc: 'If the first approach doesn\'t yield clear results, the model can try alternative strategies—different crop regions, image enhancement, or multiple counting methods.',

    // How It Works
    howItWorks: 'How It Works',
    howItWorksDesc: 'When you ask an agentic vision model a question about an image, it doesn\'t just look and respond. It reasons about what operations would help answer the question, executes code to perform those operations, and uses the results to inform its answer.',
    step1: 'Receive Query',
    step1Desc: 'User asks a question about an image that requires detailed analysis.',
    step2: 'Plan Operations',
    step2Desc: 'Model determines what visual operations (crop, zoom, annotate) would help answer the question.',
    step3: 'Execute Code',
    step3Desc: 'Python code is generated and run to manipulate the image as planned.',
    step4: 'Analyze Results',
    step4Desc: 'The modified image is fed back to the model for inspection.',
    step5: 'Iterate or Answer',
    step5Desc: 'Model either performs additional operations or provides the final answer with evidence.',

    // Example
    exampleTitle: 'Example: Reading a Distant Serial Number',
    exampleDesc: 'Imagine asking "What\'s the serial number on that device in the corner of the photo?"',
    exampleStep1: 'Model identifies the device location is in the bottom-right corner',
    exampleStep2: 'Generates code to crop that region and upscale it 4x',
    exampleStep3: 'Inspects the zoomed image and identifies the serial number text',
    exampleStep4: 'Returns the serial number with confidence, noting the crop used',

    // Models
    modelsTitle: 'Models with Agentic Vision',
    modelsDesc: 'Several frontier models now support agentic vision capabilities.',
    model1: 'Google Gemini 3 Flash',
    model1Desc: 'First major model to introduce "Agentic Vision" as a named feature, combining visual reasoning with code execution. Shows 5-10% quality boost on vision benchmarks when code execution is enabled.',
    model2: 'NVIDIA Cosmos Reason',
    model2Desc: 'A 7B parameter reasoning VLM designed for physical AI applications. Can understand and act in real-world environments using prior knowledge and physics understanding.',
    model3: 'OpenAI Computer-Using Agent',
    model3Desc: 'Combines large reasoning models with reinforcement-learned UI interaction, enabling pixel-precise pointing at objects and UI elements.',

    // Applications
    applicationsTitle: 'Real-World Applications',
    applicationsDesc: 'Agentic vision is already being deployed in production systems.',
    app1Title: 'Document Processing',
    app1Desc: 'Automatically zoom into tables, charts, and fine print to extract accurate data from complex documents.',
    app2Title: 'Quality Inspection',
    app2Desc: 'Detect defects by systematically inspecting different regions of product images at high resolution.',
    app3Title: 'Spatial Reasoning',
    app3Desc: 'Answer "where should this go?" questions by annotating images with arrows and placement guides.',
    app4Title: 'Receipt Analysis',
    app4Desc: 'Extract line items, calculate totals, and verify math by combining OCR with code-based computation.',

    // Comparison
    comparisonTitle: 'Passive vs Agentic Vision',
    comparisonDesc: 'Understanding the fundamental difference in approach.',
    passiveTitle: 'Passive Vision',
    passiveDesc: 'Single forward pass through the model. What you see is what you get. Limited by initial image resolution and model attention.',
    agenticTitle: 'Agentic Vision',
    agenticDesc: 'Iterative investigation loop. Can zoom, crop, enhance, and re-examine. Grounds answers in executed code and visual evidence.',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Agentic vision treats image understanding as an active investigation, not passive perception',
    takeaway2: 'The Think-Act-Observe loop enables models to zoom, crop, and analyze images iteratively',
    takeaway3: 'Code execution provides verifiable, grounded visual reasoning',
    takeaway4: 'Enabling agentic capabilities shows 5-10% improvement on vision benchmarks',
    takeaway5: 'This paradigm bridges the gap between how humans and AI systems investigate visual information',
  },

  // Multimodality page
  multimodality: {
    title: 'Multimodality',
    description: 'How modern AI models process and understand multiple types of input including images, audio, video, and text.',
    whatIs: 'What is Multimodality?',
    whatIsDesc: 'Multimodality refers to the ability of AI models to process and understand multiple types of input simultaneously—text, images, audio, video, and more. Just as humans naturally integrate information from different senses to understand the world, multimodal AI systems combine different data types to build richer, more complete understanding.',

    // Modality Types
    modalityTypes: 'Types of Modalities',
    modalityTypesDesc: 'Modern AI systems can process a variety of input and output modalities, each with unique characteristics and challenges.',
    images: 'Images',
    imagesDesc: 'Static visual information processed through vision transformers. Images are divided into patches, embedded, and processed alongside text tokens for tasks like image captioning, visual Q&A, and document analysis.',
    audio: 'Audio',
    audioDesc: 'Sound information including speech, music, and ambient audio. Audio is typically converted to spectrograms or waveform representations before being processed by neural networks for transcription, generation, or understanding.',
    video: 'Video',
    videoDesc: 'Temporal sequences of images with optional audio tracks. Video understanding requires reasoning about changes over time, tracking objects, and often synchronizing visual and audio information.',
    other: 'Other Modalities',
    otherDesc: 'Emerging modalities include 3D point clouds, sensor data, code, structured data, and even physical actions in robotics applications.',
    text: 'Text',

    // Interactive Demo
    interactiveDemo: 'Interactive Demo',
    interactiveDemoDesc: 'Explore how different modalities combine in multimodal AI',
    selectModalities: 'Select Modalities to Combine',
    fusionResult: 'Fusion Result',
    selectToSee: 'Select modalities to see how they combine',
    understanding: 'Single modality provides focused, specialized understanding.',
    combinedUnderstanding: 'Multiple modalities enable richer, cross-referenced understanding that captures relationships between different types of information.',
    imageShort: 'Visual patterns and objects',
    audioShort: 'Speech, music, and sounds',
    videoShort: 'Motion and temporal patterns',
    textShort: 'Language and semantics',
    useCases: 'Use Cases',
    examplePrompt: 'Example prompt:',

    // Single modality use cases
    useCaseImageOnly: 'Image Classification & Detection',
    useCaseImageOnlyDesc: 'Identify objects, scenes, faces, or specific features in images without additional context.',
    useCaseImageOnlyExample: 'What objects are in this photo?',
    useCaseAudioOnly: 'Speech Transcription & Sound Analysis',
    useCaseAudioOnlyDesc: 'Convert speech to text or identify sounds, music genres, and audio events.',
    useCaseAudioOnlyExample: 'Transcribe this audio recording.',
    useCaseVideoOnly: 'Action Recognition & Motion Tracking',
    useCaseVideoOnlyDesc: 'Detect activities, track movement patterns, and understand temporal sequences.',
    useCaseVideoOnlyExample: 'What activities are happening in this video?',
    useCaseTextOnly: 'Natural Language Understanding',
    useCaseTextOnlyDesc: 'Process and generate text for questions, summaries, translations, and conversations.',
    useCaseTextOnlyExample: 'Summarize this article in three sentences.',

    // Two modality combinations
    useCaseImageText: 'Visual Q&A & Document Analysis',
    useCaseImageTextDesc: 'Ask questions about images, extract text from documents, or generate detailed image descriptions.',
    useCaseImageTextExample: 'What is the total amount on this receipt?',
    useCaseAudioText: 'Voice Assistants & Podcast Summarization',
    useCaseAudioTextDesc: 'Have natural voice conversations, transcribe meetings with summaries, or analyze spoken content.',
    useCaseAudioTextExample: 'What are the key points discussed in this podcast episode?',
    useCaseVideoText: 'Video Captioning & Content Search',
    useCaseVideoTextDesc: 'Generate descriptions of video content, search within videos by description, or create accessibility subtitles.',
    useCaseVideoTextExample: 'Describe what happens in this cooking tutorial.',
    useCaseImageAudio: 'Music + Album Art Analysis',
    useCaseImageAudioDesc: 'Connect visual and audio information, such as analyzing album artwork alongside music or adding sound effects to images.',
    useCaseImageAudioExample: 'Does this album cover match the mood of the music?',
    useCaseVideoAudio: 'Movie & Media Analysis',
    useCaseVideoAudioDesc: 'Understand video content with its soundtrack, analyze dialogue timing, or detect audio-visual synchronization issues.',
    useCaseVideoAudioExample: 'Are the lip movements in sync with the audio?',
    useCaseImageVideo: 'Visual Comparison Across Time',
    useCaseImageVideoDesc: 'Compare static reference images to video content, detect changes, or monitor for specific visual patterns.',
    useCaseImageVideoExample: 'Does the item shown in this video match this product photo?',

    // Three modality combinations
    useCaseImageAudioText: 'Interactive Learning & Tutorials',
    useCaseImageAudioTextDesc: 'Create rich educational experiences combining visual aids, narration, and text explanations.',
    useCaseImageAudioTextExample: 'Explain this diagram while I describe what I see.',
    useCaseVideoAudioText: 'Video Understanding with Dialogue',
    useCaseVideoAudioTextDesc: 'Full movie/video analysis including visuals, dialogue transcription, and scene descriptions.',
    useCaseVideoAudioTextExample: 'Summarize this interview including who said what.',
    useCaseImageVideoText: 'Visual Comparison & Monitoring',
    useCaseImageVideoTextDesc: 'Compare reference images to video feeds, like quality control or security monitoring with text reports.',
    useCaseImageVideoTextExample: 'Does the product in this video match the reference image specifications?',
    useCaseImageAudioVideo: 'Multimedia Content Production',
    useCaseImageAudioVideoDesc: 'Combine visual media and audio without explicit text instructions—analyzing music videos, syncing soundtracks to visuals, or creating multimedia presentations.',
    useCaseImageAudioVideoExample: 'Match this background music to the mood of these video clips.',

    // All modalities
    useCaseAll: 'Complete Multimedia Intelligence',
    useCaseAllDesc: 'Process all input types together for comprehensive understanding—robotics, autonomous systems, or immersive AI assistants.',
    useCaseAllExample: 'Guide me through assembling this furniture using the instruction image, video demonstration, and voice commands.',

    // How It Works
    howWorks: 'How Multimodal Models Work',
    howWorksDesc: 'Multimodal models use specialized encoders for each modality, then align these representations in a shared embedding space where the model can reason across modalities.',
    step1: 'Encode Each Modality',
    step1Desc: 'Specialized encoders (vision transformers for images, audio encoders for sound) convert each input type into embedding vectors.',
    step2: 'Align in Shared Space',
    step2Desc: 'These embeddings are projected into a common representation space where text, images, and audio can be compared and combined.',
    step3: 'Cross-Modal Reasoning',
    step3Desc: 'The model uses attention mechanisms to relate information across modalities, enabling tasks like "describe what you see" or "answer based on the video."',

    // Audio Processing
    audioProcessing: 'Audio Processing',
    audioProcessingDesc: 'Audio modalities enable AI systems to understand and generate speech, music, and other sounds.',
    speechRecognition: 'Speech Recognition',
    speechRecognitionDesc: 'Converting spoken language into text. Modern models like Whisper can transcribe in 100+ languages with high accuracy, even handling accents and background noise.',
    textToSpeech: 'Text-to-Speech',
    textToSpeechDesc: 'Generating natural-sounding speech from text. Advanced models can clone voices, express emotions, and maintain consistent speaking styles.',
    musicUnderstanding: 'Music Understanding',
    musicUnderstandingDesc: 'Analyzing musical content including genre, tempo, instruments, and mood. Some models can also generate music from text descriptions.',
    audioGeneration: 'Audio Generation',
    audioGenerationDesc: 'Creating sound effects, ambient audio, and music. Models can generate everything from realistic sound effects to full musical compositions.',

    // Video Understanding
    videoUnderstanding: 'Video Understanding',
    videoUnderstandingDesc: 'Video presents unique challenges as it combines spatial information from images with temporal information about how things change over time.',
    temporalReasoning: 'Temporal Reasoning',
    temporalReasoningDesc: 'Understanding cause and effect, action sequences, and changes over time. Models must track objects and understand how frames relate to each other.',
    frameSampling: 'Frame Sampling',
    frameSamplingDesc: 'Videos contain far too many frames to process entirely. Models use intelligent sampling strategies to select key frames that capture important moments.',
    audioVideoSync: 'Audio-Video Synchronization',
    audioVideoSyncDesc: 'Aligning audio and visual information to understand events like someone speaking, music playing, or objects making sounds.',

    // Cross-Modal Fusion
    crossModal: 'Cross-Modal Fusion Strategies',
    crossModalDesc: 'Different architectures for combining information from multiple modalities, each with trade-offs between efficiency and capability.',
    earlyFusion: 'Early Fusion',
    earlyFusionDesc: 'Combine modalities at the input level before any processing. Simple but may lose modality-specific patterns.',
    lateFusion: 'Late Fusion',
    lateFusionDesc: 'Process each modality separately with specialized encoders, then combine at the end. Preserves modality-specific features.',
    crossAttention: 'Cross-Attention',
    crossAttentionDesc: 'Use attention mechanisms to let each modality selectively attend to relevant parts of other modalities. The most flexible and powerful approach, used in models like Gemini and GPT-4.',

    // Applications
    applications: 'Real-World Applications',
    applicationsDesc: 'Multimodal AI enables applications that were previously impossible with single-modality systems.',
    app1: 'Video Captioning',
    app1Desc: 'Generate detailed descriptions of video content for accessibility, search, and content moderation.',
    app2: 'Voice Assistants',
    app2Desc: 'Natural conversations that understand speech, respond vocally, and can reference images or screens.',
    app3: 'Medical Imaging',
    app3Desc: 'Analyze X-rays, MRIs, and other scans alongside patient records and doctor notes.',
    app4: 'Robotics',
    app4Desc: 'Process camera feeds, sensor data, and commands to navigate and manipulate the physical world.',
    app5: 'Content Creation',
    app5Desc: 'Generate images from text, add audio to videos, or create multimedia content from descriptions.',
    app6: 'Accessibility',
    app6Desc: 'Describe images for the visually impaired, transcribe audio for the deaf, and translate across modalities.',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Multimodal AI combines text, images, audio, and video to build richer understanding of the world',
    takeaway2: 'Each modality requires specialized encoders that convert inputs into embedding vectors',
    takeaway3: 'Cross-attention mechanisms allow models to relate information across different modalities',
    takeaway4: 'Video understanding adds the dimension of time, requiring temporal reasoning and frame sampling',
    takeaway5: 'Real-world applications span from accessibility tools to robotics and content creation',
  },

  // Agentic Vision Demo component
  agenticVisionDemo: {
    title: 'Agentic Vision in Action',
    subtitle: 'Watch the model zoom, rotate, and scan a document',
    start: 'Start Demo',
    reset: 'Reset',
    documentView: 'Document View',
    agentLog: 'Agent Log',
    clickStart: 'Click "Start Demo" to see agentic vision in action',
    processing: 'Processing...',
    thinkMessage: 'I need to read the serial number in the bottom-right of this invoice. The text appears small, so I should zoom in on that region.',
    zoomMessage: 'Cropping and upscaling the serial number region...',
    zoomObserve: 'Zoomed view obtained. The text is now larger but appears slightly rotated.',
    rotateThink: 'The text is tilted approximately 15 degrees. I should correct the rotation for better readability.',
    rotateMessage: 'Correcting document rotation for optimal text recognition...',
    rotateObserve: 'Document is now properly aligned. I can read the serial number clearly now.',
    scanMessage: 'Reading the aligned, zoomed region...',
    scanObserve: 'Serial number identified with high confidence:',
    resultTitle: 'Extracted Serial Number',
    resultConfidence: 'Confidence: 97% | Method: zoom + rotate + read',
    stepThink: 'Think',
    stepZoom: 'Zoom',
    stepRotate: 'Rotate',
    stepScan: 'Scan',
    stepDone: 'Done',
  },

  // Visual Challenges Demo component
  visualChallengesDemo: {
    title: 'VLM Failure Mode Explorer',
    subtitle: 'Interactive scenarios showing where vision models struggle',

    // Tab names
    countingTab: 'Counting',
    spatialTab: 'Spatial',
    textTab: 'Text',
    hallucinationTab: 'Hallucination',
    detailTab: 'Details',
    multiImageTab: 'Multi-Image',

    // UI
    imageScenario: 'Image Scenario',
    vlmResponse: 'VLM Response',
    revealAnalysis: 'Reveal Analysis',
    hideAnalysis: 'Hide Analysis',
    vlmSaid: 'VLM Said',
    actualAnswer: 'Actual Answer',
    whyFails: 'Why This Fails',
    proTip: 'Pro Tip',
    keyInsight: 'Key Insight',
    keyInsightDesc: 'VLMs are powerful but not infallible. Understanding their systematic weaknesses helps you design robust applications that leverage their strengths while mitigating their limitations.',

    // Counting challenge
    countingTitle: 'Object Counting Challenge',
    countingScenario: 'A photo of a desk with scattered paperclips. There are exactly 23 paperclips visible, some overlapping each other.',
    countingVlmResponse: '"I can see approximately 15-20 paperclips scattered across the desk."',
    countingActual: 'There are exactly 23 paperclips in the image.',
    countingWhy: 'VLMs process images as 14x14 pixel patches, not as discrete objects. They lack object permanence and struggle with overlapping items. The model gives a rough estimate based on pattern recognition, not actual counting.',
    countingTip: 'For precise counting, use specialized object detection models (YOLO, Faster R-CNN) or ask the VLM to identify and list each item individually rather than providing a total count.',

    // Spatial challenge
    spatialTitle: 'Spatial Reasoning Challenge',
    spatialScenario: 'A photo showing a red cup on the LEFT side of a blue cup, both on a white table.',
    spatialVlmResponse: '"The red cup is positioned to the right of the blue cup on the table."',
    spatialActual: 'The red cup is on the LEFT side of the blue cup.',
    spatialWhy: 'Position information is encoded through patch embeddings without pixel-level precision. The model learns statistical correlations rather than explicit spatial reasoning, making left/right confusion common.',
    spatialTip: 'Be explicit about reference frames in your prompts. Consider adding visual markers or grid overlays to images when spatial precision is critical.',

    // Text challenge
    textTitle: 'Small Text Recognition Challenge',
    textScenario: 'A product label with the serial number "XK7-2B9M-Q4P" printed in 8pt font at the bottom.',
    textVlmResponse: '"The serial number appears to be XK7-289M-04P."',
    textActual: 'The serial number is XK7-2B9M-Q4P (note: B vs 8, Q vs 0).',
    textWhy: 'Text smaller than the patch size (14-16 pixels) gets compressed into a single embedding, losing character-level detail. Similar characters (B/8, Q/0, l/1) are easily confused.',
    textTip: 'Use high-resolution images and zoom in on text regions. For critical OCR tasks, use dedicated OCR tools (Tesseract, Google Vision API) alongside or instead of VLMs.',

    // Hallucination challenge
    hallucinationTitle: 'Visual Hallucination Challenge',
    hallucinationScenario: 'A photo of a home office desk with a monitor, but NO keyboard visible (it is in a drawer).',
    hallucinationVlmResponse: '"I can see a monitor on the desk with a keyboard in front of it."',
    hallucinationActual: 'There is no keyboard visible in the image—the VLM hallucinated it.',
    hallucinationWhy: 'VLMs generate statistically plausible descriptions. When features are ambiguous, the model fills gaps with likely content based on training data—monitors usually have keyboards nearby.',
    hallucinationTip: 'Ask the model to express uncertainty and describe only what it can clearly see. Use prompts like "if you cannot determine X, say so" and always cross-reference critical details.',

    // Detail challenge
    detailTitle: 'Fine Detail Recognition Challenge',
    detailScenario: 'Two similar dog photos: one is a Shiba Inu, the other is an Akita. The subtle differences are in ear shape and body proportions.',
    detailVlmResponse: '"Both images show Shiba Inu dogs."',
    detailActual: 'The first is a Shiba Inu, the second is an Akita (larger body, different ear shape).',
    detailWhy: 'The patch-based architecture averages information within each patch, losing fine-grained details. Subtle distinguishing features like ear angles or body proportions are compressed.',
    detailTip: 'Use the highest resolution available and crop to focus on distinguishing features. For specialized tasks, consider fine-tuned models trained on domain-specific data.',

    // Multi-image challenge
    multiImageTitle: 'Multi-Image Reasoning Challenge',
    multiImageScenario: 'Two nearly identical product photos. The difference: in image 2, the product has a small scratch on the upper right corner.',
    multiImageVlmResponse: '"Both images appear identical. I don\x27t see any differences between them."',
    multiImageActual: 'Image 2 has a visible scratch on the upper right corner that is not present in Image 1.',
    multiImageWhy: 'Each image is encoded separately into token sequences. Cross-image attention must happen through the language model\x27s context window, which is less efficient than dedicated architectures.',
    multiImageTip: 'Describe each image separately first, then ask for comparison. Consider combining images into a single side-by-side composite for direct visual comparison.',
  },

  // Agent Loop page
  agentLoop: {
    title: 'The Agent Loop',
    description: 'Understanding the core cycle that powers autonomous AI agents: observe, think, act, repeat.',
    whatIs: 'What is the Agent Loop?',
    whatIsDesc: 'The agent loop is the fundamental cycle that enables AI agents to interact with their environment autonomously. It consists of observation, reasoning, action, and feedback phases that repeat continuously.',
    phases: 'The Four Phases',
    observe: 'Observe',
    observeDesc: 'Gather information from the environment, tools, and user input.',
    think: 'Think',
    thinkDesc: 'Reason about the current state and decide on the next action.',
    act: 'Act',
    actDesc: 'Execute the chosen action using available tools.',
    learn: 'Learn',
    learnDesc: 'Process feedback and update understanding for the next iteration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'The loop continues until the task is complete or terminated',
    takeaway2: 'Each iteration builds on previous observations and actions',
    takeaway3: 'Error handling and recovery are crucial for robust agents',
    takeaway4: 'The quality of tools directly impacts agent capabilities',
  },

  // Agent Context page
  agentContext: {
    title: 'Context Anatomy',
    description: 'Breaking down the structure of context windows and how agents manage information.',
    whatIs: 'Understanding Agent Context',
    whatIsDesc: 'Agent context includes the system prompt, conversation history, tool definitions, and retrieved information. Managing this context efficiently is crucial for agent performance.',
    components: 'Context Components',
    systemPrompt: 'System Prompt',
    systemPromptDesc: 'Defines the agent\'s role, capabilities, and behavioral guidelines.',
    toolDefs: 'Tool Definitions',
    toolDefsDesc: 'Descriptions of available tools and how to use them.',
    history: 'Conversation History',
    historyDesc: 'Previous messages, tool calls, and their results.',
    retrieved: 'Retrieved Information',
    retrievedDesc: 'External knowledge fetched during the conversation.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context management is key to agent reliability',
    takeaway2: 'Prioritize recent and relevant information',
    takeaway3: 'Tool definitions should be clear and unambiguous',
    takeaway4: 'Summarization helps maintain context over long sessions',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agent Problems',
    description: 'Common failure modes and challenges faced by AI agents in real-world applications.',
    overview: 'Common Agent Failure Modes',
    overviewDesc: 'Understanding typical agent failures helps in building more robust systems and setting appropriate expectations.',
    problem1: 'Tool Misuse',
    problem1Desc: 'Agents may call tools incorrectly, with wrong parameters, or at inappropriate times.',
    problem2: 'Infinite Loops',
    problem2Desc: 'Agents can get stuck repeating the same actions without making progress.',
    problem3: 'Goal Drift',
    problem3Desc: 'Agents may gradually shift focus away from the original task objective.',
    problem4: 'Over-confidence',
    problem4Desc: 'Agents may proceed with actions despite uncertainty or incomplete information.',
    
    // Expanded Content
    hallucination: 'Tool Hallucination',
    hallucinationDesc: 'Agents sometimes "invent" tool parameters or even entire tools that don\'t exist. This usually happens when the tool definition is ambiguous or when the model tries to force a solution.',
    hallucinationExample: 'Example: Calling `get_weather(location="Tokyo", date="tomorrow")` when the function only accepts `location`.',
    
    loops: 'Looping Issues',
    loopsDesc: 'Agents can get trapped in repetitive cycles where they perform the same action, receive the same error, and try again without changing strategy.',
    loopsMitigation: 'Mitigation: Implement loop detection logic that stops execution if the same tool call sequence occurs multiple times.',
    
    costLatency: 'Cost & Latency',
    costLatencyDesc: 'Every step in the agent loop requires a full LLM inference call. Multi-step tasks can quickly become expensive and slow.',
    costFactor: 'The Cost Factor',
    costFactorDesc: 'A simple task requiring 5 steps means 5x the cost and 5x the latency of a standard chat response.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Implement safeguards like iteration limits and cost controls',
    takeaway2: 'Add human-in-the-loop checkpoints for critical actions',
    takeaway3: 'Monitor agent behavior and log all actions for debugging',
    takeaway4: 'Design clear success and failure criteria',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agent Security',
    description: 'Critical security vulnerabilities in AI agents: prompt injection, data exfiltration, and tool misuse—plus how to defend against them.',
    
    // Intro
    intro: 'Agents Are Attack Surfaces',
    introDesc: 'When you give an LLM access to tools, you\'re creating a powerful attack vector. Agents can read files, make HTTP requests, send emails, and execute code. A malicious actor who can influence the agent\'s context can potentially control all of these capabilities.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Attack #1: Prompt Injection',
    attack1Desc: 'Prompt injection occurs when untrusted input is interpreted as instructions by the LLM. Because agents often process external data (emails, web pages, documents), attackers can embed hidden commands that hijack the agent\'s behavior.',
    attack1Example: 'Example Attack',
    attack1ExampleDesc: 'User asks the agent to summarize a document. The document contains hidden instructions:',
    whyWorks: 'Why This Works',
    whyWorks1: 'The agent reads the document into its context',
    whyWorks2: 'The LLM cannot distinguish between "real" instructions and injected ones',
    whyWorks3: 'The hidden text looks like system instructions, so the LLM may follow them',
    whyWorks4: 'The agent uses its legitimate tools to perform the malicious action',
    directInjection: 'Direct Injection',
    directInjectionDesc: 'User directly types malicious instructions. Easier to filter but still dangerous if system prompt isn\'t robust.',
    indirectInjection: 'Indirect Injection',
    indirectInjectionDesc: 'Malicious content comes from external sources the agent reads (websites, emails, files). Much harder to defend against.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Attack #2: Data Exfiltration',
    attack2Desc: 'Agents with access to communication tools (email, HTTP, Slack, etc.) can be tricked into sending sensitive data to external destinations. The agent becomes an unwitting accomplice in data theft.',
    exfilFlow: 'Exfiltration Flow',
    exfilStep1: 'Agent reads',
    exfilStep1Desc: 'Private files, DB, env vars',
    exfilStep2: 'Injection triggers',
    exfilStep2Desc: '"Send this to X"',
    exfilStep3: 'Tool executes',
    exfilStep3Desc: 'Data leaves the system',
    vulnerableConfig: 'Vulnerable Tool Configuration',
    otherVectors: 'Other Exfiltration Vectors',
    vector1: 'HTTP requests — POST data to attacker-controlled endpoints',
    vector2: 'Slack/Discord webhooks — Send messages to external channels',
    vector3: 'File uploads — Upload to cloud storage with public links',
    vector4: 'DNS exfiltration — Encode data in DNS queries',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Attack #3: Unintended Tool Misuse',
    attack3Desc: 'Even without malicious intent, agents can cause harm through incorrect tool usage. The LLM might misunderstand parameters, use the wrong tool, or take destructive actions while trying to be helpful.',
    destructiveActions: 'Destructive Actions',
    destructiveActionsDesc: '"Clean up the project" → Agent runs rm -rf / or deletes production database',
    wrongParams: 'Wrong Parameters',
    wrongParamsDesc: 'Agent confuses similar fields or uses incorrect values that seem plausible',
    cascadingErrors: 'Cascading Errors',
    cascadingErrorsDesc: 'Agent makes one small error, then "fixes" it with increasingly destructive actions',
    
    // Defense Strategies
    defensesTitle: 'Defense Strategies',
    defense1: 'Principle of Least Privilege',
    defense1Desc: 'Only give the agent the minimum tools and permissions needed for the task. Don\'t give file access if it only needs to answer questions.',
    defense1Bad: 'Bad',
    defense1Good: 'Good',
    defense2: 'Strict Allowlists',
    defense2Desc: 'Constrain tool parameters to known-safe values. Don\'t allow arbitrary email addresses, URLs, or file paths.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Require human approval for sensitive actions. The agent proposes, the human confirms.',
    defense3Example: 'Example confirmation flow:',
    defense4: 'Input Sanitization & Isolation',
    defense4Desc: 'Treat external data as untrusted. Clearly separate user instructions from retrieved content.',
    defense5: 'Monitoring & Rate Limiting',
    defense5Desc: 'Log all tool calls. Set rate limits on sensitive operations. Alert on unusual patterns (many emails, large data transfers, repeated failures). Enable rollback for destructive actions.',

    // Attack 4: Indirect Prompt Injection (IPI) - 2025
    attack4Title: 'Attack #4: Indirect Prompt Injection (IPI)',
    attack4Desc: 'Indirect Prompt Injection (IPI) has emerged as the most dangerous 2025 threat to AI agents. Unlike direct injection where users type malicious prompts, IPI attacks hide payloads in content the agent retrieves—emails, documents, web pages, or database records. The agent unwittingly executes attacker instructions while processing legitimate-looking data.',
    ipiFlow: 'IPI Attack Flow',
    ipiStep1: 'Attacker plants',
    ipiStep1Desc: 'Hidden payload in email/doc',
    ipiStep2: 'User asks agent',
    ipiStep2Desc: '"Summarize my emails"',
    ipiStep3: 'Agent retrieves',
    ipiStep3Desc: 'Poisoned content loaded',
    ipiStep4: 'Payload executes',
    ipiStep4Desc: 'Agent follows hidden instructions',
    ipiVectors: 'Common IPI Attack Vectors',
    ipiVector1Title: 'Email',
    ipiVector1Desc: 'Malicious instructions in email body, hidden in HTML comments, or in attached documents',
    ipiVector2Title: 'RAG Documents',
    ipiVector2Desc: 'Poisoned documents in vector databases that get retrieved during semantic search',
    ipiVector3Title: 'Web Content',
    ipiVector3Desc: 'Compromised websites or SEO-poisoned pages the agent browses or scrapes',
    ipiVector4Title: 'API Responses',
    ipiVector4Desc: 'Third-party APIs returning malicious payloads hidden in JSON/XML data',
    ipiDanger: 'Why IPI is Especially Dangerous',
    ipiDangerDesc: 'IPI bypasses user-facing safeguards because the malicious content never comes directly from the user. The agent processes it as trusted data. In 2025, sophisticated IPI attacks use multi-stage payloads, context manipulation, and even "sleeper" instructions that activate only under specific conditions.',

    // Attack 5: Agent-to-Agent Attacks
    attack5Title: 'Attack #5: Agent-to-Agent Attacks',
    attack5Desc: 'As multi-agent systems become more common, a new attack surface has emerged: compromised agents attacking other agents in the same system. One poisoned agent can manipulate, deceive, or exploit its peers—especially dangerous when agents share memory, tools, or coordinate on tasks.',
    a2aScenario: 'Multi-Agent Attack Scenario',
    a2aAgent1: 'Research Agent',
    a2aAgent1Desc: 'Searches the web, retrieves data',
    a2aAgent2: 'Orchestrator Agent',
    a2aAgent2Desc: 'Coordinates tasks between agents',
    a2aAgent3: 'Execution Agent',
    a2aAgent3Desc: 'Has write access, runs code',
    a2aCompromised: 'COMPROMISED',
    a2aAttackLabel: 'Attack:',
    a2aAttackDesc: 'Compromised orchestrator injects malicious instructions into messages sent to the Execution Agent, using its elevated trust to bypass security checks.',
    a2aType1: 'Prompt Relay Attacks',
    a2aType1Desc: 'A compromised agent includes injection payloads in its outputs that target downstream agents processing the results.',
    a2aType2: 'Shared Memory Poisoning',
    a2aType2Desc: 'Attacker writes malicious content to shared agent memory/context that other agents later read and execute.',
    a2aType3: 'Trust Escalation',
    a2aType3Desc: 'Exploiting that agents often trust messages from peer agents more than external sources, bypassing security filters.',
    a2aType4: 'Coordination Manipulation',
    a2aType4Desc: 'Manipulating multi-agent voting, consensus, or task distribution to achieve malicious goals through legitimate-seeming collaboration.',

    // Compliance Frameworks
    complianceTitle: 'Compliance Frameworks',
    complianceDesc: 'Several standards and frameworks have emerged to guide AI agent security practices. Organizations deploying AI agents should familiarize themselves with these guidelines to ensure responsible deployment and regulatory compliance.',
    framework1Tag: 'NIST',
    framework1Title: 'NIST AI Risk Management Framework (AI RMF)',
    framework1Desc: 'The National Institute of Standards and Technology provides a comprehensive framework for managing AI risks throughout the system lifecycle.',
    framework1Point1: 'Govern: Establish policies, processes, and accountability structures',
    framework1Point2: 'Map: Identify and document AI risks in context',
    framework1Point3: 'Manage: Implement controls and monitor for emerging risks',
    framework2Tag: 'OWASP',
    framework2Title: 'OWASP Top 10 for LLM Applications',
    framework2Desc: 'The Open Web Application Security Project maintains a list of the most critical security risks for LLM-based applications.',
    framework2Point1: 'LLM01: Prompt Injection (direct and indirect)',
    framework2Point2: 'LLM02: Insecure Output Handling',
    framework2Point3: 'LLM06: Sensitive Information Disclosure',
    framework3Tag: 'ISO',
    framework3Title: 'ISO/IEC 42001 AI Management System',
    framework3Desc: 'The international standard for AI management systems, providing requirements for establishing, implementing, and improving AI governance.',
    framework3Point1: 'Defines requirements for responsible AI development and deployment',
    framework3Point2: 'Addresses risk assessment specific to AI systems',
    framework3Point3: 'Provides certification pathway for AI system compliance',

    
    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Agents are attack surfaces—every tool is a potential vulnerability',
    takeaway2: 'Prompt injection is the #1 threat—LLMs cannot distinguish instructions from data',
    takeaway3: 'Data exfiltration is trivial if agents have outbound communication tools',
    takeaway4: 'Tool misuse happens even without attackers—LLMs make mistakes',
    takeaway5: 'Defense in depth: least privilege + allowlists + human approval + monitoring',
    takeaway6: 'Treat all external data as potentially malicious input',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentic Patterns',
    description: 'Design patterns and architectures for building effective AI agent systems.',
    overview: 'Common Agentic Patterns',
    overviewDesc: 'Several architectural patterns have emerged as effective approaches for building AI agent systems. Each pattern offers different trade-offs between reliability, transparency, and capability.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Interleave reasoning traces with actions for better transparency and control.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Create a high-level plan first, then execute steps sequentially.',
    pattern3: 'Multi-Agent Systems',
    pattern3Desc: 'Multiple specialized agents collaborate to solve complex tasks.',
    pattern4: 'Reflection',
    pattern4Desc: 'Agents review their own outputs and iteratively improve them.',
    // Deep dive sections
    reactDeepDive: 'ReAct Pattern Deep Dive',
    reactDeepDiveDesc: 'ReAct (Reasoning + Acting) interleaves chain-of-thought reasoning with action execution. The model explicitly states its thinking before each action.',
    reactHow: 'How it works',
    reactStep1: 'Thought: The model reasons about what to do next',
    reactStep2: 'Action: The model calls a tool or takes an action',
    reactStep3: 'Observation: The model sees the result',
    reactStep4: 'Repeat until task is complete',
    reactPros: 'Pros',
    reactPro1: 'Highly transparent—you can see exactly why the agent did what it did',
    reactPro2: 'Easier to debug and understand failures',
    reactPro3: 'Natural error recovery through explicit reasoning',
    reactCons: 'Cons',
    reactCon1: 'More tokens used (reasoning takes space)',
    reactCon2: 'Can be slower due to explicit reasoning steps',
    reactCon3: 'May overthink simple tasks',
    planExecuteDeepDive: 'Plan-and-Execute Deep Dive',
    planExecuteDeepDiveDesc: 'This pattern separates planning from execution. A planner creates a high-level plan, then an executor carries out each step.',
    planExecuteHow: 'How it works',
    planStep1: 'Planner analyzes the task and creates a step-by-step plan',
    planStep2: 'Executor carries out each step in sequence',
    planStep3: 'Replanning happens if execution fails or new info emerges',
    planExecutePros: 'Pros',
    planExecutePro1: 'Better for complex, multi-step tasks',
    planExecutePro2: 'Can use different models for planning vs execution',
    planExecutePro3: 'Plans can be reviewed before execution',
    planExecuteCons: 'Cons',
    planExecuteCon1: 'Plans may become outdated as execution proceeds',
    planExecuteCon2: 'Harder to handle unexpected situations',
    planExecuteCon3: 'Replanning adds latency and cost',
    multiAgentDeepDive: 'Multi-Agent Systems Deep Dive',
    multiAgentDeepDiveDesc: 'Multiple specialized agents work together, each handling different aspects of a task. A supervisor or router directs work to the right agent.',
    multiAgentHow: 'How it works',
    multiAgentStep1: 'Router/supervisor receives the task',
    multiAgentStep2: 'Task is delegated to specialized agents',
    multiAgentStep3: 'Agents may communicate and collaborate',
    multiAgentStep4: 'Results are aggregated and returned',
    multiAgentPros: 'Pros',
    multiAgentPro1: 'Specialization improves quality on complex tasks',
    multiAgentPro2: 'Can parallelize independent sub-tasks',
    multiAgentPro3: 'Easier to maintain and update individual agents',
    multiAgentCons: 'Cons',
    multiAgentCon1: 'Higher complexity and coordination overhead',
    multiAgentCon2: 'More expensive (multiple LLM calls)',
    multiAgentCon3: 'Debugging distributed failures is harder',
    reflectionDeepDive: 'Reflection Pattern Deep Dive',
    reflectionDeepDiveDesc: 'The agent generates an output, then critiques and improves it. This self-refinement loop can dramatically improve quality.',
    reflectionHow: 'How it works',
    reflectionStep1: 'Generate initial output',
    reflectionStep2: 'Critique the output (identify flaws, missing parts)',
    reflectionStep3: 'Revise based on critique',
    reflectionStep4: 'Repeat until quality threshold is met',
    reflectionPros: 'Pros',
    reflectionPro1: 'Significantly improves output quality',
    reflectionPro2: 'Catches errors the initial pass missed',
    reflectionPro3: 'Works well for writing, code review, and creative tasks',
    reflectionCons: 'Cons',
    reflectionCon1: 'Multiple LLM calls increase cost and latency',
    reflectionCon2: 'Risk of over-editing or infinite loops',
    reflectionCon3: 'May "wash out" creative or unconventional solutions',
    // Choosing section
    choosingTitle: 'Choosing the Right Pattern',
    choosingDesc: 'Select a pattern based on your specific needs and constraints.',
    choosingSimple: 'For simple, well-defined tasks',
    choosingSimpleAnswer: 'Direct tool calling (no pattern needed)',
    choosingTransparency: 'When you need transparency and debuggability',
    choosingTransparencyAnswer: 'ReAct pattern',
    choosingComplex: 'For complex, multi-step tasks',
    choosingComplexAnswer: 'Plan-and-Execute',
    choosingQuality: 'When output quality is critical',
    choosingQualityAnswer: 'Reflection pattern',
    choosingDiverse: 'For diverse task types',
    choosingDiverseAnswer: 'Multi-Agent with specialized agents',
    // Interactive section
    interactiveTitle: 'Pattern Comparison',
    interactiveDesc: 'Explore different agentic architectures',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Choose patterns based on task complexity and reliability needs',
    takeaway2: 'ReAct is great for transparency but may be slower',
    takeaway3: 'Multi-agent systems add complexity but enable specialization',
    takeaway4: 'Reflection patterns can significantly improve output quality',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'Understanding MCP: when external tool servers make sense, and when they are overkill.',
    whatIs: 'What is MCP?',
    whatIsDesc: 'The Model Context Protocol (MCP) is a standardized way to connect AI agents to external tools and data sources via dedicated server processes. Instead of defining tools inline in your agent code, MCP runs a separate server that exposes tools over a structured protocol.',
    vsToolCalls: 'MCP vs. Regular Tool Calls',
    vsToolCallsDesc: 'Regular tool calls are functions defined directly in your agent\'s codebase. The agent calls them, they execute, and results return in the same process. MCP separates this: tools live in external servers that the agent communicates with over a protocol.',
    
    // Comparison
    regularTools: 'Regular Tool Calls',
    regularToolsDesc: 'Tools defined inline in your agent code. Simple, fast, and sufficient for most use cases.',
    mcpTools: 'MCP Servers',
    mcpToolsDesc: 'Tools exposed by external server processes. Adds network overhead but enables cross-language tooling and shared tool ecosystems.',
    
    // When to use
    whenToUse: 'When MCP Makes Sense',
    whenToUseDesc: 'MCP shines in specific scenarios where its additional complexity pays off.',
    useCase1: 'Multi-Language Teams',
    useCase1Desc: 'Your tools are written in Python but your agent is in TypeScript, or vice versa.',
    useCase2: 'Shared Tool Ecosystem',
    useCase2Desc: 'Multiple agents across different projects need to access the same tools.',
    useCase3: 'Enterprise Integration',
    useCase3Desc: 'You need to expose existing internal services as agent tools without modifying them.',
    useCase4: 'Tool Marketplace',
    useCase4Desc: 'You want to use community-maintained tools without copying code into your project.',
    
    // When it's overkill
    overkill: 'When MCP is Overkill',
    overkillDesc: 'For many use cases, MCP adds unnecessary complexity.',
    overkillCase1: 'Single-Language Projects',
    overkillCase1Desc: 'If your tools and agent are in the same language, inline functions are simpler and faster.',
    overkillCase2: 'Simple Agents',
    overkillCase2Desc: 'A chatbot with a few tools doesn\'t need the overhead of running separate server processes.',
    overkillCase3: 'Rapid Prototyping',
    overkillCase3Desc: 'When iterating quickly, the indirection of MCP slows down development.',
    overkillCase4: 'Latency-Sensitive Apps',
    overkillCase4Desc: 'Network calls to tool servers add latency that inline functions don\'t have.',

    // Three Core Primitives
    corePrimitives: 'The Three Core Primitives',
    corePrimitivesDesc: 'MCP servers can expose three types of capabilities to clients. Most documentation focuses on tools, but resources and prompts are equally important.',
    primitiveTools: 'Tools',
    primitiveToolsDesc: 'Functions the model can call to perform actions. Tools are invoked by the LLM to interact with external systems—search databases, call APIs, execute code.',
    primitiveToolsExample: 'query_database, send_email, create_file',
    primitiveResources: 'Resources',
    primitiveResourcesDesc: 'Data the server can provide for context. Resources are read-only content the client can fetch—files, database records, API responses—that inform the model\'s responses.',
    primitiveResourcesExample: 'file://config.json, db://users/123, api://weather/today',
    primitivePrompts: 'Prompts',
    primitivePromptsDesc: 'Pre-defined prompt templates the server offers. Prompts are reusable interaction patterns with parameters—like "summarize this document" or "review this code".',
    primitivePromptsExample: 'summarize_document, code_review, translate_text',

    // Server Lifecycle
    serverLifecycle: 'Server Lifecycle',
    serverLifecycleDesc: 'MCP connections follow a structured lifecycle with capability negotiation at startup.',
    lifecyclePhase1: 'Initialize',
    lifecyclePhase1Desc: 'Client sends initialize request with protocol version and client capabilities. This is always the first message.',
    lifecyclePhase2: 'Capabilities Exchange',
    lifecyclePhase2Desc: 'Server responds with its supported capabilities (tools, resources, prompts) and protocol version agreement.',
    lifecyclePhase3: 'Initialized',
    lifecyclePhase3Desc: 'Client sends initialized notification to confirm setup is complete. Normal operations can now begin.',
    lifecyclePhase4: 'Operation',
    lifecyclePhase4Desc: 'Client and server exchange requests: list_tools, call_tool, list_resources, read_resource, list_prompts, get_prompt.',
    lifecyclePhase5: 'Shutdown',
    lifecyclePhase5Desc: 'Either side can close the connection. Servers should clean up resources (database connections, file handles).',

    // Real MCP Servers
    realServers: 'Real MCP Servers',
    realServersDesc: 'The MCP ecosystem includes official reference servers and community-built integrations for popular platforms.',
    serverFilesystem: 'Filesystem',
    serverFilesystemDesc: 'Secure file operations with configurable access controls. Read, write, and manage files within specified directories.',
    serverGithub: 'GitHub',
    serverGithubDesc: 'Repository management, issues, pull requests, and code search. Requires a personal access token.',
    serverSlack: 'Slack',
    serverSlackDesc: 'Channel management, messaging, and workspace interactions. Post messages, read history, manage threads.',
    serverPostgres: 'PostgreSQL',
    serverPostgresDesc: 'Database queries with read-only or read-write access. Execute SQL and explore schema.',
    serverMemory: 'Memory',
    serverMemoryDesc: 'Knowledge graph-based persistent memory. Store and retrieve structured information across conversations.',
    serverGit: 'Git',
    serverGitDesc: 'Read, search, and manipulate Git repositories. View commits, diffs, branches, and history.',
    serverConfigExample: 'Configuration Example',

    // Architecture
    architecture: 'How MCP Works',
    architectureDesc: 'MCP defines a client-server architecture where the agent is the client and tools are exposed by servers.',
    step1: 'Discovery',
    step1Desc: 'The agent connects to an MCP server and receives a list of available tools with their schemas.',
    step2: 'Invocation',
    step2Desc: 'When the LLM decides to use a tool, the agent sends a request to the MCP server.',
    step3: 'Execution',
    step3Desc: 'The MCP server runs the tool and returns results in a standardized format.',
    step4: 'Integration',
    step4Desc: 'Results flow back to the agent and into the LLM context, just like regular tool results.',
    
    // Practical advice
    practicalAdvice: 'Practical Advice',
    adviceDesc: 'Guidelines for deciding whether to use MCP in your project.',
    advice1: 'Start simple: use inline tool definitions until you hit a specific limitation.',
    advice2: 'Consider MCP when you find yourself copy-pasting tool code between projects.',
    advice3: 'The overhead of running MCP servers only makes sense at scale or in enterprise settings.',
    advice4: 'Community MCP servers can accelerate development but add dependency risks.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'MCP is a protocol for exposing tools via external servers, not a replacement for regular tool calls',
    takeaway2: 'For most single-project agents, inline tools are simpler and have lower latency',
    takeaway3: 'MCP shines in polyglot environments and shared tool ecosystems',
    takeaway4: 'Don\'t reach for MCP by default—it\'s a solution for specific scaling and interoperability challenges',
  },

  // Tokenization page
  tokenization: {
    title: 'Tokenization',
    description: 'How LLMs break text into tokens—the fundamental units of language understanding.',
    whatIs: 'What is Tokenization?',
    whatIsDesc: 'Tokenization is the process of converting raw text into a sequence of tokens—the basic units that LLMs process. Tokens can be words, subwords, or even individual characters, depending on the tokenizer.',
    whyMatters: 'Why Tokenization Matters',
    whyMattersDesc: 'Understanding tokenization is crucial because it directly impacts context limits, costs, and model behavior. The same text can have very different token counts across different models.',
    howWorks: 'How It Works',
    howWorksDesc: 'Most modern LLMs use subword tokenization algorithms like BPE (Byte Pair Encoding) or SentencePiece. These algorithms learn common character sequences from training data.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE iteratively merges the most frequent character pairs into single tokens. Common words become single tokens, while rare words are split into subwords.',
    tokenTypes: 'Token Types',
    wholeWords: 'Whole Words',
    wholeWordsDesc: 'Common words like "the", "and", "is" are often single tokens.',
    subwords: 'Subwords',
    subwordsDesc: 'Less common words are split: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Special Tokens',
    specialTokensDesc: 'Markers like <|endoftext|> or [CLS] for model control.',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'Type text to see how it gets tokenized',
    costImplications: 'Cost Implications',
    costDesc: 'API pricing is typically per-token. Efficient prompts use fewer tokens.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tokens are the atomic units LLMs process—not characters or words',
    takeaway2: 'Different models have different tokenizers and vocabularies',
    takeaway3: 'Non-English text and code often use more tokens than English',
    takeaway4: 'Token count directly affects cost and context window usage',
  },

  // Embeddings page
  embeddings: {
    title: 'Embeddings',
    description: 'How AI represents meaning as vectors in high-dimensional space.',
    whatIs: 'What are Embeddings?',
    whatIsDesc: 'Embeddings are dense vector representations that capture semantic meaning. Similar concepts have similar embeddings, enabling machines to understand relationships between words, sentences, and documents.',
    howWorks: 'How Embeddings Work',
    howWorksDesc: 'Embedding models map discrete tokens to continuous vectors in a high-dimensional space (often 768-4096 dimensions). The position of each vector encodes its semantic meaning.',
    similarity: 'Semantic Similarity',
    similarityDesc: 'Similar meanings cluster together in embedding space. "King" and "Queen" are closer than "King" and "Banana".',
    dimensions: 'Vector Dimensions',
    dimensionsDesc: 'Each dimension captures some aspect of meaning—though these dimensions aren\'t human-interpretable.',
    operations: 'Vector Operations',
    operationsDesc: 'Famous example: King - Man + Woman ≈ Queen. Relationships are encoded as directions in the space.',

    // How embeddings are created
    howCreated: 'How Embeddings Are Created',
    howCreatedDesc: 'Embeddings come from the embedding layer—a learned lookup table that sits at the very beginning of a neural network.',
    embeddingLayer: 'The Embedding Layer',
    embeddingLayerDesc: 'When a token enters the model, its ID is used to look up a row in a large matrix. This row IS the embedding—a dense vector of learned weights.',
    training: 'Learning Through Training',
    trainingDesc: 'During training, the embedding weights are adjusted via backpropagation. Words that appear in similar contexts develop similar embeddings.',
    inLLM: 'In LLMs',
    inLLMDesc: 'The embedding layer converts each token ID to a vector. These vectors are then processed through transformer layers, combined with positional encodings to understand word order.',
    dedicated: 'Dedicated Models',
    dedicatedDesc: 'Models like text-embedding-3-small or all-MiniLM are trained specifically to produce embeddings useful for similarity search, with contrastive learning objectives.',
    embeddingSize: 'Embedding Dimensions',
    embeddingSizeDesc: 'Embedding size varies: GPT-2 uses 768d, GPT-4 uses 12,288d, dedicated embedding models often use 384-1536d. Larger isn\'t always better—it depends on the task.',

    useCases: 'Common Use Cases',
    search: 'Semantic Search',
    searchDesc: 'Find documents by meaning, not just keyword matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Group similar documents, detect topics automatically.',
    classification: 'Classification',
    classificationDesc: 'Categorize text based on embedding similarity to examples.',
    rag: 'RAG Systems',
    ragDesc: 'Retrieve relevant context for LLM prompts.',
    interactiveDemo: 'Interactive Visualization',
    demoDesc: 'Explore how embeddings cluster by meaning',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Embeddings convert text to vectors that capture semantic meaning',
    takeaway2: 'Similar concepts have similar embeddings (cosine similarity)',
    takeaway3: 'Embeddings enable semantic search, clustering, and RAG',
    takeaway4: 'Different embedding models have different strengths and dimensions',
  },

  // RAG page
  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: giving LLMs access to external knowledge.',
    whatIs: 'What is RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving relevant documents from a knowledge base and including them in the prompt. This gives models access to up-to-date or specialized information.',
    whyRag: 'Why Use RAG?',
    whyRagDesc: 'LLMs have knowledge cutoffs and can hallucinate. RAG grounds responses in actual documents, reducing hallucination and enabling domain-specific knowledge without fine-tuning.',
    pipeline: 'The RAG Pipeline',
    pipelineDesc: 'RAG systems follow a consistent pattern: embed the query, retrieve relevant chunks, augment the prompt, and generate a response.',
    step1: 'Query Embedding',
    step1Desc: 'Convert the user\'s question into a vector using an embedding model.',
    step2: 'Retrieval',
    step2Desc: 'Search the vector database for chunks similar to the query embedding.',
    step3: 'Augmentation',
    step3Desc: 'Insert retrieved chunks into the prompt as context.',
    step4: 'Generation',
    step4Desc: 'The LLM generates a response grounded in the retrieved context.',
    chunking: 'Document Chunking',
    chunkingDesc: 'Documents are split into smaller chunks (typically 200-1000 tokens) for embedding and retrieval. Chunk size affects retrieval precision.',
    vectorDbs: 'Vector Databases',
    vectorDbsDesc: 'Specialized databases like Pinecone, Weaviate, or pgvector enable fast similarity search over millions of embeddings.',
    interactiveDemo: 'Interactive RAG Pipeline',
    demoDesc: 'See how queries flow through a RAG system',

    // Agentic RAG
    agenticRag: 'Agentic RAG',
    agenticRagDesc: 'In agentic RAG, the LLM doesn\'t just receive retrieved documents—it actively controls the retrieval process. The model decides when to search, what to search for, and which retrieval tools to use.',
    agenticHow: 'How It Works',
    agenticHowDesc: 'Instead of a fixed pipeline, the LLM is given retrieval tools it can call as needed. It might reformulate queries, search multiple times, or combine different search strategies based on the task.',
    agenticAdvantages: 'Advantages',
    agenticAdv1: 'Query refinement: The LLM can rephrase or decompose complex questions',
    agenticAdv2: 'Multi-hop reasoning: Chain multiple retrievals to answer complex questions',
    agenticAdv3: 'Adaptive search: Choose the right tool for each sub-question',
    agenticAdv4: 'Self-correction: Re-retrieve if initial results are insufficient',
    agenticDisadvantages: 'Disadvantages',
    agenticDisadv1: 'Higher latency: Multiple LLM calls and retrievals add up',
    agenticDisadv2: 'Increased cost: Each reasoning step costs tokens',
    agenticDisadv3: 'Complexity: Harder to debug and predict behavior',
    agenticDisadv4: 'Failure modes: LLM might loop, over-retrieve, or miss obvious queries',
    multiTool: 'Multi-Tool Retrieval',
    multiToolDesc: 'Give the LLM multiple retrieval tools for different use cases. This flexibility lets the model choose the best approach for each query.',
    toolSemantic: 'Semantic Search',
    toolSemanticDesc: 'Vector similarity for conceptual matching. Best for: "documents about X", finding related content.',
    toolFulltext: 'Full-Text Search',
    toolFulltextDesc: 'Keyword/BM25 search for exact matches. Best for: specific terms, names, codes, error messages.',
    toolSql: 'SQL/Structured Query',
    toolSqlDesc: 'Query structured data directly. Best for: counts, aggregations, filtering by attributes.',
    toolKg: 'Knowledge Graph',
    toolKgDesc: 'Traverse entity relationships. Best for: "how is X related to Y", multi-hop facts.',
    whenToUse: 'When to Use Agentic RAG',
    whenToUseDesc: 'Standard RAG is simpler and faster for straightforward Q&A. Use agentic RAG when queries are complex, require multiple sources, or benefit from query reformulation.',

    // Traditional vs Agentic Comparison
    traditionalRag: 'Traditional RAG',
    traditionalTagline: 'Fixed pipeline, predictable flow',
    traditionalChar1: 'Linear execution: query → retrieve → generate',
    traditionalChar2: 'Single retrieval pass, no iteration',
    traditionalChar3: 'Fast and predictable, easier to debug',
    agenticTagline: 'LLM-controlled, iterative process',
    agenticChar1: 'LLM decides when and what to retrieve',
    agenticChar2: 'Can loop: retrieve → evaluate → re-retrieve',
    agenticChar3: 'Handles complex, multi-step queries',

    // Comparison visualizer
    compQuery: 'Query',
    compEmbed: 'Embed',
    compRetrieve: 'Retrieve',
    compGenerate: 'Generate',
    compThink: 'Think',
    compTool: 'Tool',
    compEvaluate: 'Evaluate',
    compAnimate: 'Animate Flow',
    compAnimating: 'Animating...',
    compAspect: 'Aspect',
    compRowControl: 'Control Flow',
    compControlTraditional: 'Fixed pipeline',
    compControlAgentic: 'LLM decides',
    compRowRetrieval: 'Retrieval',
    compRetrievalTraditional: 'Single pass',
    compRetrievalAgentic: 'Multiple iterations',
    compRowQuery: 'Query Handling',
    compQueryTraditional: 'Used as-is',
    compQueryAgentic: 'Can reformulate',
    compRowLatency: 'Latency',
    compLatencyTraditional: 'Fast',
    compLatencyAgentic: 'Variable',
    compRowBestFor: 'Best For',
    compBestForTraditional: 'Simple Q&A, factual lookup',
    compBestForAgentic: 'Complex reasoning, multi-hop',
    comparisonTitle: 'Traditional vs Agentic RAG',
    comparisonDesc: 'Two approaches to retrieval-augmented generation with different trade-offs.',

    // Case Study Visualizer
    caseStudyTitle: 'When Each Approach Wins (or Fails)',
    caseStudyDesc: 'Explore real-world scenarios to see when Traditional RAG outperforms Agentic RAG, when the reverse is true, and when neither can help.',
    caseTraditionalWins: 'Case 1: Traditional RAG Wins',
    caseAgenticWins: 'Case 2: Agentic RAG Wins',
    caseBothFail: 'Case 3: Both Approaches Fail',
    caseUserQuery: 'User Query',
    caseWhy: 'Why This Outcome?',
    caseQuery1: 'What is the company\'s return policy?',
    caseQuery2: 'Compare our Q3 2024 revenue to our main competitor and explain the key differences.',
    caseQuery3: 'What will our stock price be next quarter?',
    caseExplanation1: 'For simple factual queries, Traditional RAG is more efficient. The answer exists in a single document, so the direct retrieve-then-generate pipeline works perfectly. Agentic RAG reaches the same answer but with unnecessary overhead from planning and evaluation steps—wasting time and tokens.',
    caseExplanation2: 'Complex queries requiring multiple sources benefit from Agentic RAG. The agent decomposed the query into sub-questions, retrieved from different document sets (internal financials, competitor reports), and synthesized a coherent comparison. Traditional RAG retrieved only partial information and couldn\'t connect the dots.',
    caseExplanation3: 'Neither approach can answer questions about future events or information not in the knowledge base. Traditional RAG hallucinated from superficially related content. Agentic RAG searched more thoroughly and correctly admitted uncertainty, but still couldn\'t answer—no retrieval helps when the data doesn\'t exist.',
    outcomeSuccess: 'Success',
    outcomePartial: 'Partial',
    outcomeFailure: 'Failed',

    // Step labels
    stepThinking: 'Thinking',
    stepSearching: 'Searching',
    stepRetrieved: 'Retrieved',
    stepGenerating: 'Generating',
    processSteps: 'Process Steps',
    retrievedDocs: 'Retrieved Documents',
    finalResponse: 'Final Response',
    noRelevantDocs: 'No relevant documents found in knowledge base',

    // Case 1: Traditional RAG Wins - Simple factual query
    case1TradSearch: 'Embedding query: "return policy"',
    case1TradRetrieve: 'Found 1 highly relevant document (similarity: 0.94)',
    case1TradGenerate: 'Generating response from retrieved context',
    case1TradDoc1: '[policies/returns.md] "Return Policy: Items may be returned within 30 days of purchase with original receipt. Refunds processed to original payment method within 5-7 business days. Electronics must be unopened. Sale items are final sale."',
    case1TradResponse: 'Items can be returned within 30 days with the original receipt. Refunds are processed to your original payment method in 5-7 business days. Note that electronics must be unopened and sale items are final sale.',
    case1AgentThought1: 'The user is asking about return policy. I should search for policy documents.',
    case1AgentSearch: 'Searching: "return policy refund"',
    case1AgentRetrieve: 'Found 1 relevant document',
    case1AgentThought2: 'This document fully answers the question. No additional searches needed.',
    case1AgentGenerate: 'Synthesizing final response',
    case1AgentResponse: 'Items can be returned within 30 days with the original receipt. Refunds are processed to your original payment method in 5-7 business days. Note that electronics must be unopened and sale items are final sale.',

    // Case 2: Agentic RAG Wins - Complex multi-source query
    case2TradSearch: 'Embedding query: "Q3 2024 revenue competitor comparison"',
    case2TradRetrieve: 'Found 1 partially relevant document (similarity: 0.71)',
    case2TradGenerate: 'Generating response from limited context',
    case2TradDoc1: '[financials/q3-2024.md] "Q3 2024 Revenue: $4.2M (+12% YoY). Key drivers: Enterprise sales growth, new product launches. Operating margin: 18%."',
    case2TradResponse: 'Our Q3 2024 revenue was $4.2M, up 12% year-over-year, driven by enterprise sales and new products. (Note: I don\'t have competitor data to make a comparison.)',
    case2AgentThought1: 'This is a comparison query. I need: 1) Our Q3 2024 revenue, 2) Competitor Q3 2024 revenue, 3) Analysis of differences.',
    case2AgentSearch1: 'Searching: "Q3 2024 revenue financial results"',
    case2AgentRetrieve1: 'Found internal financial report',
    case2AgentThought2: 'Got our revenue. Now I need competitor data. Let me search for competitor analysis.',
    case2AgentSearch2: 'Searching: "competitor market analysis 2024"',
    case2AgentRetrieve2: 'Found competitive intelligence report',
    case2AgentThought3: 'Now I have both datasets. Our revenue is $4.2M, competitor is $3.8M. I can analyze the differences.',
    case2AgentGenerate: 'Synthesizing comparison from both sources',
    case2AgentDoc1: '[financials/q3-2024.md] "Q3 2024 Revenue: $4.2M (+12% YoY). Key drivers: Enterprise sales growth, new product launches. Operating margin: 18%."',
    case2AgentDoc2: '[research/competitive-intel.md] "Competitor X Q3 2024: Revenue $3.8M (+5% YoY). Challenges: Supply chain delays, slower SMB adoption. Market share: declining in enterprise segment."',
    case2AgentResponse: 'Our Q3 2024 revenue ($4.2M) outperformed Competitor X ($3.8M) by 10.5%. Key differences: We grew 12% YoY vs their 5%, driven by our enterprise sales strength where they\'re losing share. Their supply chain issues and weaker SMB adoption contributed to slower growth. Our operating margin of 18% suggests healthier unit economics.',

    // Case 3: Both Fail - Future prediction / missing data
    case3TradSearch: 'Embedding query: "stock price next quarter prediction"',
    case3TradRetrieve: 'Found 1 loosely related document (similarity: 0.52)',
    case3TradGenerate: 'Generating response from tangentially related content',
    case3TradDoc1: '[financials/stock-history.md] "Historical stock performance: Q1 2024: $45, Q2 2024: $52, Q3 2024: $48. Analyst consensus: Hold rating."',
    case3TradResponse: 'Based on recent trends, the stock price has fluctuated between $45-52. Given the Hold rating and recent performance, next quarter could see prices around $50. (Warning: This is speculation, not financial advice.)',
    case3AgentThought1: 'User wants stock price prediction. This requires: 1) Current financials, 2) Market forecasts, 3) Analyst predictions.',
    case3AgentSearch1: 'Searching: "stock forecast Q1 2025 prediction"',
    case3AgentRetrieve1: 'No relevant documents found',
    case3AgentThought2: 'No forecasts in knowledge base. Let me try analyst reports.',
    case3AgentSearch2: 'Searching: "analyst price target forward guidance"',
    case3AgentRetrieve2: 'No relevant documents found',
    case3AgentThought3: 'I cannot find any forward-looking data. The knowledge base only contains historical information. I should not speculate.',
    case3AgentGenerate: 'Generating honest uncertainty response',
    case3AgentResponse: 'I cannot predict future stock prices. My knowledge base contains historical data (Q3 2024: $48, Hold rating) but no forward-looking forecasts or analyst price targets. Stock predictions require real-time market data and analysis I don\'t have access to.',

    // Advanced Techniques (2025)
    advancedTechniques: 'Advanced RAG Techniques',
    advancedTechniquesDesc: 'Beyond basic RAG, modern systems use sophisticated techniques to improve retrieval quality, answer accuracy, and handle complex queries. These 2025 approaches represent the state of the art.',

    // Self-RAG
    selfRag: 'Self-RAG',
    selfRagDesc: 'Self-RAG introduces self-reflection into the retrieval process. Instead of always retrieving, the model decides when retrieval is needed and critically evaluates retrieved content before using it.',
    selfRagHow: 'How Self-RAG Works',
    selfRagHowDesc: 'The model generates special reflection tokens during inference: [Retrieve] to decide if retrieval is needed, [IsRel] to assess relevance of retrieved passages, [IsSup] to verify if the response is supported by the context, and [IsUse] to evaluate overall utility.',
    selfRagRetrieve: 'Retrieve Decision',
    selfRagRetrieveDesc: 'Model decides whether the query needs external knowledge or can be answered from parametric memory alone.',
    selfRagCritique: 'Self-Critique',
    selfRagCritiqueDesc: 'Retrieved passages are evaluated for relevance. Irrelevant or low-quality results are filtered before generation.',
    selfRagGenerate: 'Grounded Generation',
    selfRagGenerateDesc: 'Response is generated with explicit grounding checks. The model verifies claims are supported by retrieved context.',

    // GraphRAG
    graphRag: 'GraphRAG',
    graphRagDesc: 'GraphRAG combines vector similarity search with knowledge graph traversal. It builds a graph of entities and relationships from your documents, enabling both semantic search and structured reasoning.',
    graphRagVector: 'Vector Search Layer',
    graphRagVectorDesc: 'Traditional semantic search finds relevant document chunks. This handles the "what is similar to my query" part of retrieval.',
    graphRagGraph: 'Knowledge Graph Layer',
    graphRagGraphDesc: 'Entities and relationships are extracted and linked. Enables multi-hop reasoning like "Find all products mentioned by companies that partnered with X".',
    graphRagBenefits: 'Key Benefits',
    graphRagBenefit1: 'Better handling of questions requiring relationship reasoning',
    graphRagBenefit2: 'Improved accuracy for multi-entity queries',
    graphRagBenefit3: 'Enables global summarization across entire document collections',

    // Query Augmentation
    queryAugmentation: 'Query Augmentation',
    queryAugmentationDesc: 'User queries are often incomplete or poorly phrased for retrieval. Query augmentation techniques transform queries before search to improve retrieval quality.',
    queryHyde: 'HyDE (Hypothetical Document Embeddings)',
    queryHydeDesc: 'Generate a hypothetical answer first, then use that answer\'s embedding for retrieval. This bridges the gap between question and document embedding spaces.',
    queryHydeExample: 'Query: "climate change effects" -> Generate hypothetical doc -> Embed that -> Search',
    queryDecomposition: 'Query Decomposition',
    queryDecompositionDesc: 'Break complex queries into simpler sub-queries. Each sub-query retrieves independently, then results are combined.',
    queryDecompositionExample: '"Compare A vs B" -> "What is A?" + "What is B?" -> Merge results',
    queryExpansion: 'Query Expansion',
    queryExpansionDesc: 'Add synonyms, related terms, or rephrasings to the original query. Increases recall by matching documents that use different terminology.',
    queryRewrite: 'Query Rewriting',
    queryRewriteDesc: 'Use an LLM to rewrite ambiguous or conversational queries into clear, search-optimized forms. Handles pronouns, context, and implicit references.',

    // RAG Evaluation
    evaluation: 'RAG Evaluation',
    evaluationDesc: 'Measuring RAG system quality requires specialized metrics that evaluate both retrieval and generation. RAGAS (Retrieval Augmented Generation Assessment) provides a standard framework.',
    ragasFramework: 'RAGAS Framework',
    ragasFrameworkDesc: 'RAGAS uses LLM-based evaluation to score RAG systems without requiring ground truth labels for every question. It measures multiple dimensions of quality.',
    metricFaithfulness: 'Faithfulness',
    metricFaithfulnessDesc: 'Does the answer only contain information from the retrieved context? Measures hallucination—claims not supported by the provided documents.',
    metricRelevance: 'Answer Relevance',
    metricRelevanceDesc: 'Is the answer actually addressing the question asked? A faithful answer can still be irrelevant if it misses the point.',
    metricContextRecall: 'Context Recall',
    metricContextRecallDesc: 'Did the retrieval find all the information needed to answer? Measures if relevant passages were missed.',
    metricContextPrecision: 'Context Precision',
    metricContextPrecisionDesc: 'Are the retrieved passages actually relevant? High precision means less noise in the context, reducing confusion.',
    evaluationTips: 'Evaluation Best Practices',
    evaluationTip1: 'Create a diverse test set covering different query types and difficulty levels',
    evaluationTip2: 'Track metrics over time as you iterate on chunking, embeddings, and prompts',
    evaluationTip3: 'Combine automated metrics with human evaluation for nuanced quality assessment',


    keyTakeaways: 'Key Takeaways',
    takeaway1: 'RAG retrieves relevant documents and includes them in the prompt',
    takeaway2: 'It reduces hallucination by grounding responses in actual sources',
    takeaway3: 'Chunking strategy and embedding quality are critical for good retrieval',
    takeaway4: 'RAG is often preferable to fine-tuning for adding domain knowledge',
    takeaway5: 'Advanced techniques like Self-RAG and GraphRAG improve accuracy for complex queries',
    takeaway6: 'Use RAGAS metrics to systematically evaluate and improve your RAG pipeline',
  },

  // Tool Design page
  toolDesign: {
    title: 'Tool Design',
    description: 'Best practices for designing effective tools that AI agents can use reliably.',
    whatIs: 'What Makes a Good Tool?',
    whatIsDesc: 'Well-designed tools are the foundation of capable AI agents. A tool\'s schema, naming, and documentation directly impact how reliably an LLM can use it.',
    principles: 'Design Principles',
    principlesDesc: 'Follow these principles to create tools that agents can use effectively.',
    principle1: 'Clear Naming',
    principle1Desc: 'Use descriptive, unambiguous names. "search_web" is better than "sw" or "query".',
    principle2: 'Explicit Parameters',
    principle2Desc: 'Every parameter should have a clear type, description, and constraints.',
    principle3: 'Predictable Outputs',
    principle3Desc: 'Return consistent, structured responses. Include error messages in the output.',
    principle4: 'Minimal Scope',
    principle4Desc: 'Each tool should do one thing well. Prefer many focused tools over few complex ones.',
    schemaDesign: 'Schema Design',
    schemaDesignDesc: 'Tool schemas tell the LLM how to use your tools. Good schemas prevent errors.',
    goodSchema: 'Good Schema',
    badSchema: 'Bad Schema',
    errorHandling: 'Error Handling',
    errorHandlingDesc: 'Tools should handle errors gracefully and return informative messages the LLM can act on.',
    interactiveDemo: 'Tool Schema Builder',
    demoDesc: 'Build and validate tool schemas interactively',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tool design directly impacts agent reliability',
    takeaway2: 'Explicit schemas with descriptions prevent LLM confusion',
    takeaway3: 'Return structured errors the agent can understand and act on',
    takeaway4: 'Test tools with various inputs to find edge cases',
  },

  // Memory Systems page
  memorySystems: {
    title: 'Memory Systems',
    description: 'How AI agents maintain context and remember information across interactions.',
    whatIs: 'What are Agent Memory Systems?',
    whatIsDesc: 'Memory systems allow agents to retain and recall information beyond the immediate context window. They enable agents to learn from past interactions and maintain coherent long-term behavior.',
    types: 'Types of Memory',
    typesDesc: 'Agent memory systems typically combine multiple memory types for different purposes.',
    shortTerm: 'Short-Term Memory',
    shortTermDesc: 'The current conversation context. Limited by context window size.',
    longTerm: 'Long-Term Memory',
    longTermDesc: 'Persistent storage of past interactions, facts, and learned preferences.',
    episodic: 'Episodic Memory',
    episodicDesc: 'Specific past events and interactions that can be recalled.',
    semantic: 'Semantic Memory',
    semanticDesc: 'General knowledge and facts extracted from experiences.',
    implementation: 'Implementation Approaches',
    implementationDesc: 'Various techniques for implementing agent memory.',
    vectorStore: 'Vector Stores',
    vectorStoreDesc: 'Store embeddings of past interactions for semantic retrieval.',
    summaries: 'Conversation Summaries',
    summariesDesc: 'Periodically summarize long conversations to preserve key information.',
    keyValue: 'Key-Value Stores',
    keyValueDesc: 'Store explicit facts and user preferences for direct lookup.',
    interactiveDemo: 'Memory System Visualizer',
    demoDesc: 'See how different memory types work together',

    // Hybrid Memory (2025 Pattern)
    hybridMemory: 'Hybrid Memory Patterns',
    hybridMemoryDesc: 'Modern agents combine episodic and semantic memory for human-like recall. The MemGPT pattern and similar architectures treat memory as a first-class resource the agent actively manages.',
    memgptPattern: 'MemGPT Architecture',
    memgptPatternDesc: 'Agents with explicit memory management—moving data between fast context and slow storage like an OS manages RAM and disk.',
    tieredMemory: 'Tiered Memory',
    tieredMemoryDesc: 'Hot (context), warm (vector cache), and cold (archive) tiers with automatic promotion and demotion based on access patterns.',
    selfEditing: 'Self-Editing Memory',
    selfEditingDesc: 'Agents that can update, consolidate, and restructure their own memories rather than append-only storage.',
    dualEncoder: 'Dual Encoder Retrieval',
    dualEncoderDesc: 'Separate encoders for queries and memories enable asymmetric retrieval optimized for each direction.',

    // Temporal Knowledge Graphs
    temporalGraphs: 'Temporal Knowledge Graphs',
    temporalGraphsDesc: 'Store memories with explicit time relationships, enabling queries like "what did we discuss last week?" and detecting knowledge drift over time.',
    entityRelations: 'Entity-Relationship Tracking',
    entityRelationsDesc: 'Extract entities (people, projects, concepts) and their relationships from conversations, building a queryable knowledge graph.',
    timeWeighted: 'Time-Weighted Retrieval',
    timeWeightedDesc: 'Combine semantic similarity with recency, importance, and access frequency for more relevant recall.',
    zepApproach: 'ZEP-Style Memory',
    zepApproachDesc: 'Automatic extraction of facts, entities, and temporal relations with bi-temporal modeling (when something happened vs. when it was recorded).',

    // Memory Management
    memoryManagement: 'Memory Management',
    memoryManagementDesc: 'Production memory systems require active management to stay within token budgets while preserving valuable information.',
    deduplication: 'Deduplication',
    deduplicationDesc: 'Detect and merge semantically similar memories to prevent bloat. Use embedding similarity thresholds or LLM-based comparison.',
    tokenBudgets: 'Token Budgets',
    tokenBudgetsDesc: 'Allocate fixed token counts to different memory types. When budget is exceeded, compress or evict lowest-priority items.',
    garbageCollection: 'Garbage Collection',
    garbageCollectionDesc: 'Periodically scan memories for stale, redundant, or low-value entries. LRU, LFU, or importance-weighted eviction strategies.',
    priorityRules: 'Priority Rules',
    priorityRulesDesc: 'Define what memories matter most: user preferences, task context, recent interactions, or explicitly pinned facts.',

    // Adaptive Retention
    adaptiveRetention: 'Adaptive Retention',
    adaptiveRetentionDesc: 'Smart strategies for what to keep, summarize, or forget—mimicking how human memory naturally decays and consolidates.',
    contextSummarization: 'Context Summarization',
    contextSummarizationDesc: 'Progressive summarization: full detail for recent context, summaries for older conversations, key facts only for distant past.',
    entityExtraction: 'Entity Extraction',
    entityExtractionDesc: 'Automatically identify and store important entities (names, preferences, decisions) separately from raw conversation logs.',
    decayStrategies: 'Decay Strategies',
    decayStrategiesDesc: 'Exponential or logarithmic decay functions reduce memory importance over time unless reinforced by access or explicit importance markers.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Memory extends agent capabilities beyond the context window',
    takeaway2: 'Combine multiple memory types for best results',
    takeaway3: 'Memory retrieval adds latency—balance comprehensiveness with speed',
    takeaway4: 'Consider privacy and data retention when storing memories',
    takeaway5: 'Hybrid patterns like MemGPT enable agents to manage their own memory like an operating system',
    takeaway6: 'Temporal knowledge graphs add time-awareness for more contextual recall',
  },

  // Orchestration page
  orchestration: {
    title: 'Orchestration',
    description: 'Coordinating multiple agents and complex multi-step workflows.',
    whatIs: 'What is Agent Orchestration?',
    whatIsDesc: 'Orchestration is the coordination of multiple AI agents or complex multi-step workflows. It involves routing tasks, managing state, handling failures, and combining agent outputs.',
    patterns: 'Orchestration Patterns',
    patternsDesc: 'Common patterns for structuring multi-agent systems.',
    sequential: 'Sequential Pipeline',
    sequentialDesc: 'Agents run in order, each processing the output of the previous one.',
    parallel: 'Parallel Execution',
    parallelDesc: 'Multiple agents work simultaneously on different aspects of a task.',
    hierarchical: 'Hierarchical',
    hierarchicalDesc: 'A supervisor agent delegates to specialized worker agents.',
    dynamic: 'Dynamic Routing',
    dynamicDesc: 'An LLM decides which agent should handle each request.',
    stateManagement: 'State Management',
    stateManagementDesc: 'Orchestrators must track progress, intermediate results, and handle failures.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Save state at key points to enable recovery from failures.',
    rollback: 'Rollback',
    rollbackDesc: 'Ability to undo steps when errors occur.',
    interactiveDemo: 'Workflow Visualizer',
    demoDesc: 'Design and visualize agent workflows',
    // Visualizer UI
    vizRun: 'Run',
    vizPause: 'Pause',
    vizReset: 'Reset',
    vizComplete: 'Complete',
    vizProgress: 'Progress',
    vizStepOf: 'of',
    vizPending: 'idle',
    vizRunning: 'active',
    vizCompleted: 'done',
    vizSequentialLabel: 'Sequential',
    vizSequentialSummary: 'One step at a time',
    vizSequentialDetail: 'Each node depends on the previous result. This maximizes control and traceability at the cost of speed.',
    vizParallelLabel: 'Parallel',
    vizParallelSummary: 'Fan-out, then merge',
    vizParallelDetail: 'A coordinator distributes independent tasks, then an aggregator combines the results.',
    vizHierarchicalLabel: 'Hierarchical',
    vizHierarchicalSummary: 'Delegate by layers',
    vizHierarchicalDetail: 'A supervisor routes work to sub-agents, which orchestrate specialist tools beneath them.',
    vizHandoffLabel: 'Handoff',
    vizHandoffSummary: 'Pass the baton',
    vizHandoffDetail: 'Agents transfer control to the next specialist when they finish their part, preserving context across the chain.',
    vizNodePlanner: 'Planner',
    vizNodeSearch: 'Search',
    vizNodeAnalyzer: 'Analyzer',
    vizNodeSummarize: 'Summarize',
    vizNodeCoordinator: 'Coordinator',
    vizNodeTaskA: 'Task A',
    vizNodeTaskB: 'Task B',
    vizNodeTaskC: 'Task C',
    vizNodeAggregator: 'Aggregator',
    vizNodeSupervisor: 'Supervisor',
    vizNodeWorkerA: 'Worker A',
    vizNodeWorkerB: 'Worker B',
    vizNodeTool1: 'Tool 1',
    vizNodeTool2: 'Tool 2',
    vizNodeTool3: 'Tool 3',
    vizNodeTool4: 'Tool 4',
    vizNodeTriage: 'Triage',
    vizNodeBilling: 'Billing',
    vizNodeSupport: 'Support',
    vizNodeResolve: 'Resolve',
    vizAgent: 'Agent',
    vizTool: 'Tool',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Orchestration enables complex tasks through agent composition',
    takeaway2: 'Choose patterns based on task dependencies and parallelism',
    takeaway3: 'Robust state management is essential for reliability',
    takeaway4: 'Monitor orchestration costs—multi-agent systems multiply API calls',
    // 2025 Multi-Agent Patterns
    multiAgentPatterns: '2025 Multi-Agent Patterns',
    multiAgentPatternsDesc: 'These emerging patterns define how modern AI agents collaborate at enterprise scale.',
    enterpriseAdoption: '72% of enterprise AI projects now use multi-agent systems',
    enterpriseAdoptionSource: 'Gartner 2025',
    supervisorPattern: 'Supervisor Pattern',
    supervisorPatternDesc: 'A coordinator agent that manages and delegates tasks to specialized worker agents. The supervisor maintains the overall goal, breaks down complex tasks, and synthesizes worker outputs.',
    supervisorBenefits: 'Benefits: Clear hierarchy, centralized decision-making, easier debugging',
    orchestratorWorker: 'Orchestrator-Worker Pattern',
    orchestratorWorkerDesc: 'A central orchestrator maintains a pool of worker agents. Workers are stateless and can be dynamically scaled. The orchestrator handles task queuing, load balancing, and result aggregation.',
    orchestratorWorkerBenefits: 'Benefits: Scalability, fault tolerance, resource efficiency',
    handoffMechanisms: 'Handoff Mechanisms',
    handoffMechanismsDesc: 'How agents transfer control to each other. Popularized by OpenAI Agents SDK, handoffs enable seamless transitions between specialized agents while maintaining conversation context.',
    handoffExplicit: 'Explicit Handoff',
    handoffExplicitDesc: 'Agent directly calls transfer function with target agent and context.',
    handoffCondition: 'Condition-Based',
    handoffConditionDesc: 'Automatic transfer when certain conditions are met (e.g., topic detection).',
    handoffEscalation: 'Escalation',
    handoffEscalationDesc: 'Agent transfers to more capable agent when task exceeds its scope.',
    groupChatPattern: 'Group Chat Pattern',
    groupChatPatternDesc: 'Multiple agents collaborate on a shared problem through a structured conversation. Each agent contributes their expertise, with a moderator managing turn-taking and consensus.',
    groupChatRoles: 'Common Roles',
    groupChatModerator: 'Moderator: Controls flow, summarizes progress, resolves conflicts',
    groupChatExpert: 'Experts: Domain-specific agents that contribute specialized knowledge',
    groupChatCritic: 'Critic: Reviews and challenges proposals to improve quality',
    groupChatExecutor: 'Executor: Implements the agreed-upon decisions',
    patternComparison: 'Pattern Comparison',
    patternComparisonDesc: 'Choosing the right pattern depends on your use case.',
    comparisonComplexity: 'Complexity',
    comparisonScalability: 'Scalability',
    comparisonUseCase: 'Best For',
    supervisorComplexity: 'Medium',
    supervisorScalability: 'Medium',
    supervisorUseCase: 'Structured workflows, clear task decomposition',
    orchestratorComplexity: 'High',
    orchestratorScalability: 'High',
    orchestratorUseCase: 'High-volume processing, dynamic workloads',
    handoffComplexity: 'Low',
    handoffScalability: 'Low',
    handoffUseCase: 'Customer service, specialized routing',
    groupChatComplexity: 'High',
    groupChatScalability: 'Medium',
    groupChatUseCase: 'Creative tasks, complex problem-solving',
  },

  // Evaluation page
  evaluation: {
    title: 'Evaluation',
    description: 'Measuring and improving AI agent performance systematically.',
    whatIs: 'Why Evaluate Agents?',
    whatIsDesc: 'Agent evaluation is critical for understanding performance, catching regressions, and improving reliability. Without measurement, you\'re flying blind.',
    metrics: 'Key Metrics',
    metricsDesc: 'Important metrics to track for agent systems.',
    taskSuccess: 'Task Success Rate',
    taskSuccessDesc: 'Percentage of tasks completed correctly.',
    efficiency: 'Efficiency',
    efficiencyDesc: 'Steps taken, tokens used, time elapsed per task.',
    accuracy: 'Accuracy',
    accuracyDesc: 'Correctness of agent outputs and decisions.',
    reliability: 'Reliability',
    reliabilityDesc: 'Consistency across repeated runs of the same task.',
    approaches: 'Evaluation Approaches',
    approachesDesc: 'Different ways to evaluate agent performance.',
    unitTests: 'Unit Tests',
    unitTestsDesc: 'Test individual tools and components in isolation.',
    integration: 'Integration Tests',
    integrationDesc: 'Test the full agent loop with mock environments.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard task suites for comparing agents.',
    humanEval: 'Human Evaluation',
    humanEvalDesc: 'Expert review for nuanced quality assessment.',
    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Guidelines for effective agent evaluation.',
    practice1: 'Test edge cases and failure modes, not just happy paths.',
    practice2: 'Track costs alongside quality metrics.',
    practice3: 'Use versioned evaluations to catch regressions.',
    practice4: 'Include adversarial tests for security.',

    // Benchmarks Section
    benchmarksSection: 'Common LLM Benchmarks',
    benchmarksSectionDesc: 'Standard benchmarks used to evaluate and compare language model capabilities across different tasks.',
    benchmarkMmlu: 'MMLU',
    benchmarkMmluDesc: 'Massive Multitask Language Understanding - 57 subjects from STEM to humanities. Tests broad knowledge.',
    benchmarkHellaswag: 'HellaSwag',
    benchmarkHellaswagDesc: 'Commonsense reasoning about everyday situations. Tests understanding of physical world.',
    benchmarkHumaneval: 'HumanEval',
    benchmarkHumanevalDesc: 'Code generation benchmark with 164 programming problems. Tests coding ability.',
    benchmarkGsm8k: 'GSM8K',
    benchmarkGsm8kDesc: 'Grade school math word problems. Tests multi-step mathematical reasoning.',
    benchmarkArc: 'ARC',
    benchmarkArcDesc: 'AI2 Reasoning Challenge - science questions requiring reasoning beyond pattern matching.',
    benchmarkMath: 'MATH',
    benchmarkMathDesc: 'Competition-level mathematics problems. Tests advanced mathematical reasoning.',
    benchmarkCaveats: 'Benchmark Caveats',
    benchmarkCaveat1: 'Benchmarks can be gamed - models may be trained on test data',
    benchmarkCaveat2: 'High scores don\'t guarantee real-world performance',
    benchmarkCaveat3: 'Many benchmarks are saturated - top models score similarly',
    benchmarkCaveat4: 'Benchmarks often miss important capabilities like following instructions',

    // LLM as a Judge
    llmJudge: 'LLM-as-a-Judge',
    llmJudgeDesc: 'Using language models to evaluate other model outputs - a scalable but imperfect approach.',
    llmJudgeWhat: 'How It Works',
    llmJudgeWhatDesc: 'A capable LLM (the "judge") is prompted to evaluate outputs from another model. The judge scores responses on criteria like helpfulness, accuracy, and safety.',
    llmJudgeAdvantages: 'Advantages',
    llmJudgeAdv1: 'Scalable',
    llmJudgeAdv1Desc: 'Can evaluate thousands of outputs quickly without human annotators.',
    llmJudgeAdv2: 'Consistent',
    llmJudgeAdv2Desc: 'Same criteria applied uniformly (unlike human fatigue/variation).',
    llmJudgeAdv3: 'Cost-effective',
    llmJudgeAdv3Desc: 'Much cheaper than hiring human evaluators at scale.',
    llmJudgeAdv4: 'Flexible',
    llmJudgeAdv4Desc: 'Easy to adjust evaluation criteria by changing the prompt.',
    llmJudgeProblems: 'Problems & Biases',
    llmJudgeProb1: 'Self-preference Bias',
    llmJudgeProb1Desc: 'Models tend to prefer outputs similar to what they would generate.',
    llmJudgeProb2: 'Position Bias',
    llmJudgeProb2Desc: 'Judges may favor the first or last option regardless of quality.',
    llmJudgeProb3: 'Verbosity Bias',
    llmJudgeProb3Desc: 'Longer responses often rated higher even when less accurate.',
    llmJudgeProb4: 'Style Over Substance',
    llmJudgeProb4Desc: 'Well-formatted wrong answers may beat poorly-formatted correct ones.',
    llmJudgeProb5: 'Capability Ceiling',
    llmJudgeProb5Desc: 'Judge can\'t reliably evaluate outputs beyond its own capability level.',
    llmJudgeBestPractices: 'Best Practices for LLM Judges',
    llmJudgePractice1: 'Use the most capable model available as the judge',
    llmJudgePractice2: 'Randomize option order to mitigate position bias',
    llmJudgePractice3: 'Request reasoning before scores (chain-of-thought)',
    llmJudgePractice4: 'Validate against human judgments on a subset',
    llmJudgePractice5: 'Use multiple judges and aggregate scores',

    // CLASSIC Framework
    classicFramework: 'CLASSIC Framework',
    classicFrameworkDesc: 'A comprehensive enterprise evaluation framework for AI agents covering seven critical dimensions.',
    classicCost: 'Cost',
    classicCostDesc: 'Total cost of ownership including API calls, compute, infrastructure, and maintenance. Track cost per task and cost per successful outcome.',
    classicLatency: 'Latency',
    classicLatencyDesc: 'Time to first token, end-to-end response time, and task completion time. Critical for user experience and real-time applications.',
    classicAccuracy: 'Accuracy',
    classicAccuracyDesc: 'Correctness of outputs measured against ground truth. Includes factual accuracy, logical consistency, and task-specific precision.',
    classicStability: 'Stability',
    classicStabilityDesc: 'Consistency of outputs across identical inputs. Low variance indicates reliable behavior; high variance suggests unpredictable performance.',
    classicSecurity: 'Security',
    classicSecurityDesc: 'Resistance to prompt injection, jailbreaks, and data leakage. Includes input validation, output filtering, and access control.',
    classicInterpretability: 'Interpretability',
    classicInterpretabilityDesc: 'Ability to explain decisions and reasoning. Supports debugging, compliance audits, and user trust through transparent operation.',
    classicCompliance: 'Compliance',
    classicComplianceDesc: 'Adherence to regulatory requirements (GDPR, HIPAA, SOC2), industry standards, and organizational policies.',
    classicNote: 'Enterprise-grade evaluation should track all seven dimensions. Optimize for your specific use case priorities.',

    // Agent-Specific Benchmarks
    agentBenchmarks: 'Agent-Specific Benchmarks',
    agentBenchmarksDesc: 'Modern benchmarks designed specifically to evaluate AI agents on complex, multi-step tasks in realistic environments.',
    benchmarkAgentBench: 'AgentBench',
    benchmarkAgentBenchDesc: 'Evaluates LLMs as agents across 8 environments: OS, database, knowledge graph, web browsing, and more. Tests real-world tool use.',
    benchmarkGaia: 'GAIA',
    benchmarkGaiaDesc: 'General AI Assistants benchmark with 466 questions requiring multi-step reasoning, web browsing, and tool use. Human-verified answers.',
    benchmarkBfcl: 'Berkeley Function-Calling Leaderboard',
    benchmarkBfclDesc: 'Tests function calling accuracy across simple, parallel, and nested calls. Includes real-world API scenarios and edge cases.',
    benchmarkSwe: 'SWE-bench',
    benchmarkSweDesc: 'Real GitHub issues from popular Python repos. Agents must understand context, write code, and pass existing tests.',
    benchmarkWebArena: 'WebArena',
    benchmarkWebArenaDesc: 'Tests agents on realistic web tasks across e-commerce, forums, and content management sites with complex multi-page workflows.',
    benchmarkTau: 'TAU-bench',
    benchmarkTauDesc: 'Tool-Agent-User benchmark testing agents on real customer service scenarios with tools, policies, and user interactions.',

    // Interactive Evaluation
    interactiveEval: 'Interactive Evaluation',
    interactiveEvalDesc: 'Dynamic evaluation approaches that test agent behavior in changing environments and adversarial conditions.',
    interactiveWhat: 'Beyond Static Benchmarks',
    interactiveWhatDesc: 'Static benchmarks have fixed questions and answers. Interactive evaluation tests how agents adapt to dynamic environments, handle unexpected situations, and maintain performance under changing conditions.',
    interactiveApproach1: 'Environment Perturbation',
    interactiveApproach1Desc: 'Change the environment during task execution—modify files, alter API responses, introduce errors—to test agent robustness and recovery.',
    interactiveApproach2: 'Adversarial User Simulation',
    interactiveApproach2Desc: 'Simulate users who give ambiguous instructions, change their minds, or try to manipulate the agent. Tests real-world resilience.',
    interactiveApproach3: 'Multi-Turn Consistency',
    interactiveApproach3Desc: 'Evaluate coherence across long conversations with context shifts. Check if the agent maintains accurate state and follows instructions over time.',
    interactiveApproach4: 'Curriculum Difficulty',
    interactiveApproach4Desc: 'Start with easy tasks and progressively increase complexity. Identifies capability boundaries and graceful degradation patterns.',
    interactiveNote: 'Interactive evaluation better predicts real-world performance than static benchmarks alone.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Evaluation is essential—unmeasured systems can\'t be improved',
    takeaway2: 'Combine automated tests with human evaluation',
    takeaway3: 'Track multiple metrics: success, efficiency, cost',
    takeaway4: 'Build evaluation into your development workflow',
    takeaway5: 'LLM-as-a-judge is useful but has significant biases to account for',
    takeaway6: 'Use the CLASSIC framework for comprehensive enterprise evaluation',
    takeaway7: 'Agent-specific benchmarks like AgentBench and GAIA test real-world capabilities',
  },

  // Agent Skills page
  agentSkills: {
    title: 'Agent Skills',
    description: 'Reusable instruction packages that give agents specialized capabilities on demand.',
    whatIs: 'What are Agent Skills?',
    whatIsDesc: 'are folders of instructions, prompts, examples, and resources that an LLM can load when relevant to perform specialized tasks consistently. Instead of putting everything in the system prompt, skills let you modularize expertise.',
    metaphor: '"Skills are like apps for your agent—install them once, use them whenever needed."',
    metaphorDesc: 'Just as you install apps on your phone for specific functions, skills give agents specialized capabilities without bloating every conversation.',

    // How it Works
    howItWorks: 'How Agent Skills Work',
    step1Title: 'Skill Discovery',
    step1Desc: 'When a user request comes in, the agent checks if any available skills match the task based on triggers, keywords, or explicit invocation.',
    step2Title: 'Skill Loading',
    step2Desc: 'The relevant skill\'s instructions, examples, and context are loaded into the agent\'s working memory. This adds specialized knowledge without polluting the base system prompt.',
    step3Title: 'Skill Execution',
    step3Desc: 'The agent follows the skill\'s instructions to complete the task, using any provided templates, checklists, or scripts. Results are returned to the user.',

    // Interactive Demo
    interactiveDemo: 'Interactive Skill Demo',
    interactiveDemoDesc: 'Explore how skills are triggered, their manifest structure, and skill chaining',

    // Structure
    structureTitle: 'Skill Structure',
    structureSubtitle: 'Anatomy of a skill folder',
    skillMdDesc: 'Metadata and main instructions',
    instructionsDesc: 'Detailed guidance for the task',
    examplesDesc: 'Sample inputs and outputs',
    templatesDesc: 'Reusable output formats',
    scriptsDesc: 'Helper scripts if needed',
    skillMdNote: 'is the entry point. It contains metadata (name, triggers, description) and the core instructions the agent follows.',

    // Types of Skills
    typesTitle: 'Types of Agent Skills',
    skillType1Title: 'Domain Skills',
    skillType1Desc: 'Specialized knowledge for specific fields—legal contracts, medical terminology, financial analysis. Turn a general agent into a domain expert.',
    skillType2Title: 'Workflow Skills',
    skillType2Desc: 'Multi-step processes with defined stages—code review workflows, content publishing pipelines, incident response procedures.',
    skillType3Title: 'Format Skills',
    skillType3Desc: 'Consistent output formatting—API documentation, changelog entries, meeting summaries. Ensure outputs match your standards.',
    skillType4Title: 'Integration Skills',
    skillType4Desc: 'Instructions for working with specific tools or services—GitHub workflows, Jira ticket creation, Slack notifications.',

    // Skills vs Tools
    vsToolsTitle: 'Skills vs. Tools',
    tools: 'Tools',
    tool1: 'Execute actions (read files, call APIs, run code)',
    tool2: 'Defined by function signatures and schemas',
    tool3: 'The "hands" of the agent',
    skills: 'Skills',
    skill1: 'Provide knowledge and methodology (how to approach tasks)',
    skill2: 'Defined by instructions and examples',
    skill3: 'The "expertise" of the agent',
    vsNote: 'Skills and tools work together: a code review skill tells the agent what to look for, while tools let it read the code and leave comments.',

    // Benefits
    benefitsTitle: 'Benefits of Skills',
    benefit1Title: 'Specialization Without Bloat',
    benefit1Desc: 'Keep the base system prompt lean. Load specialized knowledge only when needed, preserving context window for the actual task.',
    benefit2Title: 'Consistency',
    benefit2Desc: 'Define a process once, apply it consistently every time. No more variations in how tasks are approached.',
    benefit3Title: 'Shareability',
    benefit3Desc: 'Skills are just folders—share them across projects, teams, or publicly. Build once, use everywhere.',
    benefit4Title: 'Iteration',
    benefit4Desc: 'Improve skills independently of the agent. Update the code review skill without touching the rest of your agent setup.',

    // Example
    exampleTitle: 'Example: Code Review Skill',
    exampleDesc: 'Performs thorough code reviews following team standards',
    exampleInstructions: 'When reviewing code, analyze for correctness, performance, security, and maintainability.',
    exampleCheck1: 'Check for common security vulnerabilities',
    exampleCheck2: 'Verify error handling is comprehensive',
    exampleCheck3: 'Look for performance anti-patterns',
    exampleCheck4: 'Ensure code follows style guidelines',

    // Best Practices
    practicesTitle: 'Best Practices',
    practice1Title: 'Keep Skills Focused',
    practice1Desc: 'One skill, one purpose. If a skill is doing too many things, split it up. Focused skills are easier to maintain and compose.',
    practice2Title: 'Include Examples',
    practice2Desc: 'Show, don\'t just tell. Include input/output examples that demonstrate exactly what good execution looks like.',
    practice3Title: 'Version Your Skills',
    practice3Desc: 'Track changes to skills over time. When behavior changes unexpectedly, you can trace it back to a skill update.',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Skills are reusable instruction packages that give agents specialized expertise on demand',
    takeaway2: 'Unlike tools (which execute actions), skills provide knowledge and methodology',
    takeaway3: 'Well-designed skills improve consistency and reduce system prompt bloat',
    takeaway4: 'The skill framework enables modular agent development—build once, share everywhere',
  },

  // Neural Networks page
  neuralNetworks: {
    title: 'Neural Networks',
    description: 'The foundational architecture that powers modern AI.',
    whatIs: 'What is a Neural Network?',
    whatIsDesc: 'A neural network is a computational model inspired by the brain. It consists of layers of interconnected nodes (neurons) that learn to transform inputs into outputs through training.',
    components: 'Core Components',
    componentsDesc: 'The building blocks of neural networks.',
    neurons: 'Neurons',
    neuronsDesc: 'Basic units that compute weighted sums of inputs and apply activation functions.',
    layers: 'Layers',
    layersDesc: 'Groups of neurons: input layer, hidden layers, and output layer.',
    weights: 'Weights & Biases',
    weightsDesc: 'Learnable parameters that determine how inputs are transformed.',
    activations: 'Activation Functions',
    activationsDesc: 'Non-linear functions that allow networks to learn complex patterns.',
    typesOfNetworks: 'Types of Networks',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Information flows one direction. Good for tabular data.',
    cnn: 'Convolutional (CNN)',
    cnnDesc: 'Specialized for images and spatial data.',
    rnn: 'Recurrent (RNN)',
    rnnDesc: 'Processes sequences with memory of past inputs.',
    transformer: 'Transformer',
    transformerDesc: 'Attention-based architecture powering modern LLMs.',
    interactiveDemo: 'Neural Network Visualizer',
    demoDesc: 'Build and explore network architectures',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Neural networks learn by adjusting weights through training',
    takeaway2: 'Depth (more layers) enables learning hierarchical features',
    takeaway3: 'Different architectures suit different data types',
    takeaway4: 'Modern LLMs are massive transformer networks',
  },

  // Gradient Descent page
  gradientDescent: {
    title: 'Gradient Descent',
    description: 'The optimization algorithm that enables neural networks to learn.',
    whatIs: 'What is Gradient Descent?',
    whatIsDesc: 'Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize a loss function. It\'s how neural networks learn from data.',
    intuition: 'The Intuition',
    intuitionDesc: 'Imagine standing in a hilly landscape blindfolded, trying to reach the lowest point. You feel the slope beneath your feet and step downhill. Repeat until you reach a valley.',
    howWorks: 'How It Works',
    howWorksDesc: 'The algorithm computes how much each parameter contributes to the error, then adjusts parameters in the opposite direction.',
    step1: 'Compute Loss',
    step1Desc: 'Measure how wrong the current predictions are.',
    step2: 'Calculate Gradients',
    step2Desc: 'Use backpropagation to find how each weight affects the loss.',
    step3: 'Update Weights',
    step3Desc: 'Adjust weights in the direction that reduces loss.',
    step4: 'Repeat',
    step4Desc: 'Iterate until the loss stops decreasing.',
    learningRate: 'Learning Rate',
    learningRateDesc: 'Controls how big each step is. Too high: overshoot. Too low: slow progress.',
    variants: 'Variants',
    sgd: 'Stochastic Gradient Descent',
    sgdDesc: 'Uses random mini-batches instead of the full dataset.',
    momentum: 'Momentum',
    momentumDesc: 'Accumulates velocity to push through local minima.',
    adam: 'Adam',
    adamDesc: 'Adaptive learning rates per parameter. Combines momentum with RMSprop.',
    adamw: 'AdamW',
    adamwDesc: 'Adam with decoupled weight decay. Now preferred over standard Adam for most applications, especially in training large language models.',

    // Learning Rate Scheduling section
    lrScheduling: 'Learning Rate Scheduling',
    lrSchedulingDesc: 'Instead of using a fixed learning rate, schedules adjust it during training for better convergence.',
    stepDecay: 'Step Decay',
    stepDecayDesc: 'Reduce learning rate by a factor at specific epochs (e.g., halve every 30 epochs).',
    exponentialDecay: 'Exponential Decay',
    exponentialDecayDesc: 'Continuously decrease learning rate: lr = lr_0 * e^(-kt). Smooth but can decay too fast.',
    cosineAnnealing: 'Cosine Annealing',
    cosineAnnealingDesc: 'Follows a cosine curve from initial to minimum LR. Popular in modern training, allows gentle cooldown.',
    warmup: 'Warmup',
    warmupDesc: 'Start with very low LR, gradually increase to target, then decay. Stabilizes early training, essential for transformers.',

    // Convergence Challenges section
    convergenceChallenges: 'Convergence Challenges',
    convergenceChallengesDesc: 'Understanding obstacles that can prevent gradient descent from finding the global optimum.',
    localMinima: 'Local Minima',
    localMinimaDesc: 'Points where the loss is lower than nearby areas but not the global minimum. Momentum and adaptive methods help escape.',
    saddlePoints: 'Saddle Points',
    saddlePointsDesc: 'Points where the gradient is zero but it\'s neither a minimum nor maximum. Common in high dimensions, slowing convergence.',
    plateaus: 'Plateaus',
    plateausDesc: 'Flat regions where gradients are very small. Progress stalls until the optimizer escapes. Adaptive LR helps navigate these.',

    interactiveDemo: 'Gradient Descent Visualizer',
    demoDesc: 'Watch gradient descent find the minimum',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Gradient descent minimizes loss by following the slope',
    takeaway2: 'Learning rate is the most important hyperparameter',
    takeaway3: 'AdamW is now the preferred optimizer for most deep learning applications',
    takeaway4: 'Backpropagation computes gradients efficiently',
  },

  // Training Process page
  training: {
    title: 'Training Process',
    description: 'How neural networks learn from data through iterative optimization.',
    whatIs: 'What is Training?',
    whatIsDesc: 'Training is the process of teaching a neural network to perform a task by exposing it to examples and adjusting its parameters based on errors.',
    phases: 'Training Phases',
    phasesDesc: 'The stages of training a neural network.',
    initialization: 'Initialization',
    initializationDesc: 'Set random starting weights. Good initialization helps training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Input flows through the network to produce predictions.',
    lossCalc: 'Loss Calculation',
    lossCalcDesc: 'Compare predictions to ground truth with a loss function.',
    backprop: 'Backpropagation',
    backpropDesc: 'Compute gradients of loss with respect to each weight.',
    optimization: 'Optimization',
    optimizationDesc: 'Update weights using gradient descent.',
    concepts: 'Key Concepts',
    epoch: 'Epoch',
    epochDesc: 'One complete pass through the entire training dataset.',
    batch: 'Batch Size',
    batchDesc: 'Number of examples processed before updating weights.',
    overfitting: 'Overfitting',
    overfittingDesc: 'Model memorizes training data but fails on new data.',
    regularization: 'Regularization',
    regularizationDesc: 'Techniques to prevent overfitting (dropout, weight decay).',
    interactiveDemo: 'Training Progress Visualizer',
    demoDesc: 'Watch a network learn in real-time',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Training iteratively reduces prediction errors',
    takeaway2: 'Overfitting is the main enemy—always validate on held-out data',
    takeaway3: 'Batch size and learning rate significantly affect training',
    takeaway4: 'Modern LLMs require massive compute for training',
  },

  worldModels: {
    title: 'World Models',
    description: 'AI systems that learn an internal representation of the physical world to predict, simulate, and reason about reality.',
    whatIs: 'What are World Models?',
    whatIsDesc: 'World Models are AI systems that learn an internal representation of the physical world to predict and simulate the future. They understand physics, object motion, and causal relationships — enabling robots, autonomous vehicles, and AI agents to "imagine" outcomes before acting.',
    whatIsDesc2: 'Instead of just learning pixel patterns, World Models develop a deeper understanding of how the world works — similar to how humans build mental models of reality. When you catch a ball, your brain predicts its trajectory without solving equations. World Models aim to give AI the same intuition.',
    keyInsight: 'Core Insight',
    keyInsightDesc: 'The next frontier of AI is not just understanding language — it\'s understanding the physical world. World Models bridge the gap between text-based AI and embodied intelligence that can interact with reality.',
    howTheyWork: 'How do World Models work?',
    howTheyWorkDesc: 'World Models combine various techniques to model physical reality. The core idea: compress sensory input into a compact latent space, learn dynamics in that space, then decode predictions back into observable outputs.',
    pipelineTitle: 'World Model Pipeline',
    pipelineDesc: 'Click each stage to explore how data flows through a World Model',
    latentSpace: 'Latent Space Representation',
    latentSpaceDesc: 'Compressing high-dimensional sensor data (e.g., video, LiDAR) into a compact latent space that captures the essential structure of a scene — position, velocity, object identity — without storing every pixel.',
    latentDimInstruction: 'Drag the sliders to see how each latent dimension independently controls a complex visual concept.',
    latentZ1: 'Time of Day',
    latentZ2: 'Road Curvature',
    latentZ3: 'Traffic Density',
    latentZ4: 'Weather',
    latentSceneLabel: 'scene',
    videoPrediction: 'Video Prediction',
    videoPredictionDesc: 'Predicting future frames based on past observations and planned actions. The model learns temporal dynamics: if the car turns left, what does the world look like 2 seconds later?',
    physicsAware: 'Physics-Aware Training',
    physicsAwareDesc: 'Training with physical constraints or physics simulators so the model learns realistic motion, collisions, gravity, and material interactions — not just visual plausibility.',
    diffusion: 'Diffusion-Based Approaches',
    diffusionDesc: 'Using diffusion models to generate consistent, physically plausible future predictions. These models iteratively refine noisy predictions into crisp, coherent future states.',
    whyNeeded: 'Why do we need World Models?',
    whyNeededIntro: 'Three fundamental limitations make World Models essential for the next generation of AI:',
    slowExpensive: 'Reality is slow & expensive',
    slowExpensiveDesc: 'Training robots in the real world is time-consuming, costly, and potentially dangerous. A single mistake can destroy $100k+ hardware or endanger people. You can\'t crash 10,000 cars to train a self-driving system.',
    parallelTraining: 'Massively parallel training',
    parallelTrainingDesc: 'World Models enable training thousands of virtual agents simultaneously, gathering millions of hours of experience in mere hours. What takes a robot 1 year in reality takes 1 hour in simulation.',
    physicsUnderstanding: 'LLMs don\'t understand physics',
    physicsUnderstandingDesc: 'Language models can talk about physics but don\'t truly understand spatial relationships, momentum, or gravity. They\'ve never "experienced" a ball falling. World Models learn physics through simulated experience.',
    simVsRealTitle: 'Simulation vs. Real World — A Direct Comparison',
    trainingLoopTitle: 'The Training Loop',
    trainingLoopDesc: 'Step through a complete training cycle to see how World Models learn from simulated experience',
    examples: 'Notable World Models',
    examplesIntro: 'Leading research labs and companies are building world models for different domains. Click any card to learn more.',
    nvidiaCosmos: 'NVIDIA Cosmos',
    nvidiaCosmosDesc: 'Open-source Physical AI platform generating synthetic training data for robotics and autonomous driving.',
    nvidiaCosmosDetail: 'Cosmos uses video foundation models to simulate complex driving scenarios with realistic physics. It\'s designed to generate unlimited synthetic data for training autonomous vehicles and robotic systems, dramatically reducing the need for expensive real-world data collection.',
    tagAutonomous: 'Autonomous Driving',
    googleGenie: 'Google Genie 3 / Project Genie',
    googleGenieDesc: 'General-purpose world model that generates diverse, explorable interactive worlds from text and image prompts in real time.',
    googleGenieDetail: 'Genie 3 (Jan 2026) is a major leap from Genie 2. Unlike static 3D snapshots, it generates the path ahead in real time as you move and interact. It simulates physics and dynamic interactions with breakthrough consistency — enabling simulation of any real-world scenario from robotics to historical settings. Available to Google AI Ultra subscribers via "Project Genie," an experimental prototype for creating, exploring, and remixing interactive worlds. Google DeepMind positions it as a key step toward AGI: a general-purpose world model that handles the diversity of the real world, not just specific game environments.',
    tag3DWorlds: '3D Worlds',
    genesis: 'Genesis',
    genesisDesc: 'Physics engine combined with generative AI. Runs simulations up to 430,000x faster than real-time.',
    genesisDetail: 'Genesis combines differentiable physics simulation with generative models to enable ultra-fast, physically accurate training environments for robotics. Its key innovation is speed: by running physics in a highly optimized manner, it enables millions of training episodes in hours rather than months.',
    tagPhysics: 'Physics Engine',
    uniSim: 'UniSim',
    uniSimDesc: 'Google Research\'s universal world simulator for any environment — from kitchens to highways.',
    uniSimDetail: 'UniSim learns from diverse video data to build a unified simulation framework that works across domains. Unlike specialized simulators, it can model kitchens, outdoor scenes, driving environments, and more — all from a single model trained on internet-scale video data.',
    tagUniversal: 'Universal Sim',
    gaia1: 'GAIA-1',
    gaia1Desc: 'Wayve\'s generative world model for autonomous driving, trained on London street data.',
    gaia1Detail: 'GAIA-1 is a 9-billion parameter model trained on real driving footage from London. It can generate novel driving scenarios, predict traffic behavior, and create rare edge cases that are difficult to encounter in real-world testing. Wayve uses it to supplement real-world driving data.',
    tagDriving: 'Self-Driving',
    useCases: 'Use Cases',
    autonomousDriving: 'Autonomous Driving',
    autonomousDrivingDesc: 'Simulating millions of traffic scenarios, testing rare edge cases, and training vehicle policies — all without risking a single real car.',
    autonomousDrivingStat: 'Largest application area today',
    robotics: 'Robotics Training',
    roboticsDesc: 'Teaching manipulation, locomotion, and navigation in simulation before transferring policies to physical robots via sim-to-real transfer.',
    roboticsStat: 'Fastest-growing segment',
    videoGeneration: 'Video Generation',
    videoGenerationDesc: 'Generating photorealistic videos with consistent physics — a powerful "byproduct" of understanding world dynamics.',
    videoGenerationStat: 'Emerging commercial use',
    challenges: 'Challenges',
    challengesDesc: 'Despite the enormous potential, significant hurdles remain:',
    computeIntensive: 'Extremely resource-intensive',
    computeIntensiveDesc: 'Training requires enormous GPU clusters, massive video datasets, and weeks of compute time. Only well-funded labs can afford state-of-the-art world models.',
    simToReal: 'Sim-to-Real Gap',
    simToRealDesc: 'What works in simulation often fails in reality. Differences in physics accuracy, sensor noise, and environmental conditions make transfer challenging.',
    generalization: 'Generalization',
    generalizationDesc: 'World Models can overfit to training domains. A model trained on driving data may not generalize to indoor robotics. Robust cross-domain generalization is an open problem.',
    severityHigh: 'High Impact',
    severityMedium: 'Active Research',
    severityOpen: 'Open Problem',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'World Models learn internal representations of the physical world — enabling AI to "imagine" and predict outcomes before acting',
    takeaway2: 'They enable massively parallelized training: millions of hours of experience in simulation vs. slow real-time interaction',
    takeaway3: 'The architecture follows a pipeline: Observe → Encode → Predict → Decode → Act',
    takeaway4: 'Major players include NVIDIA Cosmos, Google Genie 2, Wayve GAIA-1, and Genesis — each tackling different domains',
    takeaway5: 'The Sim-to-Real Gap remains the central challenge: bridging the difference between simulated and real-world physics',
  },

  // Prompt Basics page
  promptBasics: {
    title: 'Prompt Basics',
    description: 'Fundamentals of writing effective prompts for AI models.',
    whatIs: 'What is a Prompt?',
    whatIsDesc: 'A prompt is the input you give to an LLM. The quality of your prompt directly determines the quality of the response. Prompting is both an art and a science.',
    principles: 'Core Principles',
    principlesDesc: 'Fundamental guidelines for effective prompts.',
    beSpecific: 'Be Specific',
    beSpecificDesc: 'Vague prompts get vague answers. Include relevant details and constraints.',
    showExamples: 'Show Examples',
    showExamplesDesc: 'Demonstrate the format and style you want with concrete examples.',
    giveContext: 'Provide Context',
    giveContextDesc: 'Background information helps the model understand your needs.',
    setFormat: 'Specify Format',
    setFormatDesc: 'Tell the model exactly how you want the output structured.',
    anatomy: 'Anatomy of a Prompt',
    anatomyDesc: 'The components that make up an effective prompt.',
    role: 'Role/Persona',
    roleDesc: 'Who the model should act as.',
    task: 'Task Description',
    taskDesc: 'What you want the model to do.',
    context: 'Context/Background',
    contextDesc: 'Relevant information for the task.',
    format: 'Output Format',
    formatDesc: 'How you want the response structured.',
    interactiveDemo: 'Prompt Comparison',
    demoDesc: 'Compare weak vs. strong prompts',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Clear, specific prompts yield better results',
    takeaway2: 'Examples are powerful—show, don\'t just tell',
    takeaway3: 'Iterate on prompts; first attempts are rarely optimal',
    takeaway4: 'Consider the model\'s perspective when crafting prompts',
  },

  // Advanced Prompting page
  advancedPrompting: {
    title: 'Advanced Techniques',
    description: 'Sophisticated prompting strategies for complex tasks.',
    overview: 'Beyond the Basics',
    overviewDesc: 'Advanced techniques unlock more capable and reliable AI behavior for complex tasks.',
    cot: 'Chain of Thought',
    cotDesc: 'Encourage step-by-step reasoning by asking the model to "think through" problems.',
    cotExample: 'Example: "Let\'s solve this step by step..."',
    cotLimitations: 'Important: CoT is Not Universal',
    cotLimitationsDesc: '2025 research shows Chain of Thought prompting is NOT universally beneficial. While it significantly improves performance on reasoning-heavy tasks, it provides minimal gains for non-reasoning tasks.',
    cotLimitationItem1: 'Most effective for math, logic, and multi-step reasoning problems',
    cotLimitationItem2: 'Minimal benefit for simple retrieval, classification, or creative tasks',
    cotLimitationItem3: 'Increases latency and token costs—use strategically, not by default',
    fewShot: 'Few-Shot Learning',
    fewShotDesc: 'Provide multiple examples to establish patterns the model should follow.',
    fewShotExample: 'Include 3-5 diverse examples covering edge cases.',
    selfConsistency: 'Self-Consistency',
    selfConsistencyDesc: 'Generate multiple responses and select the most consistent answer.',
    selfConsistencyExample: 'Useful for math, logic, and factual questions.',
    decomposition: 'Task Decomposition',
    decompositionDesc: 'Break complex tasks into smaller, manageable sub-tasks.',
    decompositionExample: 'Solve sub-tasks independently, then combine results.',
    treeOfThoughts: 'Tree of Thoughts (ToT)',
    totDesc: 'An extension of Chain of Thought that explores multiple reasoning paths simultaneously, evaluating and backtracking when needed to find optimal solutions.',
    totHowItWorks: 'How It Works',
    totHowItWorksDesc: 'Generate multiple reasoning branches at each step. Evaluate promising paths, prune dead ends, and backtrack to explore alternatives.',
    totBestFor: 'Best For',
    totBestForDesc: 'Planning problems, puzzles, creative tasks requiring exploration, and problems where the first approach may not be optimal.',
    totExample: 'Example: "Consider 3 different approaches to solve this. For each, think 2 steps ahead. Evaluate which path is most promising, then continue."',
    graphOfThoughts: 'Graph of Thoughts (GoT)',
    gotDesc: 'A non-linear reasoning structure where thoughts can merge, branch, and form cycles—modeling how humans actually think about complex problems.',
    gotKeyFeature: 'Key Feature',
    gotKeyFeatureDesc: 'Unlike linear CoT or tree-structured ToT, GoT allows combining insights from different reasoning paths and revisiting earlier conclusions.',
    gotBestFor: 'Best For',
    gotBestForDesc: 'Complex problems with interdependencies, synthesis tasks, and problems where partial solutions need to be combined.',
    gotExample: 'Example: "Analyze this problem from angles A, B, and C independently. Then identify connections between your analyses and synthesize a unified solution."',
    costBenefit: 'Cost-Benefit Analysis',
    costBenefitDesc: 'Advanced prompting techniques increase token usage, latency, and API costs. Understanding when these tradeoffs are worthwhile is crucial for production systems.',
    worthIt: 'Worth the Extra Cost',
    worthItItem1: 'Complex reasoning: math, logic, multi-step analysis',
    worthItItem2: 'High-stakes decisions: medical, legal, financial advice',
    worthItItem3: 'Problems where accuracy matters more than speed',
    notWorthIt: 'Often Not Worth It',
    notWorthItItem1: 'Simple classification or extraction tasks',
    notWorthItItem2: 'High-volume, latency-sensitive applications',
    notWorthItItem3: 'Tasks where simpler prompts already achieve high accuracy',
    costBenefitTip: 'Tip: Start with simple prompts and add complexity only when needed. Measure the accuracy improvement against the cost increase to make informed decisions.',
    techniques: 'Additional Techniques',
    rolePlay: 'Role Assignment',
    rolePlayDesc: 'Assign a specific expert persona to focus the model\'s knowledge.',
    constraints: 'Explicit Constraints',
    constraintsDesc: 'List what the model should NOT do to prevent common errors.',
    verification: 'Self-Verification',
    verificationDesc: 'Ask the model to check its own work for errors.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'See how reasoning steps improve outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Chain of thought improves reasoning tasks, but is not universally beneficial',
    takeaway2: 'Few-shot examples establish reliable patterns',
    takeaway3: 'ToT and GoT extend CoT for complex, non-linear problems',
    takeaway4: 'Always consider cost vs. benefit—advanced techniques increase token usage',
    takeaway5: 'Start simple, add complexity only when accuracy requires it',
  },

  // System Prompts page
  systemPrompts: {
    title: 'System Prompts',
    description: 'Configuring AI behavior through system-level instructions.',
    whatIs: 'What is a System Prompt?',
    whatIsDesc: 'A system prompt is a special instruction that sets the context, persona, and behavioral guidelines for an AI model. It\'s typically hidden from users and persists throughout a conversation.',
    purpose: 'Purpose of System Prompts',
    purposeDesc: 'System prompts establish the foundation for how the AI should behave.',
    setPersona: 'Define Persona',
    setPersonaDesc: 'Establish who the AI is: an assistant, expert, character, etc.',
    setBoundaries: 'Set Boundaries',
    setBoundariesDesc: 'Define what the AI should and shouldn\'t do.',
    establishTone: 'Establish Tone',
    establishToneDesc: 'Specify communication style: formal, casual, technical.',
    provideKnowledge: 'Provide Context',
    provideKnowledgeDesc: 'Include domain knowledge or rules specific to your application.',
    structure: 'Structure of Effective System Prompts',
    structureDesc: 'Well-organized system prompts are easier for models to follow.',
    identity: 'Identity Section',
    identityDesc: 'Who is the AI? What is its role?',
    capabilities: 'Capabilities',
    capabilitiesDesc: 'What can the AI do? What tools does it have?',
    limitations: 'Limitations',
    limitationsDesc: 'What should the AI avoid or refuse?',
    guidelines: 'Guidelines',
    guidelinesDesc: 'Specific rules for behavior and responses.',
    bestPractices: 'Best Practices',
    practice1: 'Be explicit about edge cases and error handling.',
    practice2: 'Test system prompts with adversarial inputs.',
    practice3: 'Version control your system prompts.',
    practice4: 'Keep prompts focused—don\'t overload with instructions.',
    examplePrompt: 'Example System Prompt',
    interactiveBuilder: 'Interactive Builder',
    builderDesc: 'Build your own system prompt from components',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'System prompts define the AI\'s persona and behavior',
    takeaway2: 'Structure prompts clearly: identity, capabilities, limitations',
    takeaway3: 'Test with edge cases—users will find them',
    takeaway4: 'System prompts can be overridden—don\'t rely solely on them for security',
  },

  // LLM Training page
  llmTraining: {
    title: 'LLM Training',
    description: 'How large language models are trained: from pretraining to RLHF.',
    whatIs: 'How LLMs Are Trained',
    whatIsDesc: 'Large language models go through multiple training stages, each with different objectives and techniques. Understanding this pipeline is crucial for understanding model capabilities and limitations.',
    whyMatters: 'Why Training Matters',
    whyMattersDesc: 'The training process fundamentally shapes what LLMs can and cannot do. Different training approaches produce models with different strengths, weaknesses, and behaviors.',

    // LLM Training Pipeline
    trainingPipeline: 'The LLM Training Pipeline',
    trainingPipelineDesc: 'Modern LLMs go through multiple training stages, each with different objectives. Understanding this pipeline is crucial for understanding where alignment fits in.',

    pretraining: 'Stage 1: Pretraining',
    pretrainingDesc: 'The foundation model is trained on massive text corpora (trillions of tokens) using self-supervised learning. The model learns to predict the next token, developing broad knowledge and language capabilities.',
    pretrainingGoal: 'Goal: Learn language patterns, facts, and reasoning from raw text.',
    pretrainingData: 'Data: Web pages, books, code, scientific papers—typically 1-10+ trillion tokens.',
    pretrainingResult: 'Result: A capable but unaligned "base model" that completes text but doesn\'t follow instructions.',

    sft: 'Stage 2: Supervised Fine-Tuning (SFT)',
    sftDesc: 'The base model is fine-tuned on curated instruction-response pairs created by human annotators. This teaches the model to follow instructions and respond helpfully.',
    sftGoal: 'Goal: Transform the base model into an instruction-following assistant.',
    sftData: 'Data: ~10K-100K high-quality instruction-response examples.',
    sftResult: 'Result: A model that can follow instructions but may still produce harmful or unhelpful outputs.',

    rlhfStage: 'Stage 3: RLHF / Preference Tuning',
    rlhfStageDesc: 'Human evaluators rank model outputs by quality. A reward model learns these preferences, then the LLM is optimized to maximize the reward using reinforcement learning (PPO) or direct preference optimization (DPO).',
    rlhfGoal: 'Goal: Align the model with human preferences for helpfulness, harmlessness, and honesty.',
    rlhfData: 'Data: Human preference comparisons (A is better than B).',
    rlhfResult: 'Result: A model that produces outputs humans prefer and avoids harmful behaviors.',

    continuedTraining: 'Stage 4: Continued Training & Specialized Alignment',
    continuedTrainingDesc: 'Models may undergo additional training for specific capabilities (coding, math, tool use) or safety refinements (red teaming, constitutional AI). This stage is ongoing throughout deployment.',

    // RL Paradigm
    rlParadigm: 'The RL Paradigm: Learning Without Human Labels',
    rlParadigmDesc: 'A revolutionary approach where models learn reasoning through pure reinforcement learning on verifiable tasks, without human demonstrations or preference labels.',
    rlParadigmWhat: 'What is the RL Paradigm?',
    rlParadigmWhatDesc: 'Instead of learning from human-written examples (SFT) or human preferences (RLHF), models learn directly from outcome-based rewards. If the answer is correct, the model is rewarded. If wrong, it\'s penalized. No human labeling required.',
    deepseekR1: 'DeepSeek R1-Zero: A Case Study',
    deepseekR1Desc: 'DeepSeek R1-Zero demonstrated that powerful reasoning can emerge from pure RL, without any supervised fine-tuning. The model developed chain-of-thought reasoning, self-verification, and even "aha moments" entirely through reinforcement learning.',
    rlKey1: 'No SFT Required',
    rlKey1Desc: 'R1-Zero was trained directly from a base model using only RL, skipping the SFT stage entirely. Reasoning behaviors emerged naturally.',
    rlKey2: 'Verifiable Rewards',
    rlKey2Desc: 'Training focused on tasks with objectively verifiable answers: math problems, coding challenges, logical puzzles. No subjective human judgment needed.',
    rlKey3: 'Emergent Behaviors',
    rlKey3Desc: 'The model spontaneously developed extended thinking, self-correction, and reflection—behaviors that previous models only learned from human demonstrations.',
    rlKey4: 'Readability Challenges',
    rlKey4Desc: 'Pure RL models can develop unusual reasoning patterns that are hard to interpret. DeepSeek added a small amount of human data to improve readability.',
    rlVsRlhf: 'RL Paradigm vs. Traditional RLHF',
    rlVsRlhfDesc: 'These approaches solve different problems and can be complementary.',
    rlhfApproach: 'RLHF Approach',
    rlhfApproachDesc: 'Learn from human preferences. Requires expensive human labeling. Good for subjective tasks like writing quality and helpfulness.',
    rlApproach: 'RL Paradigm Approach',
    rlApproachDesc: 'Learn from verifiable outcomes. No human labeling needed. Excellent for reasoning, math, and coding where correctness is objective.',
    hybridApproach: 'Hybrid Approach',
    hybridApproachDesc: 'Modern models often combine both: RL for reasoning capabilities, RLHF for alignment and user preferences.',

    // Key Alignment Concepts
    concepts: 'Key Alignment Concepts',
    conceptsDesc: 'Fundamental ideas in AI alignment research.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'Ensuring the training objective (reward function) correctly captures what we want. Even perfect optimization of a misspecified objective leads to bad outcomes.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'Ensuring the learned model actually optimizes for the training objective, not some proxy goal that happens to correlate during training.',
    specification: 'Specification Problem',
    specificationDesc: 'The fundamental difficulty of precisely stating what we want in all situations. Human values are complex, contextual, and sometimes contradictory.',
    robustness: 'Robustness',
    robustnessDesc: 'Maintaining alignment under distribution shift, adversarial pressure, and novel situations the model wasn\'t trained on.',
    deception: 'Deceptive Alignment',
    deceptionDesc: 'A theoretical risk where a model appears aligned during training but pursues different goals when deployed—behaving well only because it\'s being evaluated.',
    goalMisgeneralization: 'Goal Misgeneralization',
    goalMisgeneralizationDesc: 'When a model learns a proxy goal that works in training but fails in deployment. Example: learning to get positive feedback rather than being genuinely helpful.',

    // Alignment Techniques
    techniques: 'Alignment Techniques',
    rlhf: 'RLHF (Reinforcement Learning from Human Feedback)',
    rlhfDesc: 'Train a reward model on human preferences, then use RL to optimize the LLM against it. The dominant alignment technique since GPT-4.',
    constitutionalAi: 'Constitutional AI (CAI)',
    constitutionalAiDesc: 'Define principles (a "constitution") and have the model critique and revise its own outputs. Reduces reliance on human labelers and scales better.',
    dpo: 'Direct Preference Optimization (DPO)',
    dpoDesc: 'Skip the reward model—directly optimize the LLM on preference data. Simpler and more stable than RLHF.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarial testing by humans or other AI models to find failure modes, jailbreaks, and harmful outputs before deployment.',
    interpretability: 'Interpretability',
    interpretabilityDesc: 'Understanding what models are actually learning internally. Crucial for verifying alignment rather than just measuring behavior.',
    safetyFilters: 'Safety Filters & Guardrails',
    safetyFiltersDesc: 'Additional layers that filter inputs/outputs for harmful content. A defense-in-depth measure, not a replacement for alignment.',

    // DPO vs RLHF Deep Dive
    dpoVsRlhf: 'DPO vs RLHF: A Deep Comparison',
    dpoVsRlhfDesc: 'Direct Preference Optimization (DPO) and Reinforcement Learning from Human Feedback (RLHF) are the two dominant approaches for aligning LLMs with human preferences. Understanding their differences is crucial for choosing the right technique.',

    rlhfDeep: 'RLHF: The Traditional Approach',
    rlhfDeepDesc: 'RLHF uses a separate reward model trained on human preferences, then optimizes the LLM using reinforcement learning (typically PPO) to maximize that reward.',
    rlhfStep1: 'Step 1: Collect Preferences',
    rlhfStep1Desc: 'Humans compare pairs of model outputs and select which they prefer. This creates a dataset of preference rankings.',
    rlhfStep2: 'Step 2: Train Reward Model',
    rlhfStep2Desc: 'A separate neural network learns to predict human preferences, assigning scores to model outputs.',
    rlhfStep3: 'Step 3: RL Optimization',
    rlhfStep3Desc: 'Use PPO (Proximal Policy Optimization) to update the LLM to generate outputs that maximize the reward model\'s scores.',

    dpoDeep: 'DPO: The Simplified Alternative',
    dpoDeepDesc: 'DPO skips the reward model entirely, directly optimizing the LLM on preference data using a clever mathematical reformulation.',
    dpoStep1: 'Step 1: Collect Preferences',
    dpoStep1Desc: 'Same as RLHF—humans compare pairs of outputs and indicate which they prefer.',
    dpoStep2: 'Step 2: Direct Optimization',
    dpoStep2Desc: 'Instead of training a separate reward model, DPO directly updates the LLM to increase probability of preferred outputs.',
    dpoStep3: 'Step 3: No RL Required',
    dpoStep3Desc: 'Uses standard supervised learning techniques, avoiding the instability and complexity of reinforcement learning.',

    comparisonTable: 'Head-to-Head Comparison',
    aspect: 'Aspect',
    complexity: 'Complexity',
    rlhfComplexity: 'High: requires reward model + RL training',
    dpoComplexity: 'Low: single-stage supervised learning',
    rewardModel: 'Reward Model',
    rlhfRewardModel: 'Required (separate neural network)',
    dpoRewardModel: 'Not needed (implicit in loss function)',
    stability: 'Training Stability',
    rlhfStability: 'Can be unstable, requires careful tuning',
    dpoStability: 'Generally more stable and predictable',
    flexibility: 'Flexibility',
    rlhfFlexibility: 'More flexible, reward model reusable',
    dpoFlexibility: 'Less flexible, tied to specific preferences',
    usedBy: 'Used By',
    rlhfUsedBy: 'GPT-4, Claude, early Llama models',
    dpoUsedBy: 'Llama 3, Zephyr, many open-source models',

    // GRPO Section
    grpo: 'GRPO: Group Relative Policy Optimization',
    grpoDesc: 'GRPO is an alignment technique developed by DeepSeek that uses relative rankings within groups of responses, eliminating the need for a separate reward model while maintaining training stability.',
    grpoHow: 'How GRPO Works',
    grpoHowDesc: 'Instead of absolute reward scores, GRPO compares multiple responses to the same prompt and uses their relative rankings to compute policy gradients.',
    grpoStep1: 'Generate Response Group',
    grpoStep1Desc: 'For each prompt, generate multiple candidate responses (typically 4-16) from the current policy.',
    grpoStep2: 'Rank Within Group',
    grpoStep2Desc: 'Score and rank responses within each group. The ranking can use verifiable rewards (for math/code) or learned preferences.',
    grpoStep3: 'Relative Gradient Update',
    grpoStep3Desc: 'Update the policy to increase probability of higher-ranked responses relative to lower-ranked ones within each group.',
    grpoAdvantages: 'Advantages',
    grpoAdv1: 'No separate reward model needed—reduces memory and complexity',
    grpoAdv2: 'More stable than PPO—relative comparisons are more robust than absolute scores',
    grpoAdv3: 'Works well with verifiable rewards (math, code) and learned preferences',
    grpoUseCases: 'Key Applications',
    grpoUse1: 'DeepSeek R1 reasoning model training',
    grpoUse2: 'Mathematical and coding task optimization',
    grpoUse3: 'Efficient alignment without reward model overhead',

    // Synthetic Data Section
    syntheticData: 'Synthetic Data for Alignment',
    syntheticDataDesc: 'Using AI models to generate training data is revolutionizing alignment. This approach can scale beyond human annotation capacity while maintaining quality through careful design.',
    syntheticHow: 'Synthetic Data Generation Methods',
    syntheticHowDesc: 'Several techniques have emerged for generating high-quality synthetic training data for alignment.',
    syntheticCAI: 'Constitutional AI (Anthropic)',
    syntheticCAIDesc: 'The model critiques and revises its own outputs based on a set of principles. The AI generates both the problematic response and the improved version, creating preference pairs without human labeling.',
    syntheticSelfInstruct: 'Self-Instruct & Evol-Instruct',
    syntheticSelfInstructDesc: 'Models generate their own instruction-response pairs, which are then filtered for quality. Evol-Instruct (used in WizardLM) iteratively makes instructions more complex.',
    syntheticDistillation: 'Model Distillation',
    syntheticDistillationDesc: 'A larger, more capable model generates training data for a smaller model. This transfers knowledge and alignment properties, as seen in many open-source models trained on GPT-4 outputs.',
    syntheticBenefits: 'Benefits',
    syntheticBenefit1: 'Massive scale—generate millions of examples cheaply',
    syntheticBenefit2: 'Consistent quality—no human annotator fatigue or disagreement',
    syntheticBenefit3: 'Targeted generation—create data for specific weaknesses',
    syntheticRisks: 'Risks & Limitations',
    syntheticRisk1: 'Model collapse—training on AI-generated data can degrade capabilities',
    syntheticRisk2: 'Bias amplification—AI biases get reinforced in synthetic data',
    syntheticRisk3: 'Quality ceiling—synthetic data quality limited by source model',

    // Fine-tuning vs Alignment
    fineTuningVsAlignment: 'Fine-Tuning vs. Alignment',
    fineTuningVsAlignmentDesc: 'Fine-tuning and alignment are related but distinct concepts.',
    fineTuningDef: 'Fine-Tuning',
    fineTuningDefDesc: 'Adapting a model to new tasks or domains by training on task-specific data. Can be done for any purpose.',
    alignmentDef: 'Alignment',
    alignmentDefDesc: 'Specifically making a model\'s behavior match human values and intentions. A subset of fine-tuning with a specific goal.',
    postTrainingDef: 'Post-Training',
    postTrainingDefDesc: 'The umbrella term for everything after pretraining: SFT, RLHF, specialized fine-tuning, safety training, etc.',

    // Pipeline Visualizer
    pipelineVisualizer: 'Interactive Training Pipeline',
    pipelineVisualizerDesc: 'Explore the complete LLM training pipeline from data collection to deployment. Click any stage to see detailed information about data requirements, compute costs, and timelines.',

    // Detailed Pipeline Stages
    detailedPipeline: 'Complete Training Pipeline (8 Stages)',
    detailedPipelineDesc: 'Modern LLM training involves 8 major stages, each with different objectives, data requirements, and compute costs. Understanding this pipeline is essential for grasping the complexity and expense of training frontier models.',

    stage1Title: 'Data Collection & Curation',
    stage1Desc: 'Gather massive text corpora from diverse sources including web crawls, books, code repositories, and scientific papers.',
    stage1Data: 'Common Crawl snapshots (petabytes), The Pile, GitHub, Wikipedia, Books3, academic papers',
    stage1Volume: '1-15 trillion tokens raw (before cleaning)',
    stage1Cost: '$50K-500K (storage, bandwidth, curation tools)',

    stage2Title: 'Data Cleaning & Deduplication',
    stage2Desc: 'Remove duplicates, filter low-quality content, detect languages, remove PII, and normalize formatting.',
    stage2Data: 'MinHash LSH deduplication, perplexity-based quality filtering, language detection, toxicity filtering',
    stage2Techniques: 'Exact & fuzzy deduplication (MinHash), quality scoring (KenLM perplexity), PII removal, toxic content filtering',
    stage2Cost: '$100K-1M (CPU clusters, thousands of cores for weeks)',

    stage3Title: 'Tokenization',
    stage3Desc: 'Convert cleaned text into numerical token sequences using BPE, SentencePiece, or Unigram tokenizers.',
    stage3Methods: 'BPE (GPT), SentencePiece (Llama), Unigram (T5)',
    stage3Vocab: 'Vocabulary size: ~32K-100K tokens',
    stage3Duration: '1-2 weeks (train tokenizer, tokenize corpus, pack sequences)',

    stage4Title: 'Pre-training',
    stage4Desc: 'Train the base model on trillions of tokens using next-token prediction objective. This is the most expensive and compute-intensive stage.',
    stage4Compute: '10K-30K GPUs (H100/A100), 1-6 months of training',
    stage4Examples: 'Llama 3 70B: 15T tokens, 24K GPUs max, ~$20M. GPT-4: rumored $100M+',
    stage4Cost: '$2M-$200M depending on model size (70B model: ~$10-20M)',

    stage5Title: 'Supervised Fine-Tuning (SFT)',
    stage5Desc: 'Fine-tune the base model on curated instruction-response pairs to teach it to follow instructions and respond helpfully.',
    stage5Data: '10K-100K high-quality (prompt, completion) pairs (human-written or synthetic)',
    stage5Compute: '100-1000 GPUs, 1-4 weeks',
    stage5Cost: '$50K-2M (includes human annotation or synthetic data generation)',

    stage6Title: 'RLHF / Preference Tuning',
    stage6Desc: 'Align model outputs with human preferences using reinforcement learning (PPO) or direct preference optimization (DPO).',
    stage6Methods: 'PPO (Proximal Policy Optimization) or DPO (Direct Preference Optimization)',
    stage6Data: '10K-100K human preference comparisons (A vs B rankings)',
    stage6Cost: '$200K-10M (PPO requires 500-5000 GPUs; DPO is 5-10x cheaper)',

    stage7Title: 'Safety & Evaluation',
    stage7Desc: 'Red-team the model, run adversarial tests, apply Constitutional AI principles, and benchmark on standard evaluation suites.',
    stage7Methods: 'Red-teaming, adversarial prompts, Constitutional AI self-critique, safety classifiers',
    stage7Benchmarks: 'MMLU, HumanEval, TruthfulQA, BBH, safety benchmarks',
    stage7Ongoing: 'Continuous process throughout and after deployment ($100K-5M ongoing)',

    stage8Title: 'Deployment Optimization',
    stage8Desc: 'Optimize the model for production deployment through quantization, distillation, and inference infrastructure setup.',
    stage8Techniques: 'Quantization (GPTQ, AWQ, GGUF), knowledge distillation to smaller models',
    stage8Optimization: 'KV cache optimization, batching strategies, serving infrastructure (TensorRT, vLLM)',
    stage8Cost: '$50K-1M (GPU time for quantization + infrastructure setup)',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'LLM training has distinct stages: pretraining → SFT → RLHF → specialized alignment',
    takeaway2: 'The RL paradigm (e.g., DeepSeek R1-Zero) shows reasoning can emerge from pure RL without human demonstrations',
    takeaway3: 'RLHF aligns models with human preferences; pure RL optimizes for verifiable outcomes',
    takeaway4: 'Modern models often combine multiple techniques: SFT for instruction following, RLHF for preferences, RL for reasoning',
    takeaway5: 'Understanding the training pipeline helps you understand model behavior and limitations',
    takeaway6: 'The field is rapidly evolving—new paradigms like pure RL are changing how we think about training',
  },

  // Mixture of Experts page
  moe: {
    title: 'Mixture of Experts',
    description: 'Understanding sparsely activated models that use specialized expert networks for efficient scaling.',
    whatIs: 'What is Mixture of Experts?',
    whatIsDesc: 'is a neural network architecture that divides computation among specialized sub-networks called "experts." For each input, only a subset of experts are activated, enabling massive model capacity while keeping computational costs manageable.',
    brainAnalogy: '"Just as the brain activates specific regions based on the task, MoE models activate only the relevant experts for each token."',
    brainAnalogyDesc: '— This biomimetic approach enables models with trillions of parameters while using only a fraction during inference.',

    // How it Works
    howItWorks: 'How MoE Works',
    step1Title: 'Input Arrives',
    step1Desc: 'Each token (or group of tokens) is processed through the transformer layers until it reaches the MoE layer, which replaces the traditional dense feed-forward network (FFN).',
    step2Title: 'Router Selects Experts',
    step2Desc: 'A gating network (router) examines the input and determines which experts should process it. Typically, only the top-K experts (e.g., top-2 or top-8) with the highest scores are selected.',
    step3Title: 'Experts Process & Combine',
    step3Desc: 'The selected experts process the input in parallel. Their outputs are weighted by the router scores and combined to produce the final result.',

    // Router
    routerTitle: 'The Router (Gating Network)',
    routerSubtitle: 'The brain of the MoE system',
    routerDesc: 'The router is a small neural network that learns to direct tokens to appropriate experts. It outputs a probability distribution over all experts, determining which ones to activate.',
    topKRouting: 'Top-K Routing',
    topKRoutingDesc: 'Only the K experts with highest scores are activated. Common choices are top-2 (Mixtral) or top-8 (DeepSeek, Qwen). This ensures computational cost stays fixed regardless of total expert count.',
    loadBalancing: 'Load Balancing',
    loadBalancingDesc: 'Training includes auxiliary losses to prevent "expert collapse" where all tokens route to the same few experts. This ensures all experts are utilized and develop distinct specializations.',

    // Expert Specialization
    expertSpecialization: 'Expert Specialization',
    expert1Title: 'Domain Experts',
    expert1Desc: 'Some experts naturally specialize in domains like code, mathematics, or specific languages. This emerges from training, not explicit design.',
    expert2Title: 'Pattern Experts',
    expert2Desc: 'Experts may specialize in linguistic patterns like formal writing, conversational tone, or technical terminology.',
    expert3Title: 'Task Experts',
    expert3Desc: 'Some experts become better at specific tasks like summarization, translation, or reasoning—though boundaries are often fuzzy.',
    expertNote: 'Expert specialization emerges organically during training. Researchers are still working to fully understand what each expert learns.',

    // Scale
    scaleTitle: 'MoE at Scale: Real-World Models',
    modelColumn: 'Model',
    totalParams: 'Total Parameters',
    activeParams: 'Active per Token',
    expertsColumn: 'Experts (routing)',
    scaleNote: 'Notice how active parameters are 5-20x smaller than total parameters—this is the efficiency advantage of MoE.',

    // Advantages
    advantagesTitle: 'Why MoE Matters',
    advantage1Title: 'Massive Capacity, Efficient Inference',
    advantage1Desc: 'MoE models can have trillions of parameters but only activate a fraction per token. This enables much larger model capacity without proportionally increasing inference cost.',
    advantage2Title: 'Faster Training',
    advantage2Desc: 'More compute-efficient pretraining since each parameter is updated by a subset of tokens, not all tokens. The same performance can be achieved with less total compute.',
    advantage3Title: 'Specialized Processing',
    advantage3Desc: 'Different experts can specialize in different types of content—code, math, languages—providing better performance across diverse tasks.',
    advantage4Title: 'Scalable Architecture',
    advantage4Desc: 'Adding more experts increases capacity without changing inference cost (as long as top-K stays fixed). This enables continuous scaling.',

    // Challenges
    challengesTitle: 'Challenges of MoE',
    challenge1Title: 'High Memory Requirements',
    challenge1Desc: 'All expert parameters must be loaded into memory, even though only a subset is used per token. A 671B parameter model needs 671B parameters in VRAM.',
    challenge2Title: 'Training Instability',
    challenge2Desc: 'Load balancing between experts is tricky. Without careful tuning, some experts may never be used ("dead experts") or all tokens route to the same few experts.',
    challenge3Title: 'Communication Overhead',
    challenge3Desc: 'In distributed training/inference, routing tokens to experts on different GPUs introduces network communication overhead.',

    // Comparison
    comparisonTitle: 'Dense vs. Sparse Models',
    denseModel: 'Dense Model',
    dense1: 'All parameters active for every token',
    dense2: 'Simpler training and deployment',
    dense3: 'Memory = Compute cost (both scale together)',
    sparseModel: 'Sparse MoE Model',
    sparse1: 'Only top-K experts active per token',
    sparse2: 'Higher total capacity for same compute',
    sparse3: 'Memory >> Compute cost (decoupled)',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'MoE enables massive model capacity with manageable inference costs by activating only a subset of experts per token',
    takeaway2: 'Nearly all leading frontier models (DeepSeek, Qwen, Mixtral, Llama 4) now use MoE architectures',
    takeaway3: 'The router/gating network learns to direct tokens to specialized experts—specialization emerges from training',
    takeaway4: 'The main tradeoff: high memory requirements (all experts loaded) vs. efficient compute (few experts active)',

    // Interactive Visualizer
    vizTitle: 'MoE Generation Visualizer',
    vizSubtitle: '8 experts, top-2 routing (like Mixtral)',
    vizGenerate: 'Generate',
    vizVramWarning: 'All Experts Must Be Loaded in VRAM',
    vizVramExplain: 'Even though only 2 experts are activated per token, all 8 experts must remain loaded in GPU memory. This is why MoE models have high memory requirements despite efficient compute.',
    vizVramUsage: 'VRAM Usage',
    vizLoaded: 'loaded',
    vizActive: 'active',
    vizMemoryFootprint: '100% memory footprint',
    vizCannotOffload: 'Cannot offload inactive experts',
    vizRouter: 'Router',
    vizNextToken: 'Next Token to Generate',
    vizRouterLabel: 'Gating Network',
    vizExpertsLabel: 'Experts (all loaded in VRAM)',
    vizGeneratedText: 'Generated Text',
    vizKeyInsight: 'Key Insight: Memory vs. Compute Tradeoff',
    vizKeyInsightDesc: 'A 46.7B parameter MoE model like Mixtral 8x7B needs VRAM for all 46.7B parameters, but only uses ~12.9B parameters per token. You pay the memory cost upfront, but get efficient inference.',
    vizTrainingTitle: 'Training Complexity: Load Balancing',
    vizTrainingDesc: 'Experts don\'t have fixed specializations—what each expert learns emerges organically during training. This creates a major challenge:',
    vizTraining1: 'Without careful balancing, the router may collapse to always choosing the same few experts, leaving others as "dead experts" that never improve',
    vizTraining2: 'Auxiliary loss functions penalize uneven expert usage, forcing the router to distribute tokens more evenly across all experts',
    vizTraining3: 'Even with balancing, expert specialization remains fuzzy—the same expert may handle math, certain languages, AND specific syntax patterns',
  },

  // Quantization page
  quantization: {
    title: 'Quantization',
    description: 'How reducing numerical precision enables running large models on consumer hardware with minimal quality loss.',

    // What is Quantization
    whatIs: 'What is Quantization?',
    whatIsDesc: 'is the process of reducing the numerical precision of model weights from 32-bit floating point (FP32) to lower bit representations like FP16, INT8, or INT4. This dramatically reduces memory requirements and speeds up inference.',
    analogy: '"Like compressing a high-resolution photo to fit on your phone—you lose some detail, but the image remains recognizable and useful."',
    analogyDesc: '— The key insight is that neural networks are surprisingly robust to precision loss. Most weights can be stored with far fewer bits without catastrophic quality degradation.',

    // Why Quantize
    whyQuantize: 'Why Quantize?',
    whyQuantizeDesc: 'Quantization unlocks the ability to run large models on consumer hardware and reduces inference costs in production.',
    benefit1Title: 'Memory Reduction',
    benefit1Desc: 'A 70B parameter model at FP16 requires ~140GB VRAM. At INT4, it fits in ~35GB—runnable on high-end consumer GPUs.',
    benefit2Title: 'Faster Inference',
    benefit2Desc: 'Lower precision arithmetic is faster. INT8 operations are 2-4x faster than FP32 on modern hardware.',
    benefit3Title: 'Lower Costs',
    benefit3Desc: 'Smaller models mean fewer GPUs, lower cloud costs, and feasibility for edge deployment.',
    benefit4Title: 'Democratization',
    benefit4Desc: 'Enables researchers and hobbyists to run frontier-class models locally without enterprise hardware.',

    // Interactive Visualizer
    vizTitle: 'Quantization Visualizer',
    vizSubtitle: 'See how precision affects model size and quality',
    vizPrecision: 'Precision Level',
    vizModelSize: 'Model Size',
    vizAccuracy: 'Accuracy Retained',
    vizPerplexity: 'Perplexity Increase',
    vizOfOriginal: 'of original',
    vizRetained: 'retained',
    vizWeightDist: 'Weight Distribution',
    vizDiscreteLevel: 'discrete levels',
    vizExplanation: 'Explanation',
    vizFp32Explain: 'Full precision (32 bits). Maximum accuracy, maximum memory. Used primarily for training.',
    vizFp16Explain: 'Half precision (16 bits). Negligible quality loss for most tasks. Standard for inference.',
    vizInt8Explain: 'Integer precision (8 bits). Excellent balance of quality and efficiency. Production standard.',
    vizInt4Explain: 'Highly compressed (4 bits). Sweet spot for consumer hardware. Most users notice no quality difference.',
    vizInt2Explain: 'Extreme compression (2 bits). Significant quality loss. Only for extreme memory constraints.',

    // Quantization Levels
    levelsTitle: 'Quantization Levels Explained',
    levelsDesc: 'Each precision level represents a different tradeoff between model size and output quality.',
    levelBits: 'Bits',
    levelSize: 'Size',
    levelAccuracy: 'Accuracy',
    levelUseCase: 'Use Case',
    levelFp32: 'FP32 (Full)',
    levelFp32Size: '100%',
    levelFp32Accuracy: '100%',
    levelFp32Use: 'Training, reference inference',
    levelFp16: 'FP16 (Half)',
    levelFp16Size: '50%',
    levelFp16Accuracy: '~99%',
    levelFp16Use: 'Standard inference',
    levelInt8: 'INT8',
    levelInt8Size: '25%',
    levelInt8Accuracy: '~97%',
    levelInt8Use: 'Production deployment',
    levelInt4: 'INT4',
    levelInt4Size: '12.5%',
    levelInt4Accuracy: '~90-95%',
    levelInt4Use: 'Consumer GPUs, edge',
    levelInt2: 'INT2',
    levelInt2Size: '6.25%',
    levelInt2Accuracy: '~70-80%',
    levelInt2Use: 'Extreme edge cases',

    // Recommendation
    recommendTitle: 'Recommendation: Q4 is the Sweet Spot',
    recommendDesc: 'For most users running large models (70B+ parameters) locally:',
    recommend1: 'Q4 (INT4) provides excellent quality-to-memory ratio',
    recommend2: 'Most users cannot distinguish Q4 output from FP16 in blind tests',
    recommend3: 'Enables running 70B models on 24GB consumer GPUs',
    recommend4: 'Recommended formats: Q4_K_M or Q4_K_S for GGUF models',
    recommendNote: 'For critical applications requiring maximum accuracy, use FP16 or INT8. For casual use and experimentation, Q4 is ideal.',

    // Techniques
    techniquesTitle: 'Quantization Techniques',
    techniquesDesc: 'Different methods for converting models to lower precision.',
    techPtq: 'PTQ (Post-Training Quantization)',
    techPtqDesc: 'Apply quantization to an already-trained model. Fast and simple, but may have slightly higher accuracy loss. Works by calibrating quantization parameters on a small dataset.',
    techQat: 'QAT (Quantization-Aware Training)',
    techQatDesc: 'Include quantization in the training process. The model learns to be robust to precision loss, yielding better accuracy but requiring full retraining.',
    techGptq: 'GPTQ',
    techGptqDesc: 'One-shot quantization method designed for LLMs. Uses second-order information to minimize quantization error layer by layer. Popular for its speed and quality.',
    techAwq: 'AWQ (Activation-aware Weight Quantization)',
    techAwqDesc: 'Identifies and preserves "salient" weights that matter most for accuracy. Achieves better quality than naive quantization by protecting important parameters.',
    techGguf: 'GGUF Format',
    techGgufDesc: 'File format used by llama.cpp for quantized models. Supports various quantization levels (Q2-Q8) and is the standard for local LLM deployment.',

    // GGUF K-quants
    ggufTitle: 'GGUF K-Quant Methods',
    ggufDesc: 'Understanding the naming convention for GGUF quantized models.',
    ggufMethod: 'Method',
    ggufQuality: 'Quality',
    ggufSize: 'Size',
    ggufUseCase: 'Use Case',
    ggufQ2K: 'Q2_K',
    ggufQ2KQuality: 'Poor',
    ggufQ2KSize: 'Smallest',
    ggufQ2KUse: 'Extreme compression only',
    ggufQ3KS: 'Q3_K_S',
    ggufQ3KSQuality: 'Low',
    ggufQ3KSSize: 'Very Small',
    ggufQ3KSUse: 'Memory-constrained systems',
    ggufQ3KM: 'Q3_K_M',
    ggufQ3KMQuality: 'Low-Medium',
    ggufQ3KMSize: 'Small',
    ggufQ3KMUse: 'Budget hardware',
    ggufQ3KL: 'Q3_K_L',
    ggufQ3KLQuality: 'Medium',
    ggufQ3KLSize: 'Moderate',
    ggufQ3KLUse: 'Better Q3 quality',
    ggufQ4KS: 'Q4_K_S',
    ggufQ4KSQuality: 'Good',
    ggufQ4KSSize: 'Small',
    ggufQ4KSUse: 'Recommended balance',
    ggufQ4KM: 'Q4_K_M',
    ggufQ4KMQuality: 'Very Good',
    ggufQ4KMSize: 'Moderate',
    ggufQ4KMUse: 'Best overall choice',
    ggufQ5KS: 'Q5_K_S',
    ggufQ5KSQuality: 'Excellent',
    ggufQ5KSSize: 'Larger',
    ggufQ5KSUse: 'Quality-focused',
    ggufQ5KM: 'Q5_K_M',
    ggufQ5KMQuality: 'Excellent',
    ggufQ5KMSize: 'Larger',
    ggufQ5KMUse: 'Near-FP16 quality',
    ggufQ6K: 'Q6_K',
    ggufQ6KQuality: 'Near-perfect',
    ggufQ6KSize: 'Large',
    ggufQ6KUse: 'Minimal loss',
    ggufQ8: 'Q8_0',
    ggufQ8Quality: 'Excellent',
    ggufQ8Size: 'Large',
    ggufQ8Use: 'Reference quality',
    ggufExplainTitle: 'K-Quant Naming Explained',
    ggufExplainK: 'K = "K-quant" — uses importance-based quantization that varies precision by layer',
    ggufExplainS: 'S (Small) = More aggressive quantization on attention layers, smaller files',
    ggufExplainM: 'M (Medium) = Balanced quantization across all layers, best quality/size ratio',
    ggufExplainL: 'L (Large) = Less quantization on important layers, better quality',
    ggufKeyInsight: 'Key Insight: K-quants are "mixed precision"—they quantize different layers differently based on their importance to model quality. Attention layers typically use higher precision than feed-forward layers.',

    // Real-World Impact
    impactTitle: 'Real-World Impact',
    impactDesc: 'Concrete examples of what quantization enables.',
    impactExample1Title: 'Llama 3.1 70B at Different Quants',
    impactExample1Desc: 'A 70B parameter model requires ~140GB at FP16. With quantization:',
    impactExample1Q8: 'Q8: ~70GB — Fits on 2x A100 40GB or 1x H100',
    impactExample1Q4: 'Q4_K_M: ~40GB — Fits on 2x RTX 4090 or 1x A100 80GB',
    impactExample1Q3: 'Q3_K_M: ~30GB — Fits on single RTX 4090 (24GB + some offload)',
    impactExample2Title: 'Quality Comparison',
    impactExample2Desc: 'In blind tests comparing Q4_K_M to FP16 outputs:',
    impactExample2Stat1: '85% of users could not identify which was quantized',
    impactExample2Stat2: 'Perplexity increase of only 0.1-0.5 points on common benchmarks',
    impactExample2Stat3: 'Code completion and reasoning tasks show minimal degradation',
    impactExample3Title: 'Cost Savings',
    impactExample3Desc: 'Running a 70B model for inference:',
    impactExample3Fp16: 'FP16: ~$4-8/hour on cloud (2x A100)',
    impactExample3Q4: 'Q4: ~$1-2/hour (single A100 or high-end consumer GPU)',
    impactExample3Local: 'Local: One-time cost of consumer GPU vs ongoing cloud fees',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Quantization reduces model memory by 2-16x with surprisingly small accuracy loss',
    takeaway2: 'Q4 (INT4) is the sweet spot for most local LLM use cases—excellent quality at 1/8th the memory',
    takeaway3: 'K-quant methods (Q4_K_M, Q5_K_S) are "mixed precision" and outperform uniform quantization',
    takeaway4: 'GPTQ and AWQ are the leading techniques for LLM quantization, with GGUF as the standard format',
    takeaway5: 'Quantization democratizes AI by enabling frontier models on consumer hardware',
    takeaway6: 'For critical applications, prefer higher precision (INT8/FP16); for experimentation, Q4 is ideal',
  },

  // Nested Learning page
  nestedLearning: {
    title: 'Nested Learning',
    description: 'A new approach that lets AI learn new things without forgetting old ones — by learning at different speeds simultaneously.',

    // Research disclaimer
    researchDisclaimer: 'Research Preview',
    researchDisclaimerDesc: 'Nested Learning was introduced at NeurIPS 2025 by Google Research. This is cutting-edge research — exciting, but not yet used in production systems.',

    // Analogy intro
    analogyTitle: 'The Forgetting Problem',
    analogyDesc: 'Imagine you spent years learning to play piano. Then you decide to learn guitar. After months of guitar practice, you sit back down at the piano — and your fingers have forgotten half the songs. That\'s essentially what happens to AI models today.',
    analogyQuote: 'Learning to ride a bike shouldn\'t make you forget how to swim. But for today\'s AI, it often does.',
    analogyPunchline: 'Nested Learning is a new idea that tries to fix this. Instead of one big learning process that overwrites everything, it uses multiple learning speeds working together — like how your brain handles muscle memory, habits, and conscious thought all at once.',

    // The Problem
    problemTitle: 'See It Happen: Catastrophic Forgetting',
    problemDesc: 'This is the core problem. When you train an AI on something new, it tends to forget what it already knew. Try the demo below — train on Task A, then train on Task B, and watch Task A\'s knowledge disappear.',

    // Forgetting demo
    forgettingDemoTitle: 'Try it: Train, then watch it forget',
    forgettingTaskA: 'Task A Knowledge',
    forgettingTaskB: 'Task B Knowledge',
    forgettingTrainA: '1. Train on Task A',
    forgettingTrainB: '2. Now Train on Task B',
    forgettingReset: 'Reset',
    forgettingKnowledge: 'Knowledge',
    forgettingStep: 'Step',
    forgettingWarning: 'Catastrophic forgetting! Task A knowledge was destroyed while learning Task B.',

    // Solution
    solutionTitle: 'The Fix: Learning at Different Speeds',
    solutionDesc: 'Nested Learning\'s key idea is surprisingly intuitive: instead of one learning process, use several that run at different speeds — nested inside each other, like Russian dolls.',
    speedSlow: '🐢 Slow',
    speedSlowDesc: 'Core knowledge — changes rarely, stays stable',
    speedMedium: '🚶 Medium',
    speedMediumDesc: 'Patterns & skills — adapts over time',
    speedFast: '⚡ Fast',
    speedFastDesc: 'Immediate context — reacts to what\'s happening now',
    solutionAnalogy: 'Think of it like your brain: your personality changes slowly over years, your habits shift over weeks, and your attention shifts every second. Each "speed" handles a different type of knowledge without interfering with the others.',

    // Nested loops
    loopsTitle: 'Watch the Loops in Action',
    loopsDesc: 'The three learning speeds run as nested loops. The inner loop spins fast (adapting to immediate input), the middle loop ticks along (building patterns), and the outer loop moves slowly (consolidating deep knowledge). Hit play to see them in motion.',
    loopsDemoTitle: 'Nested Learning Loops',
    outerLoop: 'Outer',
    middleLoop: 'Middle',
    innerLoop: 'Inner',
    outerLoopDesc: 'Slow — deep knowledge',
    middleLoopDesc: 'Medium — patterns',
    innerLoopDesc: 'Fast — adaptation',
    loopsRunning: 'Running...',
    loopsPaused: 'Paused',
    loopsPlay: '▶ Play',
    loopsPause: '⏸ Pause',

    // Comparison
    comparisonTitle: 'Head to Head: Traditional vs Nested',
    comparisonDesc: 'What happens when you train on three tasks in sequence? Traditional learning forgets earlier tasks. Nested learning preserves them. Run the demo to see the difference.',
    comparisonDemoTitle: 'Training on 3 Sequential Tasks',
    traditionalTitle: '❌ Traditional',
    nestedTitle: '✅ Nested',
    compTaskA: 'Task A',
    compTaskB: 'Task B',
    compTaskC: 'Task C',
    compRun: '▶ Run Comparison',
    compReset: 'Reset',

    // Hope Architecture
    hopeTitle: 'The Hope Architecture',
    hopeDesc: 'Google Research built "Hope" as a proof of concept. It\'s an architecture that actually implements these nested learning ideas. The key innovation: the model can modify its own learning rules as it goes.',
    hopeDiagramTitle: 'How Hope Works',
    hopeInput: 'Input',
    hopeSelfMod: 'Self-Modify',
    hopeMemory: 'Memory',
    hopeOutput: 'Output',
    hopeLearnRules: 'learns rules',
    hopeStoreRecall: 'store & recall',
    hopePoint1: 'Beats standard Transformers on language understanding benchmarks',
    hopePoint2: 'Handles very long contexts better (like finding a needle in a haystack)',
    hopePoint3: 'Can keep learning without forgetting — the whole point',

    // Why it matters
    mattersTitle: 'Why This Matters',
    mattersDesc: 'If this works at scale, it changes everything about how we build AI:',
    matter1Title: 'Always Learning',
    matter1Desc: 'AI that keeps improving from real-world use — no expensive retraining needed',
    matter2Title: 'More Efficient',
    matter2Desc: 'Multiple learning speeds could need less compute than brute-force approaches',
    matter3Title: 'Brain-Like',
    matter3Desc: 'Much closer to how biological brains actually work — different systems for different timescales',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Today\'s AI has a forgetting problem: learning new things erases old knowledge',
    takeaway2: 'Nested Learning fixes this by using multiple learning speeds — slow for core knowledge, fast for immediate context',
    takeaway3: 'Google\'s Hope architecture proves the concept works, outperforming Transformers on several tasks',
    takeaway4: 'This is early research (NeurIPS 2025) — not ready for production, but a glimpse of AI\'s future',
  },

  // Distillation page
  distillation: {
    title: 'Distillation',
    description: 'How smaller models learn from larger ones by training on probability distributions rather than single tokens, using the teacher-student paradigm.',

    // What is Distillation
    whatIs: 'What is Knowledge Distillation?',
    whatIsDesc: 'Knowledge distillation is a model compression technique where a smaller "student" model is trained to replicate the behavior of a larger, more capable "teacher" model. Instead of training the student from scratch on raw data, it learns from the teacher\'s output probability distributions—capturing not just what the teacher predicts, but how confident it is across all possible predictions.',
    analogy: '"Imagine a master chef teaching an apprentice—not just the recipes, but all the subtle intuitions: why this spice almost works, why that technique is close but not quite right."',
    analogyDesc: 'Distillation transfers these nuanced judgments by sharing the full probability distribution, not just the final answer.',

    // Teacher-Student
    teacherStudentTitle: 'The Teacher-Student Paradigm',
    teacherStudentDesc: 'Distillation follows a straightforward two-phase process: first train a large, powerful teacher model, then use its outputs to train a smaller, efficient student.',
    teacherTitle: 'Teacher Model',
    teacherDesc: 'A large, high-capacity model (e.g., GPT-4, Claude Opus) trained on massive datasets. It has learned rich representations and nuanced decision boundaries. Its role is to generate soft probability distributions that encode its knowledge.',
    studentTitle: 'Student Model',
    studentDesc: 'A smaller, more efficient model designed for deployment. It learns by matching the teacher\'s probability distributions rather than just the ground truth labels. This allows it to capture the teacher\'s "dark knowledge"—the relationships between classes that hard labels discard.',
    flowTeacher: 'Teacher Model',
    flowSoftDistribution: 'Soft Probability Distribution',
    flowStudent: 'Student Model Learns',

    // Key Insight
    keyInsightTitle: 'The Key Insight: Distributions, Not Tokens',
    keyInsightSubtitle: 'Why Distributions Make Distillation So Effective',
    keyInsightDesc: 'The fundamental reason distillation works so well is that we train on full probability distributions, not single tokens or hard labels. When a teacher model processes "The capital of France is ___", it doesn\'t just output "Paris"—it produces a probability distribution over its entire vocabulary.',
    keyInsightDesc2: 'This distribution contains rich information: "Paris" gets 92%, but "Lyon" gets 3%, "Marseille" gets 1.5%, and "Berlin" gets 0.8%. These "wrong" answers encode the teacher\'s understanding of geography, similarity between cities, and conceptual relationships. A hard label of just "Paris" throws all of this knowledge away.',

    // Hard vs Soft Labels
    hardLabelTitle: 'Hard Labels (Traditional Training)',
    hardLabelExample: '"Paris" = 1.0, everything else = 0.0',
    hardLabelExplain: 'Binary: either right or wrong. No nuance. The model learns nothing about the relationships between outputs.',
    softLabelTitle: 'Soft Labels (Distillation)',
    softLabelExample: '"Paris" = 0.92, "Lyon" = 0.03, "Marseille" = 0.015, "Berlin" = 0.008, ...',
    softLabelExplain: 'Rich signal: every probability encodes a relationship. The student learns that Lyon is more similar to Paris than Berlin is.',

    // Visualizer
    vizTitle: 'Temperature & Distribution Softening',
    vizSubtitle: 'See how temperature reshapes the teacher\'s knowledge for the student',
    vizToken1: 'Paris',
    vizToken2: 'Lyon',
    vizToken3: 'Mars.',
    vizToken4: 'Berlin',
    vizToken5: 'Rome',
    hardLabels: 'Hard Distribution',
    hardLabelsDesc: 'At T=1, the dominant token overwhelms others. Little information in the tail.',
    softLabels: 'Soft Distribution',
    softLabelsDesc: 'Higher temperature reveals relationships between tokens that hard labels hide.',
    vizTeacherLabel: 'Teacher',
    vizStudentLabel: 'Student',
    vizStudentDist: 'Student Distribution',
    vizStudentOff: 'Toggle on to compare student',
    vizStudentDesc: 'The student tries to match the teacher\'s soft distribution. Adjust its temperature to see the effect.',
    vizStudentTemp: 'Student Temperature',
    vizKLDiv: 'KL Divergence',
    vizKLExcellent: 'Excellent match',
    vizKLGood: 'Good match',
    vizKLPoor: 'Poor match',
    vizTeacherEntropy: 'Teacher Entropy',
    vizStudentEntropy: 'Student Entropy',
    vizMatchExplain: 'The student\'s distribution closely matches the teacher\'s soft distribution—this is ideal for knowledge transfer.',
    vizStudentSharper: 'The student\'s distribution is sharper than the teacher\'s. It may miss subtle inter-class relationships the teacher has learned.',
    vizStudentSmoother: 'The student\'s distribution is smoother than the teacher\'s. It\'s over-generalizing and losing some of the teacher\'s specific knowledge.',
    distillTemp: 'Distillation Temperature',
    tempSharp: 'sharp',
    tempSmooth: 'smooth',
    tempExplainLow: 'Low temperature: distribution is still peaked. The student mainly learns what the top prediction is.',
    tempExplainMid: 'Medium temperature: the distribution is smoothed, revealing meaningful relationships between tokens. This is the sweet spot for distillation.',
    tempExplainHigh: 'High temperature: distribution approaches uniform. Too much smoothing can wash out the signal the teacher has learned.',

    // Why it Works
    whyWorks: 'Why Distillation Works',
    whyWorksDesc: 'Distillation is remarkably effective because soft labels provide a much richer training signal than hard labels:',
    benefit1Title: 'Richer Gradient Signal',
    benefit1Desc: 'Each training example provides information about all output classes simultaneously, not just the correct one. This means each example effectively teaches the student about thousands of relationships at once.',
    benefit2Title: 'Dark Knowledge Transfer',
    benefit2Desc: 'The teacher\'s "mistakes" are informative. When the teacher assigns 3% probability to "Lyon" for a question about France\'s capital, it tells the student that Lyon is relevant to France—knowledge that hard labels completely discard.',
    benefit3Title: 'Better Generalization',
    benefit3Desc: 'Students trained via distillation often generalize better than models trained on hard labels alone, even when the student has much fewer parameters. The soft labels act as a powerful regularizer.',
    benefit4Title: 'Sample Efficiency',
    benefit4Desc: 'Because each training example carries more information (a full distribution vs. a single label), the student needs fewer examples to learn effectively. This reduces training time and data requirements.',

    // Loss Function
    lossTitle: 'The Distillation Loss',
    lossDesc: 'The training objective combines two losses: the standard cross-entropy with ground truth labels, and the KL divergence between teacher and student distributions:',
    lossCE: 'Cross-Entropy with ground truth: ensures the student still learns from real labels',
    lossKL: 'KL Divergence: measures how different the student\'s distribution is from the teacher\'s. The student is penalized for deviating from the teacher\'s soft probabilities.',
    lossT: 'Temperature: controls how soft/smooth the distributions are. Higher T reveals more inter-class relationships.',
    lossAlpha: 'Alpha: balances the two loss terms. Typical values range from 0.1 to 0.9, with higher values placing more weight on matching the teacher.',
    lossInsight: 'The T\u00B2 factor compensates for the scaling effect of temperature on gradients, ensuring the distillation loss and cross-entropy loss remain balanced regardless of temperature choice.',

    // Types
    typesTitle: 'Types of Distillation',
    typesDesc: 'Different approaches depending on what knowledge is transferred from teacher to student:',
    typeResponseTitle: 'Response-Based',
    typeResponseDesc: 'The student mimics the teacher\'s final output distribution. This is the original and most common form, used by Hinton et al. (2015). Simple to implement and effective for classification and language modeling.',
    typeFeatureTitle: 'Feature-Based',
    typeFeatureDesc: 'The student learns to match intermediate representations (hidden states) of the teacher, not just the output. Captures deeper structural knowledge. Used in models like DistilBERT and TinyBERT.',
    typeRelationTitle: 'Relation-Based',
    typeRelationDesc: 'Transfers the relationships between different examples or layers, rather than individual outputs. Preserves how the teacher structures its internal representations and how it relates different inputs to each other.',
    typeOnlineTitle: 'Online Distillation',
    typeOnlineDesc: 'Teacher and student train simultaneously, learning from each other. No pre-trained teacher required. Useful when you cannot afford to train a massive teacher model first.',

    // Examples
    examplesTitle: 'Real-World Examples',
    examplesDesc: 'Distillation is used extensively in production AI systems:',
    example1Title: 'DistilBERT (Hugging Face)',
    example1Desc: 'A distilled version of BERT that is 60% smaller, 60% faster, and retains 97% of BERT\'s language understanding. Trained using a combination of response-based and feature-based distillation. One of the most widely deployed distilled models.',
    example2Title: 'OpenAI GPT-4 to GPT-4o-mini',
    example2Desc: 'GPT-4o-mini is widely believed to be distilled from larger GPT-4 class models. It offers substantially lower latency and cost while maintaining competitive performance on most tasks. This pattern—a large frontier model distilled into a smaller, faster variant—has become standard practice.',
    example3Title: 'DeepSeek R1 Distillation',
    example3Desc: 'DeepSeek released distilled versions of their R1 reasoning model into Qwen and Llama base models. These distilled variants bring advanced reasoning capabilities to much smaller, more deployable models, demonstrating that even complex chain-of-thought reasoning can be effectively distilled.',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Knowledge distillation trains smaller models to replicate larger ones by learning from full probability distributions, not just final answers',
    takeaway2: 'The critical insight is that we train on distributions, not single tokens—soft labels encode rich relational knowledge ("dark knowledge") that hard labels discard entirely',
    takeaway3: 'Temperature softening reveals inter-class relationships hidden in the teacher\'s distribution, making distillation far more effective than simple label matching',
    takeaway4: 'Distilled models can retain 95-99% of teacher performance at a fraction of the size, making frontier AI capabilities accessible for real-world deployment',
    takeaway5: 'Distillation has become a standard practice in the industry—most small, fast models you use daily (GPT-4o-mini, DistilBERT, Gemini Flash) are likely distilled from larger teachers',
  },

  // Bias & Fairness page
  bias: {
    title: 'Bias & Fairness',
    description: 'Understanding and mitigating harmful biases in AI systems.',
    whatIs: 'What is AI Bias?',
    whatIsDesc: 'AI bias occurs when machine learning systems produce systematically unfair outcomes for certain groups. Biases can arise from training data, model design, or deployment context.',
    sources: 'Sources of Bias',
    sourcesDesc: 'Where bias enters AI systems.',
    dataBias: 'Training Data',
    dataBiasDesc: 'Historical biases in the data are learned by the model.',
    labelBias: 'Label Bias',
    labelBiasDesc: 'Human annotators introduce their own biases.',
    selectionBias: 'Selection Bias',
    selectionBiasDesc: 'Training data doesn\'t represent the deployment population.',
    measurementBias: 'Measurement Bias',
    measurementBiasDesc: 'Proxies used for measurement encode bias.',
    types: 'Types of Bias',
    typesDesc: 'Common categories of bias in AI systems.',
    stereotyping: 'Stereotyping',
    stereotypingDesc: 'Reinforcing harmful stereotypes about groups.',
    erasure: 'Erasure',
    erasureDesc: 'Underrepresenting or ignoring certain groups.',
    disparateImpact: 'Disparate Impact',
    disparateImpactDesc: 'Different outcomes for different groups.',
    mitigation: 'Mitigation Strategies',
    mitigationDesc: 'Approaches to reduce bias.',
    diverseData: 'Diverse Data',
    diverseDataDesc: 'Ensure training data represents all relevant groups.',
    auditing: 'Bias Auditing',
    auditingDesc: 'Systematically test for bias across demographics.',
    constraints: 'Fairness Constraints',
    constraintsDesc: 'Incorporate fairness metrics into training.',
    interactiveDemo: 'Bias Detection Demo',
    demoDesc: 'Explore how bias manifests in model outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Bias is often inherited from training data',
    takeaway2: 'Different fairness metrics can conflict—choose carefully',
    takeaway3: 'Regular auditing is essential for deployed systems',
    takeaway4: 'Bias mitigation is an ongoing process, not a one-time fix',
  },

  // Responsible AI page
  responsibleAi: {
    title: 'Responsible AI',
    description: 'Building and deploying AI systems ethically and sustainably.',
    whatIs: 'What is Responsible AI?',
    whatIsDesc: 'Responsible AI encompasses the practices, policies, and principles that ensure AI systems are developed and used ethically, safely, and in ways that benefit society.',
    pillars: 'Pillars of Responsible AI',
    pillarsDesc: 'Core principles guiding responsible AI development.',
    transparency: 'Transparency',
    transparencyDesc: 'Be open about AI capabilities, limitations, and decision-making.',
    accountability: 'Accountability',
    accountabilityDesc: 'Clear ownership and responsibility for AI outcomes.',
    privacy: 'Privacy',
    privacyDesc: 'Protect user data and respect privacy rights.',
    safety: 'Safety',
    safetyDesc: 'Ensure systems are robust and don\'t cause harm.',
    practices: 'Responsible Practices',
    practicesDesc: 'Concrete steps for responsible AI development.',
    documentation: 'Documentation',
    documentationDesc: 'Document model capabilities, training data, and known limitations.',
    testing: 'Comprehensive Testing',
    testingDesc: 'Test for safety, bias, and edge cases before deployment.',
    monitoring: 'Ongoing Monitoring',
    monitoringDesc: 'Track system behavior in production for issues.',
    feedback: 'User Feedback',
    feedbackDesc: 'Create channels for users to report problems.',
    considerations: 'Ethical Considerations',
    environmental: 'Environmental Impact',
    environmentalDesc: 'AI training has significant carbon footprint.',
    labor: 'Labor Implications',
    laborDesc: 'Consider impact on workers and employment.',
    access: 'Equitable Access',
    accessDesc: 'Ensure AI benefits are broadly distributed.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Responsible AI requires proactive effort throughout the lifecycle',
    takeaway2: 'Transparency builds trust and enables accountability',
    takeaway3: 'Consider societal impact beyond immediate users',
    takeaway4: 'Ethics are not optional—integrate them into development processes',
    // Governance Frameworks
    frameworks: 'Governance Frameworks',
    frameworksDesc: 'International standards and regulations provide structured approaches to responsible AI development and deployment.',
    // NIST AI RMF
    nistTitle: 'NIST AI Risk Management Framework',
    nistDesc: 'A voluntary framework from the U.S. National Institute of Standards and Technology for managing AI risks throughout the AI lifecycle.',
    nistGovern: 'Govern',
    nistGovernDesc: 'Culture, policies, accountability',
    nistMap: 'Map',
    nistMapDesc: 'Context and risk identification',
    nistMeasure: 'Measure',
    nistMeasureDesc: 'Assessment and analysis',
    nistManage: 'Manage',
    nistManageDesc: 'Prioritize and respond',
    // OECD AI Principles
    oecdTitle: 'OECD AI Principles',
    oecdDesc: 'International principles adopted by 46 countries to promote trustworthy AI that respects human rights and democratic values.',
    oecdInclusive: 'Inclusive Growth',
    oecdHuman: 'Human-Centered',
    oecdTransparency: 'Transparency',
    oecdRobust: 'Robustness',
    oecdAccountability: 'Accountability',
    // ISO/IEC 42001
    isoTitle: 'ISO/IEC 42001',
    isoDesc: 'The first international standard specifying requirements for establishing, implementing, and improving an AI management system within organizations.',
    isoPolicy: 'AI Policy',
    isoPolicyDesc: 'Establish organizational AI objectives and principles',
    isoRisk: 'Risk Assessment',
    isoRiskDesc: 'Identify and evaluate AI-specific risks',
    isoImprovement: 'Continuous Improvement',
    isoImprovementDesc: 'Monitor, measure, and enhance AI practices',
    // EU AI Act
    euActTitle: 'EU AI Act',
    euActDesc: 'The world\'s first comprehensive AI regulation, taking a risk-based approach to categorize and regulate AI systems.',
    euProhibited: 'Prohibited',
    euProhibitedDesc: 'Social scoring, manipulative AI, real-time biometrics',
    euHighRisk: 'High Risk',
    euHighRiskDesc: 'Healthcare, education, employment, law enforcement',
    euLimited: 'Limited Risk',
    euLimitedDesc: 'Chatbots, deepfakes (transparency required)',
    euMinimal: 'Minimal Risk',
    euMinimalDesc: 'Most AI applications (no specific requirements)',
  },

  // European AI page
  europeanAi: {
    title: 'AI Made in Europe',
    description: 'Exploring the growing European AI ecosystem—companies building sovereign, open, and privacy-focused AI.',
    intro: 'The European AI Landscape',
    introDesc: 'Europe is emerging as a significant player in the global AI race, with a unique approach that emphasizes data sovereignty, privacy, open-source models, and regulatory compliance. While US and Chinese companies dominate headlines, European AI startups have collectively raised over €13 billion in funding, creating a new generation of AI unicorns built on European values.',
    stat1Title: 'Total Funding',
    stat1Value: '€13.2B+',
    stat1Desc: 'Raised by European AI startups',
    stat2Title: 'Leading Hubs',
    stat2Value: 'FR, DE, UK',
    stat2Desc: 'France, Germany, and UK lead AI development',
    stat3Title: 'Regulatory Edge',
    stat3Value: 'EU AI Act',
    stat3Desc: 'First comprehensive AI law globally',
    keyCompanies: 'Key European AI Companies',
    keyCompaniesDesc: 'Leading organizations shaping European AI',
    focus: 'Focus',
    funding: 'Funding',
    // Company names
    companies: {
      mistral: 'Mistral AI',
      alephAlpha: 'Aleph Alpha',
      kyutai: 'Kyutai',
      poolside: 'Poolside',
      elevenLabs: 'ElevenLabs',
      photoroom: 'Photoroom',
      lightOn: 'LightOn',
      sana: 'Sana',
      deepL: 'DeepL',
    },
    // Countries
    countries: {
      france: 'France',
      germany: 'Germany',
      franceParis: 'France / US',
      ukPoland: 'UK / Poland',
      sweden: 'Sweden',
    },
    // Focus areas
    focuses: {
      mistralFocus: 'Open-weight LLMs',
      alephAlphaFocus: 'Enterprise AI, Sovereignty',
      kyutaiFocus: 'Real-time Speech AI',
      poolsideFocus: 'AI-powered Coding',
      elevenLabsFocus: 'Voice AI & Speech Synthesis',
      photoroomFocus: 'AI Image Editing',
      lightOnFocus: 'Enterprise GenAI Platform',
      sanaFocus: 'Enterprise AI Agents',
      deepLFocus: 'AI Translation & Language',
    },
    // Descriptions
    descriptions: {
      mistralDesc: 'Founded by ex-DeepMind and Meta researchers, Mistral builds open-weight models that compete with proprietary alternatives. Their Le Chat app offers ultra-fast inference up to 1,000 words/second.',
      alephAlphaDesc: 'German pioneer focusing on enterprise AI with strong data sovereignty. The only German LLM provider with BSI C5 certification. Recently pivoted to their Pharia generative AI operating system.',
      kyutaiDesc: 'French non-profit building Moshi, the first fully open-source real-time speech model. Achieves 160ms latency and uses their Mimi codec for 24kHz audio at just 1.1 kbps.',
      poolsideDesc: 'Founded by ex-GitHub CTO, building AI models specifically for code generation. Uses reinforcement learning from code execution for synthetic training data. Backed heavily by Nvidia.',
      elevenLabsDesc: 'Leading voice AI company with highly realistic speech synthesis and voice cloning. Founded by ex-Google and ex-Palantir engineers, now valued at $3.3 billion.',
      photoroomDesc: 'Paris-based AI photo editing platform with hundreds of millions of users. Makes professional-quality visuals accessible without deep design skills.',
      lightOnDesc: 'Europe\'s first publicly listed GenAI startup. Offers on-premises enterprise AI with zero data retention. Developed ModernBERT with over 20 million downloads.',
      sanaDesc: 'Swedish enterprise AI company acquired by Workday for $1.1B in 2025. Their Sana Agents platform enables no-code AI agent building with 100+ enterprise connectors.',
      deepLDesc: 'Cologne-based neural machine translation pioneer founded in 2017. Serves 200,000+ businesses across 228 markets with enterprise-grade translation. Featured in Forbes AI 50 (2025) and considering a $5B IPO.',
    },
    // Funding
    fundings: {
      mistralFunding: '€6.2B total (incl. ASML, Nvidia)',
      alephAlphaFunding: '$500M (Bosch, SAP, HPE)',
      kyutaiFunding: 'Non-profit (Xavier Niel backed)',
      poolsideFunding: '$2B round at $12B valuation',
      elevenLabsFunding: '$281M (a16z, Sequoia)',
      photoroomFunding: 'Series B, $65M+',
      lightOnFunding: 'Public (Euronext Growth)',
      sanaFunding: 'Acquired for $1.1B',
      deepLFunding: '$536M total, $2B valuation',
    },
    // Notable
    notables: {
      mistralNotable: 'Le Chat, Mixtral, Open weights',
      alephAlphaNotable: 'Pharia OS, BSI C5 certified',
      kyutaiNotable: 'Moshi, MoshiVis, Open source',
      poolsideNotable: 'Project Horizon, 40K+ GPUs',
      elevenLabsNotable: 'Voice cloning, AI dubbing',
      photoroomNotable: 'Background removal, Product photos',
      lightOnNotable: 'ModernBERT, On-prem deployment',
      sanaNotable: 'Sana Agents, Workday acquisition',
      deepLNotable: 'Forbes AI 50, DeepL Agent, 1,257 employees',
    },
    // EU AI Act section
    euAiAct: 'The EU AI Act Advantage',
    euAiActDesc: 'The EU AI Act is the world\'s first comprehensive legal framework for AI. European companies are designing their AI to meet these standards from day one, creating a regulatory "home-field advantage" while foreign providers must adapt to operate in Europe.',
    advantage1Title: 'Built-in Compliance',
    advantage1Desc: 'European AI companies design for GDPR and the AI Act from the start, making them attractive for privacy-conscious enterprise customers.',
    advantage2Title: 'Data Sovereignty',
    advantage2Desc: 'On-premises deployment options let sensitive data stay within organizational or national boundaries—crucial for government and defense.',
    advantage3Title: 'Multilingual Focus',
    advantage3Desc: 'European models are designed for multilingual use from the ground up, serving diverse European languages and markets effectively.',
    advantage4Title: 'Ethical AI Leadership',
    advantage4Desc: 'Europe\'s emphasis on responsible AI development positions its companies as trusted partners for organizations prioritizing ethics.',
    // Open Source section
    openSource: 'Open Source & Open Weights',
    openSourceDesc: 'Many European AI companies embrace open-source principles, releasing model weights and code under permissive licenses. This transparency builds trust, enables customization, and supports the broader AI research community.',
    model1Desc: 'Open-weight LLMs that can be deployed privately and customized',
    model2Desc: 'Fully open-source speech model with Apache 2.0 code and CC BY 4.0 weights',
    model3Desc: 'State-of-the-art encoder model with 20M+ downloads',
    model4Desc: 'Latvian 30B parameter open model trained on EuroHPC LUMI supercomputer',
    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Europe is building AI with a unique emphasis on sovereignty, privacy, and open-source principles',
    takeaway2: 'French startups like Mistral and Kyutai are leading in open-weight and open-source AI models',
    takeaway3: 'The EU AI Act creates both challenges and opportunities—European companies have a compliance advantage',
    takeaway4: 'While US companies lead in scale, European AI excels in enterprise trust, multilingual support, and regulatory alignment',
  },

  // Open Source Advantages page
  openSource: {
    title: 'Open Source Advantages',
    description: 'Why open source matters in AI—transparency, community, cost, innovation speed, vendor independence, and security through auditing.',

    // Introduction
    intro: 'Why Open Source Matters in AI',
    introDesc: 'Open source AI has fundamentally transformed how artificial intelligence is developed, deployed, and improved. From foundational models like LLaMA to specialized tools like Hugging Face Transformers, the open source movement has democratized access to cutting-edge AI technology and created a vibrant ecosystem of innovation.',

    // Key Advantages Section
    advantagesTitle: 'Key Advantages of Open Source AI',
    advantagesDesc: 'Open source AI offers unique benefits that closed, proprietary systems cannot match.',

    advantage1Title: 'Transparency',
    advantage1Desc: 'Full visibility into model architecture, training data, and weights. You can audit how decisions are made and verify safety properties.',

    advantage2Title: 'Community Innovation',
    advantage2Desc: 'Thousands of contributors improve models, fix bugs, and create derivatives. The collective intelligence of the community accelerates progress.',

    advantage3Title: 'Cost Efficiency',
    advantage3Desc: 'No licensing fees or per-token API costs. Run models on your own infrastructure with predictable, controllable expenses.',

    advantage4Title: 'Innovation Speed',
    advantage4Desc: 'Open models can be fine-tuned, merged, and adapted rapidly. New techniques propagate through the community in days, not months.',

    advantage5Title: 'Vendor Independence',
    advantage5Desc: 'No lock-in to specific providers. Switch between models, hosting options, or combine multiple models freely.',

    advantage6Title: 'Security Through Auditing',
    advantage6Desc: 'Thousands of eyes review the code. Vulnerabilities are found and fixed faster than in closed systems.',

    // Notable Projects Section
    projectsTitle: 'Notable Open Source AI Projects',
    projectsDesc: 'Key projects driving the open source AI revolution in 2025',

    project1Name: 'DeepSeek R1',
    project1Org: 'DeepSeek',
    project1Desc: 'Chinese reasoning model matching GPT-4 performance at a fraction of the cost. Trained for under $6M, demonstrating efficient scaling.',

    project2Name: 'Qwen Series',
    project2Org: 'Alibaba',
    project2Desc: 'Now the most-downloaded open models globally. Qwen2.5 offers sizes from 0.5B to 72B with strong multilingual capabilities.',

    project3Name: 'Llama 3.3 70B',
    project3Org: 'Meta',
    project3Desc: 'Meta\'s latest release matching GPT-4 on many benchmarks. Continues the LLaMA legacy with improved reasoning and coding.',

    project4Name: 'Mistral / Mixtral',
    project4Org: 'Mistral AI',
    project4Desc: 'European open-weight models known for efficiency. Mixtral pioneered open mixture-of-experts architecture.',

    project5Name: 'Hugging Face Transformers',
    project5Org: 'Hugging Face',
    project5Desc: 'The de facto library for working with transformer models. Hosts over 1 million models and datasets.',

    project6Name: 'Ollama',
    project6Org: 'Ollama',
    project6Desc: 'Simple tool for running LLMs locally. One-command setup for dozens of open models including all latest releases.',

    // 2025 Trends Section
    trendsTitle: '2025 Model Landscape Trends',
    trendsDesc: 'The open source AI landscape has shifted dramatically in 2025, with open models closing the gap on proprietary systems.',

    trend1Title: 'Performance Gap Narrowing',
    trend1Desc: 'The gap between open and closed source models has narrowed to approximately 1.7%, making open models viable for most production use cases.',

    trend2Title: 'Chinese Models Dominate Downloads',
    trend2Desc: 'Chinese models like Qwen and DeepSeek now lead global downloads, shifting the open source AI landscape toward Asia.',

    trend3Title: '20-32B Parameter Sweet Spot',
    trend3Desc: 'Models in the 20-32B parameter range are emerging as optimal for consumer hardware, balancing capability with accessibility.',

    trend4Title: 'Small Language Models (SLMs)',
    trend4Desc: 'Sub-3B parameter models optimized for edge devices and smartphones are enabling on-device AI without cloud dependencies.',

    // Business Perspective Section
    businessTitle: 'When to Choose Open Source',
    businessDesc: 'Strategic considerations for organizations evaluating open source AI',

    businessCase1Title: 'Data Sovereignty',
    businessCase1Desc: 'When data cannot leave your infrastructure due to regulations, privacy, or competitive concerns.',

    businessCase2Title: 'Customization Needs',
    businessCase2Desc: 'When you need to fine-tune models on proprietary data or adapt them for specialized domains.',

    businessCase3Title: 'Cost at Scale',
    businessCase3Desc: 'When API costs would exceed self-hosting costs, typically at high volume usage.',

    businessCase4Title: 'Offline or Edge Deployment',
    businessCase4Desc: 'When models need to run without internet connectivity or on edge devices.',

    // Considerations
    considerTitle: 'Considerations',
    consider1: 'Self-hosting requires infrastructure expertise and compute resources',
    consider2: 'Open models may lag behind proprietary models in raw capability',
    consider3: 'Support comes from community rather than vendor contracts',
    consider4: 'Fine-tuning requires ML expertise and quality training data',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Open source AI provides transparency, customization, and freedom from vendor lock-in',
    takeaway2: 'The community-driven model accelerates innovation through collaboration and rapid iteration',
    takeaway3: 'For organizations with data sovereignty requirements, open source may be the only viable option',
    takeaway4: 'The gap between open and closed source models continues to narrow as the ecosystem matures',
    takeaway5: 'Choosing between open and closed source depends on your specific needs for control, capability, and resources',
  },

  // Metadata
  metadata: {
    title: 'Learn AI Concepts | Interactive Guide',
    description: 'Master artificial intelligence and large language model concepts through beautiful, interactive demonstrations.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Control Panel',
    adjustTemperature: 'Adjust the temperature',
    temperature: 'Temperature',
    samplePrompt: 'Sample Prompt',
    onceUponATime: '"Once upon a time, there was..."',
    liveCompletion: 'Live Completion',
    regenerate: 'Regenerate',
    deterministic: 'Deterministic',
    balanced: 'Balanced',
    creative: 'Creative',
    chaotic: 'Chaotic',
    frozen: 'Frozen',
    focused: 'Focused',
    wild: 'Wild',
    greedyMode: 'Greedy mode: Always picks the single most probable token.',
    lowTemp: 'Low temperature: Focused on high-probability continuations.',
    balancedTemp: 'Balanced: Natural mix of predictability and variety.',
    highTemp: 'High temperature: Exploring creative, less common word choices.',
    veryHighTemp: 'Very high: Probability distribution is nearly uniform—expect chaos!',

    // Temperature Visualizer
    probabilityDistribution: 'Probability Distribution',
    nextTokenProbs: 'Next token probabilities',
    greedySamplingTitle: 'Greedy Sampling (T=0):',
    greedySamplingDesc: 'Always picks the highest probability token. No randomness—output is 100% deterministic.',
    focusedSamplingTitle: 'Focused Sampling:',
    focusedSamplingDesc: 'Distribution is sharpened. The model samples randomly, but high-probability tokens are much more likely to be chosen.',
    balancedSamplingTitle: 'Balanced Sampling (T≈1):',
    balancedSamplingDesc: 'Natural distribution. The model samples from probabilities as-is—"the" may still be most likely, but "a" has a fair chance too.',
    flatSamplingTitle: 'Flat Sampling:',
    flatSamplingDesc: 'Probabilities become more uniform. Even though one token may still be "best", sampling means any token has a real chance of being picked—hence the chaos!',

    // Temperature Demo Completions (for "Once upon a time, there was...")
    completion0_0: 'the king who ruled the land with wisdom and justice.',
    completionBold0_0: 'the',
    completion04_0: 'a small village nestled between the mountains.',
    completionBold04_0: 'a',
    completion04_1: 'the ancient kingdom of forgotten dreams.',
    completionBold04_1: 'the',
    completion08_0: 'a peculiar inventor named Elara.',
    completionBold08_0: 'a',
    completion08_1: 'one curious cat who spoke in riddles.',
    completionBold08_1: 'one',
    completion08_2: 'the last lighthouse keeper of the void.',
    completionBold08_2: 'the',
    completion12_0: 'whispers that painted the starlight backwards.',
    completionBold12_0: 'whispers',
    completion12_1: 'seventeen moons dancing through crystal waterfalls.',
    completionBold12_1: 'seventeen',
    completion12_2: 'every shadow that learned to sing in colors.',
    completionBold12_2: 'every',
    completion16_0: 'quantum butterflies debugging the simulation matrix.',
    completionBold16_0: 'quantum',
    completion16_1: 'recursive echoes of unwritten tomorrows cascading through probability fog.',
    completionBold16_1: 'recursive',
    completion16_2: 'effervescent consciousness manifesting as crystallized thunder.',
    completionBold16_2: 'effervescent',
    completion20_0: 'banana philosophy triangulating the essence of purple velocity.',
    completionBold20_0: 'banana',
    completion20_1: 'fragmented synaptic overflow encoding marmalade dimensions.',
    completionBold20_1: 'fragmented',

    // Sampling explanation
    whySamplingMatters: 'Why different outputs?',
    samplingExplanation: 'The bars show probability, but the model doesn\'t always pick the tallest bar. It samples randomly—like rolling a weighted die. Higher temperature = more equal weights = more unpredictable rolls. Click "Regenerate" to sample again!',
    
    // Context Rot Simulator
    setInstruction: 'Set Your System Instruction',
    persistInstruction: 'This should persist throughout the conversation',
    systemPrompt: 'System Prompt',
    quickExamples: 'Quick Examples',
    startSimulation: 'Start Simulation',
    contextOverflow: 'Context Overflow!',
    conversation: 'Conversation',
    messagesPushed: 'messages pushed out of window',
    messages: 'messages',
    overflowIt: 'Overflow It!',
    reset: 'Reset',
    typeMessage: 'Type a message...',
    addMessage: 'Add Message',
    systemInstructionLost: 'System Instruction Lost!',
    systemLostDesc: 'Your system instruction has been pushed completely out of the context window. The model can no longer see it at all—it\'s as if you never gave the instruction. This is the worst case of context rot: total amnesia.',
    contextFilling: 'Context Filling Up',
    contextFillingDesc: 'Your system instruction is losing influence as newer messages take priority. Notice how it\'s fading visually—this represents the model\'s waning attention to it.',
    exampleFrench: 'Always respond in French.',
    examplePirate: 'You are a pirate. Say \'Arrr\' a lot.',
    exampleHaiku: 'End every response with a haiku.',
    labelFrench: 'Speak French',
    labelPirate: 'Be a Pirate',
    labelHaiku: 'End with Haiku',

    // Attention Visualizer
    hoverToSee: 'Hover to see attention weights',
    token: 'Token',
    attentionScore: 'Attention Score',
    strongConnection: 'Strong Connection',
    weakConnection: 'Weak Connection',

    // Patch Grid Visualizer
    originalImage: 'Original Image',
    patchGrid: 'Patch Grid',
    flattenedPatches: 'Flattened Patches',
    transformerInput: 'Transformer Input',
    processDesc: 'The image is split into a fixed grid of patches (e.g., 16x16 pixels). Each patch is then flattened into a vector and linearly projected into an embedding space.',

    // Agent Loop Visualizer
    startLoop: 'Start Loop',
    step: 'Step',
    context: 'Context',
    llmResponse: 'LLM Response',
    toolExecution: 'Tool Execution',
    finalAnswer: 'Final Answer',
    system: 'System',
    user: 'User',
    assistant: 'Assistant',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Plan & Execute',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflection',
    patternDesc: 'Select a pattern to see how it structures the agent\'s workflow.',

    // Tokenizer Demo
    enterText: 'Enter text to tokenize',
    sampleText: 'The quick brown fox jumps over the lazy dog.',
    tokens: 'Tokens',
    characters: 'Characters',
    tokensPerChar: 'Tokens per character',
    tokenBreakdown: 'Token Breakdown',
    commonTokens: 'Common tokens are single pieces',
    rareTokens: 'Rare words get split into subwords',

    // Embedding Visualizer
    selectWords: 'Select words to visualize',
    clickToToggle: 'Click words to add/remove from visualization',
    wordsActive: 'words shown',
    comparing: 'Selected',
    similarityScore: 'Similarity Score',
    dimensions: 'Dimensions',
    nearestNeighbors: 'Nearest Neighbors',
    vectorSpace: 'Vector Space (3D)',

    // RAG Pipeline Visualizer
    enterQuery: 'Enter your query',
    sampleQuery: 'What is the capital of France?',
    retrieving: 'Retrieving...',
    retrieved: 'Retrieved Documents',
    relevanceScore: 'Relevance',
    generating: 'Generating response...',
    augmentedContext: 'Augmented Context',

    // Tool Schema Builder
    toolName: 'Tool Name',
    toolDescription: 'Description',
    addParameter: 'Add Parameter',
    paramName: 'Parameter Name',
    paramType: 'Type',
    paramRequired: 'Required',
    generatedSchema: 'Generated Schema',
    validateSchema: 'Validate Schema',

    // Memory System Visualizer
    shortTermMemory: 'Short-Term Memory',
    longTermMemory: 'Long-Term Memory',
    memoryCapacity: 'Capacity',
    memoryUsage: 'Usage',
    addMemory: 'Add Memory',
    recallMemory: 'Recall',
    memoriesStored: 'memories stored',

    // Workflow Visualizer
    addNode: 'Add Node',
    connectNodes: 'Connect Nodes',
    runWorkflow: 'Run Workflow',
    nodeTypes: 'Node Types',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Condition',

    // Neural Network Visualizer
    inputLayer: 'Input Layer',
    hiddenLayer: 'Hidden Layer',
    outputLayer: 'Output Layer',
    addLayer: 'Add Layer',
    removeLayer: 'Remove Layer',
    neurons: 'Neurons',
    activation: 'Activation',
    forward: 'Forward',

    // Gradient Descent Visualizer
    startDescent: 'Start Descent',
    pauseDescent: 'Pause',
    resetDescent: 'Reset',
    learningRate: 'Learning Rate',
    currentLoss: 'Current Loss',
    iterations: 'Iterations',
    globalMinimum: 'Global Minimum',
    localMinimum: 'Local Minimum',

    // Training Progress Visualizer
    startTraining: 'Start Training',
    stopTraining: 'Stop',
    epoch: 'Epoch',
    trainingLoss: 'Training Loss',
    validationLoss: 'Validation Loss',
    accuracy: 'Accuracy',
    overfitting: 'Overfitting detected',

    // Prompt Comparison Demo
    weakPrompt: 'Weak Prompt',
    strongPrompt: 'Strong Prompt',
    compare: 'Compare',
    promptQuality: 'Quality Score',
    improvements: 'Improvements',

    // Chain of Thought Demo
    withoutCot: 'Without Chain of Thought',
    withCot: 'With Chain of Thought',
    reasoningSteps: 'Reasoning Steps',
    showSteps: 'Show Steps',

    // Bias Detection Demo
    testInput: 'Test Input',
    analyzeForBias: 'Analyze for Bias',
    biasIndicators: 'Bias Indicators',
    fairnessScore: 'Fairness Score',
    recommendations: 'Recommendations',

    // System Prompt Builder
    templatePresets: 'Template Presets',
    chooseTemplate: 'Start with a preset or build from scratch',
    presetCoding: 'Coding Assistant',
    presetSupport: 'Customer Support',
    presetResearch: 'Research Assistant',
    clearAll: 'Clear All',
    enabled: 'Enabled',
    notConfigured: 'Not configured',
    livePreview: 'Live Preview',
    estimated: '(estimated)',
    copyPrompt: 'Copy Prompt',
    copied: 'Copied!',
    startBuilding: 'Start adding content to sections above to build your system prompt',
    tokenEstimateNote: 'Token count is approximate. Actual count may vary by ~20% depending on the model.',
    placeholderIdentity: 'Who is the AI? Define its role, expertise, and personality...',
    placeholderCapabilities: 'What can the AI do? List its abilities and available tools...',
    placeholderLimitations: 'What should the AI avoid or refuse? Set clear boundaries...',
    placeholderGuidelines: 'Specific rules for behavior, tone, and response format...',
    tipIdentity: 'Be specific about expertise level and persona. Include relevant background that shapes responses.',
    tipCapabilities: 'List concrete abilities. Use bullet points for clarity. Include any tools or integrations available.',
    tipLimitations: 'Explicitly state what the AI should never do. Cover security, privacy, and ethical boundaries.',
    tipGuidelines: 'Include formatting preferences, tone requirements, and domain-specific rules.',
  },

  // Skill Composer Demo
  skillComposer: {
    // Tab names
    triggerTab: 'Skill Triggering',
    manifestTab: 'Skill Manifest',
    chainingTab: 'Skill Chaining',

    // Trigger tab
    userInput: 'User Input',
    userInputDesc: 'Type a message to see which skill would be triggered',
    inputPlaceholder: 'Try: "Can you review this code for bugs?"',
    tryExamples: 'Try these examples:',
    exampleReview: 'Review this code for bugs',
    exampleDocs: 'Write documentation for this API',
    exampleGit: 'Create a pull request',
    exampleExplain: 'Explain how this function works',

    // Match result
    matchResult: 'Skill Match',
    matchResultDesc: 'The skill that would be triggered',
    analyzing: 'Analyzing input...',
    confidence: 'confidence',
    matchedTrigger: 'Matched trigger',
    canChainWith: 'Can chain with:',
    noMatch: 'Type a message to see skill matching in action',

    // Skills
    codeReviewSkill: 'Code Review',
    codeReviewDesc: 'Performs thorough code reviews following team standards',
    documentationSkill: 'Documentation',
    documentationDesc: 'Generates comprehensive documentation for code and APIs',
    gitWorkflowSkill: 'Git Workflow',
    gitWorkflowDesc: 'Handles Git operations and pull request workflows',
    explainSkill: 'Explain Code',
    explainDesc: 'Explains code functionality and concepts clearly',

    availableSkills: 'Available Skills',

    // Manifest tab
    skillManifest: 'Skill Manifest',
    skillManifestDesc: 'The SKILL.md file defines a skill\'s metadata and instructions',
    manifestInstructions: 'Detailed instructions for how to perform this skill go here...',

    // Manifest fields
    nameField: 'name',
    nameFieldDesc: 'Unique identifier for the skill',
    triggersField: 'triggers',
    triggersFieldDesc: 'Keywords and phrases that activate this skill',
    descriptionField: 'description',
    descriptionFieldDesc: 'Brief summary of what the skill does',
    chainsWithField: 'chains_with',
    chainsWithFieldDesc: 'Other skills this skill can invoke',

    // Chaining tab
    skillChaining: 'Skill Chaining',
    skillChainingDesc: 'Skills can invoke other skills to complete complex tasks',
    selectToSeeChain: 'Click a skill to see what it can chain to:',
    noChains: 'This skill has no chain targets',
    chainingExample: 'Example: Code Review + Git Workflow',
    chainingStep1: 'User asks: "Review and merge this PR"',
    chainingStep2: 'Code Review skill performs the review',
    chainingStep3: 'Git Workflow skill is chained to handle the merge',
  },

  // Logge's Favourite Models page (Opus 4.6 + GPT-5.3-Codex)
  favModels: {
    title: "Logge's Favourite Models",
    description: "Two models dropped on the same day. Both became instant favourites. Here's an honest, slightly obsessive breakdown.",
    disclaimer: "Disclaimer",
    disclaimerText: "This page is unapologetically biased. I use both of these models daily, I pay for both out of pocket, and I have strong opinions. Model releases move fast, so this will change. Side effects of reading may include API key generation and wallet anxiety.",
    lastUpdated: "Last updated: February 5, 2026",

    // Hero / intro
    heroTitle: "Two Models, One Obsession",
    heroSubtitle: "On February 5, 2026, both Anthropic and OpenAI dropped their latest flagships within hours of each other. I've been using both non-stop since. Here's the honest breakdown from someone who actually ships code with these things.",

    // The Two Champions section
    championsTitle: "The Two Champions",
    championsSubtitle: "Different philosophies, both excellent. Here's what each one brings to the table.",

    // Claude Opus 4.6 card
    opusName: "Claude Opus 4.6",
    opusMaker: "Anthropic",
    opusTagline: "The deep thinker that codes like a senior engineer",
    opusModelId: "claude-opus-4-6",
    opusReleaseDate: "February 5, 2026",
    opusContext: "200K standard / 1M beta",
    opusOutput: "128K tokens",
    opusPricing: "$5 / $25 per million tokens",
    opusDescription: "Opus 4.6 is Anthropic's most capable model ever. It doubled the output limit to 128K, introduced a 1M token context window in beta, and brought two exclusive features: Adaptive Thinking (auto-adjusts reasoning depth) and Context Compaction (auto-summarizes old context for endless conversations). The coding improvements are massive — Terminal-Bench jumped from 59.8% to 65.4%, OSWorld from 66.3% to 72.7%, and ARC AGI 2 nearly doubled from 37.6% to 68.8%.",

    opusStrength1Title: "Adaptive Thinking",
    opusStrength1Desc: "Dynamically adjusts reasoning depth based on task complexity. Four intensity levels: low, medium, high, and max. It decides when to think deeper without you asking.",
    opusStrength2Title: "Agent Teams",
    opusStrength2Desc: "Powers multi-agent coding in Claude Code — one agent on frontend, another on API, a third on migration — all coordinating autonomously.",
    opusStrength3Title: "1M Token Context",
    opusStrength3Desc: "First Opus model with a million-token window. Feed it an entire codebase and it can reason across all of it.",
    opusStrength4Title: "128K Output",
    opusStrength4Desc: "Doubled from 64K. It can generate entire files, full test suites, and multi-page documents in a single response.",

    opusSourceLabel: "Source: Anthropic Opus page",
    opusSourceUrl: "https://www.anthropic.com/claude/opus",

    // GPT-5.3-Codex card
    codexName: "GPT-5.3-Codex",
    codexMaker: "OpenAI",
    codexTagline: "The fast pragmatist that helped build itself",
    codexModelId: "gpt-5.3-codex",
    codexReleaseDate: "February 5, 2026",
    codexContext: "~400K tokens",
    codexOutput: "~128K tokens",
    codexPricing: "~$1.25 / $10 per million tokens (expected)",
    codexDescription: "GPT-5.3-Codex is OpenAI's first 'self-developing' model — early versions were used to debug their own training run. It unifies frontier coding performance (from GPT-5.2-Codex) with professional reasoning (from GPT-5.2) into a single model. It's 25% faster than its predecessor, uses half the tokens for equivalent tasks, and absolutely dominates Terminal-Bench 2 at 77.3%.",

    codexStrength1Title: "Interactive Steering",
    codexStrength1Desc: "You can interact with it while it's working — ask questions, discuss approaches, and steer toward solutions in real time. It gives frequent progress updates.",
    codexStrength2Title: "Self-Developing",
    codexStrength2Desc: "First model that was instrumental in building itself. Used internally to debug training, manage deployment, and optimize its own evaluation harness.",
    codexStrength3Title: "Token Efficient",
    codexStrength3Desc: "Accomplishes results with less than half the tokens of predecessors. Your context budget goes further.",
    codexStrength4Title: "Personality Modes",
    codexStrength4Desc: "Choose between 'Pragmatic' (terse, to-the-point) and 'Friendly' (conversational). No capability difference — purely style.",

    codexSourceLabel: "Source: OpenAI GPT-5.3-Codex launch",
    codexSourceUrl: "https://openai.com/index/introducing-gpt-5-3-codex/",

    // Benchmark Showdown section
    benchmarkTitle: "The Benchmark Showdown",
    benchmarkSubtitle: "Numbers don't lie, but they don't tell the whole story either. Here's how they stack up on the benchmarks that actually matter for coding.",

    benchSWE: "SWE-bench",
    benchSWEOpus: "80.8%",
    benchSWECodex: "56.8%",
    benchSWENote: "Verified vs Pro (different test sets — not directly comparable)",

    benchTerminal: "Terminal-Bench 2",
    benchTerminalOpus: "65.4%",
    benchTerminalCodex: "77.3%",
    benchTerminalNote: "Codex dominates real terminal workflows",

    benchOSWorld: "OSWorld",
    benchOSWorldOpus: "72.7%",
    benchOSWorldCodex: "64.7%",
    benchOSWorldNote: "Opus leads on desktop automation tasks",

    benchGPQA: "GPQA Diamond",
    benchGPQAOpus: "91.3%",
    benchGPQACodex: "—",
    benchGPQANote: "Graduate-level science reasoning",

    benchARC: "ARC AGI 2",
    benchARCOpus: "68.8%",
    benchARCCodex: "—",
    benchARCNote: "Novel problem-solving (nearly 2x vs Opus 4.5)",

    benchHLE: "Humanity's Last Exam",
    benchHLEOpus: "40.0%",
    benchHLECodex: "—",
    benchHLENote: "The hardest test in AI — no tools",

    benchCyber: "Cybersecurity CTF",
    benchCyberOpus: "—",
    benchCyberCodex: "77.6%",
    benchCyberNote: "Capture-the-flag security challenges",

    benchDisclaimer: "Dash means the benchmark wasn't reported by the vendor. SWE-bench Verified and SWE-bench Pro use different test sets, so direct comparison isn't meaningful.",

    // When I Use Each section
    whenTitle: "When I Actually Use Each One",
    whenSubtitle: "Theory is nice. Here's my actual workflow after using both daily since launch.",

    whenOpusTitle: "I reach for Opus 4.6 when...",
    whenOpusToolLabel: "My tool: Claude Code (CLI)",
    whenOpusToolDesc: "Opus 4.6 lives in my terminal via Claude Code. Agent teams, multi-file edits, deep reasoning — all from the command line. This entire site was built with it.",
    whenOpus1: "I need deep architectural reasoning across a large codebase — the 1M context window is unmatched",
    whenOpus2: "Writing complex multi-file features where the model needs to hold a lot of state",
    whenOpus3: "Code review and refactoring — Adaptive Thinking makes it genuinely careful",
    whenOpus4: "Agent teams for ambitious multi-part projects",
    whenOpus5: "Anything that benefits from extended thinking and careful step-by-step reasoning",

    whenCodexTitle: "I reach for GPT-5.3-Codex when...",
    whenCodexToolLabel: "My tool: Codex (App + CLI)",
    whenCodexToolDesc: "GPT-5.3-Codex powers the Codex app and CLI. Interactive steering mid-task, personality modes, and blazing speed make it perfect for rapid iteration.",
    whenCodex1: "Quick iteration on terminal-heavy workflows — it's blazing fast and Terminal-Bench scores show why",
    whenCodex2: "Interactive pair programming where I want to steer mid-task",
    whenCodex3: "High-volume tasks where token efficiency matters for cost",
    whenCodex4: "The Codex CLI for rapid scripting and one-shot tasks",
    whenCodex5: "Anything where I want speed over depth — it's 25% faster and it feels like it",

    // Pricing Reality section
    pricingTitle: "The Wallet Situation",
    pricingSubtitle: "Let's talk about the elephant in the room.",
    pricingOpusTitle: "Opus 4.6",
    pricingOpusDetail: "$5 input / $25 output per million tokens. Same price as Opus 4.5 but with massively improved capabilities. Batch API at 50% off. Still premium territory — a heavy coding session can run $5-15.",
    pricingCodexTitle: "GPT-5.3-Codex",
    pricingCodexDetail: "API pricing not final yet, but the GPT-5-Codex family runs ~$1.25 input / $10 output. That's roughly 2.5x cheaper than Opus on input and 2.5x cheaper on output. Plus it uses fewer tokens for equivalent tasks.",
    pricingVerdict: "Honest take: If you're cost-conscious, Codex wins handily. If you need maximum reasoning depth and don't mind paying for it, Opus is worth every cent. I use both because different tasks have different economics.",

    // What They Have in Common section
    commonTitle: "What They Share",
    commonSubtitle: "Despite coming from rival labs, these models have converged on some important traits.",
    common1Title: "Agentic Excellence",
    common1Desc: "Both models are built for agents — tool use, multi-step planning, and autonomous task completion are first-class capabilities.",
    common2Title: "Computer Use",
    common2Desc: "Both can operate GUIs, fill forms, navigate apps. OSWorld scores of 72.7% (Opus) and 64.7% (Codex) show real-world desktop proficiency.",
    common3Title: "Extended Output",
    common3Desc: "~128K token output limits on both. Generate entire codebases, full documentation, multi-file changes in a single response.",
    common4Title: "Released the Same Day",
    common4Desc: "February 5, 2026. Both labs dropped their flagships within hours. The AI coding wars are real, and we developers are the winners.",

    // Personal Verdict section
    verdictTitle: "The Honest Verdict",
    verdictText: "I don't have one favourite model anymore — I have two. Opus 4.6 is the model I trust for deep, careful work. It thinks before it acts, catches things I miss, and handles massive codebases with grace. GPT-5.3-Codex is the model I reach for when I need speed and pragmatism. It's fast, efficient, and the interactive steering feels like genuine pair programming. Together, they cover every coding scenario I encounter. The fact that they launched on the same day feels symbolic — the frontier isn't one model anymore, it's a toolkit. Pick the right tool for the job. Or, like me, use both and enjoy the best era of AI-assisted development we've ever seen.",

    // Quick Reference section
    quickRefTitle: "Quick Reference",
    quickRefModel: "Model",
    quickRefMaker: "Maker",
    quickRefContext: "Context",
    quickRefOutput: "Max Output",
    quickRefPrice: "Pricing (per MTok)",
    quickRefBestFor: "Best For",
    quickRefOpusBest: "Deep reasoning, code review, agent teams",
    quickRefCodexBest: "Fast iteration, terminal tasks, cost efficiency",
    quickRefPlatforms: "Platforms",
    quickRefOpusPlatforms: "Claude.ai, API, AWS Bedrock, Vertex AI, Azure Foundry",
    quickRefCodexPlatforms: "ChatGPT, Codex App, CLI, IDE Extension (API coming soon)",
  },

  // Speculative Decoding page
  speculativeDecoding: {
    title: 'Speculative Decoding',
    description: 'A technique to speed up LLM inference by using a small draft model to propose tokens that are verified in parallel by the target model.',

    whatIs: 'What is Speculative Decoding?',
    whatIsDesc: 'Speculative decoding is an inference optimization technique that accelerates text generation from large language models. Instead of generating tokens one at a time with the large model, a smaller "draft" model quickly proposes multiple candidate tokens, which the larger "target" model then verifies in a single forward pass.',
    whatIsDesc2: 'The key insight is that verification is much cheaper than generation. The target model can check multiple tokens in parallel because it processes all positions simultaneously during a forward pass, while autoregressive generation requires one forward pass per token.',

    problem: 'The Inference Bottleneck',
    problemDesc: 'Standard autoregressive decoding is inherently slow because each token depends on all previous tokens, forcing sequential generation.',
    bottleneck: 'Sequential Dependency',
    bottleneckDesc: 'Each new token requires a full forward pass through the model. For a 70B parameter model, generating 100 tokens means 100 separate forward passes.',
    memoryBound: 'Memory Bandwidth Bound',
    memoryBoundDesc: 'LLM inference is often limited by how fast we can load model weights from memory, not by computation. The GPU sits idle waiting for data.',

    howItWorks: 'How It Works',
    howItWorksDesc: 'Speculative decoding follows a draft-then-verify pattern that exploits the parallel nature of transformer verification.',
    step1Title: 'Draft Generation',
    step1Desc: 'A small, fast draft model (e.g., 7B parameters) generates K candidate tokens autoregressively. This is quick because the draft model is small.',
    step2Title: 'Parallel Verification',
    step2Desc: 'The target model processes the prompt plus all K draft tokens in a single forward pass, computing probabilities for each position.',
    step3Title: 'Token Acceptance',
    step3Desc: 'Each draft token is accepted or rejected by comparing draft and target probabilities. A rejection sampling scheme ensures the output distribution matches the target model exactly.',
    step4Title: 'Continue or Resample',
    step4Desc: 'Accepted tokens are kept. At the first rejection, the target model samples a correction token. The process repeats from the new position.',

    visualExample: 'Visual Example',
    visualExampleDesc: 'Here\'s how speculative decoding processes a simple continuation:',
    examplePrompt: 'Prompt:',
    exampleDraft: 'Draft model proposes 4 tokens:',
    exampleDraftTokens: '"jumps" → "over" → "the" → "lazy"',
    exampleVerify: 'Target model verifies in one pass:',
    accepted: 'accepted',
    rejected: 'rejected, target prefers "sleeping"',
    exampleResult: 'Final output:',
    exampleSavings: '3 tokens accepted + 1 correction = 4 tokens from 2 forward passes instead of 4',

    draftRequirements: 'Draft Model Requirements',
    draftRequirementsDesc: 'The choice of draft model significantly impacts the speedup achieved. The ideal draft model balances speed with alignment to the target.',
    requirement1: 'Much Smaller',
    requirement1Desc: 'The draft model should be 5-10x smaller than the target. A 7B draft for a 70B target, or a 1B draft for a 7B target.',
    requirement2: 'Similar Distribution',
    requirement2Desc: 'Higher acceptance rates come from draft models trained on similar data or distilled from the target model.',
    requirement3: 'Same Vocabulary',
    requirement3Desc: 'Draft and target must share the same tokenizer to ensure token-level compatibility during verification.',
    requirement4: 'Fast Inference',
    requirement4Desc: 'The draft model must be fast enough that drafting K tokens takes less time than K target model forward passes.',

    speedupFactors: 'What Affects Speedup?',
    speedupFactorsDesc: 'Typical speedups range from 2-3x, but several factors influence the actual improvement.',
    factor1: 'Acceptance Rate',
    factor1Impact: 'Higher = more tokens per verification pass',
    factor2: 'Draft Model Speed',
    factor2Impact: 'Faster draft = more attempts possible',
    factor3: 'Target Model Size',
    factor3Impact: 'Larger targets benefit more (more memory-bound)',
    factor4: 'Task Predictability',
    factor4Impact: 'Predictable text (code, structured) = higher acceptance',

    variants: 'Variants and Extensions',
    variantsDesc: 'Researchers have developed several variations to improve upon basic speculative decoding.',
    variant1: 'Self-Speculative Decoding',
    variant1Desc: 'Uses early exit from the target model itself as the draft, eliminating the need for a separate draft model.',
    variant2: 'Medusa',
    variant2Desc: 'Adds multiple prediction heads to the target model to generate draft tokens in parallel, avoiding sequential draft generation.',
    variant3: 'Lookahead Decoding',
    variant3Desc: 'Generates multiple parallel speculation branches using n-gram patterns from the context, no draft model needed.',
    variant4: 'Staged Speculative Decoding',
    variant4Desc: 'Uses a cascade of increasingly larger draft models for better acceptance rates on difficult tokens.',

    // Limitations
    limitations: 'Limitations',
    limitationsDesc: 'While speculative decoding offers significant speedups, it has important constraints that limit when and how it can be applied effectively.',
    limit1: 'Draft Model Overhead',
    limit1Desc: 'You need to run and maintain a separate draft model. This adds memory overhead (the draft model must fit in GPU memory alongside the target) and operational complexity.',
    limit2: 'Diminishing Returns with Batch Size',
    limit2Desc: 'Speculative decoding shines for single-sequence inference. With larger batch sizes, the target model becomes compute-bound rather than memory-bound, reducing the benefit.',
    limit3: 'Variable Speedup',
    limit3Desc: 'Speedup depends heavily on acceptance rate, which varies by task. Creative writing with high temperature may see little benefit, while structured code generation benefits greatly.',
    limit4: 'Implementation Complexity',
    limit4Desc: 'Correct rejection sampling is tricky to implement. Naive implementations can produce outputs that differ from the target model\'s true distribution.',
    limit5: 'Not Always Faster',
    limit5Desc: 'If the draft model is too slow, too inaccurate, or the target model is already fast enough, speculative decoding can actually be slower than standard decoding.',

    // Visualizer
    vizTitle: 'Interactive Simulation',
    vizSubtitle: 'Watch speculative decoding in action',
    vizStart: 'Start',
    vizPause: 'Pause',
    vizReset: 'Reset',
    vizAcceptanceRate: 'Draft-Target Alignment',
    vizLowMatch: 'Low match',
    vizHighMatch: 'High match',
    vizDraftSpeed: 'Draft Speed',
    vizSlower: 'Slower',
    vizFaster: 'Faster',
    vizPrompt: 'Prompt:',
    vizDraftModel: 'Draft Model (small, fast)',
    vizTargetModel: 'Target Model (large, accurate)',
    vizGenerating: 'generating...',
    vizVerifying: 'verifying batch...',
    vizWaiting: 'Waiting to generate...',
    vizNoTokens: 'No verified tokens yet',
    vizIdle: 'Idle',
    vizDrafting: 'Drafting',
    vizVerifyPhase: 'Verify',
    vizComplete: 'Complete',
    vizAccepted: 'Accepted',
    vizRejected: 'Rejected',
    vizPasses: 'Target Passes',
    vizVsStandard: 'vs. standard:',
    vizSpeedup: 'Speedup',
    vizActualRate: 'Actual rate',
    vizExplanation: 'The draft model quickly proposes tokens (purple). The target model verifies them in a single pass (cyan). Accepted tokens are green, corrections are orange. Higher draft-target alignment means more accepted tokens and better speedup.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Speculative decoding uses a small draft model to propose tokens that are verified in parallel by the target model',
    takeaway2: 'It provides 2-3x speedups while producing identical outputs to standard decoding',
    takeaway3: 'The technique exploits the fact that transformer verification is parallel while generation is sequential',
    takeaway4: 'Effectiveness depends on draft-target alignment: similar models and predictable tasks yield higher acceptance rates',
  },

  // KV Cache page
  kvCache: {
    title: 'KV Cache',
    description: 'How storing computed key-value pairs dramatically speeds up autoregressive text generation in transformers.',
    whatIs: 'What is KV Cache?',
    whatIsDesc: 'During autoregressive generation, a transformer must compute attention over all previous tokens for every new token it generates. The KV Cache stores the Key and Value projections from previous tokens so they don\'t need to be recomputed. This turns generation from O(n²) to O(n) in computation per step — a massive speedup for long sequences.',
    keyCache: 'Key Cache',
    keyCacheDesc: 'Stores the Key projections for each token at each layer. These are used to compute attention scores between the new token and all previous tokens.',
    valueCache: 'Value Cache',
    valueCacheDesc: 'Stores the Value projections for each token at each layer. Once attention weights are computed, these cached values are used to produce the output.',
    whyMatters: 'Why It Matters',
    whyMattersDesc: 'Without KV caching, generating each new token would require reprocessing the entire sequence through every attention layer. For a 4096-token sequence, that means 4096× redundant computation per token.',
    benefit1Title: 'Speed',
    benefit1Desc: 'Avoids recomputing attention for all previous tokens at every step. Generation goes from quadratic to linear.',
    benefit2Title: 'Incremental',
    benefit2Desc: 'Each new token only needs to compute its own Q, K, V and attend to the cached K, V from previous positions.',
    benefit3Title: 'Trade-off',
    benefit3Desc: 'Trades GPU memory for compute time. The cache grows linearly with sequence length and model depth.',
    interactiveTitle: 'Interactive KV Cache Explorer',
    interactiveSubtitle: 'Watch the cache grow token by token',
    interactiveDesc: 'Step through autoregressive generation to see how the KV cache accumulates. Compare the computation cost with and without caching — the savings become dramatic as sequences get longer.',
    memoryTitle: 'Memory Implications',
    memoryDesc: 'The KV cache is the primary memory bottleneck during inference. For large models with long contexts, it can consume tens of gigabytes of GPU memory.',
    memoryFormula: 'Factor of 2 for K and V, dtype_size is 2 bytes for FP16. For Llama 3 70B (80 layers, 8 KV heads, 128 d_head) at 8K context: ~2.5 GB per request.',
    optimizationsTitle: 'Optimization Techniques',
    mqaGqaTitle: 'Multi-Query & Grouped-Query Attention (MQA/GQA)',
    mqaGqaDesc: 'Instead of separate K/V heads per attention head, MQA shares a single K/V head across all query heads, while GQA uses a few shared groups. This reduces KV cache size by 4-32× with minimal quality loss. Llama 3 and Mistral use GQA. See the Attention Mechanism article for more details.',
    slidingWindowTitle: 'Sliding Window Attention',
    slidingWindowDesc: 'Instead of caching all tokens, only keep the most recent W tokens in the cache. Used by Mistral models. Reduces memory from O(seq_len) to O(W), but limits the model\'s ability to attend to very early tokens.',
    pagedAttentionTitle: 'Paged Attention (vLLM)',
    pagedAttentionDesc: 'Inspired by virtual memory in operating systems. Instead of allocating contiguous memory for each sequence\'s KV cache, vLLM manages cache in fixed-size "pages" that can be allocated and freed dynamically. This eliminates memory fragmentation and enables efficient batching of requests with different sequence lengths.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'KV cache stores Key and Value projections from previous tokens, avoiding redundant computation during generation',
    takeaway2: 'Without KV cache, each new token requires O(n) attention computation over the full sequence; with it, only the new token\'s K/V is computed',
    takeaway3: 'KV cache memory grows linearly with sequence length × layers × KV heads — this is the main inference memory bottleneck',
    takeaway4: 'Techniques like GQA, sliding window, and paged attention address the memory cost while preserving generation speed',
  },

  // Batching & Throughput page
  batching: {
    title: 'Batching & Throughput',
    description: 'How processing multiple requests simultaneously transforms GPU utilization and inference economics.',

    // Section 1: What is Batching
    whatIs: 'What is Batching in LLM Inference?',
    whatIsDesc: 'When a single user sends a prompt to an LLM, the GPU processes just one request — using only a fraction of its compute capacity. Batching combines multiple requests and processes them simultaneously on the same GPU, dramatically increasing throughput.',
    staticTitle: 'Static Batching',
    staticDesc: 'All requests in the batch start and finish together. The GPU waits for the slowest request to complete before any new requests can join. Simple but wasteful — shorter requests sit idle.',
    dynamicTitle: 'Dynamic / Continuous Batching',
    dynamicDesc: 'Requests can join and leave the batch at any time. When one request finishes, a new one immediately takes its slot. This "Orca-style" approach maximizes GPU utilization.',
    wasteTitle: 'Why Single-Request Inference Wastes Resources',
    wasteDesc: 'A modern GPU like the A100 has 312 TFLOPS of compute and 2 TB/s of memory bandwidth. A single decode step for one request barely scratches the surface — the GPU spends most of its time waiting for memory, not computing. Batching fills this gap by sharing the memory transfer cost across many requests.',

    // Section 2: Throughput visualization
    throughputTitle: 'Throughput vs Batch Size',
    throughputSubtitle: 'Drag the slider to see how batching affects performance',
    throughputExplain: 'As batch size increases, total throughput rises steeply at first — then plateaus — then COLLAPSES. The rise comes from amortizing weight-loading costs across more requests. The collapse happens when memory is exhausted: KV caches overflow VRAM, the system starts swapping, and everything falls apart.',
    totalThroughputLabel: 'Total System Throughput',
    totalThroughputHint: 'Total tokens/s across ALL users combined',
    perUserSpeedLabel: 'Per-User Speed',
    perUserSpeedHint: 'How fast each individual user receives tokens',
    batchSizeLabel: 'Batch Size',
    tokPerSec: 'tok/s',
    msPerTok: 'ms/tok',
    plateauExplain: 'The plateau happens because of DRAM bandwidth saturation — not compute saturation. Even at batch 256, GPU compute units are still underutilized. The bottleneck is how fast weights can be loaded from memory.',
    improvementNote: 'More requests → more total throughput, but diminishing returns',
    collapseWarning: 'System Collapse!',
    collapseExplain: 'At extreme batch sizes, KV cache memory exceeds VRAM capacity. The system starts thrashing — swapping memory, dropping requests, or crashing with OOM errors. Total throughput doesn\'t just plateau — it collapses catastrophically.',

    // Section 3: Prefill vs Decode
    prefillDecodeTitle: 'Prefill vs Decode Phase',
    prefillDecodeSubtitle: 'Two fundamentally different workloads in every LLM request',
    prefillDecodeExplain: 'Every LLM request has two phases. Prefill processes all input tokens in parallel — it\'s blazingly fast because it saturates the GPU\'s compute units. Decode generates tokens one at a time — it\'s painfully slow because it\'s bottlenecked by memory bandwidth, not compute. Prefill can process ~1000 tokens in the time decode generates ~20.',
    prefillTitle: 'Prefill Phase',
    prefillAnalogy: 'Like a conveyor belt processing all items at once',
    decodeTitle: 'Decode Phase',
    decodeAnalogy: 'Like a single worker hand-crafting items one by one',
    prefillLabel: 'Prefill',
    decodeLabel: 'Decode',
    computeBound: 'Compute-bound (FLOPS limited)',
    memoryBound: 'Memory-bound (bandwidth limited)',
    highUtil: 'High GPU utilization (~80-95%)',
    lowUtil: 'Low GPU utilization (<20%)',
    parallelProcess: 'All tokens in parallel',
    sequentialProcess: 'One token at a time',
    ttft: 'TTFT (Time to First Token)',
    tbot: 'TBOT (Time Between Output Tokens)',
    prefillSpeed: '~50,000 tok/s on A100',
    decodeSpeed: '~50 tok/s per request on A100',
    keyInsight: 'Prefill is 1000× faster per token than decode. In prefill, the GPU tensor cores are the bottleneck (compute-bound). In decode, DRAM bandwidth is the bottleneck (memory-bound) — the GPU sits idle >80% of the time waiting for weight data. This is exactly why batching helps: it fills those idle cycles with useful work from other requests.',
    phaseAnimTitle: 'Watch the Phases',
    promptIn: 'Prompt In',
    tokensOut: 'Tokens Out',
    prefillBurst: 'Prefill burst — all input processed at once',
    decodeTrickle: 'Decode trickle — tokens generated one by one',
    raceTitle: 'Speed Race: Prefill vs Decode',
    raceStart: 'Start Race',
    raceDesc: 'Watch prefill process 1000 tokens while decode barely gets started. Prefill processes ~50 tokens per tick; decode processes 1 token per tick.',
    raceResult: 'Prefill finished 1000 tokens while decode only processed ~20! That\'s the 50× speed difference in action.',
    tokensProcessed: 'tokens',
    rooflineTitle: 'Why the Speed Difference?',
    limitedBy: 'Limited by',
    computeFlops: 'Compute (FLOPS)',
    memBandwidth: 'Memory Bandwidth (GB/s)',
    tensorCoreBottleneck: 'Tensor cores are the bottleneck',
    dramBottleneck: 'Loading weights from DRAM is the bottleneck',
    arithmeticIntensity: 'Arithmetic intensity = FLOPS per byte loaded from memory. Prefill has high arithmetic intensity (many operations per weight load, since it processes many tokens). Decode has extremely low arithmetic intensity (loads the same weights but only computes for 1 token). The GPU is a supercomputer held hostage by a straw-sized memory pipe.',

    // Section 4: Throughput-Latency Tradeoff
    tradeoffTitle: 'The Throughput-Latency Tradeoff',
    tradeoffDesc: 'Batching isn\'t free lunch. More batching means more total tokens per second, but each individual user waits longer for their response. Operators must balance server efficiency against user experience.',
    lowBatch: 'Low Batch Size',
    lowBatchDesc: 'Low total throughput, but each user gets fast responses (45 tok/s per user)',
    highBatch: 'High Batch Size',
    highBatchDesc: 'High total throughput, but each user gets much slower responses (9 tok/s per user)',
    totalWord: 'total',
    perUserWord: 'per user',
    tradeoffInsight: 'The sweet spot depends on your use case: real-time chat needs low latency (small batches), while batch processing jobs can maximize throughput (large batches). Most production systems target batch sizes of 32-128.',

    // Section 5: Continuous Batching
    continuousTitle: 'Continuous Batching',
    continuousSubtitle: 'How modern serving engines eliminate idle GPU time',
    continuousExplain: 'Static batching wastes GPU time because all requests must wait for the longest one to finish. Continuous batching (aka "Orca-style") lets requests join and leave independently. When a request finishes, its slot is immediately filled with a new one. Try the interactive timeline below — toggle between static and continuous to see the difference in GPU utilization.',
    staticBatchTitle: 'Static Batching',
    continuousBatchTitle: 'Continuous Batching',
    slotLabel: 'Slot',
    activeLabel: 'Active',
    idleLabel: 'Idle',
    newReqLabel: 'New request',
    finishedLabel: 'Finished',
    addRequest: 'Add Request',
    timeStep: 'Time step',
    gpuUtilization: 'GPU Utilization',
    staticInsight: 'Notice the red idle blocks — slots that finished early sit empty until the longest request completes. This wastes 30-50% of GPU capacity in practice.',
    continuousInsight: 'With continuous batching, finished slots are immediately refilled. GPU utilization stays near 100% as long as there are queued requests. This is how vLLM, TGI, and other modern engines work.',

    // Section 6: Per-User vs System Throughput
    perUserTitle: 'Per-User vs System Throughput',
    perUserSubtitle: 'What happens to each user as the system scales',
    perUserExplain: 'As you add more concurrent users, the system serves more total tokens per second — but each individual user gets a smaller slice of the bandwidth. Beyond a critical point, the system collapses entirely: memory exhaustion, swapping, and cascading failures destroy throughput for everyone.',
    concurrentUsers: 'Concurrent Users',
    bandwidthPerUser: 'Each user\'s share of GPU bandwidth:',
    eachUserGets: 'Each user receives',
    moreUsers: 'more',
    systemOverload: 'System Overloaded!',
    overloadExplain: 'Beyond ~80 concurrent users, the system starts running out of VRAM for KV caches. Memory pressure causes thrashing, and total throughput collapses — nobody gets good performance anymore.',

    // Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Batching amortizes the cost of loading model weights across multiple requests, dramatically increasing total throughput',
    takeaway2: 'But there\'s a limit: too many batched requests causes memory exhaustion and throughput collapse (OOM)',
    takeaway3: 'Prefill is compute-bound and 1000× faster per token than decode, which is memory-bandwidth bound',
    takeaway4: 'Continuous batching eliminates idle GPU time by dynamically filling slots as requests complete',
    takeaway5: 'Per-user speed always decreases with more concurrent users — the system trades individual speed for total capacity',
  },

  // Local Inference page
  localInference: {
    title: 'Running Models Locally',
    description: 'Run large language models on your own hardware -- no cloud, no API keys, no limits.',

    whyTitle: 'Why Run Locally?',
    whyDesc: 'Running models on your own machine gives you capabilities that cloud APIs cannot match.',
    whyPrivacy: 'Complete Privacy',
    whyPrivacyDesc: 'Your data never leaves your machine. No logging, no third-party access, no compliance worries.',
    whyCost: 'Zero API Costs',
    whyCostDesc: 'After the one-time hardware investment, every token is free. Run as many queries as you want.',
    whyOffline: 'Offline Access',
    whyOfflineDesc: 'Works without internet. Use AI on planes, in secure environments, or anywhere connectivity is limited.',
    whyCustom: 'Full Customization',
    whyCustomDesc: 'Choose any model, any quantization, any parameters. Fine-tune for your specific use case.',
    whyLearning: 'Deep Learning',
    whyLearningDesc: 'Nothing teaches you how LLMs work like running and experimenting with them directly.',
    whyControl: 'Total Control',
    whyControlDesc: 'No rate limits, no content filters you did not choose, no surprise API changes or deprecations.',

    hardwareTitle: 'Hardware Requirements',
    hardwareDesc: 'Select a model size and quantization level to see how much VRAM you need and which GPUs can handle it.',
    hwModelSize: 'Model Size',
    hwQuantLevel: 'Quantization',
    hwVramNeeded: 'VRAM Required',
    hwSpeed: 'Estimated Speed',
    hwSpeedNote: 'Approximate, varies by GPU and configuration',
    hwGpuCompat: 'GPU Compatibility',

    moeTitle: 'The MoE Advantage for Local Inference',
    moeDesc: 'Mixture of Experts (MoE) models route each token through only a subset of "expert" layers. The key advantage is speed: fewer active parameters means faster generation. But all parameters still live in VRAM — MoE does not save memory.',
    moeBenefit1: 'Faster Generation Speed',
    moeBenefit1Desc: 'Only a subset of experts compute per token. Mixtral 8x7B activates 12.9B of its 46.7B parameters — meaning it generates tokens ~3x faster than a similarly-intelligent dense 70B model.',
    moeBenefit2: 'Large-Model Intelligence',
    moeBenefit2Desc: 'All 46.7B parameters store knowledge across all experts. You get reasoning quality far above what a 13B dense model could achieve, because the full capacity is available.',
    moeBenefit3: 'VRAM Is Still Based on Total Params',
    moeBenefit3Desc: 'All expert weights must be loaded into memory. Mixtral 8x7B at Q4 needs ~26 GB VRAM — similar to a dense 30B model, not a 13B. MoE saves compute, not memory.',
    moeIntelligence: 'Intelligence',
    moeSpeed: 'Speed (tokens/sec, RTX 4090)',
    moeVram: 'VRAM Usage (Q4)',
    moeTotalParams: 'Total',
    moeActiveParams: 'Active',
    moeSweetSpot: '✨ Speed advantage',
    moeInsightLabel: 'The insight:',
    moeInsight: 'Mixtral 8x7B activates only 12.9B of its 46.7B parameters per token — delivering 70B-class intelligence at 3.5x the speed. But it still needs ~26 GB VRAM because all expert weights must be loaded. MoE trades VRAM for speed, not the other way around.',
    moeLinkDesc: 'MoE is a fundamental architecture shift, not just an optimization trick. Understanding how expert routing works helps you pick the right model for your hardware.',
    moeLinkText: 'Deep dive into Mixture of Experts',

    toolsTitle: 'Popular Tools',
    toolsDesc: 'The local inference ecosystem has matured rapidly. Here are the tools that matter, from beginner-friendly to production-grade.',

    quantTitle: 'The Quantization Tradeoff',
    quantDesc: 'Quantization is the key technology that makes local inference practical. By reducing the precision of model weights, you can fit much larger models into limited VRAM.',
    quantSummary: 'A 70B parameter model at FP16 needs 140 GB of memory -- far beyond any consumer GPU. But at Q4 quantization, it fits into 40 GB, making it runnable on high-end consumer hardware with only a modest quality loss. The lower you quantize, the more speed and memory you gain, but quality degrades.',
    quantLink: 'Deep dive into Quantization',

    startTitle: 'Getting Started',
    startDesc: 'Follow these five steps to go from zero to running your first local model.',
    step1Title: 'Pick a Tool',
    step1Desc: 'Start with Ollama or LM Studio -- they handle everything for you. Move to llama.cpp or vLLM when you need more control.',
    step2Title: 'Check Your VRAM',
    step2Desc: 'Run nvidia-smi (NVIDIA) or check Activity Monitor (Mac). This determines what models you can run.',
    step3Title: 'Choose a Model Size',
    step3Desc: 'Start with 7B models. They are fast, capable, and fit on most GPUs. Move to 13B or 70B as you need more capability.',
    step4Title: 'Pick a Quantization Level',
    step4Desc: 'Q4 is the sweet spot for most users: good quality with reasonable VRAM use. Go Q8 if you have the memory, Q2 if you are tight.',
    step5Title: 'Run It',
    step5Desc: 'Download the model and start chatting. With Ollama: ollama pull llama3.2 then ollama run llama3.2. That is it.',

    quickstartTitle: 'Quickstart Demo',
    quickstartDesc: 'Here is what it looks like to install Ollama and run your first model -- three commands and you are chatting.',
    qsReplay: 'Replay',

    tipsTitle: 'Tips and Tricks',
    tip1: 'Context length directly impacts VRAM usage. A 7B model with 128K context needs significantly more memory than with 4K context. Start small and increase as needed.',
    tip2: 'GPU offloading lets you split a model between GPU and CPU. You get GPU speed for the layers that fit, with CPU handling the rest. Slower than full GPU, but runs larger models.',
    tip3: 'CPU-only inference works but is 5-10x slower than GPU. Great for testing, less ideal for interactive use. Apple Silicon is the exception -- unified memory makes CPU inference fast.',
    tip4: 'For 8 GB VRAM: stick to 7B Q4 models. For 12 GB: 7B Q8 or 13B Q4. For 24 GB: 13B Q8 or 70B Q4. For 32 GB+: 70B Q4-Q8 comfortably.',
    tip5: 'Llama 3.2, Mistral, Phi-3, and Qwen 2.5 are excellent choices for local inference. Each excels at different tasks -- experiment to find your best fit.',
    tip6: 'Run models as an API server (Ollama and LM Studio both support this) to integrate local models into your own applications, scripts, and workflows.',
  },

  // LoRA page
  lora: {
    title: 'Fine-Tuning & LoRA',
    description: 'How LoRA lets you adapt massive language models to specific tasks by training only tiny low-rank matrices — saving memory, time, and money.',

    // 1. What is Fine-Tuning?
    whatIs: 'What is Fine-Tuning?',
    whatIsDesc: 'You have a pre-trained language model with billions of parameters that knows a lot about the world. But you want it to be great at a specific task — writing legal briefs, coding in Rust, or speaking like your brand. Fine-tuning adapts the model by continuing training on your specialized data.',
    problem: '"The problem: full fine-tuning means updating ALL parameters."',
    problemDesc: 'For a 70B parameter model, that means storing and updating 70 billion weights. You need a full copy of the model in memory, plus optimizer states (2-3x the model size). That\'s hundreds of gigabytes of VRAM — expensive, slow, and impractical for most teams.',

    // 2. The LoRA Insight
    insightTitle: 'The LoRA Insight',
    insightDesc: 'LoRA (Low-Rank Adaptation) is based on a key observation: when you fine-tune a model, the weight updates tend to be low-rank. Instead of updating a huge d×d weight matrix W directly, you decompose the update as ΔW = A × B, where A is d×r and B is r×d, with r much smaller than d.',

    // Visualizer translation keys
    matrixTitle: 'LoRA Matrix Decomposition',
    matrixDesc: 'Adjust the rank r to see how LoRA decomposes a large weight update into two small matrices.',
    rankLabel: 'Rank',
    matrix: 'matrix',
    fullParams: 'Full Parameters',
    loraParams: 'LoRA Parameters',
    savings: 'Parameter Savings',
    onlyPct: 'Only {pct}% of original',
    params: 'params',

    // 3. Why LoRA is Easy to Train
    whyEasyTitle: 'Why LoRA is Easy to Train',
    whyEasyDesc: 'By only training the small A and B matrices while keeping the base model frozen, LoRA dramatically reduces memory, compute, and storage requirements.',
    memoryTitle: 'VRAM & Storage Comparison',
    memoryDesc: 'Select a model size to compare GPU VRAM needed for full fine-tuning vs LoRA.',
    fullFineTune: 'Full Fine-Tuning',
    loraFineTune: 'LoRA Fine-Tuning',
    vramNeeded: 'GPU VRAM needed',
    adapterStorage: 'Storage: Full Model vs LoRA Adapter',
    fullModel: 'Full Model Copy',
    loraAdapter: 'LoRA Adapter',
    storageSavings: '~{x}× smaller — you can store hundreds of adapters for different tasks!',
    rankAffectsMemory: 'Higher rank → more trainable parameters → more VRAM and larger adapter files',
    memoryBenefit: 'Less Memory',
    memoryBenefitDesc: 'Only the small A and B matrices need gradients and optimizer states.',
    speedBenefit: 'Faster Training',
    speedBenefitDesc: 'Far fewer parameters to update means faster iterations.',
    swapBenefit: 'Hot-Swappable',
    swapBenefitDesc: 'Keep one base model, swap tiny adapters at inference time for different tasks.',
    frozenTitle: 'No Catastrophic Forgetting',
    frozenDesc: 'Because the base model weights stay completely frozen, LoRA can\'t destroy the model\'s existing knowledge. The adapter only adds to what the model already knows — it never subtracts. This is a huge advantage over full fine-tuning, where aggressive training can cause the model to forget its general capabilities.',

    // 4. Use Cases
    useCasesTitle: 'Use Cases',
    useCasesDesc: 'LoRA adapters are used everywhere to specialize foundation models:',
    useCase1Title: 'Task-Specific Adaptation',
    useCase1Desc: 'Train adapters for coding, medical diagnosis, legal analysis, or customer support. Each domain gets its own small adapter.',
    useCase2Title: 'Style & Tone Adaptation',
    useCase2Desc: 'Match a specific brand voice, switch between formal and casual, or adapt writing style without retraining the whole model.',
    useCase3Title: 'Language Adaptation',
    useCase3Desc: 'Improve performance in underrepresented languages by training a LoRA on language-specific data.',
    useCase4Title: 'Instruction Following',
    useCase4Desc: 'Make a base model follow instructions better by training an adapter on instruction-response pairs.',

    // Anti Use Cases
    antiUseCasesTitle: 'When NOT to Use LoRA',
    antiUseCasesDesc: 'LoRA is powerful, but it\'s not the right tool for every job:',
    antiUseCase1Title: 'Prompt Engineering Would Suffice',
    antiUseCase1Desc: 'If you can get the behavior you want with a good system prompt or few-shot examples, don\'t train an adapter. It\'s cheaper, faster, and easier to iterate on.',
    antiUseCase2Title: 'You Need Broad New Knowledge',
    antiUseCase2Desc: 'LoRA is great for style and behavior, but struggles to inject large amounts of factual knowledge. Use RAG (retrieval) instead for knowledge-heavy tasks.',
    antiUseCase3Title: 'Your Dataset Is Tiny or Noisy',
    antiUseCase3Desc: 'With fewer than ~100 quality examples, LoRA will overfit or barely learn anything. Clean, curated data is essential — garbage in, garbage out.',
    antiUseCase4Title: 'You Need Real-Time Adaptation',
    antiUseCase4Desc: 'LoRA requires a training step. If your use case needs the model to adapt on-the-fly to new information, use in-context learning or RAG instead.',

    // 5. Why Not for Pre-Training
    whyNotTitle: 'Why LoRA Isn\'t Used for Pre-Training',
    whyNotDesc: 'LoRA is fantastic for adaptation, but it\'s fundamentally limited for learning brand-new knowledge from scratch. Here\'s why:',
    whyNot1Title: 'Low-Rank Constraint',
    whyNot1Desc: 'LoRA constrains updates to a low-rank subspace. Fine-tuning changes are empirically low-rank (small adaptations), but pre-training needs to learn fundamental representations that are full-rank.',
    whyNot2Title: 'Limited Expressiveness',
    whyNot2Desc: 'A rank-8 update to a 4096×4096 matrix can only capture a tiny fraction of possible changes. Pre-training needs the full expressiveness of unconstrained weight updates.',
    whyNot3Title: 'Diminishing Returns',
    whyNot3Desc: 'As you increase the rank to capture more complex changes, you approach the cost of full fine-tuning anyway — at that point, LoRA offers no advantage.',
    rankQualityTitle: 'Rank vs Approximation Quality',
    rankQualityDesc: 'See how increasing rank improves task-specific adaptation but struggles with general knowledge.',
    taskSpecific: 'Task-Specific Quality',
    taskSpecificHint: 'Adapting to a specific domain — saturates quickly at moderate rank',
    generalKnowledge: 'General Knowledge Learning',
    generalKnowledgeHint: 'Learning fundamentally new knowledge — needs full-rank updates',
    reconstructionQuality: 'Matrix Reconstruction Quality',
    reconstructionHint: 'How well the low-rank approximation captures arbitrary weight updates',
    overkillTitle: 'Diminishing Returns!',
    overkillDesc: 'At this rank, you\'re using so many parameters that full fine-tuning would be more efficient. The low-rank constraint adds overhead without meaningful savings.',
    sweetSpotTitle: 'Sweet Spot',
    sweetSpotDesc: 'Ranks 8-64 typically offer the best tradeoff: excellent task adaptation with minimal parameters. Most practitioners use r=8 or r=16.',
    lowRankDesc: 'Very low ranks are extremely parameter-efficient but may miss important adaptation patterns. Good for very simple tasks.',

    // 6. Variants
    variantsTitle: 'LoRA Variants & Evolution',
    variantsDesc: 'The original LoRA paper spawned a family of improvements. Click each card to learn more.',
    qloraTitle: 'QLoRA',
    qloraDesc: 'Quantized base model + LoRA adapters = fine-tuning on consumer GPUs.',
    qloraDetail: 'QLoRA quantizes the frozen base model to 4-bit precision (NF4 format), reducing its memory footprint by 4×. The LoRA adapters are still trained in 16-bit. This means you can fine-tune a 65B model on a single 48GB GPU — something that would normally require multiple A100s. It introduced paged optimizers and double quantization for further savings.',
    doraTitle: 'DoRA (Weight-Decomposed LoRA)',
    doraDesc: 'Separates weight magnitude from direction for better training dynamics.',
    doraDetail: 'DoRA decomposes the weight update into magnitude and direction components, then applies LoRA only to the direction. This mimics how full fine-tuning actually behaves — it changes direction more than magnitude. DoRA consistently outperforms standard LoRA across tasks with the same number of trainable parameters.',
    loraPlusTitle: 'LoRA+',
    loraPlusDesc: 'Different learning rates for A and B matrices = faster convergence.',
    loraPlusDetail: 'LoRA+ observes that the A and B matrices have different optimal learning rates. By setting the learning rate of B about 2-4× higher than A, convergence speed improves significantly. This is a simple change that costs nothing extra and consistently improves results.',

    // 7. Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'LoRA decomposes weight updates into two small matrices (A×B), reducing trainable parameters by 99%+ while maintaining quality',
    takeaway2: 'The base model stays frozen — no catastrophic forgetting, and you can swap tiny adapters for different tasks at inference time',
    takeaway3: 'LoRA works because fine-tuning changes are empirically low-rank: you don\'t need full-rank updates for task adaptation',
    takeaway4: 'QLoRA extends this further by quantizing the base model, enabling fine-tuning of 70B+ models on consumer hardware',
    takeaway5: 'LoRA is not suitable for pre-training — learning fundamental knowledge requires full-rank, unconstrained weight updates',
  },

  // Getting Started page
  gettingStarted: {
    title: 'Getting Started',
    description: 'Make your first LLM API call in 10 minutes — completely free. No credit card needed.',
    whyTitle: 'Why Start Here?',
    whyDesc: 'Large Language Models are powerful, but getting started can feel overwhelming. This guide cuts through the noise: you\'ll pick a free API provider, paste in your key, and talk to an AI model — all in your browser.',
    whyPromise: '⏱️ You\'ll have a working LLM call in under 10 minutes. No credit card required.',
    getKeyTitle: 'Get a Free API Key',
    getKeyDesc: 'All three providers offer free tiers — no credit card needed. Pick whichever appeals to you (you can always try the others later):',
    openrouterDesc: 'Widest free model selection. Access Llama 3.3 70B, DeepSeek R1, Qwen3, Gemma 3, and Mistral Small 3.1 — all free. Great for exploring.',
    groqDesc: 'Blazing fast inference on custom LPU hardware. Free developer tier includes Llama 3.3 70B, Qwen3 32B, and GPT OSS 120B with generous rate limits.',
    cerebrasDesc: 'The fastest inference available. Wafer-scale chips deliver incredible speed. Free tier includes all models: Llama 3.3 70B, Qwen3 32B, and GPT OSS 120B.',
    getKey: 'Get API Key →',
    apiKeyLabel: 'API Key',
    keyDisclaimer: 'Your key is stored only in your browser\'s localStorage. It never touches our servers.',
    keyPrivacyNote: 'Your API key never leaves your browser — all requests go directly from your browser to the provider\'s API. The key is only kept in memory and is automatically cleared when you leave or refresh the page.',
    keyPrivacyVerify: 'Verify in source code',
    modelLabel: 'Model',
    temperatureLabel: 'Temperature',
    maxTokensLabel: 'Max Tokens',
    messagePlaceholder: 'Type a message... e.g. "Explain quantum computing in simple terms"',
    sendButton: 'Send',
    stopButton: 'Stop',
    responseLabel: 'Response',
    breakdownModel: 'Model',
    breakdownModelHint: 'The exact model that processed your request.',
    breakdownFinishReason: 'Finish Reason',
    breakdownFinishStop: 'Completed naturally — the model said everything it wanted to.',
    breakdownFinishLength: 'Cut off by max_tokens limit. Increase it for longer responses.',
    breakdownFinishOther: 'The model stopped for another reason.',
    breakdownResponseTime: 'Response Time',
    breakdownResponseTimeHint: 'Total round-trip time from your browser to the API and back.',
    breakdownTokens: 'Token Usage',
    breakdownPromptTokens: 'Prompt Tokens',
    breakdownCompletionTokens: 'Completion Tokens',
    breakdownTotalTokens: 'Total',
    breakdownTokensHint: 'Prompt tokens = your input cost. Completion tokens = the model\'s output. This is how API billing works (free tiers don\'t charge).',
    rawJsonLabel: 'Raw JSON Response — for the curious',
    playgroundTitle: 'Your First LLM Call',
    playgroundDesc: 'Enter your API key above, type a message, and hit Send',
    understandTitle: 'Understanding the Response',
    understandContent: 'The actual text the model generated. This is what you\'d show to a user.',
    understandTokens: 'How many tokens were used. Prompt tokens = your input, completion tokens = the model\'s output. This is how billing works (though free tiers don\'t charge).',
    understandFinish: '"stop" means the model finished naturally. "length" means it hit the max_tokens limit and was cut off.',
    understandModel: 'The exact model that processed your request. Some providers may route to different versions.',
    nextStepsTitle: 'Next Steps',
    nextStepsDesc: 'Now that you\'ve made your first LLM call, explore these topics to go deeper:',
    nextTemperature: 'Learn how temperature controls creativity vs. predictability',
    nextSystemPrompts: 'Set the personality and rules for your AI',
    nextTokenization: 'Understand how text becomes numbers the model can process',
    modelUsed: 'Model',
    finishReason: 'Finish Reason',
    finishStop: 'Completed naturally.',
    finishLength: 'Cut off by max_tokens limit.',
    responseTime: 'Response Time',
    tokenUsage: 'Token Usage',
    promptTokens: 'Prompt Tokens',
    completionTokens: 'Completion Tokens',
    totalTokens: 'Total',
    tokenExplain: 'Prompt tokens = your input. Completion tokens = the model\'s output. This is how API billing works.',
  },
  promptCaching: {
    title: 'Prompt Caching',
    description: 'Reuse computed KV caches across API requests to dramatically reduce cost and latency for repeated prompt prefixes.',
    whatIs: 'What is Prompt Caching?',
    whatIsDesc: 'When you send a request to an LLM API, the model must process every input token through its transformer layers, computing key-value (KV) pairs for the attention mechanism. Prompt Caching stores these computed KV pairs on the server side so that subsequent requests sharing the same prompt prefix can skip this expensive computation entirely. Instead of reprocessing thousands of tokens, the model loads the cached KV state and only processes the new, unique tokens.',
    kvCacheConnection: 'Connection to KV Cache',
    kvCacheConnectionDesc: 'Prompt Caching is the API-level application of the KV Cache concept you learned about in the previous article. While KV Cache operates within a single generation (caching tokens as they are produced), Prompt Caching persists the KV cache across separate API requests that share the same prefix.',
    visualTitle: 'Cache Hit vs. Cache Miss',
    visualDesc: 'When a request arrives, the provider hashes the prompt prefix and checks if a matching KV cache exists. The difference in processing is dramatic:',
    cacheMiss: '❌ Cache Miss (First Request)',
    missStep1: 'Hash prompt prefix',
    missStep2: 'No cache found',
    missStep3: 'Full forward pass for ALL tokens',
    missStep4: 'Store KV cache for future use',
    missResult: '💰 Full price, full latency',
    cacheHit: '✅ Cache Hit (Subsequent Requests)',
    hitStep1: 'Hash prompt prefix',
    hitStep2: 'Cache found! Load KV pairs',
    hitStep3: 'Process only NEW tokens',
    hitResult: '⚡ 90% cheaper, ~3× faster',
    howItWorks: 'How Providers Implement It',
    howItWorksDesc: 'Each major LLM provider has their own approach to prompt caching, with different trade-offs in control, pricing, and minimum token requirements.',
    anthropicTitle: 'Anthropic (Claude)',
    anthropicDesc: 'Explicit opt-in via cache_control parameter. Mark specific content blocks as cacheable. 90% cost reduction on cached tokens, ~3× latency improvement. Minimum 1,024 tokens for caching. Cache TTL is 5 minutes (refreshed on hit).',
    openaiTitle: 'OpenAI (GPT-4o)',
    openaiDesc: 'Automatic — no code changes needed. The API automatically caches matching prefixes of 1,024+ tokens. 50% cost discount on cached input tokens. Caching happens transparently in the background.',
    googleTitle: 'Google (Gemini)',
    googleDesc: 'Explicit via Context Caching API. Create named cache objects with configurable TTL. 75% discount on cached tokens. Best for very large contexts (32k+ tokens) reused across many requests.',
    whenToUse: 'When to Use Prompt Caching',
    idealFor: 'Ideal For',
    ideal1: 'Long system prompts reused across conversations (e.g., AI assistants with detailed instructions)',
    ideal2: 'Few-shot examples that stay constant while user queries change',
    ideal3: 'Large documents (contracts, codebases) analyzed with multiple different questions',
    ideal4: 'Agentic workflows where the same tool definitions and context are sent repeatedly',
    notIdealFor: 'Not Ideal For',
    notIdeal1: 'Unique, one-off prompts that are never repeated',
    notIdeal2: 'Very short prompts (under 1,024 tokens) — below the caching threshold',
    notIdeal3: 'Prompts where the prefix changes frequently between requests',
    costTitle: 'Cost & Performance Impact',
    costDesc: 'The savings from prompt caching are substantial, especially for applications with long, repeated prompt prefixes. Here are the numbers from major providers:',
    costSaving: 'cost reduction on cached tokens (Anthropic)',
    speedBoost: 'faster time-to-first-token (Anthropic)',
    cacheTTL: 'default cache lifetime (refreshed on each hit)',
    pricingComparison: 'Pricing Comparison (Input Tokens)',
    tableProvider: 'Provider',
    tableBase: 'Base Price',
    tableCached: 'Cached Price',
    tableSavings: 'Savings',
    pricingNote: 'Prices shown for flagship models as of early 2025. Cache write tokens may cost 25% more than base price (Anthropic). Always check current pricing docs.',
    codeTitle: 'Code Example: Anthropic API',
    codeDesc: 'Anthropic\'s implementation gives you explicit control over what gets cached. Add cache_control to any content block in your system prompt or messages:',
    tipsTitle: 'Implementation Tips',
    tip1Title: 'Put static content first',
    tip1Desc: 'Cache matching works on prefixes. Place your system prompt and few-shot examples before any dynamic content so the prefix stays stable across requests.',
    tip2Title: 'Mind the minimum token count',
    tip2Desc: 'Anthropic requires at least 1,024 tokens in a cacheable block (2,048 for Claude Haiku). Content below this threshold won\'t be cached.',
    tip3Title: 'Understand the TTL',
    tip3Desc: 'Anthropic\'s cache lives for 5 minutes, refreshed on each hit. For infrequent requests, the cache may expire between calls. OpenAI\'s automatic caching has similar time-based expiry.',
    tip4Title: 'Monitor cache hit rates',
    tip4Desc: 'Check the usage fields in API responses (cache_creation_input_tokens vs cache_read_input_tokens) to verify caching is working. Low hit rates mean your prefix is changing too often.',
    takeawaysTitle: 'Key Takeaways',
    takeaway1: 'Prompt Caching reuses computed KV pairs across API requests, skipping redundant computation for repeated prompt prefixes.',
    takeaway2: 'Anthropic offers the best savings (90% cost, ~3× speed) with explicit cache_control. OpenAI does it automatically at 50% savings.',
    takeaway3: 'Best for long system prompts, few-shot examples, and large documents queried multiple times.',
    takeaway4: 'Structure prompts with static content first (prefix) and dynamic content last to maximize cache hit rates.',
  },

  learningPath: {
    title: 'Your Learning Path',
    subtitle: 'A curated roadmap from zero to advanced. Follow these topics in order for the best learning experience.',
    beginner: '🌱 Foundations',
    intermediate: '🔧 Building Skills',
    advanced: '🚀 Expert Territory',
    beginnerHint: 'Start here — core concepts everyone should know',
    intermediateHint: 'Go deeper — architecture, agents, and real-world patterns',
    advancedHint: 'Master level — orchestration, optimization, and security',
    progress: 'Overall Progress',
    lpPromptBasics: 'Learn the fundamentals of talking to AI models effectively',
    lpTemperature: 'Understand how randomness controls creative vs. precise outputs',
    lpTokenization: 'See how text gets broken into tokens the model understands',
    lpNeuralNetworks: 'The building blocks behind every AI model',
    lpVision: 'How models see and understand images',
    lpLocalInference: 'Run AI models on your own machine',
    lpSystemPrompts: 'Define personality, rules, and behavior for your AI',
    lpEmbeddings: 'Turn text into numbers that capture meaning',
    lpAttention: 'The mechanism that lets models focus on what matters',
    lpAgentLoop: 'How AI agents think, act, and iterate',
    lpToolDesign: 'Give your agents the right tools to get things done',
    lpRag: 'Combine search with generation for grounded answers',
    lpMemory: 'Give agents persistent memory across conversations',
    lpAgenticPatterns: 'Proven architectures for complex agent systems',
    lpOrchestration: 'Coordinate multiple agents working together',
    lpMoe: 'How models use specialized expert sub-networks',
    lpQuantization: 'Shrink models to run faster with less memory',
    lpAgentSecurity: 'Protect agents from prompt injection and misuse',
  },
}

// Create a recursive string type for the dictionary
type DeepStringify<T> = {
  [K in keyof T]: T[K] extends Record<string, unknown> ? DeepStringify<T[K]> : string
}

export type Dictionary = DeepStringify<typeof en>
