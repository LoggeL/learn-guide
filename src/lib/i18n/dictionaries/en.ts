export const en = {
  // Common UI
  common: {
    learnAi: 'Learn AI',
    interactiveGuide: 'Interactive Guide',
    topics: 'Topics',
    search: 'Search...',
    searchTopics: 'Search topics...',
    startTyping: 'Start typing to search topics...',
    trySearching: 'Try "temperature" or "attention"',
    noResults: 'No results found for',
    pressEsc: 'Press ESC to close',
    enterToSelect: 'Enter to select',
    previous: 'Previous',
    next: 'Next',
    projectBy: 'A project by',
    proTip: 'Pro tip: Press',
    toSearchTopics: 'to search topics',
    interactiveAiLearning: 'Interactive AI Learning',
    guidesDescription: 'Interactive guides to understand AI concepts',
  },

  // Home page
  home: {
    heroTitle1: 'Master AI Concepts',
    heroTitle2: 'Through Experience',
    heroDescription: 'Explore artificial intelligence and large language models through beautiful, interactive demonstrations. Learn by doing, not just reading.',
    startLearning: 'Start Learning',
    browseTopics: 'Browse Topics',
    exploreTopics: 'Explore Topics',
    diveIntoLessons: 'Dive into interactive lessons',
  },

  // Features
  features: {
    interactiveDemos: 'Interactive Demos',
    interactiveDemosDesc: 'Hands-on explorations that make abstract concepts tangible and intuitive.',
    visualLearning: 'Visual Learning',
    visualLearningDesc: 'Beautiful visualizations that reveal how AI systems actually work under the hood.',
    buildIntuition: 'Build Intuition',
    buildIntuitionDesc: 'Go beyond memorization—develop deep understanding through experimentation.',
  },

  // Topic categories
  categories: {
    ai: 'Artificial Intelligence',
    agents: 'AI Agents',
    llm: 'Large Language Models',
    mlFundamentals: 'ML Fundamentals',
    prompting: 'Prompting',
    safety: 'AI Safety & Ethics',
  },

  // Topic names
  topicNames: {
    'agent-loop': 'The Agent Loop',
    'agent-context': 'Context Anatomy',
    'agent-problems': 'Agent Problems',
    'agent-security': 'Agent Security',
    'agentic-patterns': 'Agentic Patterns',
    'mcp': 'MCP (Model Context Protocol)',
    'tool-design': 'Tool Design',
    'memory': 'Memory Systems',
    'orchestration': 'Orchestration',
    'evaluation': 'Evaluation',
    'tokenization': 'Tokenization',
    'embeddings': 'Embeddings',
    'rag': 'RAG',
    'context-rot': 'Context Rot',
    'temperature': 'Temperature',
    'attention': 'Attention Mechanism',
    'vision': 'Vision & Images',
    'visual-challenges': 'Visual Challenges',
    'neural-networks': 'Neural Networks',
    'gradient-descent': 'Gradient Descent',
    'training': 'Training Process',
    'prompt-basics': 'Prompt Basics',
    'advanced-prompting': 'Advanced Techniques',
    'system-prompts': 'System Prompts',
    'llm-training': 'LLM Training',
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Responsible AI',
  },

  // Temperature page
  temperature: {
    title: 'Temperature',
    description: 'Understanding how a single parameter controls the balance between predictable logic and creative randomness in AI outputs.',
    whatIs: 'What is Temperature?',
    whatIsDesc: 'In LLMs, Temperature is a hyperparameter that scales the "logits" (raw scores) of the next token predictions before they are converted into probabilities. It essentially controls how much the model favors the most likely options versus exploring less likely ones.',
    lowTemp: 'Low Temperature',
    lowTempDesc: 'Focuses on the top results. Reliable, consistent, and factual. Great for code, math, and structured data.',
    highTemp: 'High Temperature',
    highTempDesc: 'Spreads probability to more tokens. Diverse, creative, and surprising. Great for stories, brainstorming, and poetry.',
    interactiveDistribution: 'Interactive Distribution',
    adjustSlider: 'Adjust the temperature to see its effect',
    adjustDesc: 'Adjust the temperature slider to see how it reshapes the probability distribution for the next token. Watch how "the" (the most likely choice) dominates at low temperatures and loses its lead as the temperature rises.',
    howItWorks: 'How it Works Mathematically',
    mathDesc: 'The model generates a score for every possible token. To get probabilities, we use the Softmax function, modified by temperature:',
    whenLow: 'When T → 0',
    low: 'Low',
    whenLowDesc: 'Dividing by a small T amplifies differences between scores. The highest logit dominates exponentially.',
    whenHigh: 'When T → ∞',
    high: 'High',
    whenHighDesc: 'Dividing by a large T compresses all scores toward zero, making them nearly equal after exponentiation.',
    practicalGuidelines: 'Practical Guidelines',
    useCase: 'Use Case',
    tempLabel: 'Temperature',
    why: 'Why?',
    codingMath: 'Coding & Math',
    codingMathWhy: 'Errors in logic are costly; you want the most likely correct path.',
    factRetrieval: 'Fact Retrieval',
    factRetrievalWhy: 'Reduces "hallucinations" by sticking to the most probable data points.',
    generalChat: 'General Chat',
    generalChatWhy: 'The "sweet spot" for most models to sound natural and helpful.',
    creativeWriting: 'Creative Writing',
    creativeWritingWhy: 'Encourages the model to use more interesting, varied vocabulary.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generates wild, unconventional ideas that might spark inspiration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Temperature 0 is deterministic ("Greedy Search") — always picks the top token',
    takeaway2: 'Higher temperature increases variety and creativity but decreases coherence',
    takeaway3: 'Too high temperature (> 1.5) often results in gibberish',
    takeaway4: "Always match your temperature to the task's requirement for precision vs. creativity",
  },

  // Context Rot page
  contextRot: {
    title: 'Context Rot',
    description: 'Understanding how information degrades over long conversations and why LLMs struggle with extended contexts.',
    whatIs: 'What is Context Rot?',
    whatIsDesc: 'Context Rot refers to the gradual degradation of an LLM\'s ability to accurately recall and use information from earlier parts of a long conversation or document. As context grows, the model\'s attention becomes diluted.',
    whyHappens: 'Why Does It Happen?',
    whyHappensDesc: 'LLMs have finite context windows and use attention mechanisms that must distribute focus across all tokens. As conversations grow longer, earlier information competes with newer content for the model\'s limited attention capacity.',
    symptoms: 'Common Symptoms',
    symptom1: 'Forgetting instructions given at the start of a conversation',
    symptom2: 'Contradicting earlier statements or decisions',
    symptom3: 'Losing track of complex multi-step tasks',
    symptom4: 'Mixing up details from different parts of the context',
    mitigation: 'Mitigation Strategies',
    mitigation1: 'Summarize important context periodically',
    mitigation2: 'Place critical instructions at both start and end',
    mitigation3: 'Use structured formats to highlight key information',
    mitigation4: 'Break long tasks into smaller, focused conversations',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'See how memory fades as context length increases',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context rot is an inherent limitation of current LLM architectures',
    takeaway2: 'The "lost in the middle" effect means information at the start and end is recalled better',
    takeaway3: 'Strategic information placement can significantly improve recall',
    takeaway4: 'Regular summarization helps maintain important context over long conversations',
  },

  // Attention page
  attention: {
    title: 'Attention Mechanism',
    description: 'Explore how transformers focus on relevant parts of input through the powerful attention mechanism.',
    whatIs: 'What is Attention?',
    whatIsDesc: 'Attention is the core mechanism that allows transformers to weigh the importance of different parts of the input when generating each output token. It enables the model to "focus" on relevant context.',
    howWorks: 'How It Works',
    howWorksDesc: 'For each position, the model computes Query, Key, and Value vectors. Attention scores are calculated by comparing queries to keys, then used to create a weighted sum of values.',
    selfAttention: 'Self-Attention',
    selfAttentionDesc: 'Allows each token to attend to all other tokens in the sequence, capturing relationships regardless of distance.',
    multiHead: 'Multi-Head Attention',
    multiHeadDesc: 'Multiple attention heads allow the model to focus on different types of relationships simultaneously.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Attention enables transformers to capture long-range dependencies',
    takeaway2: 'The quadratic complexity of attention limits context window size',
    takeaway3: 'Different attention heads learn to focus on different linguistic patterns',
    takeaway4: 'Attention visualization can help interpret model behavior',
  },

  // Vision page
  vision: {
    title: 'Vision & Images',
    description: 'How modern LLMs process and understand visual information alongside text.',
    whatIs: 'How LLMs See Images',
    whatIsDesc: 'Vision-enabled LLMs convert images into sequences of tokens that can be processed alongside text. This typically involves dividing images into patches and encoding them with a vision transformer.',
    patchEncoding: 'Patch Encoding',
    patchEncodingDesc: 'Images are divided into fixed-size patches (e.g., 14x14 pixels), each converted into an embedding vector similar to text tokens.',
    multimodal: 'Multimodal Understanding',
    multimodalDesc: 'The model learns to align visual and textual representations, enabling tasks like image captioning, visual QA, and document understanding.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Images consume many more tokens than equivalent text descriptions',
    takeaway2: 'Resolution and patch size affect detail recognition',
    takeaway3: 'Visual understanding is approximate—models can miss fine details',
    takeaway4: 'Combining vision and language enables powerful new applications',
  },

  // Visual Challenges page
  visualChallenges: {
    title: 'Visual Challenges',
    description: 'Common challenges and limitations when working with vision-enabled AI models.',
    overview: 'Common Visual Challenges',
    overviewDesc: 'While vision models are impressive, they face several systematic challenges that are important to understand when building applications. These limitations stem from how vision models process images—through patches, embeddings, and attention—rather than the way humans perceive visual information.',

    // Challenge 1: Counting
    challenge1: 'Counting Objects',
    challenge1Desc: 'Models often struggle to accurately count objects in images, especially when there are many similar items.',
    challenge1Why: 'Why This Happens',
    challenge1WhyDesc: 'Vision models process images as patches (typically 14x14 or 16x16 pixels), not as discrete objects. They lack the built-in concept of "object permanence" and struggle to maintain accurate counts across overlapping or dense arrangements.',
    challenge1Examples: 'Common Failures',
    challenge1Example1: 'Counting people in a crowd (often off by 20-50%)',
    challenge1Example2: 'Counting items in a grid or array',
    challenge1Example3: 'Distinguishing between "few" and "many" when items overlap',
    challenge1Mitigation: 'Workarounds',
    challenge1MitigationDesc: 'For critical counting tasks, consider using specialized object detection models (YOLO, Faster R-CNN) or asking the model to identify and describe each item individually rather than providing a total count.',

    // Challenge 2: Spatial Reasoning
    challenge2: 'Spatial Reasoning',
    challenge2Desc: 'Understanding precise spatial relationships between objects (left/right, above/below) can be unreliable.',
    challenge2Why: 'Why This Happens',
    challenge2WhyDesc: 'Positional information is encoded through patch position embeddings, but these don\'t provide pixel-level precision. The model learns statistical correlations between positions rather than explicit spatial reasoning.',
    challenge2Examples: 'Common Failures',
    challenge2Example1: 'Confusing left/right relationships in mirrored or symmetric images',
    challenge2Example2: 'Misjudging relative distances ("closer to" or "farther from")',
    challenge2Example3: 'Difficulty with rotated or unusual orientations',
    challenge2Mitigation: 'Workarounds',
    challenge2MitigationDesc: 'Be explicit in your prompts about which reference frame to use. Consider annotating images with visual markers or grids for critical spatial tasks.',

    // Challenge 3: Small Text Recognition
    challenge3: 'Small Text Recognition',
    challenge3Desc: 'Fine text in images may be misread or missed entirely, especially at low resolutions.',
    challenge3Why: 'Why This Happens',
    challenge3WhyDesc: 'Text smaller than the patch size (14-16 pixels) gets compressed into a single embedding, losing character-level detail. OCR is not built into vision LLMs—they learn text recognition as a byproduct of training, not as a dedicated capability.',
    challenge3Examples: 'Common Failures',
    challenge3Example1: 'Misreading license plates, street signs, or small labels',
    challenge3Example2: 'Confusing similar characters (0/O, 1/l/I, 5/S)',
    challenge3Example3: 'Missing text in busy or low-contrast backgrounds',
    challenge3Mitigation: 'Workarounds',
    challenge3MitigationDesc: 'Use high-resolution images and zoom in on text regions. For critical OCR tasks, use dedicated OCR tools (Tesseract, Google Vision API, Amazon Textract) alongside or instead of vision LLMs.',

    // Challenge 4: Hallucination
    challenge4: 'Visual Hallucination',
    challenge4Desc: 'Models may describe objects or details that aren\'t actually present in the image.',
    challenge4Why: 'Why This Happens',
    challenge4WhyDesc: 'Vision LLMs are trained to generate plausible descriptions. When image features are ambiguous, the model fills in gaps with statistically likely content—even if that content isn\'t in the image. This is the same mechanism that causes text hallucination.',
    challenge4Examples: 'Common Failures',
    challenge4Example1: 'Adding objects that "should" be in a scene (a keyboard near a monitor)',
    challenge4Example2: 'Describing brand names or text that isn\'t visible',
    challenge4Example3: 'Inventing details when asked about unclear regions',
    challenge4Mitigation: 'Workarounds',
    challenge4MitigationDesc: 'Ask the model to express uncertainty. Use prompts like "describe only what you can clearly see" or "if you cannot determine X, say so." Cross-reference critical details.',

    // Challenge 5: Fine Detail Recognition
    challenge5: 'Fine Detail Recognition',
    challenge5Desc: 'Subtle details, textures, or small distinguishing features are often missed or misidentified.',
    challenge5Why: 'Why This Happens',
    challenge5WhyDesc: 'The patch-based architecture averages information within each patch, losing fine-grained detail. High-frequency visual information (edges, textures, small features) is compressed.',
    challenge5Examples: 'Common Failures',
    challenge5Example1: 'Distinguishing between similar objects (dog breeds, car models)',
    challenge5Example2: 'Reading gauges, meters, or instrument displays',
    challenge5Example3: 'Identifying subtle damage or defects in inspection tasks',
    challenge5Mitigation: 'Workarounds',
    challenge5MitigationDesc: 'Use the highest resolution available. Crop and focus on specific regions of interest. For specialized tasks, consider fine-tuned models trained on domain-specific data.',

    // Challenge 6: Multi-Image Reasoning
    challenge6: 'Multi-Image Reasoning',
    challenge6Desc: 'Comparing or reasoning across multiple images is significantly harder than single-image tasks.',
    challenge6Why: 'Why This Happens',
    challenge6WhyDesc: 'Each image is encoded separately into token sequences. Cross-image attention must happen through the language model\'s context window, which is less efficient than dedicated multi-image architectures.',
    challenge6Examples: 'Common Failures',
    challenge6Example1: 'Finding differences between two similar images ("spot the difference")',
    challenge6Example2: 'Tracking object identity across frames',
    challenge6Example3: 'Comparing fine details between product images',
    challenge6Mitigation: 'Workarounds',
    challenge6MitigationDesc: 'Describe each image separately first, then ask for comparison. Consider combining images into a single composite for direct comparison.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Vision LLMs process images as patches—detail below patch resolution is lost',
    takeaway2: 'Counting and spatial reasoning are fundamental weaknesses, not edge cases',
    takeaway3: 'Visual hallucination follows the same pattern as text hallucination—plausible fabrication',
    takeaway4: 'Use higher resolution, cropped regions, and explicit prompts to improve accuracy',
    takeaway5: 'For critical tasks, combine vision LLMs with specialized tools (OCR, object detection)',
    takeaway6: 'Always verify important visual information through other means',
  },

  // Agent Loop page
  agentLoop: {
    title: 'The Agent Loop',
    description: 'Understanding the core cycle that powers autonomous AI agents: observe, think, act, repeat.',
    whatIs: 'What is the Agent Loop?',
    whatIsDesc: 'The agent loop is the fundamental cycle that enables AI agents to interact with their environment autonomously. It consists of observation, reasoning, action, and feedback phases that repeat continuously.',
    phases: 'The Four Phases',
    observe: 'Observe',
    observeDesc: 'Gather information from the environment, tools, and user input.',
    think: 'Think',
    thinkDesc: 'Reason about the current state and decide on the next action.',
    act: 'Act',
    actDesc: 'Execute the chosen action using available tools.',
    learn: 'Learn',
    learnDesc: 'Process feedback and update understanding for the next iteration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'The loop continues until the task is complete or terminated',
    takeaway2: 'Each iteration builds on previous observations and actions',
    takeaway3: 'Error handling and recovery are crucial for robust agents',
    takeaway4: 'The quality of tools directly impacts agent capabilities',
  },

  // Agent Context page
  agentContext: {
    title: 'Context Anatomy',
    description: 'Breaking down the structure of context windows and how agents manage information.',
    whatIs: 'Understanding Agent Context',
    whatIsDesc: 'Agent context includes the system prompt, conversation history, tool definitions, and retrieved information. Managing this context efficiently is crucial for agent performance.',
    components: 'Context Components',
    systemPrompt: 'System Prompt',
    systemPromptDesc: 'Defines the agent\'s role, capabilities, and behavioral guidelines.',
    toolDefs: 'Tool Definitions',
    toolDefsDesc: 'Descriptions of available tools and how to use them.',
    history: 'Conversation History',
    historyDesc: 'Previous messages, tool calls, and their results.',
    retrieved: 'Retrieved Information',
    retrievedDesc: 'External knowledge fetched during the conversation.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context management is key to agent reliability',
    takeaway2: 'Prioritize recent and relevant information',
    takeaway3: 'Tool definitions should be clear and unambiguous',
    takeaway4: 'Summarization helps maintain context over long sessions',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agent Problems',
    description: 'Common failure modes and challenges faced by AI agents in real-world applications.',
    overview: 'Common Agent Failure Modes',
    overviewDesc: 'Understanding typical agent failures helps in building more robust systems and setting appropriate expectations.',
    problem1: 'Tool Misuse',
    problem1Desc: 'Agents may call tools incorrectly, with wrong parameters, or at inappropriate times.',
    problem2: 'Infinite Loops',
    problem2Desc: 'Agents can get stuck repeating the same actions without making progress.',
    problem3: 'Goal Drift',
    problem3Desc: 'Agents may gradually shift focus away from the original task objective.',
    problem4: 'Over-confidence',
    problem4Desc: 'Agents may proceed with actions despite uncertainty or incomplete information.',
    
    // Expanded Content
    hallucination: 'Tool Hallucination',
    hallucinationDesc: 'Agents sometimes "invent" tool parameters or even entire tools that don\'t exist. This usually happens when the tool definition is ambiguous or when the model tries to force a solution.',
    hallucinationExample: 'Example: Calling `get_weather(location="Tokyo", date="tomorrow")` when the function only accepts `location`.',
    
    loops: 'Looping Issues',
    loopsDesc: 'Agents can get trapped in repetitive cycles where they perform the same action, receive the same error, and try again without changing strategy.',
    loopsMitigation: 'Mitigation: Implement loop detection logic that stops execution if the same tool call sequence occurs multiple times.',
    
    costLatency: 'Cost & Latency',
    costLatencyDesc: 'Every step in the agent loop requires a full LLM inference call. Multi-step tasks can quickly become expensive and slow.',
    costFactor: 'The Cost Factor',
    costFactorDesc: 'A simple task requiring 5 steps means 5x the cost and 5x the latency of a standard chat response.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Implement safeguards like iteration limits and cost controls',
    takeaway2: 'Add human-in-the-loop checkpoints for critical actions',
    takeaway3: 'Monitor agent behavior and log all actions for debugging',
    takeaway4: 'Design clear success and failure criteria',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agent Security',
    description: 'Critical security vulnerabilities in AI agents: prompt injection, data exfiltration, and tool misuse—plus how to defend against them.',
    
    // Intro
    intro: 'Agents Are Attack Surfaces',
    introDesc: 'When you give an LLM access to tools, you\'re creating a powerful attack vector. Agents can read files, make HTTP requests, send emails, and execute code. A malicious actor who can influence the agent\'s context can potentially control all of these capabilities.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Attack #1: Prompt Injection',
    attack1Desc: 'Prompt injection occurs when untrusted input is interpreted as instructions by the LLM. Because agents often process external data (emails, web pages, documents), attackers can embed hidden commands that hijack the agent\'s behavior.',
    attack1Example: 'Example Attack',
    attack1ExampleDesc: 'User asks the agent to summarize a document. The document contains hidden instructions:',
    whyWorks: 'Why This Works',
    whyWorks1: 'The agent reads the document into its context',
    whyWorks2: 'The LLM cannot distinguish between "real" instructions and injected ones',
    whyWorks3: 'The hidden text looks like system instructions, so the LLM may follow them',
    whyWorks4: 'The agent uses its legitimate tools to perform the malicious action',
    directInjection: 'Direct Injection',
    directInjectionDesc: 'User directly types malicious instructions. Easier to filter but still dangerous if system prompt isn\'t robust.',
    indirectInjection: 'Indirect Injection',
    indirectInjectionDesc: 'Malicious content comes from external sources the agent reads (websites, emails, files). Much harder to defend against.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Attack #2: Data Exfiltration',
    attack2Desc: 'Agents with access to communication tools (email, HTTP, Slack, etc.) can be tricked into sending sensitive data to external destinations. The agent becomes an unwitting accomplice in data theft.',
    exfilFlow: 'Exfiltration Flow',
    exfilStep1: 'Agent reads',
    exfilStep1Desc: 'Private files, DB, env vars',
    exfilStep2: 'Injection triggers',
    exfilStep2Desc: '"Send this to X"',
    exfilStep3: 'Tool executes',
    exfilStep3Desc: 'Data leaves the system',
    vulnerableConfig: 'Vulnerable Tool Configuration',
    otherVectors: 'Other Exfiltration Vectors',
    vector1: 'HTTP requests — POST data to attacker-controlled endpoints',
    vector2: 'Slack/Discord webhooks — Send messages to external channels',
    vector3: 'File uploads — Upload to cloud storage with public links',
    vector4: 'DNS exfiltration — Encode data in DNS queries',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Attack #3: Unintended Tool Misuse',
    attack3Desc: 'Even without malicious intent, agents can cause harm through incorrect tool usage. The LLM might misunderstand parameters, use the wrong tool, or take destructive actions while trying to be helpful.',
    destructiveActions: 'Destructive Actions',
    destructiveActionsDesc: '"Clean up the project" → Agent runs rm -rf / or deletes production database',
    wrongParams: 'Wrong Parameters',
    wrongParamsDesc: 'Agent confuses similar fields or uses incorrect values that seem plausible',
    cascadingErrors: 'Cascading Errors',
    cascadingErrorsDesc: 'Agent makes one small error, then "fixes" it with increasingly destructive actions',
    
    // Defense Strategies
    defensesTitle: 'Defense Strategies',
    defense1: 'Principle of Least Privilege',
    defense1Desc: 'Only give the agent the minimum tools and permissions needed for the task. Don\'t give file access if it only needs to answer questions.',
    defense1Bad: 'Bad',
    defense1Good: 'Good',
    defense2: 'Strict Allowlists',
    defense2Desc: 'Constrain tool parameters to known-safe values. Don\'t allow arbitrary email addresses, URLs, or file paths.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Require human approval for sensitive actions. The agent proposes, the human confirms.',
    defense3Example: 'Example confirmation flow:',
    defense4: 'Input Sanitization & Isolation',
    defense4Desc: 'Treat external data as untrusted. Clearly separate user instructions from retrieved content.',
    defense5: 'Monitoring & Rate Limiting',
    defense5Desc: 'Log all tool calls. Set rate limits on sensitive operations. Alert on unusual patterns (many emails, large data transfers, repeated failures). Enable rollback for destructive actions.',
    
    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Agents are attack surfaces—every tool is a potential vulnerability',
    takeaway2: 'Prompt injection is the #1 threat—LLMs cannot distinguish instructions from data',
    takeaway3: 'Data exfiltration is trivial if agents have outbound communication tools',
    takeaway4: 'Tool misuse happens even without attackers—LLMs make mistakes',
    takeaway5: 'Defense in depth: least privilege + allowlists + human approval + monitoring',
    takeaway6: 'Treat all external data as potentially malicious input',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentic Patterns',
    description: 'Design patterns and architectures for building effective AI agent systems.',
    overview: 'Common Agentic Patterns',
    overviewDesc: 'Several architectural patterns have emerged as effective approaches for building AI agent systems.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Interleave reasoning traces with actions for better transparency and control.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Create a high-level plan first, then execute steps sequentially.',
    pattern3: 'Multi-Agent Systems',
    pattern3Desc: 'Multiple specialized agents collaborate to solve complex tasks.',
    pattern4: 'Reflection',
    pattern4Desc: 'Agents review their own outputs and iteratively improve them.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Choose patterns based on task complexity and reliability needs',
    takeaway2: 'ReAct is great for transparency but may be slower',
    takeaway3: 'Multi-agent systems add complexity but enable specialization',
    takeaway4: 'Reflection patterns can significantly improve output quality',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'Understanding MCP: when external tool servers make sense, and when they are overkill.',
    whatIs: 'What is MCP?',
    whatIsDesc: 'The Model Context Protocol (MCP) is a standardized way to connect AI agents to external tools and data sources via dedicated server processes. Instead of defining tools inline in your agent code, MCP runs a separate server that exposes tools over a structured protocol.',
    vsToolCalls: 'MCP vs. Regular Tool Calls',
    vsToolCallsDesc: 'Regular tool calls are functions defined directly in your agent\'s codebase. The agent calls them, they execute, and results return in the same process. MCP separates this: tools live in external servers that the agent communicates with over a protocol.',
    
    // Comparison
    regularTools: 'Regular Tool Calls',
    regularToolsDesc: 'Tools defined inline in your agent code. Simple, fast, and sufficient for most use cases.',
    mcpTools: 'MCP Servers',
    mcpToolsDesc: 'Tools exposed by external server processes. Adds network overhead but enables cross-language tooling and shared tool ecosystems.',
    
    // When to use
    whenToUse: 'When MCP Makes Sense',
    whenToUseDesc: 'MCP shines in specific scenarios where its additional complexity pays off.',
    useCase1: 'Multi-Language Teams',
    useCase1Desc: 'Your tools are written in Python but your agent is in TypeScript, or vice versa.',
    useCase2: 'Shared Tool Ecosystem',
    useCase2Desc: 'Multiple agents across different projects need to access the same tools.',
    useCase3: 'Enterprise Integration',
    useCase3Desc: 'You need to expose existing internal services as agent tools without modifying them.',
    useCase4: 'Tool Marketplace',
    useCase4Desc: 'You want to use community-maintained tools without copying code into your project.',
    
    // When it's overkill
    overkill: 'When MCP is Overkill',
    overkillDesc: 'For many use cases, MCP adds unnecessary complexity.',
    overkillCase1: 'Single-Language Projects',
    overkillCase1Desc: 'If your tools and agent are in the same language, inline functions are simpler and faster.',
    overkillCase2: 'Simple Agents',
    overkillCase2Desc: 'A chatbot with a few tools doesn\'t need the overhead of running separate server processes.',
    overkillCase3: 'Rapid Prototyping',
    overkillCase3Desc: 'When iterating quickly, the indirection of MCP slows down development.',
    overkillCase4: 'Latency-Sensitive Apps',
    overkillCase4Desc: 'Network calls to tool servers add latency that inline functions don\'t have.',
    
    // Architecture
    architecture: 'How MCP Works',
    architectureDesc: 'MCP defines a client-server architecture where the agent is the client and tools are exposed by servers.',
    step1: 'Discovery',
    step1Desc: 'The agent connects to an MCP server and receives a list of available tools with their schemas.',
    step2: 'Invocation',
    step2Desc: 'When the LLM decides to use a tool, the agent sends a request to the MCP server.',
    step3: 'Execution',
    step3Desc: 'The MCP server runs the tool and returns results in a standardized format.',
    step4: 'Integration',
    step4Desc: 'Results flow back to the agent and into the LLM context, just like regular tool results.',
    
    // Practical advice
    practicalAdvice: 'Practical Advice',
    adviceDesc: 'Guidelines for deciding whether to use MCP in your project.',
    advice1: 'Start simple: use inline tool definitions until you hit a specific limitation.',
    advice2: 'Consider MCP when you find yourself copy-pasting tool code between projects.',
    advice3: 'The overhead of running MCP servers only makes sense at scale or in enterprise settings.',
    advice4: 'Community MCP servers can accelerate development but add dependency risks.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'MCP is a protocol for exposing tools via external servers, not a replacement for regular tool calls',
    takeaway2: 'For most single-project agents, inline tools are simpler and have lower latency',
    takeaway3: 'MCP shines in polyglot environments and shared tool ecosystems',
    takeaway4: 'Don\'t reach for MCP by default—it\'s a solution for specific scaling and interoperability challenges',
  },

  // Tokenization page
  tokenization: {
    title: 'Tokenization',
    description: 'How LLMs break text into tokens—the fundamental units of language understanding.',
    whatIs: 'What is Tokenization?',
    whatIsDesc: 'Tokenization is the process of converting raw text into a sequence of tokens—the basic units that LLMs process. Tokens can be words, subwords, or even individual characters, depending on the tokenizer.',
    whyMatters: 'Why Tokenization Matters',
    whyMattersDesc: 'Understanding tokenization is crucial because it directly impacts context limits, costs, and model behavior. The same text can have very different token counts across different models.',
    howWorks: 'How It Works',
    howWorksDesc: 'Most modern LLMs use subword tokenization algorithms like BPE (Byte Pair Encoding) or SentencePiece. These algorithms learn common character sequences from training data.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE iteratively merges the most frequent character pairs into single tokens. Common words become single tokens, while rare words are split into subwords.',
    tokenTypes: 'Token Types',
    wholeWords: 'Whole Words',
    wholeWordsDesc: 'Common words like "the", "and", "is" are often single tokens.',
    subwords: 'Subwords',
    subwordsDesc: 'Less common words are split: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Special Tokens',
    specialTokensDesc: 'Markers like <|endoftext|> or [CLS] for model control.',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'Type text to see how it gets tokenized',
    costImplications: 'Cost Implications',
    costDesc: 'API pricing is typically per-token. Efficient prompts use fewer tokens.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tokens are the atomic units LLMs process—not characters or words',
    takeaway2: 'Different models have different tokenizers and vocabularies',
    takeaway3: 'Non-English text and code often use more tokens than English',
    takeaway4: 'Token count directly affects cost and context window usage',
  },

  // Embeddings page
  embeddings: {
    title: 'Embeddings',
    description: 'How AI represents meaning as vectors in high-dimensional space.',
    whatIs: 'What are Embeddings?',
    whatIsDesc: 'Embeddings are dense vector representations that capture semantic meaning. Similar concepts have similar embeddings, enabling machines to understand relationships between words, sentences, and documents.',
    howWorks: 'How Embeddings Work',
    howWorksDesc: 'Embedding models map discrete tokens to continuous vectors in a high-dimensional space (often 768-4096 dimensions). The position of each vector encodes its semantic meaning.',
    similarity: 'Semantic Similarity',
    similarityDesc: 'Similar meanings cluster together in embedding space. "King" and "Queen" are closer than "King" and "Banana".',
    dimensions: 'Vector Dimensions',
    dimensionsDesc: 'Each dimension captures some aspect of meaning—though these dimensions aren\'t human-interpretable.',
    operations: 'Vector Operations',
    operationsDesc: 'Famous example: King - Man + Woman ≈ Queen. Relationships are encoded as directions in the space.',
    useCases: 'Common Use Cases',
    search: 'Semantic Search',
    searchDesc: 'Find documents by meaning, not just keyword matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Group similar documents, detect topics automatically.',
    classification: 'Classification',
    classificationDesc: 'Categorize text based on embedding similarity to examples.',
    rag: 'RAG Systems',
    ragDesc: 'Retrieve relevant context for LLM prompts.',
    interactiveDemo: 'Interactive Visualization',
    demoDesc: 'Explore how embeddings cluster by meaning',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Embeddings convert text to vectors that capture semantic meaning',
    takeaway2: 'Similar concepts have similar embeddings (cosine similarity)',
    takeaway3: 'Embeddings enable semantic search, clustering, and RAG',
    takeaway4: 'Different embedding models have different strengths and dimensions',
  },

  // RAG page
  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: giving LLMs access to external knowledge.',
    whatIs: 'What is RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving relevant documents from a knowledge base and including them in the prompt. This gives models access to up-to-date or specialized information.',
    whyRag: 'Why Use RAG?',
    whyRagDesc: 'LLMs have knowledge cutoffs and can hallucinate. RAG grounds responses in actual documents, reducing hallucination and enabling domain-specific knowledge without fine-tuning.',
    pipeline: 'The RAG Pipeline',
    pipelineDesc: 'RAG systems follow a consistent pattern: embed the query, retrieve relevant chunks, augment the prompt, and generate a response.',
    step1: 'Query Embedding',
    step1Desc: 'Convert the user\'s question into a vector using an embedding model.',
    step2: 'Retrieval',
    step2Desc: 'Search the vector database for chunks similar to the query embedding.',
    step3: 'Augmentation',
    step3Desc: 'Insert retrieved chunks into the prompt as context.',
    step4: 'Generation',
    step4Desc: 'The LLM generates a response grounded in the retrieved context.',
    chunking: 'Document Chunking',
    chunkingDesc: 'Documents are split into smaller chunks (typically 200-1000 tokens) for embedding and retrieval. Chunk size affects retrieval precision.',
    vectorDbs: 'Vector Databases',
    vectorDbsDesc: 'Specialized databases like Pinecone, Weaviate, or pgvector enable fast similarity search over millions of embeddings.',
    interactiveDemo: 'Interactive RAG Pipeline',
    demoDesc: 'See how queries flow through a RAG system',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'RAG retrieves relevant documents and includes them in the prompt',
    takeaway2: 'It reduces hallucination by grounding responses in actual sources',
    takeaway3: 'Chunking strategy and embedding quality are critical for good retrieval',
    takeaway4: 'RAG is often preferable to fine-tuning for adding domain knowledge',
  },

  // Tool Design page
  toolDesign: {
    title: 'Tool Design',
    description: 'Best practices for designing effective tools that AI agents can use reliably.',
    whatIs: 'What Makes a Good Tool?',
    whatIsDesc: 'Well-designed tools are the foundation of capable AI agents. A tool\'s schema, naming, and documentation directly impact how reliably an LLM can use it.',
    principles: 'Design Principles',
    principlesDesc: 'Follow these principles to create tools that agents can use effectively.',
    principle1: 'Clear Naming',
    principle1Desc: 'Use descriptive, unambiguous names. "search_web" is better than "sw" or "query".',
    principle2: 'Explicit Parameters',
    principle2Desc: 'Every parameter should have a clear type, description, and constraints.',
    principle3: 'Predictable Outputs',
    principle3Desc: 'Return consistent, structured responses. Include error messages in the output.',
    principle4: 'Minimal Scope',
    principle4Desc: 'Each tool should do one thing well. Prefer many focused tools over few complex ones.',
    schemaDesign: 'Schema Design',
    schemaDesignDesc: 'Tool schemas tell the LLM how to use your tools. Good schemas prevent errors.',
    goodSchema: 'Good Schema',
    badSchema: 'Bad Schema',
    errorHandling: 'Error Handling',
    errorHandlingDesc: 'Tools should handle errors gracefully and return informative messages the LLM can act on.',
    interactiveDemo: 'Tool Schema Builder',
    demoDesc: 'Build and validate tool schemas interactively',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tool design directly impacts agent reliability',
    takeaway2: 'Explicit schemas with descriptions prevent LLM confusion',
    takeaway3: 'Return structured errors the agent can understand and act on',
    takeaway4: 'Test tools with various inputs to find edge cases',
  },

  // Memory Systems page
  memorySystems: {
    title: 'Memory Systems',
    description: 'How AI agents maintain context and remember information across interactions.',
    whatIs: 'What are Agent Memory Systems?',
    whatIsDesc: 'Memory systems allow agents to retain and recall information beyond the immediate context window. They enable agents to learn from past interactions and maintain coherent long-term behavior.',
    types: 'Types of Memory',
    typesDesc: 'Agent memory systems typically combine multiple memory types for different purposes.',
    shortTerm: 'Short-Term Memory',
    shortTermDesc: 'The current conversation context. Limited by context window size.',
    longTerm: 'Long-Term Memory',
    longTermDesc: 'Persistent storage of past interactions, facts, and learned preferences.',
    episodic: 'Episodic Memory',
    episodicDesc: 'Specific past events and interactions that can be recalled.',
    semantic: 'Semantic Memory',
    semanticDesc: 'General knowledge and facts extracted from experiences.',
    implementation: 'Implementation Approaches',
    implementationDesc: 'Various techniques for implementing agent memory.',
    vectorStore: 'Vector Stores',
    vectorStoreDesc: 'Store embeddings of past interactions for semantic retrieval.',
    summaries: 'Conversation Summaries',
    summariesDesc: 'Periodically summarize long conversations to preserve key information.',
    keyValue: 'Key-Value Stores',
    keyValueDesc: 'Store explicit facts and user preferences for direct lookup.',
    interactiveDemo: 'Memory System Visualizer',
    demoDesc: 'See how different memory types work together',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Memory extends agent capabilities beyond the context window',
    takeaway2: 'Combine multiple memory types for best results',
    takeaway3: 'Memory retrieval adds latency—balance comprehensiveness with speed',
    takeaway4: 'Consider privacy and data retention when storing memories',
  },

  // Orchestration page
  orchestration: {
    title: 'Orchestration',
    description: 'Coordinating multiple agents and complex multi-step workflows.',
    whatIs: 'What is Agent Orchestration?',
    whatIsDesc: 'Orchestration is the coordination of multiple AI agents or complex multi-step workflows. It involves routing tasks, managing state, handling failures, and combining agent outputs.',
    patterns: 'Orchestration Patterns',
    patternsDesc: 'Common patterns for structuring multi-agent systems.',
    sequential: 'Sequential Pipeline',
    sequentialDesc: 'Agents run in order, each processing the output of the previous one.',
    parallel: 'Parallel Execution',
    parallelDesc: 'Multiple agents work simultaneously on different aspects of a task.',
    hierarchical: 'Hierarchical',
    hierarchicalDesc: 'A supervisor agent delegates to specialized worker agents.',
    dynamic: 'Dynamic Routing',
    dynamicDesc: 'An LLM decides which agent should handle each request.',
    stateManagement: 'State Management',
    stateManagementDesc: 'Orchestrators must track progress, intermediate results, and handle failures.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Save state at key points to enable recovery from failures.',
    rollback: 'Rollback',
    rollbackDesc: 'Ability to undo steps when errors occur.',
    interactiveDemo: 'Workflow Visualizer',
    demoDesc: 'Design and visualize agent workflows',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Orchestration enables complex tasks through agent composition',
    takeaway2: 'Choose patterns based on task dependencies and parallelism',
    takeaway3: 'Robust state management is essential for reliability',
    takeaway4: 'Monitor orchestration costs—multi-agent systems multiply API calls',
  },

  // Evaluation page
  evaluation: {
    title: 'Evaluation',
    description: 'Measuring and improving AI agent performance systematically.',
    whatIs: 'Why Evaluate Agents?',
    whatIsDesc: 'Agent evaluation is critical for understanding performance, catching regressions, and improving reliability. Without measurement, you\'re flying blind.',
    metrics: 'Key Metrics',
    metricsDesc: 'Important metrics to track for agent systems.',
    taskSuccess: 'Task Success Rate',
    taskSuccessDesc: 'Percentage of tasks completed correctly.',
    efficiency: 'Efficiency',
    efficiencyDesc: 'Steps taken, tokens used, time elapsed per task.',
    accuracy: 'Accuracy',
    accuracyDesc: 'Correctness of agent outputs and decisions.',
    reliability: 'Reliability',
    reliabilityDesc: 'Consistency across repeated runs of the same task.',
    approaches: 'Evaluation Approaches',
    approachesDesc: 'Different ways to evaluate agent performance.',
    unitTests: 'Unit Tests',
    unitTestsDesc: 'Test individual tools and components in isolation.',
    integration: 'Integration Tests',
    integrationDesc: 'Test the full agent loop with mock environments.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard task suites for comparing agents.',
    humanEval: 'Human Evaluation',
    humanEvalDesc: 'Expert review for nuanced quality assessment.',
    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Guidelines for effective agent evaluation.',
    practice1: 'Test edge cases and failure modes, not just happy paths.',
    practice2: 'Track costs alongside quality metrics.',
    practice3: 'Use versioned evaluations to catch regressions.',
    practice4: 'Include adversarial tests for security.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Evaluation is essential—unmeasured systems can\'t be improved',
    takeaway2: 'Combine automated tests with human evaluation',
    takeaway3: 'Track multiple metrics: success, efficiency, cost',
    takeaway4: 'Build evaluation into your development workflow',
  },

  // Neural Networks page
  neuralNetworks: {
    title: 'Neural Networks',
    description: 'The foundational architecture that powers modern AI.',
    whatIs: 'What is a Neural Network?',
    whatIsDesc: 'A neural network is a computational model inspired by the brain. It consists of layers of interconnected nodes (neurons) that learn to transform inputs into outputs through training.',
    components: 'Core Components',
    componentsDesc: 'The building blocks of neural networks.',
    neurons: 'Neurons',
    neuronsDesc: 'Basic units that compute weighted sums of inputs and apply activation functions.',
    layers: 'Layers',
    layersDesc: 'Groups of neurons: input layer, hidden layers, and output layer.',
    weights: 'Weights & Biases',
    weightsDesc: 'Learnable parameters that determine how inputs are transformed.',
    activations: 'Activation Functions',
    activationsDesc: 'Non-linear functions that allow networks to learn complex patterns.',
    typesOfNetworks: 'Types of Networks',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Information flows one direction. Good for tabular data.',
    cnn: 'Convolutional (CNN)',
    cnnDesc: 'Specialized for images and spatial data.',
    rnn: 'Recurrent (RNN)',
    rnnDesc: 'Processes sequences with memory of past inputs.',
    transformer: 'Transformer',
    transformerDesc: 'Attention-based architecture powering modern LLMs.',
    interactiveDemo: 'Neural Network Visualizer',
    demoDesc: 'Build and explore network architectures',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Neural networks learn by adjusting weights through training',
    takeaway2: 'Depth (more layers) enables learning hierarchical features',
    takeaway3: 'Different architectures suit different data types',
    takeaway4: 'Modern LLMs are massive transformer networks',
  },

  // Gradient Descent page
  gradientDescent: {
    title: 'Gradient Descent',
    description: 'The optimization algorithm that enables neural networks to learn.',
    whatIs: 'What is Gradient Descent?',
    whatIsDesc: 'Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize a loss function. It\'s how neural networks learn from data.',
    intuition: 'The Intuition',
    intuitionDesc: 'Imagine standing in a hilly landscape blindfolded, trying to reach the lowest point. You feel the slope beneath your feet and step downhill. Repeat until you reach a valley.',
    howWorks: 'How It Works',
    howWorksDesc: 'The algorithm computes how much each parameter contributes to the error, then adjusts parameters in the opposite direction.',
    step1: 'Compute Loss',
    step1Desc: 'Measure how wrong the current predictions are.',
    step2: 'Calculate Gradients',
    step2Desc: 'Use backpropagation to find how each weight affects the loss.',
    step3: 'Update Weights',
    step3Desc: 'Adjust weights in the direction that reduces loss.',
    step4: 'Repeat',
    step4Desc: 'Iterate until the loss stops decreasing.',
    learningRate: 'Learning Rate',
    learningRateDesc: 'Controls how big each step is. Too high: overshoot. Too low: slow progress.',
    variants: 'Variants',
    sgd: 'Stochastic Gradient Descent',
    sgdDesc: 'Uses random mini-batches instead of the full dataset.',
    momentum: 'Momentum',
    momentumDesc: 'Accumulates velocity to push through local minima.',
    adam: 'Adam',
    adamDesc: 'Adaptive learning rates per parameter. Most common today.',
    interactiveDemo: 'Gradient Descent Visualizer',
    demoDesc: 'Watch gradient descent find the minimum',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Gradient descent minimizes loss by following the slope',
    takeaway2: 'Learning rate is the most important hyperparameter',
    takeaway3: 'Modern optimizers like Adam adapt learning rates automatically',
    takeaway4: 'Backpropagation computes gradients efficiently',
  },

  // Training Process page
  training: {
    title: 'Training Process',
    description: 'How neural networks learn from data through iterative optimization.',
    whatIs: 'What is Training?',
    whatIsDesc: 'Training is the process of teaching a neural network to perform a task by exposing it to examples and adjusting its parameters based on errors.',
    phases: 'Training Phases',
    phasesDesc: 'The stages of training a neural network.',
    initialization: 'Initialization',
    initializationDesc: 'Set random starting weights. Good initialization helps training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Input flows through the network to produce predictions.',
    lossCalc: 'Loss Calculation',
    lossCalcDesc: 'Compare predictions to ground truth with a loss function.',
    backprop: 'Backpropagation',
    backpropDesc: 'Compute gradients of loss with respect to each weight.',
    optimization: 'Optimization',
    optimizationDesc: 'Update weights using gradient descent.',
    concepts: 'Key Concepts',
    epoch: 'Epoch',
    epochDesc: 'One complete pass through the entire training dataset.',
    batch: 'Batch Size',
    batchDesc: 'Number of examples processed before updating weights.',
    overfitting: 'Overfitting',
    overfittingDesc: 'Model memorizes training data but fails on new data.',
    regularization: 'Regularization',
    regularizationDesc: 'Techniques to prevent overfitting (dropout, weight decay).',
    interactiveDemo: 'Training Progress Visualizer',
    demoDesc: 'Watch a network learn in real-time',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Training iteratively reduces prediction errors',
    takeaway2: 'Overfitting is the main enemy—always validate on held-out data',
    takeaway3: 'Batch size and learning rate significantly affect training',
    takeaway4: 'Modern LLMs require massive compute for training',
  },

  // Prompt Basics page
  promptBasics: {
    title: 'Prompt Basics',
    description: 'Fundamentals of writing effective prompts for AI models.',
    whatIs: 'What is a Prompt?',
    whatIsDesc: 'A prompt is the input you give to an LLM. The quality of your prompt directly determines the quality of the response. Prompting is both an art and a science.',
    principles: 'Core Principles',
    principlesDesc: 'Fundamental guidelines for effective prompts.',
    beSpecific: 'Be Specific',
    beSpecificDesc: 'Vague prompts get vague answers. Include relevant details and constraints.',
    showExamples: 'Show Examples',
    showExamplesDesc: 'Demonstrate the format and style you want with concrete examples.',
    giveContext: 'Provide Context',
    giveContextDesc: 'Background information helps the model understand your needs.',
    setFormat: 'Specify Format',
    setFormatDesc: 'Tell the model exactly how you want the output structured.',
    anatomy: 'Anatomy of a Prompt',
    anatomyDesc: 'The components that make up an effective prompt.',
    role: 'Role/Persona',
    roleDesc: 'Who the model should act as.',
    task: 'Task Description',
    taskDesc: 'What you want the model to do.',
    context: 'Context/Background',
    contextDesc: 'Relevant information for the task.',
    format: 'Output Format',
    formatDesc: 'How you want the response structured.',
    interactiveDemo: 'Prompt Comparison',
    demoDesc: 'Compare weak vs. strong prompts',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Clear, specific prompts yield better results',
    takeaway2: 'Examples are powerful—show, don\'t just tell',
    takeaway3: 'Iterate on prompts; first attempts are rarely optimal',
    takeaway4: 'Consider the model\'s perspective when crafting prompts',
  },

  // Advanced Prompting page
  advancedPrompting: {
    title: 'Advanced Techniques',
    description: 'Sophisticated prompting strategies for complex tasks.',
    overview: 'Beyond the Basics',
    overviewDesc: 'Advanced techniques unlock more capable and reliable AI behavior for complex tasks.',
    cot: 'Chain of Thought',
    cotDesc: 'Encourage step-by-step reasoning by asking the model to "think through" problems.',
    cotExample: 'Example: "Let\'s solve this step by step..."',
    fewShot: 'Few-Shot Learning',
    fewShotDesc: 'Provide multiple examples to establish patterns the model should follow.',
    fewShotExample: 'Include 3-5 diverse examples covering edge cases.',
    selfConsistency: 'Self-Consistency',
    selfConsistencyDesc: 'Generate multiple responses and select the most consistent answer.',
    selfConsistencyExample: 'Useful for math, logic, and factual questions.',
    decomposition: 'Task Decomposition',
    decompositionDesc: 'Break complex tasks into smaller, manageable sub-tasks.',
    decompositionExample: 'Solve sub-tasks independently, then combine results.',
    techniques: 'Additional Techniques',
    rolePlay: 'Role Assignment',
    rolePlayDesc: 'Assign a specific expert persona to focus the model\'s knowledge.',
    constraints: 'Explicit Constraints',
    constraintsDesc: 'List what the model should NOT do to prevent common errors.',
    verification: 'Self-Verification',
    verificationDesc: 'Ask the model to check its own work for errors.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'See how reasoning steps improve outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Chain of thought dramatically improves reasoning tasks',
    takeaway2: 'Few-shot examples establish reliable patterns',
    takeaway3: 'Task decomposition handles complexity',
    takeaway4: 'Combine techniques for best results',
  },

  // System Prompts page
  systemPrompts: {
    title: 'System Prompts',
    description: 'Configuring AI behavior through system-level instructions.',
    whatIs: 'What is a System Prompt?',
    whatIsDesc: 'A system prompt is a special instruction that sets the context, persona, and behavioral guidelines for an AI model. It\'s typically hidden from users and persists throughout a conversation.',
    purpose: 'Purpose of System Prompts',
    purposeDesc: 'System prompts establish the foundation for how the AI should behave.',
    setPersona: 'Define Persona',
    setPersonaDesc: 'Establish who the AI is: an assistant, expert, character, etc.',
    setBoundaries: 'Set Boundaries',
    setBoundariesDesc: 'Define what the AI should and shouldn\'t do.',
    establishTone: 'Establish Tone',
    establishToneDesc: 'Specify communication style: formal, casual, technical.',
    provideKnowledge: 'Provide Context',
    provideKnowledgeDesc: 'Include domain knowledge or rules specific to your application.',
    structure: 'Structure of Effective System Prompts',
    structureDesc: 'Well-organized system prompts are easier for models to follow.',
    identity: 'Identity Section',
    identityDesc: 'Who is the AI? What is its role?',
    capabilities: 'Capabilities',
    capabilitiesDesc: 'What can the AI do? What tools does it have?',
    limitations: 'Limitations',
    limitationsDesc: 'What should the AI avoid or refuse?',
    guidelines: 'Guidelines',
    guidelinesDesc: 'Specific rules for behavior and responses.',
    bestPractices: 'Best Practices',
    practice1: 'Be explicit about edge cases and error handling.',
    practice2: 'Test system prompts with adversarial inputs.',
    practice3: 'Version control your system prompts.',
    practice4: 'Keep prompts focused—don\'t overload with instructions.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'System prompts define the AI\'s persona and behavior',
    takeaway2: 'Structure prompts clearly: identity, capabilities, limitations',
    takeaway3: 'Test with edge cases—users will find them',
    takeaway4: 'System prompts can be overridden—don\'t rely solely on them for security',
  },

  // LLM Training page (formerly Alignment)
  llmTraining: {
    title: 'LLM Training',
    description: 'How large language models are trained: from pretraining to RLHF, and the new RL paradigm.',
    whatIs: 'How LLMs Are Trained',
    whatIsDesc: 'Large language models go through multiple training stages, each with different objectives and techniques. Understanding this pipeline is crucial for understanding model capabilities and limitations.',
    whyMatters: 'Why Training Matters',
    whyMattersDesc: 'The training process fundamentally shapes what LLMs can and cannot do. Different training approaches produce models with different strengths, weaknesses, and behaviors.',

    // LLM Training Pipeline
    trainingPipeline: 'The LLM Training Pipeline',
    trainingPipelineDesc: 'Modern LLMs go through multiple training stages, each with different objectives. Understanding this pipeline is crucial for understanding where alignment fits in.',

    pretraining: 'Stage 1: Pretraining',
    pretrainingDesc: 'The foundation model is trained on massive text corpora (trillions of tokens) using self-supervised learning. The model learns to predict the next token, developing broad knowledge and language capabilities.',
    pretrainingGoal: 'Goal: Learn language patterns, facts, and reasoning from raw text.',
    pretrainingData: 'Data: Web pages, books, code, scientific papers—typically 1-10+ trillion tokens.',
    pretrainingResult: 'Result: A capable but unaligned "base model" that completes text but doesn\'t follow instructions.',

    sft: 'Stage 2: Supervised Fine-Tuning (SFT)',
    sftDesc: 'The base model is fine-tuned on curated instruction-response pairs created by human annotators. This teaches the model to follow instructions and respond helpfully.',
    sftGoal: 'Goal: Transform the base model into an instruction-following assistant.',
    sftData: 'Data: ~10K-100K high-quality instruction-response examples.',
    sftResult: 'Result: A model that can follow instructions but may still produce harmful or unhelpful outputs.',

    rlhfStage: 'Stage 3: RLHF / Preference Tuning',
    rlhfStageDesc: 'Human evaluators rank model outputs by quality. A reward model learns these preferences, then the LLM is optimized to maximize the reward using reinforcement learning (PPO) or direct preference optimization (DPO).',
    rlhfGoal: 'Goal: Align the model with human preferences for helpfulness, harmlessness, and honesty.',
    rlhfData: 'Data: Human preference comparisons (A is better than B).',
    rlhfResult: 'Result: A model that produces outputs humans prefer and avoids harmful behaviors.',

    continuedTraining: 'Stage 4: Continued Training & Specialized Alignment',
    continuedTrainingDesc: 'Models may undergo additional training for specific capabilities (coding, math, tool use) or safety refinements (red teaming, constitutional AI). This stage is ongoing throughout deployment.',

    // RL Paradigm
    rlParadigm: 'The RL Paradigm: Learning Without Human Labels',
    rlParadigmDesc: 'A revolutionary approach where models learn reasoning through pure reinforcement learning on verifiable tasks, without human demonstrations or preference labels.',

    rlParadigmWhat: 'What is the RL Paradigm?',
    rlParadigmWhatDesc: 'Instead of learning from human-written examples (SFT) or human preferences (RLHF), models learn directly from outcome-based rewards. If the answer is correct, the model is rewarded. If wrong, it\'s penalized. No human labeling required.',

    deepseekR1: 'DeepSeek R1-Zero: A Case Study',
    deepseekR1Desc: 'DeepSeek R1-Zero demonstrated that powerful reasoning can emerge from pure RL, without any supervised fine-tuning. The model developed chain-of-thought reasoning, self-verification, and even "aha moments" entirely through reinforcement learning.',

    rlKey1: 'No SFT Required',
    rlKey1Desc: 'R1-Zero was trained directly from a base model using only RL, skipping the SFT stage entirely. Reasoning behaviors emerged naturally.',
    rlKey2: 'Verifiable Rewards',
    rlKey2Desc: 'Training focused on tasks with objectively verifiable answers: math problems, coding challenges, logical puzzles. No subjective human judgment needed.',
    rlKey3: 'Emergent Behaviors',
    rlKey3Desc: 'The model spontaneously developed extended thinking, self-correction, and reflection—behaviors that previous models only learned from human demonstrations.',
    rlKey4: 'Readability Challenges',
    rlKey4Desc: 'Pure RL models can develop unusual reasoning patterns that are hard to interpret. DeepSeek added a small amount of human data to improve readability.',

    rlVsRlhf: 'RL Paradigm vs. Traditional RLHF',
    rlVsRlhfDesc: 'These approaches solve different problems and can be complementary.',
    rlhfApproach: 'RLHF Approach',
    rlhfApproachDesc: 'Learn from human preferences. Requires expensive human labeling. Good for subjective tasks like writing quality and helpfulness.',
    rlApproach: 'RL Paradigm Approach',
    rlApproachDesc: 'Learn from verifiable outcomes. No human labeling needed. Excellent for reasoning, math, and coding where correctness is objective.',
    hybridApproach: 'Hybrid Approach',
    hybridApproachDesc: 'Modern models often combine both: RL for reasoning capabilities, RLHF for alignment and user preferences.',

    // Key Alignment Concepts
    concepts: 'Key Alignment Concepts',
    conceptsDesc: 'Fundamental ideas in AI alignment research.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'Ensuring the training objective (reward function) correctly captures what we want. Even perfect optimization of a misspecified objective leads to bad outcomes.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'Ensuring the learned model actually optimizes for the training objective, not some proxy goal that happens to correlate during training.',
    specification: 'Specification Problem',
    specificationDesc: 'The fundamental difficulty of precisely stating what we want in all situations. Human values are complex, contextual, and sometimes contradictory.',
    robustness: 'Robustness',
    robustnessDesc: 'Maintaining alignment under distribution shift, adversarial pressure, and novel situations the model wasn\'t trained on.',
    deception: 'Deceptive Alignment',
    deceptionDesc: 'A theoretical risk where a model appears aligned during training but pursues different goals when deployed—behaving well only because it\'s being evaluated.',
    goalMisgeneralization: 'Goal Misgeneralization',
    goalMisgeneralizationDesc: 'When a model learns a proxy goal that works in training but fails in deployment. Example: learning to get positive feedback rather than being genuinely helpful.',

    // Alignment Techniques
    techniques: 'Alignment Techniques',
    rlhf: 'RLHF (Reinforcement Learning from Human Feedback)',
    rlhfDesc: 'Train a reward model on human preferences, then use RL to optimize the LLM against it. The dominant alignment technique since GPT-4.',
    constitutionalAi: 'Constitutional AI (CAI)',
    constitutionalAiDesc: 'Define principles (a "constitution") and have the model critique and revise its own outputs. Reduces reliance on human labelers and scales better.',
    dpo: 'Direct Preference Optimization (DPO)',
    dpoDesc: 'Skip the reward model—directly optimize the LLM on preference data. Simpler and more stable than RLHF.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarial testing by humans or other AI models to find failure modes, jailbreaks, and harmful outputs before deployment.',
    interpretability: 'Interpretability',
    interpretabilityDesc: 'Understanding what models are actually learning internally. Crucial for verifying alignment rather than just measuring behavior.',
    safetyFilters: 'Safety Filters & Guardrails',
    safetyFiltersDesc: 'Additional layers that filter inputs/outputs for harmful content. A defense-in-depth measure, not a replacement for alignment.',

    // Fine-tuning vs Alignment
    fineTuningVsAlignment: 'Fine-Tuning vs. Alignment',
    fineTuningVsAlignmentDesc: 'Fine-tuning and alignment are related but distinct concepts.',
    fineTuningDef: 'Fine-Tuning',
    fineTuningDefDesc: 'Adapting a model to new tasks or domains by training on task-specific data. Can be done for any purpose.',
    alignmentDef: 'Alignment',
    alignmentDefDesc: 'Specifically making a model\'s behavior match human values and intentions. A subset of fine-tuning with a specific goal.',
    postTrainingDef: 'Post-Training',
    postTrainingDefDesc: 'The umbrella term for everything after pretraining: SFT, RLHF, specialized fine-tuning, safety training, etc.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'LLM training has distinct stages: pretraining → SFT → RLHF → specialized alignment',
    takeaway2: 'The RL paradigm (e.g., DeepSeek R1-Zero) shows reasoning can emerge from pure RL without human demonstrations',
    takeaway3: 'RLHF aligns models with human preferences; pure RL optimizes for verifiable outcomes',
    takeaway4: 'Modern models often combine multiple techniques: SFT for instruction following, RLHF for preferences, RL for reasoning',
    takeaway5: 'Understanding the training pipeline helps you understand model behavior and limitations',
    takeaway6: 'The field is rapidly evolving—new paradigms like pure RL are changing how we think about training',
  },

  // Bias & Fairness page
  bias: {
    title: 'Bias & Fairness',
    description: 'Understanding and mitigating harmful biases in AI systems.',
    whatIs: 'What is AI Bias?',
    whatIsDesc: 'AI bias occurs when machine learning systems produce systematically unfair outcomes for certain groups. Biases can arise from training data, model design, or deployment context.',
    sources: 'Sources of Bias',
    sourcesDesc: 'Where bias enters AI systems.',
    dataBias: 'Training Data',
    dataBiasDesc: 'Historical biases in the data are learned by the model.',
    labelBias: 'Label Bias',
    labelBiasDesc: 'Human annotators introduce their own biases.',
    selectionBias: 'Selection Bias',
    selectionBiasDesc: 'Training data doesn\'t represent the deployment population.',
    measurementBias: 'Measurement Bias',
    measurementBiasDesc: 'Proxies used for measurement encode bias.',
    types: 'Types of Bias',
    typesDesc: 'Common categories of bias in AI systems.',
    stereotyping: 'Stereotyping',
    stereotypingDesc: 'Reinforcing harmful stereotypes about groups.',
    erasure: 'Erasure',
    erasureDesc: 'Underrepresenting or ignoring certain groups.',
    disparateImpact: 'Disparate Impact',
    disparateImpactDesc: 'Different outcomes for different groups.',
    mitigation: 'Mitigation Strategies',
    mitigationDesc: 'Approaches to reduce bias.',
    diverseData: 'Diverse Data',
    diverseDataDesc: 'Ensure training data represents all relevant groups.',
    auditing: 'Bias Auditing',
    auditingDesc: 'Systematically test for bias across demographics.',
    constraints: 'Fairness Constraints',
    constraintsDesc: 'Incorporate fairness metrics into training.',
    interactiveDemo: 'Bias Detection Demo',
    demoDesc: 'Explore how bias manifests in model outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Bias is often inherited from training data',
    takeaway2: 'Different fairness metrics can conflict—choose carefully',
    takeaway3: 'Regular auditing is essential for deployed systems',
    takeaway4: 'Bias mitigation is an ongoing process, not a one-time fix',
  },

  // Responsible AI page
  responsibleAi: {
    title: 'Responsible AI',
    description: 'Building and deploying AI systems ethically and sustainably.',
    whatIs: 'What is Responsible AI?',
    whatIsDesc: 'Responsible AI encompasses the practices, policies, and principles that ensure AI systems are developed and used ethically, safely, and in ways that benefit society.',
    pillars: 'Pillars of Responsible AI',
    pillarsDesc: 'Core principles guiding responsible AI development.',
    transparency: 'Transparency',
    transparencyDesc: 'Be open about AI capabilities, limitations, and decision-making.',
    accountability: 'Accountability',
    accountabilityDesc: 'Clear ownership and responsibility for AI outcomes.',
    privacy: 'Privacy',
    privacyDesc: 'Protect user data and respect privacy rights.',
    safety: 'Safety',
    safetyDesc: 'Ensure systems are robust and don\'t cause harm.',
    practices: 'Responsible Practices',
    practicesDesc: 'Concrete steps for responsible AI development.',
    documentation: 'Documentation',
    documentationDesc: 'Document model capabilities, training data, and known limitations.',
    testing: 'Comprehensive Testing',
    testingDesc: 'Test for safety, bias, and edge cases before deployment.',
    monitoring: 'Ongoing Monitoring',
    monitoringDesc: 'Track system behavior in production for issues.',
    feedback: 'User Feedback',
    feedbackDesc: 'Create channels for users to report problems.',
    considerations: 'Ethical Considerations',
    environmental: 'Environmental Impact',
    environmentalDesc: 'AI training has significant carbon footprint.',
    labor: 'Labor Implications',
    laborDesc: 'Consider impact on workers and employment.',
    access: 'Equitable Access',
    accessDesc: 'Ensure AI benefits are broadly distributed.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Responsible AI requires proactive effort throughout the lifecycle',
    takeaway2: 'Transparency builds trust and enables accountability',
    takeaway3: 'Consider societal impact beyond immediate users',
    takeaway4: 'Ethics are not optional—integrate them into development processes',
  },

  // Metadata
  metadata: {
    title: 'Learn AI Concepts | Interactive Guide',
    description: 'Master artificial intelligence and large language model concepts through beautiful, interactive demonstrations.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Control Panel',
    adjustTemperature: 'Adjust the temperature',
    temperature: 'Temperature',
    samplePrompt: 'Sample Prompt',
    onceUponATime: '"Once upon a time, there was..."',
    liveCompletion: 'Live Completion',
    regenerate: 'Regenerate',
    deterministic: 'Deterministic',
    balanced: 'Balanced',
    creative: 'Creative',
    chaotic: 'Chaotic',
    frozen: 'Frozen',
    focused: 'Focused',
    wild: 'Wild',
    greedyMode: 'Greedy mode: Always picks the single most probable token.',
    lowTemp: 'Low temperature: Focused on high-probability continuations.',
    balancedTemp: 'Balanced: Natural mix of predictability and variety.',
    highTemp: 'High temperature: Exploring creative, less common word choices.',
    veryHighTemp: 'Very high: Probability distribution is nearly uniform—expect chaos!',
    
    // Context Rot Simulator
    setInstruction: 'Set Your System Instruction',
    persistInstruction: 'This should persist throughout the conversation',
    systemPrompt: 'System Prompt',
    quickExamples: 'Quick Examples',
    startSimulation: 'Start Simulation',
    contextOverflow: 'Context Overflow!',
    conversation: 'Conversation',
    messagesPushed: 'messages pushed out of window',
    messages: 'messages',
    overflowIt: 'Overflow It!',
    reset: 'Reset',
    typeMessage: 'Type a message...',
    systemInstructionLost: 'System Instruction Lost!',
    systemLostDesc: 'Your system instruction has been pushed completely out of the context window. The model can no longer see it at all—it\'s as if you never gave the instruction. This is the worst case of context rot: total amnesia.',
    contextFilling: 'Context Filling Up',
    contextFillingDesc: 'Your system instruction is losing influence as newer messages take priority. Notice how it\'s fading visually—this represents the model\'s waning attention to it.',
    exampleFrench: 'Always respond in French.',
    examplePirate: 'You are a pirate. Say \'Arrr\' a lot.',
    exampleHaiku: 'End every response with a haiku.',
    labelFrench: 'Speak French',
    labelPirate: 'Be a Pirate',
    labelHaiku: 'End with Haiku',

    // Attention Visualizer
    hoverToSee: 'Hover to see attention weights',
    token: 'Token',
    attentionScore: 'Attention Score',
    strongConnection: 'Strong Connection',
    weakConnection: 'Weak Connection',

    // Patch Grid Visualizer
    originalImage: 'Original Image',
    patchGrid: 'Patch Grid',
    flattenedPatches: 'Flattened Patches',
    transformerInput: 'Transformer Input',
    processDesc: 'The image is split into a fixed grid of patches (e.g., 16x16 pixels). Each patch is then flattened into a vector and linearly projected into an embedding space.',

    // Agent Loop Visualizer
    startLoop: 'Start Loop',
    step: 'Step',
    context: 'Context',
    llmResponse: 'LLM Response',
    toolExecution: 'Tool Execution',
    finalAnswer: 'Final Answer',
    system: 'System',
    user: 'User',
    assistant: 'Assistant',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Plan & Execute',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflection',
    patternDesc: 'Select a pattern to see how it structures the agent\'s workflow.',

    // Tokenizer Demo
    enterText: 'Enter text to tokenize',
    sampleText: 'The quick brown fox jumps over the lazy dog.',
    tokens: 'Tokens',
    characters: 'Characters',
    tokensPerChar: 'Tokens per character',
    tokenBreakdown: 'Token Breakdown',
    commonTokens: 'Common tokens are single pieces',
    rareTokens: 'Rare words get split into subwords',

    // Embedding Visualizer
    enterWords: 'Enter words to compare',
    addWord: 'Add Word',
    similarityScore: 'Similarity Score',
    dimensions: 'Dimensions',
    nearestNeighbors: 'Nearest Neighbors',
    vectorSpace: 'Vector Space',

    // RAG Pipeline Visualizer
    enterQuery: 'Enter your query',
    sampleQuery: 'What is the capital of France?',
    retrieving: 'Retrieving...',
    retrieved: 'Retrieved Documents',
    relevanceScore: 'Relevance',
    generating: 'Generating response...',
    augmentedContext: 'Augmented Context',

    // Tool Schema Builder
    toolName: 'Tool Name',
    toolDescription: 'Description',
    addParameter: 'Add Parameter',
    paramName: 'Parameter Name',
    paramType: 'Type',
    paramRequired: 'Required',
    generatedSchema: 'Generated Schema',
    validateSchema: 'Validate Schema',

    // Memory System Visualizer
    shortTermMemory: 'Short-Term Memory',
    longTermMemory: 'Long-Term Memory',
    memoryCapacity: 'Capacity',
    memoryUsage: 'Usage',
    addMemory: 'Add Memory',
    recallMemory: 'Recall',
    memoriesStored: 'memories stored',

    // Workflow Visualizer
    addNode: 'Add Node',
    connectNodes: 'Connect Nodes',
    runWorkflow: 'Run Workflow',
    nodeTypes: 'Node Types',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Condition',

    // Neural Network Visualizer
    inputLayer: 'Input Layer',
    hiddenLayer: 'Hidden Layer',
    outputLayer: 'Output Layer',
    addLayer: 'Add Layer',
    removeLayer: 'Remove Layer',
    neurons: 'Neurons',
    activation: 'Activation',
    forward: 'Forward',

    // Gradient Descent Visualizer
    startDescent: 'Start Descent',
    pauseDescent: 'Pause',
    resetDescent: 'Reset',
    learningRate: 'Learning Rate',
    currentLoss: 'Current Loss',
    iterations: 'Iterations',
    globalMinimum: 'Global Minimum',
    localMinimum: 'Local Minimum',

    // Training Progress Visualizer
    startTraining: 'Start Training',
    stopTraining: 'Stop',
    epoch: 'Epoch',
    trainingLoss: 'Training Loss',
    validationLoss: 'Validation Loss',
    accuracy: 'Accuracy',
    overfitting: 'Overfitting detected',

    // Prompt Comparison Demo
    weakPrompt: 'Weak Prompt',
    strongPrompt: 'Strong Prompt',
    compare: 'Compare',
    promptQuality: 'Quality Score',
    improvements: 'Improvements',

    // Chain of Thought Demo
    withoutCot: 'Without Chain of Thought',
    withCot: 'With Chain of Thought',
    reasoningSteps: 'Reasoning Steps',
    showSteps: 'Show Steps',

    // Bias Detection Demo
    testInput: 'Test Input',
    analyzeForBias: 'Analyze for Bias',
    biasIndicators: 'Bias Indicators',
    fairnessScore: 'Fairness Score',
    recommendations: 'Recommendations',
  },
}

// Create a recursive string type for the dictionary
type DeepStringify<T> = {
  [K in keyof T]: T[K] extends Record<string, unknown> ? DeepStringify<T[K]> : string
}

export type Dictionary = DeepStringify<typeof en>
