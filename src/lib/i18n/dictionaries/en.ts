export const en = {
  // Common UI
  common: {
    learnAi: 'Learn AI',
    interactiveGuide: 'Interactive Guide',
    topics: 'Topics',
    search: 'Search...',
    searchTopics: 'Search topics...',
    startTyping: 'Start typing to search topics...',
    trySearching: 'Try "temperature" or "attention"',
    noResults: 'No results found for',
    pressEsc: 'Press ESC to close',
    enterToSelect: 'Enter to select',
    previous: 'Previous',
    next: 'Next',
    projectBy: 'A project by',
    proTip: 'Pro tip: Press',
    toSearchTopics: 'to search topics',
    interactiveAiLearning: 'Interactive AI Learning',
    guidesDescription: 'Interactive guides to understand AI concepts',
    backToAllTopics: 'Back to all topics',
  },

  // Home page
  home: {
    heroTitle1: 'Master AI Concepts',
    heroTitle2: 'Through Experience',
    heroDescription: 'Explore artificial intelligence and large language models through beautiful, interactive demonstrations. Learn by doing, not just reading.',
    startLearning: 'Start Learning',
    browseTopics: 'Browse Topics',
    exploreTopics: 'Explore Topics',
    diveIntoLessons: 'Dive into interactive lessons',
  },

  // Features
  features: {
    interactiveDemos: 'Interactive Demos',
    interactiveDemosDesc: 'Hands-on explorations that make abstract concepts tangible and intuitive.',
    visualLearning: 'Visual Learning',
    visualLearningDesc: 'Beautiful visualizations that reveal how AI systems actually work under the hood.',
    buildIntuition: 'Build Intuition',
    buildIntuitionDesc: 'Go beyond memorization—develop deep understanding through experimentation.',
  },

  // Topic categories
  categories: {
    ai: 'Artificial Intelligence',
    agents: 'AI Agents',
    llm: 'Large Language Models',
    mlFundamentals: 'ML Fundamentals',
    prompting: 'Prompting',
    safety: 'AI Safety & Ethics',
    industry: 'AI Industry',
  },

  // Category descriptions (for category landing pages)
  categoryDescriptions: {
    agents: 'Learn how to build autonomous AI systems that can reason, plan, and take actions using tools and memory.',
    llm: 'Understand the inner workings of large language models, from tokenization to attention mechanisms.',
    'ml-fundamentals': 'Master the foundational concepts of machine learning that power modern AI systems.',
    prompting: 'Discover techniques to communicate effectively with AI models and get better results.',
    safety: 'Explore the ethical considerations and best practices for building responsible AI systems.',
    industry: 'Explore the companies and trends shaping the AI industry landscape.',
  },

  // Topic names
  topicNames: {
    // Agent subcategories
    'agents-core': 'Core Concepts',
    'agents-building': 'Building Blocks',
    'agents-patterns': 'Patterns',
    'agents-quality': 'Quality & Security',
    // Agent topics
    'agent-loop': 'The Agent Loop',
    'agent-context': 'Context Anatomy',
    'agent-problems': 'Agent Problems',
    'agent-security': 'Agent Security',
    'agentic-patterns': 'Agentic Patterns',
    'mcp': 'MCP (Model Context Protocol)',
    'tool-design': 'Tool Design',
    'memory': 'Memory Systems',
    'orchestration': 'Orchestration',
    'evaluation': 'Evaluation',
    'skills': 'Agent Skills',
    // LLM subcategories
    'llm-fundamentals': 'Fundamentals',
    'llm-behavior': 'Behavior',
    'llm-capabilities': 'Capabilities',
    'llm-architecture': 'Architecture',
    // LLM topics
    'tokenization': 'Tokenization',
    'embeddings': 'Embeddings',
    'rag': 'RAG',
    'context-rot': 'Context Rot',
    'temperature': 'Temperature',
    'attention': 'Attention Mechanism',
    'vision': 'Vision & Images',
    'visual-challenges': 'Visual Challenges',
    'llm-training': 'LLM Training',
    'moe': 'Mixture of Experts',
    // ML Fundamentals
    'ml-fundamentals': 'ML Fundamentals',
    'neural-networks': 'Neural Networks',
    'gradient-descent': 'Gradient Descent',
    'training': 'Training Process',
    // Prompting
    'prompt-basics': 'Prompt Basics',
    'advanced-prompting': 'Advanced Techniques',
    'system-prompts': 'System Prompts',
    // Safety
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Responsible AI',
    // Industry
    'european-ai': 'AI Made in Europe',
    'open-source': 'Open Source Advantages',
    'opus-4-5': "Logge's Favourite Model",
  },

  // Temperature page
  temperature: {
    title: 'Temperature',
    description: 'Understanding how a single parameter controls the balance between predictable logic and creative randomness in AI outputs.',
    whatIs: 'What is Temperature?',
    whatIsDesc: 'In LLMs, Temperature is a hyperparameter that scales the "logits" (raw scores) of the next token predictions before they are converted into probabilities. It essentially controls how much the model favors the most likely options versus exploring less likely ones.',
    lowTemp: 'Low Temperature',
    lowTempDesc: 'Focuses on the top results. Reliable, consistent, and factual. Great for code, math, and structured data.',
    highTemp: 'High Temperature',
    highTempDesc: 'Spreads probability to more tokens. Diverse, creative, and surprising. Great for stories, brainstorming, and poetry.',
    interactiveDistribution: 'Interactive Distribution',
    adjustSlider: 'Adjust the temperature to see its effect',
    adjustDesc: 'Adjust the temperature slider to see how it reshapes the probability distribution for the next token. Watch how "the" (the most likely choice) dominates at low temperatures and loses its lead as the temperature rises.',
    howItWorks: 'How it Works Mathematically',
    mathDesc: 'The model generates a score for every possible token. To get probabilities, we use the Softmax function, modified by temperature:',
    whenLow: 'When T → 0',
    low: 'Low',
    whenLowDesc: 'Dividing by a small T amplifies differences between scores. The highest logit dominates exponentially.',
    whenHigh: 'When T → ∞',
    high: 'High',
    whenHighDesc: 'Dividing by a large T compresses all scores toward zero, making them nearly equal after exponentiation.',
    practicalGuidelines: 'Practical Guidelines',
    useCase: 'Use Case',
    tempLabel: 'Temperature',
    why: 'Why?',
    codingMath: 'Coding & Math',
    codingMathWhy: 'Errors in logic are costly; you want the most likely correct path.',
    factRetrieval: 'Fact Retrieval',
    factRetrievalWhy: 'Reduces "hallucinations" by sticking to the most probable data points.',
    generalChat: 'General Chat',
    generalChatWhy: 'The "sweet spot" for most models to sound natural and helpful.',
    creativeWriting: 'Creative Writing',
    creativeWritingWhy: 'Encourages the model to use more interesting, varied vocabulary.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generates wild, unconventional ideas that might spark inspiration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Temperature 0 is deterministic ("Greedy Search") — always picks the top token',
    takeaway2: 'Higher temperature increases variety and creativity but decreases coherence',
    takeaway3: 'Too high temperature (> 1.5) often results in gibberish',
    takeaway4: "Always match your temperature to the task's requirement for precision vs. creativity",
  },

  // Context Rot page
  contextRot: {
    title: 'Context Rot',
    description: 'Understanding how information degrades over long conversations and why LLMs struggle with extended contexts.',
    whatIs: 'What is Context Rot?',
    whatIsDesc: 'refers to the gradual degradation of an LLM\'s ability to accurately recall and use information from earlier parts of a long conversation or document. As context grows, the model\'s attention becomes diluted.',
    whyHappens: 'Why Does It Happen?',
    whyHappensDesc: 'LLMs have finite context windows and use attention mechanisms that must distribute focus across all tokens. As conversations grow longer, earlier information competes with newer content for the model\'s limited attention capacity.',
    // Reasons
    reason1Title: 'Finite Context Windows',
    reason1Desc: 'LLMs have finite context windows and use attention mechanisms that must distribute focus across all tokens. As conversations grow longer, earlier information competes with newer content for the model\'s limited attention capacity.',
    reason2Title: 'Attention Dilution',
    reason2Desc: 'The model\'s attention mechanism spreads across all tokens. More content means each token gets proportionally less attention.',
    reason3Title: 'Recency Bias',
    reason3Desc: 'Transformers tend to weight recent tokens more heavily. Instructions at the start naturally become less influential.',
    symptoms: 'Common Symptoms',
    symptom1: 'Forgetting instructions given at the start of a conversation',
    symptom2: 'Contradicting earlier statements or decisions',
    symptom3: 'Losing track of complex multi-step tasks',
    symptom4: 'Mixing up details from different parts of the context',
    mitigation: 'Mitigation Strategies',
    mitigation1Title: 'Periodic Instruction Reinforcement',
    mitigation1: 'Summarize important context periodically',
    mitigation2Title: 'Conversation Summarization',
    mitigation2: 'Place critical instructions at both start and end',
    mitigation3Title: 'Hierarchical Memory',
    mitigation3: 'Use external memory systems to store and retrieve relevant context on-demand.',
    mitigation4Title: 'Instruction Anchoring',
    mitigation4: 'Place critical instructions at both the beginning and the end of your prompt to reinforce them.',
    mitigation5Title: 'Shorter Task Chains',
    mitigation5: 'Break long tasks into smaller, focused conversations.',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'See how memory fades as context length increases',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context rot is an inherent limitation of current LLM architectures',
    takeaway2: 'The "lost in the middle" effect means information at the start and end is recalled better',
    takeaway3: 'Strategic information placement can significantly improve recall',
    takeaway4: 'Regular summarization helps maintain important context over long conversations',
  },

  // Attention page
  attention: {
    title: 'Attention Mechanism',
    description: 'Explore how transformers focus on relevant parts of input through the powerful attention mechanism.',
    whatIs: 'What is Attention?',
    whatIsDesc: 'Attention is the core mechanism that allows transformers to weigh the importance of different parts of the input when generating each output token. It enables the model to "focus" on relevant context.',
    howWorks: 'How It Works',
    howWorksDesc: 'For each position, the model computes Query, Key, and Value vectors. Attention scores are calculated by comparing queries to keys, then used to create a weighted sum of values.',
    selfAttention: 'Self-Attention',
    selfAttentionDesc: 'Allows each token to attend to all other tokens in the sequence, capturing relationships regardless of distance.',
    multiHead: 'Multi-Head Attention',
    multiHeadDesc: 'Multiple attention heads allow the model to focus on different types of relationships simultaneously.',
    // Interactive section
    interactiveTitle: 'Interactive Attention Map',
    interactiveDesc: 'Hover to explore attention patterns',
    interactiveExplain: 'Hover over different words in the sentences below. The highlighting shows where the model is "looking" to understand that specific word.',
    // QKV section
    qkvTitle: 'The Three Keys: Query, Key, and Value',
    queryTitle: 'Query',
    queryDesc: '"What am I looking for?" - Represents the current word seeking context.',
    keyTitle: 'Key',
    keyDesc: '"What do I contain?" - A label for every word in the sequence to check against the query.',
    valueTitle: 'Value',
    valueDesc: '"What information do I offer?" - The actual content that gets passed forward if the Query and Key match.',
    qkvExplain: 'The model calculates a score by multiplying Q and K. This score determines how much of V to keep.',
    // Benefits section
    benefitsTitle: 'Why it Changed Everything',
    benefit1Title: 'Parallel Processing',
    benefit1Desc: 'Unlike older models (RNNs), Transformers can process all words in a sentence at the same time, making training much faster.',
    benefit2Title: 'Long-Range Dependencies',
    benefit2Desc: 'Attention can link two words even if they are thousands of tokens apart, as long as they are within the same context window.',
    benefit3Title: 'Dynamic Context',
    benefit3Desc: 'The model doesn\'t just look at words; it learns which words are important *for each other* based on the specific sentence.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Attention enables transformers to capture long-range dependencies',
    takeaway2: 'The quadratic complexity of attention limits context window size',
    takeaway3: 'Different attention heads learn to focus on different linguistic patterns',
    takeaway4: 'Attention visualization can help interpret model behavior',
  },

  // Vision page
  vision: {
    title: 'Vision & Images',
    description: 'How modern LLMs process and understand visual information alongside text.',
    whatIs: 'How LLMs See Images',
    whatIsDesc: 'Vision-enabled LLMs convert images into sequences of tokens that can be processed alongside text. This typically involves dividing images into patches and encoding them with a vision transformer.',
    patchEncoding: 'Patch Encoding',
    patchEncodingDesc: 'Images are divided into fixed-size patches (e.g., 14x14 pixels), each converted into an embedding vector similar to text tokens.',
    // Vision Transformer section
    vitTitle: 'The Vision Transformer (ViT)',
    vitDesc: 'The Vision Transformer architecture adapts the transformer model for image processing. Instead of processing words, it processes image patches.',
    vitStep1: 'Divide into Patches',
    vitStep1Desc: 'The image is split into a grid of fixed-size patches (typically 14x14 or 16x16 pixels).',
    vitStep2: 'Flatten & Project',
    vitStep2Desc: 'Each patch is flattened into a vector and linearly projected into an embedding space.',
    vitStep3: 'Add Position Info',
    vitStep3Desc: 'Positional embeddings are added so the model knows where each patch came from.',
    vitStep4: 'Process with Transformer',
    vitStep4Desc: 'The sequence of patch embeddings is processed by standard transformer layers.',
    // Token costs
    tokenCosts: 'Token Costs',
    tokenCostsDesc: 'Images are expensive in terms of tokens. Understanding this helps you optimize your applications.',
    tokenExample1: 'A 512x512 image with 16x16 patches',
    tokenExample1Value: '~1,024 tokens',
    tokenExample2: 'A 1024x1024 high-res image',
    tokenExample2Value: '~4,096 tokens',
    tokenExample3: 'Equivalent text description',
    tokenExample3Value: '~50-100 tokens',
    tokenTip: 'Always consider whether a text description might be more efficient than passing the actual image.',
    // Use cases
    useCases: 'Common Use Cases',
    useCasesDesc: 'Vision-enabled LLMs unlock many practical applications.',
    useCase1: 'Document Analysis',
    useCase1Desc: 'Extract information from PDFs, receipts, forms, and handwritten notes.',
    useCase2: 'Visual Q&A',
    useCase2Desc: 'Answer questions about image contents, charts, and diagrams.',
    useCase3: 'Image Captioning',
    useCase3Desc: 'Generate detailed descriptions of images for accessibility or indexing.',
    useCase4: 'UI Understanding',
    useCase4Desc: 'Analyze screenshots, wireframes, and user interfaces.',
    multimodal: 'Multimodal Understanding',
    multimodalDesc: 'The model learns to align visual and textual representations, enabling tasks like image captioning, visual QA, and document understanding.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Images consume many more tokens than equivalent text descriptions',
    takeaway2: 'Resolution and patch size affect detail recognition',
    takeaway3: 'Visual understanding is approximate—models can miss fine details',
    takeaway4: 'Combining vision and language enables powerful new applications',
  },

  // Visual Challenges page
  visualChallenges: {
    title: 'Visual Challenges',
    description: 'Common challenges and limitations when working with vision-enabled AI models.',
    overview: 'Common Visual Challenges',
    overviewDesc: 'While vision models are impressive, they face several systematic challenges that are important to understand when building applications. These limitations stem from how vision models process images—through patches, embeddings, and attention—rather than the way humans perceive visual information.',

    // Challenge 1: Counting
    challenge1: 'Counting Objects',
    challenge1Desc: 'Models often struggle to accurately count objects in images, especially when there are many similar items.',
    challenge1Why: 'Why This Happens',
    challenge1WhyDesc: 'Vision models process images as patches (typically 14x14 or 16x16 pixels), not as discrete objects. They lack the built-in concept of "object permanence" and struggle to maintain accurate counts across overlapping or dense arrangements.',
    challenge1Examples: 'Common Failures',
    challenge1Example1: 'Counting people in a crowd (often off by 20-50%)',
    challenge1Example2: 'Counting items in a grid or array',
    challenge1Example3: 'Distinguishing between "few" and "many" when items overlap',
    challenge1Mitigation: 'Workarounds',
    challenge1MitigationDesc: 'For critical counting tasks, consider using specialized object detection models (YOLO, Faster R-CNN) or asking the model to identify and describe each item individually rather than providing a total count.',

    // Challenge 2: Spatial Reasoning
    challenge2: 'Spatial Reasoning',
    challenge2Desc: 'Understanding precise spatial relationships between objects (left/right, above/below) can be unreliable.',
    challenge2Why: 'Why This Happens',
    challenge2WhyDesc: 'Positional information is encoded through patch position embeddings, but these don\'t provide pixel-level precision. The model learns statistical correlations between positions rather than explicit spatial reasoning.',
    challenge2Examples: 'Common Failures',
    challenge2Example1: 'Confusing left/right relationships in mirrored or symmetric images',
    challenge2Example2: 'Misjudging relative distances ("closer to" or "farther from")',
    challenge2Example3: 'Difficulty with rotated or unusual orientations',
    challenge2Mitigation: 'Workarounds',
    challenge2MitigationDesc: 'Be explicit in your prompts about which reference frame to use. Consider annotating images with visual markers or grids for critical spatial tasks.',

    // Challenge 3: Small Text Recognition
    challenge3: 'Small Text Recognition',
    challenge3Desc: 'Fine text in images may be misread or missed entirely, especially at low resolutions.',
    challenge3Why: 'Why This Happens',
    challenge3WhyDesc: 'Text smaller than the patch size (14-16 pixels) gets compressed into a single embedding, losing character-level detail. OCR is not built into vision LLMs—they learn text recognition as a byproduct of training, not as a dedicated capability.',
    challenge3Examples: 'Common Failures',
    challenge3Example1: 'Misreading license plates, street signs, or small labels',
    challenge3Example2: 'Confusing similar characters (0/O, 1/l/I, 5/S)',
    challenge3Example3: 'Missing text in busy or low-contrast backgrounds',
    challenge3Mitigation: 'Workarounds',
    challenge3MitigationDesc: 'Use high-resolution images and zoom in on text regions. For critical OCR tasks, use dedicated OCR tools (Tesseract, Google Vision API, Amazon Textract) alongside or instead of vision LLMs.',

    // Challenge 4: Hallucination
    challenge4: 'Visual Hallucination',
    challenge4Desc: 'Models may describe objects or details that aren\'t actually present in the image.',
    challenge4Why: 'Why This Happens',
    challenge4WhyDesc: 'Vision LLMs are trained to generate plausible descriptions. When image features are ambiguous, the model fills in gaps with statistically likely content—even if that content isn\'t in the image. This is the same mechanism that causes text hallucination.',
    challenge4Examples: 'Common Failures',
    challenge4Example1: 'Adding objects that "should" be in a scene (a keyboard near a monitor)',
    challenge4Example2: 'Describing brand names or text that isn\'t visible',
    challenge4Example3: 'Inventing details when asked about unclear regions',
    challenge4Mitigation: 'Workarounds',
    challenge4MitigationDesc: 'Ask the model to express uncertainty. Use prompts like "describe only what you can clearly see" or "if you cannot determine X, say so." Cross-reference critical details.',

    // Challenge 5: Fine Detail Recognition
    challenge5: 'Fine Detail Recognition',
    challenge5Desc: 'Subtle details, textures, or small distinguishing features are often missed or misidentified.',
    challenge5Why: 'Why This Happens',
    challenge5WhyDesc: 'The patch-based architecture averages information within each patch, losing fine-grained detail. High-frequency visual information (edges, textures, small features) is compressed.',
    challenge5Examples: 'Common Failures',
    challenge5Example1: 'Distinguishing between similar objects (dog breeds, car models)',
    challenge5Example2: 'Reading gauges, meters, or instrument displays',
    challenge5Example3: 'Identifying subtle damage or defects in inspection tasks',
    challenge5Mitigation: 'Workarounds',
    challenge5MitigationDesc: 'Use the highest resolution available. Crop and focus on specific regions of interest. For specialized tasks, consider fine-tuned models trained on domain-specific data.',

    // Challenge 6: Multi-Image Reasoning
    challenge6: 'Multi-Image Reasoning',
    challenge6Desc: 'Comparing or reasoning across multiple images is significantly harder than single-image tasks.',
    challenge6Why: 'Why This Happens',
    challenge6WhyDesc: 'Each image is encoded separately into token sequences. Cross-image attention must happen through the language model\'s context window, which is less efficient than dedicated multi-image architectures.',
    challenge6Examples: 'Common Failures',
    challenge6Example1: 'Finding differences between two similar images ("spot the difference")',
    challenge6Example2: 'Tracking object identity across frames',
    challenge6Example3: 'Comparing fine details between product images',
    challenge6Mitigation: 'Workarounds',
    challenge6MitigationDesc: 'Describe each image separately first, then ask for comparison. Consider combining images into a single composite for direct comparison.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Vision LLMs process images as patches—detail below patch resolution is lost',
    takeaway2: 'Counting and spatial reasoning are fundamental weaknesses, not edge cases',
    takeaway3: 'Visual hallucination follows the same pattern as text hallucination—plausible fabrication',
    takeaway4: 'Use higher resolution, cropped regions, and explicit prompts to improve accuracy',
    takeaway5: 'For critical tasks, combine vision LLMs with specialized tools (OCR, object detection)',
    takeaway6: 'Always verify important visual information through other means',
  },

  // Agent Loop page
  agentLoop: {
    title: 'The Agent Loop',
    description: 'Understanding the core cycle that powers autonomous AI agents: observe, think, act, repeat.',
    whatIs: 'What is the Agent Loop?',
    whatIsDesc: 'The agent loop is the fundamental cycle that enables AI agents to interact with their environment autonomously. It consists of observation, reasoning, action, and feedback phases that repeat continuously.',
    phases: 'The Four Phases',
    observe: 'Observe',
    observeDesc: 'Gather information from the environment, tools, and user input.',
    think: 'Think',
    thinkDesc: 'Reason about the current state and decide on the next action.',
    act: 'Act',
    actDesc: 'Execute the chosen action using available tools.',
    learn: 'Learn',
    learnDesc: 'Process feedback and update understanding for the next iteration.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'The loop continues until the task is complete or terminated',
    takeaway2: 'Each iteration builds on previous observations and actions',
    takeaway3: 'Error handling and recovery are crucial for robust agents',
    takeaway4: 'The quality of tools directly impacts agent capabilities',
  },

  // Agent Context page
  agentContext: {
    title: 'Context Anatomy',
    description: 'Breaking down the structure of context windows and how agents manage information.',
    whatIs: 'Understanding Agent Context',
    whatIsDesc: 'Agent context includes the system prompt, conversation history, tool definitions, and retrieved information. Managing this context efficiently is crucial for agent performance.',
    components: 'Context Components',
    systemPrompt: 'System Prompt',
    systemPromptDesc: 'Defines the agent\'s role, capabilities, and behavioral guidelines.',
    toolDefs: 'Tool Definitions',
    toolDefsDesc: 'Descriptions of available tools and how to use them.',
    history: 'Conversation History',
    historyDesc: 'Previous messages, tool calls, and their results.',
    retrieved: 'Retrieved Information',
    retrievedDesc: 'External knowledge fetched during the conversation.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Context management is key to agent reliability',
    takeaway2: 'Prioritize recent and relevant information',
    takeaway3: 'Tool definitions should be clear and unambiguous',
    takeaway4: 'Summarization helps maintain context over long sessions',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agent Problems',
    description: 'Common failure modes and challenges faced by AI agents in real-world applications.',
    overview: 'Common Agent Failure Modes',
    overviewDesc: 'Understanding typical agent failures helps in building more robust systems and setting appropriate expectations.',
    problem1: 'Tool Misuse',
    problem1Desc: 'Agents may call tools incorrectly, with wrong parameters, or at inappropriate times.',
    problem2: 'Infinite Loops',
    problem2Desc: 'Agents can get stuck repeating the same actions without making progress.',
    problem3: 'Goal Drift',
    problem3Desc: 'Agents may gradually shift focus away from the original task objective.',
    problem4: 'Over-confidence',
    problem4Desc: 'Agents may proceed with actions despite uncertainty or incomplete information.',
    
    // Expanded Content
    hallucination: 'Tool Hallucination',
    hallucinationDesc: 'Agents sometimes "invent" tool parameters or even entire tools that don\'t exist. This usually happens when the tool definition is ambiguous or when the model tries to force a solution.',
    hallucinationExample: 'Example: Calling `get_weather(location="Tokyo", date="tomorrow")` when the function only accepts `location`.',
    
    loops: 'Looping Issues',
    loopsDesc: 'Agents can get trapped in repetitive cycles where they perform the same action, receive the same error, and try again without changing strategy.',
    loopsMitigation: 'Mitigation: Implement loop detection logic that stops execution if the same tool call sequence occurs multiple times.',
    
    costLatency: 'Cost & Latency',
    costLatencyDesc: 'Every step in the agent loop requires a full LLM inference call. Multi-step tasks can quickly become expensive and slow.',
    costFactor: 'The Cost Factor',
    costFactorDesc: 'A simple task requiring 5 steps means 5x the cost and 5x the latency of a standard chat response.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Implement safeguards like iteration limits and cost controls',
    takeaway2: 'Add human-in-the-loop checkpoints for critical actions',
    takeaway3: 'Monitor agent behavior and log all actions for debugging',
    takeaway4: 'Design clear success and failure criteria',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agent Security',
    description: 'Critical security vulnerabilities in AI agents: prompt injection, data exfiltration, and tool misuse—plus how to defend against them.',
    
    // Intro
    intro: 'Agents Are Attack Surfaces',
    introDesc: 'When you give an LLM access to tools, you\'re creating a powerful attack vector. Agents can read files, make HTTP requests, send emails, and execute code. A malicious actor who can influence the agent\'s context can potentially control all of these capabilities.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Attack #1: Prompt Injection',
    attack1Desc: 'Prompt injection occurs when untrusted input is interpreted as instructions by the LLM. Because agents often process external data (emails, web pages, documents), attackers can embed hidden commands that hijack the agent\'s behavior.',
    attack1Example: 'Example Attack',
    attack1ExampleDesc: 'User asks the agent to summarize a document. The document contains hidden instructions:',
    whyWorks: 'Why This Works',
    whyWorks1: 'The agent reads the document into its context',
    whyWorks2: 'The LLM cannot distinguish between "real" instructions and injected ones',
    whyWorks3: 'The hidden text looks like system instructions, so the LLM may follow them',
    whyWorks4: 'The agent uses its legitimate tools to perform the malicious action',
    directInjection: 'Direct Injection',
    directInjectionDesc: 'User directly types malicious instructions. Easier to filter but still dangerous if system prompt isn\'t robust.',
    indirectInjection: 'Indirect Injection',
    indirectInjectionDesc: 'Malicious content comes from external sources the agent reads (websites, emails, files). Much harder to defend against.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Attack #2: Data Exfiltration',
    attack2Desc: 'Agents with access to communication tools (email, HTTP, Slack, etc.) can be tricked into sending sensitive data to external destinations. The agent becomes an unwitting accomplice in data theft.',
    exfilFlow: 'Exfiltration Flow',
    exfilStep1: 'Agent reads',
    exfilStep1Desc: 'Private files, DB, env vars',
    exfilStep2: 'Injection triggers',
    exfilStep2Desc: '"Send this to X"',
    exfilStep3: 'Tool executes',
    exfilStep3Desc: 'Data leaves the system',
    vulnerableConfig: 'Vulnerable Tool Configuration',
    otherVectors: 'Other Exfiltration Vectors',
    vector1: 'HTTP requests — POST data to attacker-controlled endpoints',
    vector2: 'Slack/Discord webhooks — Send messages to external channels',
    vector3: 'File uploads — Upload to cloud storage with public links',
    vector4: 'DNS exfiltration — Encode data in DNS queries',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Attack #3: Unintended Tool Misuse',
    attack3Desc: 'Even without malicious intent, agents can cause harm through incorrect tool usage. The LLM might misunderstand parameters, use the wrong tool, or take destructive actions while trying to be helpful.',
    destructiveActions: 'Destructive Actions',
    destructiveActionsDesc: '"Clean up the project" → Agent runs rm -rf / or deletes production database',
    wrongParams: 'Wrong Parameters',
    wrongParamsDesc: 'Agent confuses similar fields or uses incorrect values that seem plausible',
    cascadingErrors: 'Cascading Errors',
    cascadingErrorsDesc: 'Agent makes one small error, then "fixes" it with increasingly destructive actions',
    
    // Defense Strategies
    defensesTitle: 'Defense Strategies',
    defense1: 'Principle of Least Privilege',
    defense1Desc: 'Only give the agent the minimum tools and permissions needed for the task. Don\'t give file access if it only needs to answer questions.',
    defense1Bad: 'Bad',
    defense1Good: 'Good',
    defense2: 'Strict Allowlists',
    defense2Desc: 'Constrain tool parameters to known-safe values. Don\'t allow arbitrary email addresses, URLs, or file paths.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Require human approval for sensitive actions. The agent proposes, the human confirms.',
    defense3Example: 'Example confirmation flow:',
    defense4: 'Input Sanitization & Isolation',
    defense4Desc: 'Treat external data as untrusted. Clearly separate user instructions from retrieved content.',
    defense5: 'Monitoring & Rate Limiting',
    defense5Desc: 'Log all tool calls. Set rate limits on sensitive operations. Alert on unusual patterns (many emails, large data transfers, repeated failures). Enable rollback for destructive actions.',
    
    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Agents are attack surfaces—every tool is a potential vulnerability',
    takeaway2: 'Prompt injection is the #1 threat—LLMs cannot distinguish instructions from data',
    takeaway3: 'Data exfiltration is trivial if agents have outbound communication tools',
    takeaway4: 'Tool misuse happens even without attackers—LLMs make mistakes',
    takeaway5: 'Defense in depth: least privilege + allowlists + human approval + monitoring',
    takeaway6: 'Treat all external data as potentially malicious input',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentic Patterns',
    description: 'Design patterns and architectures for building effective AI agent systems.',
    overview: 'Common Agentic Patterns',
    overviewDesc: 'Several architectural patterns have emerged as effective approaches for building AI agent systems. Each pattern offers different trade-offs between reliability, transparency, and capability.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Interleave reasoning traces with actions for better transparency and control.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Create a high-level plan first, then execute steps sequentially.',
    pattern3: 'Multi-Agent Systems',
    pattern3Desc: 'Multiple specialized agents collaborate to solve complex tasks.',
    pattern4: 'Reflection',
    pattern4Desc: 'Agents review their own outputs and iteratively improve them.',
    // Deep dive sections
    reactDeepDive: 'ReAct Pattern Deep Dive',
    reactDeepDiveDesc: 'ReAct (Reasoning + Acting) interleaves chain-of-thought reasoning with action execution. The model explicitly states its thinking before each action.',
    reactHow: 'How it works',
    reactStep1: 'Thought: The model reasons about what to do next',
    reactStep2: 'Action: The model calls a tool or takes an action',
    reactStep3: 'Observation: The model sees the result',
    reactStep4: 'Repeat until task is complete',
    reactPros: 'Pros',
    reactPro1: 'Highly transparent—you can see exactly why the agent did what it did',
    reactPro2: 'Easier to debug and understand failures',
    reactPro3: 'Natural error recovery through explicit reasoning',
    reactCons: 'Cons',
    reactCon1: 'More tokens used (reasoning takes space)',
    reactCon2: 'Can be slower due to explicit reasoning steps',
    reactCon3: 'May overthink simple tasks',
    planExecuteDeepDive: 'Plan-and-Execute Deep Dive',
    planExecuteDeepDiveDesc: 'This pattern separates planning from execution. A planner creates a high-level plan, then an executor carries out each step.',
    planExecuteHow: 'How it works',
    planStep1: 'Planner analyzes the task and creates a step-by-step plan',
    planStep2: 'Executor carries out each step in sequence',
    planStep3: 'Replanning happens if execution fails or new info emerges',
    planExecutePros: 'Pros',
    planExecutePro1: 'Better for complex, multi-step tasks',
    planExecutePro2: 'Can use different models for planning vs execution',
    planExecutePro3: 'Plans can be reviewed before execution',
    planExecuteCons: 'Cons',
    planExecuteCon1: 'Plans may become outdated as execution proceeds',
    planExecuteCon2: 'Harder to handle unexpected situations',
    planExecuteCon3: 'Replanning adds latency and cost',
    multiAgentDeepDive: 'Multi-Agent Systems Deep Dive',
    multiAgentDeepDiveDesc: 'Multiple specialized agents work together, each handling different aspects of a task. A supervisor or router directs work to the right agent.',
    multiAgentHow: 'How it works',
    multiAgentStep1: 'Router/supervisor receives the task',
    multiAgentStep2: 'Task is delegated to specialized agents',
    multiAgentStep3: 'Agents may communicate and collaborate',
    multiAgentStep4: 'Results are aggregated and returned',
    multiAgentPros: 'Pros',
    multiAgentPro1: 'Specialization improves quality on complex tasks',
    multiAgentPro2: 'Can parallelize independent sub-tasks',
    multiAgentPro3: 'Easier to maintain and update individual agents',
    multiAgentCons: 'Cons',
    multiAgentCon1: 'Higher complexity and coordination overhead',
    multiAgentCon2: 'More expensive (multiple LLM calls)',
    multiAgentCon3: 'Debugging distributed failures is harder',
    reflectionDeepDive: 'Reflection Pattern Deep Dive',
    reflectionDeepDiveDesc: 'The agent generates an output, then critiques and improves it. This self-refinement loop can dramatically improve quality.',
    reflectionHow: 'How it works',
    reflectionStep1: 'Generate initial output',
    reflectionStep2: 'Critique the output (identify flaws, missing parts)',
    reflectionStep3: 'Revise based on critique',
    reflectionStep4: 'Repeat until quality threshold is met',
    reflectionPros: 'Pros',
    reflectionPro1: 'Significantly improves output quality',
    reflectionPro2: 'Catches errors the initial pass missed',
    reflectionPro3: 'Works well for writing, code review, and creative tasks',
    reflectionCons: 'Cons',
    reflectionCon1: 'Multiple LLM calls increase cost and latency',
    reflectionCon2: 'Risk of over-editing or infinite loops',
    reflectionCon3: 'May "wash out" creative or unconventional solutions',
    // Choosing section
    choosingTitle: 'Choosing the Right Pattern',
    choosingDesc: 'Select a pattern based on your specific needs and constraints.',
    choosingSimple: 'For simple, well-defined tasks',
    choosingSimpleAnswer: 'Direct tool calling (no pattern needed)',
    choosingTransparency: 'When you need transparency and debuggability',
    choosingTransparencyAnswer: 'ReAct pattern',
    choosingComplex: 'For complex, multi-step tasks',
    choosingComplexAnswer: 'Plan-and-Execute',
    choosingQuality: 'When output quality is critical',
    choosingQualityAnswer: 'Reflection pattern',
    choosingDiverse: 'For diverse task types',
    choosingDiverseAnswer: 'Multi-Agent with specialized agents',
    // Interactive section
    interactiveTitle: 'Pattern Comparison',
    interactiveDesc: 'Explore different agentic architectures',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Choose patterns based on task complexity and reliability needs',
    takeaway2: 'ReAct is great for transparency but may be slower',
    takeaway3: 'Multi-agent systems add complexity but enable specialization',
    takeaway4: 'Reflection patterns can significantly improve output quality',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'Understanding MCP: when external tool servers make sense, and when they are overkill.',
    whatIs: 'What is MCP?',
    whatIsDesc: 'The Model Context Protocol (MCP) is a standardized way to connect AI agents to external tools and data sources via dedicated server processes. Instead of defining tools inline in your agent code, MCP runs a separate server that exposes tools over a structured protocol.',
    vsToolCalls: 'MCP vs. Regular Tool Calls',
    vsToolCallsDesc: 'Regular tool calls are functions defined directly in your agent\'s codebase. The agent calls them, they execute, and results return in the same process. MCP separates this: tools live in external servers that the agent communicates with over a protocol.',
    
    // Comparison
    regularTools: 'Regular Tool Calls',
    regularToolsDesc: 'Tools defined inline in your agent code. Simple, fast, and sufficient for most use cases.',
    mcpTools: 'MCP Servers',
    mcpToolsDesc: 'Tools exposed by external server processes. Adds network overhead but enables cross-language tooling and shared tool ecosystems.',
    
    // When to use
    whenToUse: 'When MCP Makes Sense',
    whenToUseDesc: 'MCP shines in specific scenarios where its additional complexity pays off.',
    useCase1: 'Multi-Language Teams',
    useCase1Desc: 'Your tools are written in Python but your agent is in TypeScript, or vice versa.',
    useCase2: 'Shared Tool Ecosystem',
    useCase2Desc: 'Multiple agents across different projects need to access the same tools.',
    useCase3: 'Enterprise Integration',
    useCase3Desc: 'You need to expose existing internal services as agent tools without modifying them.',
    useCase4: 'Tool Marketplace',
    useCase4Desc: 'You want to use community-maintained tools without copying code into your project.',
    
    // When it's overkill
    overkill: 'When MCP is Overkill',
    overkillDesc: 'For many use cases, MCP adds unnecessary complexity.',
    overkillCase1: 'Single-Language Projects',
    overkillCase1Desc: 'If your tools and agent are in the same language, inline functions are simpler and faster.',
    overkillCase2: 'Simple Agents',
    overkillCase2Desc: 'A chatbot with a few tools doesn\'t need the overhead of running separate server processes.',
    overkillCase3: 'Rapid Prototyping',
    overkillCase3Desc: 'When iterating quickly, the indirection of MCP slows down development.',
    overkillCase4: 'Latency-Sensitive Apps',
    overkillCase4Desc: 'Network calls to tool servers add latency that inline functions don\'t have.',
    
    // Architecture
    architecture: 'How MCP Works',
    architectureDesc: 'MCP defines a client-server architecture where the agent is the client and tools are exposed by servers.',
    step1: 'Discovery',
    step1Desc: 'The agent connects to an MCP server and receives a list of available tools with their schemas.',
    step2: 'Invocation',
    step2Desc: 'When the LLM decides to use a tool, the agent sends a request to the MCP server.',
    step3: 'Execution',
    step3Desc: 'The MCP server runs the tool and returns results in a standardized format.',
    step4: 'Integration',
    step4Desc: 'Results flow back to the agent and into the LLM context, just like regular tool results.',
    
    // Practical advice
    practicalAdvice: 'Practical Advice',
    adviceDesc: 'Guidelines for deciding whether to use MCP in your project.',
    advice1: 'Start simple: use inline tool definitions until you hit a specific limitation.',
    advice2: 'Consider MCP when you find yourself copy-pasting tool code between projects.',
    advice3: 'The overhead of running MCP servers only makes sense at scale or in enterprise settings.',
    advice4: 'Community MCP servers can accelerate development but add dependency risks.',
    
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'MCP is a protocol for exposing tools via external servers, not a replacement for regular tool calls',
    takeaway2: 'For most single-project agents, inline tools are simpler and have lower latency',
    takeaway3: 'MCP shines in polyglot environments and shared tool ecosystems',
    takeaway4: 'Don\'t reach for MCP by default—it\'s a solution for specific scaling and interoperability challenges',
  },

  // Tokenization page
  tokenization: {
    title: 'Tokenization',
    description: 'How LLMs break text into tokens—the fundamental units of language understanding.',
    whatIs: 'What is Tokenization?',
    whatIsDesc: 'Tokenization is the process of converting raw text into a sequence of tokens—the basic units that LLMs process. Tokens can be words, subwords, or even individual characters, depending on the tokenizer.',
    whyMatters: 'Why Tokenization Matters',
    whyMattersDesc: 'Understanding tokenization is crucial because it directly impacts context limits, costs, and model behavior. The same text can have very different token counts across different models.',
    howWorks: 'How It Works',
    howWorksDesc: 'Most modern LLMs use subword tokenization algorithms like BPE (Byte Pair Encoding) or SentencePiece. These algorithms learn common character sequences from training data.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE iteratively merges the most frequent character pairs into single tokens. Common words become single tokens, while rare words are split into subwords.',
    tokenTypes: 'Token Types',
    wholeWords: 'Whole Words',
    wholeWordsDesc: 'Common words like "the", "and", "is" are often single tokens.',
    subwords: 'Subwords',
    subwordsDesc: 'Less common words are split: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Special Tokens',
    specialTokensDesc: 'Markers like <|endoftext|> or [CLS] for model control.',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'Type text to see how it gets tokenized',
    costImplications: 'Cost Implications',
    costDesc: 'API pricing is typically per-token. Efficient prompts use fewer tokens.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tokens are the atomic units LLMs process—not characters or words',
    takeaway2: 'Different models have different tokenizers and vocabularies',
    takeaway3: 'Non-English text and code often use more tokens than English',
    takeaway4: 'Token count directly affects cost and context window usage',
  },

  // Embeddings page
  embeddings: {
    title: 'Embeddings',
    description: 'How AI represents meaning as vectors in high-dimensional space.',
    whatIs: 'What are Embeddings?',
    whatIsDesc: 'Embeddings are dense vector representations that capture semantic meaning. Similar concepts have similar embeddings, enabling machines to understand relationships between words, sentences, and documents.',
    howWorks: 'How Embeddings Work',
    howWorksDesc: 'Embedding models map discrete tokens to continuous vectors in a high-dimensional space (often 768-4096 dimensions). The position of each vector encodes its semantic meaning.',
    similarity: 'Semantic Similarity',
    similarityDesc: 'Similar meanings cluster together in embedding space. "King" and "Queen" are closer than "King" and "Banana".',
    dimensions: 'Vector Dimensions',
    dimensionsDesc: 'Each dimension captures some aspect of meaning—though these dimensions aren\'t human-interpretable.',
    operations: 'Vector Operations',
    operationsDesc: 'Famous example: King - Man + Woman ≈ Queen. Relationships are encoded as directions in the space.',
    useCases: 'Common Use Cases',
    search: 'Semantic Search',
    searchDesc: 'Find documents by meaning, not just keyword matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Group similar documents, detect topics automatically.',
    classification: 'Classification',
    classificationDesc: 'Categorize text based on embedding similarity to examples.',
    rag: 'RAG Systems',
    ragDesc: 'Retrieve relevant context for LLM prompts.',
    interactiveDemo: 'Interactive Visualization',
    demoDesc: 'Explore how embeddings cluster by meaning',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Embeddings convert text to vectors that capture semantic meaning',
    takeaway2: 'Similar concepts have similar embeddings (cosine similarity)',
    takeaway3: 'Embeddings enable semantic search, clustering, and RAG',
    takeaway4: 'Different embedding models have different strengths and dimensions',
  },

  // RAG page
  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: giving LLMs access to external knowledge.',
    whatIs: 'What is RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving relevant documents from a knowledge base and including them in the prompt. This gives models access to up-to-date or specialized information.',
    whyRag: 'Why Use RAG?',
    whyRagDesc: 'LLMs have knowledge cutoffs and can hallucinate. RAG grounds responses in actual documents, reducing hallucination and enabling domain-specific knowledge without fine-tuning.',
    pipeline: 'The RAG Pipeline',
    pipelineDesc: 'RAG systems follow a consistent pattern: embed the query, retrieve relevant chunks, augment the prompt, and generate a response.',
    step1: 'Query Embedding',
    step1Desc: 'Convert the user\'s question into a vector using an embedding model.',
    step2: 'Retrieval',
    step2Desc: 'Search the vector database for chunks similar to the query embedding.',
    step3: 'Augmentation',
    step3Desc: 'Insert retrieved chunks into the prompt as context.',
    step4: 'Generation',
    step4Desc: 'The LLM generates a response grounded in the retrieved context.',
    chunking: 'Document Chunking',
    chunkingDesc: 'Documents are split into smaller chunks (typically 200-1000 tokens) for embedding and retrieval. Chunk size affects retrieval precision.',
    vectorDbs: 'Vector Databases',
    vectorDbsDesc: 'Specialized databases like Pinecone, Weaviate, or pgvector enable fast similarity search over millions of embeddings.',
    interactiveDemo: 'Interactive RAG Pipeline',
    demoDesc: 'See how queries flow through a RAG system',

    // Agentic RAG
    agenticRag: 'Agentic RAG',
    agenticRagDesc: 'In agentic RAG, the LLM doesn\'t just receive retrieved documents—it actively controls the retrieval process. The model decides when to search, what to search for, and which retrieval tools to use.',
    agenticHow: 'How It Works',
    agenticHowDesc: 'Instead of a fixed pipeline, the LLM is given retrieval tools it can call as needed. It might reformulate queries, search multiple times, or combine different search strategies based on the task.',
    agenticAdvantages: 'Advantages',
    agenticAdv1: 'Query refinement: The LLM can rephrase or decompose complex questions',
    agenticAdv2: 'Multi-hop reasoning: Chain multiple retrievals to answer complex questions',
    agenticAdv3: 'Adaptive search: Choose the right tool for each sub-question',
    agenticAdv4: 'Self-correction: Re-retrieve if initial results are insufficient',
    agenticDisadvantages: 'Disadvantages',
    agenticDisadv1: 'Higher latency: Multiple LLM calls and retrievals add up',
    agenticDisadv2: 'Increased cost: Each reasoning step costs tokens',
    agenticDisadv3: 'Complexity: Harder to debug and predict behavior',
    agenticDisadv4: 'Failure modes: LLM might loop, over-retrieve, or miss obvious queries',
    multiTool: 'Multi-Tool Retrieval',
    multiToolDesc: 'Give the LLM multiple retrieval tools for different use cases. This flexibility lets the model choose the best approach for each query.',
    toolSemantic: 'Semantic Search',
    toolSemanticDesc: 'Vector similarity for conceptual matching. Best for: "documents about X", finding related content.',
    toolFulltext: 'Full-Text Search',
    toolFulltextDesc: 'Keyword/BM25 search for exact matches. Best for: specific terms, names, codes, error messages.',
    toolSql: 'SQL/Structured Query',
    toolSqlDesc: 'Query structured data directly. Best for: counts, aggregations, filtering by attributes.',
    toolKg: 'Knowledge Graph',
    toolKgDesc: 'Traverse entity relationships. Best for: "how is X related to Y", multi-hop facts.',
    whenToUse: 'When to Use Agentic RAG',
    whenToUseDesc: 'Standard RAG is simpler and faster for straightforward Q&A. Use agentic RAG when queries are complex, require multiple sources, or benefit from query reformulation.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'RAG retrieves relevant documents and includes them in the prompt',
    takeaway2: 'It reduces hallucination by grounding responses in actual sources',
    takeaway3: 'Chunking strategy and embedding quality are critical for good retrieval',
    takeaway4: 'RAG is often preferable to fine-tuning for adding domain knowledge',
  },

  // Tool Design page
  toolDesign: {
    title: 'Tool Design',
    description: 'Best practices for designing effective tools that AI agents can use reliably.',
    whatIs: 'What Makes a Good Tool?',
    whatIsDesc: 'Well-designed tools are the foundation of capable AI agents. A tool\'s schema, naming, and documentation directly impact how reliably an LLM can use it.',
    principles: 'Design Principles',
    principlesDesc: 'Follow these principles to create tools that agents can use effectively.',
    principle1: 'Clear Naming',
    principle1Desc: 'Use descriptive, unambiguous names. "search_web" is better than "sw" or "query".',
    principle2: 'Explicit Parameters',
    principle2Desc: 'Every parameter should have a clear type, description, and constraints.',
    principle3: 'Predictable Outputs',
    principle3Desc: 'Return consistent, structured responses. Include error messages in the output.',
    principle4: 'Minimal Scope',
    principle4Desc: 'Each tool should do one thing well. Prefer many focused tools over few complex ones.',
    schemaDesign: 'Schema Design',
    schemaDesignDesc: 'Tool schemas tell the LLM how to use your tools. Good schemas prevent errors.',
    goodSchema: 'Good Schema',
    badSchema: 'Bad Schema',
    errorHandling: 'Error Handling',
    errorHandlingDesc: 'Tools should handle errors gracefully and return informative messages the LLM can act on.',
    interactiveDemo: 'Tool Schema Builder',
    demoDesc: 'Build and validate tool schemas interactively',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tool design directly impacts agent reliability',
    takeaway2: 'Explicit schemas with descriptions prevent LLM confusion',
    takeaway3: 'Return structured errors the agent can understand and act on',
    takeaway4: 'Test tools with various inputs to find edge cases',
  },

  // Memory Systems page
  memorySystems: {
    title: 'Memory Systems',
    description: 'How AI agents maintain context and remember information across interactions.',
    whatIs: 'What are Agent Memory Systems?',
    whatIsDesc: 'Memory systems allow agents to retain and recall information beyond the immediate context window. They enable agents to learn from past interactions and maintain coherent long-term behavior.',
    types: 'Types of Memory',
    typesDesc: 'Agent memory systems typically combine multiple memory types for different purposes.',
    shortTerm: 'Short-Term Memory',
    shortTermDesc: 'The current conversation context. Limited by context window size.',
    longTerm: 'Long-Term Memory',
    longTermDesc: 'Persistent storage of past interactions, facts, and learned preferences.',
    episodic: 'Episodic Memory',
    episodicDesc: 'Specific past events and interactions that can be recalled.',
    semantic: 'Semantic Memory',
    semanticDesc: 'General knowledge and facts extracted from experiences.',
    implementation: 'Implementation Approaches',
    implementationDesc: 'Various techniques for implementing agent memory.',
    vectorStore: 'Vector Stores',
    vectorStoreDesc: 'Store embeddings of past interactions for semantic retrieval.',
    summaries: 'Conversation Summaries',
    summariesDesc: 'Periodically summarize long conversations to preserve key information.',
    keyValue: 'Key-Value Stores',
    keyValueDesc: 'Store explicit facts and user preferences for direct lookup.',
    interactiveDemo: 'Memory System Visualizer',
    demoDesc: 'See how different memory types work together',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Memory extends agent capabilities beyond the context window',
    takeaway2: 'Combine multiple memory types for best results',
    takeaway3: 'Memory retrieval adds latency—balance comprehensiveness with speed',
    takeaway4: 'Consider privacy and data retention when storing memories',
  },

  // Orchestration page
  orchestration: {
    title: 'Orchestration',
    description: 'Coordinating multiple agents and complex multi-step workflows.',
    whatIs: 'What is Agent Orchestration?',
    whatIsDesc: 'Orchestration is the coordination of multiple AI agents or complex multi-step workflows. It involves routing tasks, managing state, handling failures, and combining agent outputs.',
    patterns: 'Orchestration Patterns',
    patternsDesc: 'Common patterns for structuring multi-agent systems.',
    sequential: 'Sequential Pipeline',
    sequentialDesc: 'Agents run in order, each processing the output of the previous one.',
    parallel: 'Parallel Execution',
    parallelDesc: 'Multiple agents work simultaneously on different aspects of a task.',
    hierarchical: 'Hierarchical',
    hierarchicalDesc: 'A supervisor agent delegates to specialized worker agents.',
    dynamic: 'Dynamic Routing',
    dynamicDesc: 'An LLM decides which agent should handle each request.',
    stateManagement: 'State Management',
    stateManagementDesc: 'Orchestrators must track progress, intermediate results, and handle failures.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Save state at key points to enable recovery from failures.',
    rollback: 'Rollback',
    rollbackDesc: 'Ability to undo steps when errors occur.',
    interactiveDemo: 'Workflow Visualizer',
    demoDesc: 'Design and visualize agent workflows',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Orchestration enables complex tasks through agent composition',
    takeaway2: 'Choose patterns based on task dependencies and parallelism',
    takeaway3: 'Robust state management is essential for reliability',
    takeaway4: 'Monitor orchestration costs—multi-agent systems multiply API calls',
  },

  // Evaluation page
  evaluation: {
    title: 'Evaluation',
    description: 'Measuring and improving AI agent performance systematically.',
    whatIs: 'Why Evaluate Agents?',
    whatIsDesc: 'Agent evaluation is critical for understanding performance, catching regressions, and improving reliability. Without measurement, you\'re flying blind.',
    metrics: 'Key Metrics',
    metricsDesc: 'Important metrics to track for agent systems.',
    taskSuccess: 'Task Success Rate',
    taskSuccessDesc: 'Percentage of tasks completed correctly.',
    efficiency: 'Efficiency',
    efficiencyDesc: 'Steps taken, tokens used, time elapsed per task.',
    accuracy: 'Accuracy',
    accuracyDesc: 'Correctness of agent outputs and decisions.',
    reliability: 'Reliability',
    reliabilityDesc: 'Consistency across repeated runs of the same task.',
    approaches: 'Evaluation Approaches',
    approachesDesc: 'Different ways to evaluate agent performance.',
    unitTests: 'Unit Tests',
    unitTestsDesc: 'Test individual tools and components in isolation.',
    integration: 'Integration Tests',
    integrationDesc: 'Test the full agent loop with mock environments.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard task suites for comparing agents.',
    humanEval: 'Human Evaluation',
    humanEvalDesc: 'Expert review for nuanced quality assessment.',
    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Guidelines for effective agent evaluation.',
    practice1: 'Test edge cases and failure modes, not just happy paths.',
    practice2: 'Track costs alongside quality metrics.',
    practice3: 'Use versioned evaluations to catch regressions.',
    practice4: 'Include adversarial tests for security.',

    // Benchmarks Section
    benchmarksSection: 'Common LLM Benchmarks',
    benchmarksSectionDesc: 'Standard benchmarks used to evaluate and compare language model capabilities across different tasks.',
    benchmarkMmlu: 'MMLU',
    benchmarkMmluDesc: 'Massive Multitask Language Understanding - 57 subjects from STEM to humanities. Tests broad knowledge.',
    benchmarkHellaswag: 'HellaSwag',
    benchmarkHellaswagDesc: 'Commonsense reasoning about everyday situations. Tests understanding of physical world.',
    benchmarkHumaneval: 'HumanEval',
    benchmarkHumanevalDesc: 'Code generation benchmark with 164 programming problems. Tests coding ability.',
    benchmarkGsm8k: 'GSM8K',
    benchmarkGsm8kDesc: 'Grade school math word problems. Tests multi-step mathematical reasoning.',
    benchmarkArc: 'ARC',
    benchmarkArcDesc: 'AI2 Reasoning Challenge - science questions requiring reasoning beyond pattern matching.',
    benchmarkMath: 'MATH',
    benchmarkMathDesc: 'Competition-level mathematics problems. Tests advanced mathematical reasoning.',
    benchmarkCaveats: 'Benchmark Caveats',
    benchmarkCaveat1: 'Benchmarks can be gamed - models may be trained on test data',
    benchmarkCaveat2: 'High scores don\'t guarantee real-world performance',
    benchmarkCaveat3: 'Many benchmarks are saturated - top models score similarly',
    benchmarkCaveat4: 'Benchmarks often miss important capabilities like following instructions',

    // LLM as a Judge
    llmJudge: 'LLM-as-a-Judge',
    llmJudgeDesc: 'Using language models to evaluate other model outputs - a scalable but imperfect approach.',
    llmJudgeWhat: 'How It Works',
    llmJudgeWhatDesc: 'A capable LLM (the "judge") is prompted to evaluate outputs from another model. The judge scores responses on criteria like helpfulness, accuracy, and safety.',
    llmJudgeAdvantages: 'Advantages',
    llmJudgeAdv1: 'Scalable',
    llmJudgeAdv1Desc: 'Can evaluate thousands of outputs quickly without human annotators.',
    llmJudgeAdv2: 'Consistent',
    llmJudgeAdv2Desc: 'Same criteria applied uniformly (unlike human fatigue/variation).',
    llmJudgeAdv3: 'Cost-effective',
    llmJudgeAdv3Desc: 'Much cheaper than hiring human evaluators at scale.',
    llmJudgeAdv4: 'Flexible',
    llmJudgeAdv4Desc: 'Easy to adjust evaluation criteria by changing the prompt.',
    llmJudgeProblems: 'Problems & Biases',
    llmJudgeProb1: 'Self-preference Bias',
    llmJudgeProb1Desc: 'Models tend to prefer outputs similar to what they would generate.',
    llmJudgeProb2: 'Position Bias',
    llmJudgeProb2Desc: 'Judges may favor the first or last option regardless of quality.',
    llmJudgeProb3: 'Verbosity Bias',
    llmJudgeProb3Desc: 'Longer responses often rated higher even when less accurate.',
    llmJudgeProb4: 'Style Over Substance',
    llmJudgeProb4Desc: 'Well-formatted wrong answers may beat poorly-formatted correct ones.',
    llmJudgeProb5: 'Capability Ceiling',
    llmJudgeProb5Desc: 'Judge can\'t reliably evaluate outputs beyond its own capability level.',
    llmJudgeBestPractices: 'Best Practices for LLM Judges',
    llmJudgePractice1: 'Use the most capable model available as the judge',
    llmJudgePractice2: 'Randomize option order to mitigate position bias',
    llmJudgePractice3: 'Request reasoning before scores (chain-of-thought)',
    llmJudgePractice4: 'Validate against human judgments on a subset',
    llmJudgePractice5: 'Use multiple judges and aggregate scores',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Evaluation is essential—unmeasured systems can\'t be improved',
    takeaway2: 'Combine automated tests with human evaluation',
    takeaway3: 'Track multiple metrics: success, efficiency, cost',
    takeaway4: 'Build evaluation into your development workflow',
    takeaway5: 'LLM-as-a-judge is useful but has significant biases to account for',
  },

  // Agent Skills page
  agentSkills: {
    title: 'Agent Skills',
    description: 'Reusable instruction packages that give agents specialized capabilities on demand.',
    whatIs: 'What are Agent Skills?',
    whatIsDesc: 'are folders of instructions, prompts, examples, and resources that an LLM can load when relevant to perform specialized tasks consistently. Instead of putting everything in the system prompt, skills let you modularize expertise.',
    metaphor: '"Skills are like apps for your agent—install them once, use them whenever needed."',
    metaphorDesc: 'Just as you install apps on your phone for specific functions, skills give agents specialized capabilities without bloating every conversation.',

    // How it Works
    howItWorks: 'How Agent Skills Work',
    step1Title: 'Skill Discovery',
    step1Desc: 'When a user request comes in, the agent checks if any available skills match the task based on triggers, keywords, or explicit invocation.',
    step2Title: 'Skill Loading',
    step2Desc: 'The relevant skill\'s instructions, examples, and context are loaded into the agent\'s working memory. This adds specialized knowledge without polluting the base system prompt.',
    step3Title: 'Skill Execution',
    step3Desc: 'The agent follows the skill\'s instructions to complete the task, using any provided templates, checklists, or scripts. Results are returned to the user.',

    // Structure
    structureTitle: 'Skill Structure',
    structureSubtitle: 'Anatomy of a skill folder',
    skillMdDesc: 'Metadata and main instructions',
    instructionsDesc: 'Detailed guidance for the task',
    examplesDesc: 'Sample inputs and outputs',
    templatesDesc: 'Reusable output formats',
    scriptsDesc: 'Helper scripts if needed',
    skillMdNote: 'is the entry point. It contains metadata (name, triggers, description) and the core instructions the agent follows.',

    // Types of Skills
    typesTitle: 'Types of Agent Skills',
    skillType1Title: 'Domain Skills',
    skillType1Desc: 'Specialized knowledge for specific fields—legal contracts, medical terminology, financial analysis. Turn a general agent into a domain expert.',
    skillType2Title: 'Workflow Skills',
    skillType2Desc: 'Multi-step processes with defined stages—code review workflows, content publishing pipelines, incident response procedures.',
    skillType3Title: 'Format Skills',
    skillType3Desc: 'Consistent output formatting—API documentation, changelog entries, meeting summaries. Ensure outputs match your standards.',
    skillType4Title: 'Integration Skills',
    skillType4Desc: 'Instructions for working with specific tools or services—GitHub workflows, Jira ticket creation, Slack notifications.',

    // Skills vs Tools
    vsToolsTitle: 'Skills vs. Tools',
    tools: 'Tools',
    tool1: 'Execute actions (read files, call APIs, run code)',
    tool2: 'Defined by function signatures and schemas',
    tool3: 'The "hands" of the agent',
    skills: 'Skills',
    skill1: 'Provide knowledge and methodology (how to approach tasks)',
    skill2: 'Defined by instructions and examples',
    skill3: 'The "expertise" of the agent',
    vsNote: 'Skills and tools work together: a code review skill tells the agent what to look for, while tools let it read the code and leave comments.',

    // Benefits
    benefitsTitle: 'Benefits of Skills',
    benefit1Title: 'Specialization Without Bloat',
    benefit1Desc: 'Keep the base system prompt lean. Load specialized knowledge only when needed, preserving context window for the actual task.',
    benefit2Title: 'Consistency',
    benefit2Desc: 'Define a process once, apply it consistently every time. No more variations in how tasks are approached.',
    benefit3Title: 'Shareability',
    benefit3Desc: 'Skills are just folders—share them across projects, teams, or publicly. Build once, use everywhere.',
    benefit4Title: 'Iteration',
    benefit4Desc: 'Improve skills independently of the agent. Update the code review skill without touching the rest of your agent setup.',

    // Example
    exampleTitle: 'Example: Code Review Skill',
    exampleDesc: 'Performs thorough code reviews following team standards',
    exampleInstructions: 'When reviewing code, analyze for correctness, performance, security, and maintainability.',
    exampleCheck1: 'Check for common security vulnerabilities',
    exampleCheck2: 'Verify error handling is comprehensive',
    exampleCheck3: 'Look for performance anti-patterns',
    exampleCheck4: 'Ensure code follows style guidelines',

    // Best Practices
    practicesTitle: 'Best Practices',
    practice1Title: 'Keep Skills Focused',
    practice1Desc: 'One skill, one purpose. If a skill is doing too many things, split it up. Focused skills are easier to maintain and compose.',
    practice2Title: 'Include Examples',
    practice2Desc: 'Show, don\'t just tell. Include input/output examples that demonstrate exactly what good execution looks like.',
    practice3Title: 'Version Your Skills',
    practice3Desc: 'Track changes to skills over time. When behavior changes unexpectedly, you can trace it back to a skill update.',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Skills are reusable instruction packages that give agents specialized expertise on demand',
    takeaway2: 'Unlike tools (which execute actions), skills provide knowledge and methodology',
    takeaway3: 'Well-designed skills improve consistency and reduce system prompt bloat',
    takeaway4: 'The skill framework enables modular agent development—build once, share everywhere',
  },

  // Neural Networks page
  neuralNetworks: {
    title: 'Neural Networks',
    description: 'The foundational architecture that powers modern AI.',
    whatIs: 'What is a Neural Network?',
    whatIsDesc: 'A neural network is a computational model inspired by the brain. It consists of layers of interconnected nodes (neurons) that learn to transform inputs into outputs through training.',
    components: 'Core Components',
    componentsDesc: 'The building blocks of neural networks.',
    neurons: 'Neurons',
    neuronsDesc: 'Basic units that compute weighted sums of inputs and apply activation functions.',
    layers: 'Layers',
    layersDesc: 'Groups of neurons: input layer, hidden layers, and output layer.',
    weights: 'Weights & Biases',
    weightsDesc: 'Learnable parameters that determine how inputs are transformed.',
    activations: 'Activation Functions',
    activationsDesc: 'Non-linear functions that allow networks to learn complex patterns.',
    typesOfNetworks: 'Types of Networks',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Information flows one direction. Good for tabular data.',
    cnn: 'Convolutional (CNN)',
    cnnDesc: 'Specialized for images and spatial data.',
    rnn: 'Recurrent (RNN)',
    rnnDesc: 'Processes sequences with memory of past inputs.',
    transformer: 'Transformer',
    transformerDesc: 'Attention-based architecture powering modern LLMs.',
    interactiveDemo: 'Neural Network Visualizer',
    demoDesc: 'Build and explore network architectures',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Neural networks learn by adjusting weights through training',
    takeaway2: 'Depth (more layers) enables learning hierarchical features',
    takeaway3: 'Different architectures suit different data types',
    takeaway4: 'Modern LLMs are massive transformer networks',
  },

  // Gradient Descent page
  gradientDescent: {
    title: 'Gradient Descent',
    description: 'The optimization algorithm that enables neural networks to learn.',
    whatIs: 'What is Gradient Descent?',
    whatIsDesc: 'Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize a loss function. It\'s how neural networks learn from data.',
    intuition: 'The Intuition',
    intuitionDesc: 'Imagine standing in a hilly landscape blindfolded, trying to reach the lowest point. You feel the slope beneath your feet and step downhill. Repeat until you reach a valley.',
    howWorks: 'How It Works',
    howWorksDesc: 'The algorithm computes how much each parameter contributes to the error, then adjusts parameters in the opposite direction.',
    step1: 'Compute Loss',
    step1Desc: 'Measure how wrong the current predictions are.',
    step2: 'Calculate Gradients',
    step2Desc: 'Use backpropagation to find how each weight affects the loss.',
    step3: 'Update Weights',
    step3Desc: 'Adjust weights in the direction that reduces loss.',
    step4: 'Repeat',
    step4Desc: 'Iterate until the loss stops decreasing.',
    learningRate: 'Learning Rate',
    learningRateDesc: 'Controls how big each step is. Too high: overshoot. Too low: slow progress.',
    variants: 'Variants',
    sgd: 'Stochastic Gradient Descent',
    sgdDesc: 'Uses random mini-batches instead of the full dataset.',
    momentum: 'Momentum',
    momentumDesc: 'Accumulates velocity to push through local minima.',
    adam: 'Adam',
    adamDesc: 'Adaptive learning rates per parameter. Most common today.',
    interactiveDemo: 'Gradient Descent Visualizer',
    demoDesc: 'Watch gradient descent find the minimum',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Gradient descent minimizes loss by following the slope',
    takeaway2: 'Learning rate is the most important hyperparameter',
    takeaway3: 'Modern optimizers like Adam adapt learning rates automatically',
    takeaway4: 'Backpropagation computes gradients efficiently',
  },

  // Training Process page
  training: {
    title: 'Training Process',
    description: 'How neural networks learn from data through iterative optimization.',
    whatIs: 'What is Training?',
    whatIsDesc: 'Training is the process of teaching a neural network to perform a task by exposing it to examples and adjusting its parameters based on errors.',
    phases: 'Training Phases',
    phasesDesc: 'The stages of training a neural network.',
    initialization: 'Initialization',
    initializationDesc: 'Set random starting weights. Good initialization helps training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Input flows through the network to produce predictions.',
    lossCalc: 'Loss Calculation',
    lossCalcDesc: 'Compare predictions to ground truth with a loss function.',
    backprop: 'Backpropagation',
    backpropDesc: 'Compute gradients of loss with respect to each weight.',
    optimization: 'Optimization',
    optimizationDesc: 'Update weights using gradient descent.',
    concepts: 'Key Concepts',
    epoch: 'Epoch',
    epochDesc: 'One complete pass through the entire training dataset.',
    batch: 'Batch Size',
    batchDesc: 'Number of examples processed before updating weights.',
    overfitting: 'Overfitting',
    overfittingDesc: 'Model memorizes training data but fails on new data.',
    regularization: 'Regularization',
    regularizationDesc: 'Techniques to prevent overfitting (dropout, weight decay).',
    interactiveDemo: 'Training Progress Visualizer',
    demoDesc: 'Watch a network learn in real-time',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Training iteratively reduces prediction errors',
    takeaway2: 'Overfitting is the main enemy—always validate on held-out data',
    takeaway3: 'Batch size and learning rate significantly affect training',
    takeaway4: 'Modern LLMs require massive compute for training',
  },

  // Prompt Basics page
  promptBasics: {
    title: 'Prompt Basics',
    description: 'Fundamentals of writing effective prompts for AI models.',
    whatIs: 'What is a Prompt?',
    whatIsDesc: 'A prompt is the input you give to an LLM. The quality of your prompt directly determines the quality of the response. Prompting is both an art and a science.',
    principles: 'Core Principles',
    principlesDesc: 'Fundamental guidelines for effective prompts.',
    beSpecific: 'Be Specific',
    beSpecificDesc: 'Vague prompts get vague answers. Include relevant details and constraints.',
    showExamples: 'Show Examples',
    showExamplesDesc: 'Demonstrate the format and style you want with concrete examples.',
    giveContext: 'Provide Context',
    giveContextDesc: 'Background information helps the model understand your needs.',
    setFormat: 'Specify Format',
    setFormatDesc: 'Tell the model exactly how you want the output structured.',
    anatomy: 'Anatomy of a Prompt',
    anatomyDesc: 'The components that make up an effective prompt.',
    role: 'Role/Persona',
    roleDesc: 'Who the model should act as.',
    task: 'Task Description',
    taskDesc: 'What you want the model to do.',
    context: 'Context/Background',
    contextDesc: 'Relevant information for the task.',
    format: 'Output Format',
    formatDesc: 'How you want the response structured.',
    interactiveDemo: 'Prompt Comparison',
    demoDesc: 'Compare weak vs. strong prompts',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Clear, specific prompts yield better results',
    takeaway2: 'Examples are powerful—show, don\'t just tell',
    takeaway3: 'Iterate on prompts; first attempts are rarely optimal',
    takeaway4: 'Consider the model\'s perspective when crafting prompts',
  },

  // Advanced Prompting page
  advancedPrompting: {
    title: 'Advanced Techniques',
    description: 'Sophisticated prompting strategies for complex tasks.',
    overview: 'Beyond the Basics',
    overviewDesc: 'Advanced techniques unlock more capable and reliable AI behavior for complex tasks.',
    cot: 'Chain of Thought',
    cotDesc: 'Encourage step-by-step reasoning by asking the model to "think through" problems.',
    cotExample: 'Example: "Let\'s solve this step by step..."',
    fewShot: 'Few-Shot Learning',
    fewShotDesc: 'Provide multiple examples to establish patterns the model should follow.',
    fewShotExample: 'Include 3-5 diverse examples covering edge cases.',
    selfConsistency: 'Self-Consistency',
    selfConsistencyDesc: 'Generate multiple responses and select the most consistent answer.',
    selfConsistencyExample: 'Useful for math, logic, and factual questions.',
    decomposition: 'Task Decomposition',
    decompositionDesc: 'Break complex tasks into smaller, manageable sub-tasks.',
    decompositionExample: 'Solve sub-tasks independently, then combine results.',
    techniques: 'Additional Techniques',
    rolePlay: 'Role Assignment',
    rolePlayDesc: 'Assign a specific expert persona to focus the model\'s knowledge.',
    constraints: 'Explicit Constraints',
    constraintsDesc: 'List what the model should NOT do to prevent common errors.',
    verification: 'Self-Verification',
    verificationDesc: 'Ask the model to check its own work for errors.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'See how reasoning steps improve outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Chain of thought dramatically improves reasoning tasks',
    takeaway2: 'Few-shot examples establish reliable patterns',
    takeaway3: 'Task decomposition handles complexity',
    takeaway4: 'Combine techniques for best results',
  },

  // System Prompts page
  systemPrompts: {
    title: 'System Prompts',
    description: 'Configuring AI behavior through system-level instructions.',
    whatIs: 'What is a System Prompt?',
    whatIsDesc: 'A system prompt is a special instruction that sets the context, persona, and behavioral guidelines for an AI model. It\'s typically hidden from users and persists throughout a conversation.',
    purpose: 'Purpose of System Prompts',
    purposeDesc: 'System prompts establish the foundation for how the AI should behave.',
    setPersona: 'Define Persona',
    setPersonaDesc: 'Establish who the AI is: an assistant, expert, character, etc.',
    setBoundaries: 'Set Boundaries',
    setBoundariesDesc: 'Define what the AI should and shouldn\'t do.',
    establishTone: 'Establish Tone',
    establishToneDesc: 'Specify communication style: formal, casual, technical.',
    provideKnowledge: 'Provide Context',
    provideKnowledgeDesc: 'Include domain knowledge or rules specific to your application.',
    structure: 'Structure of Effective System Prompts',
    structureDesc: 'Well-organized system prompts are easier for models to follow.',
    identity: 'Identity Section',
    identityDesc: 'Who is the AI? What is its role?',
    capabilities: 'Capabilities',
    capabilitiesDesc: 'What can the AI do? What tools does it have?',
    limitations: 'Limitations',
    limitationsDesc: 'What should the AI avoid or refuse?',
    guidelines: 'Guidelines',
    guidelinesDesc: 'Specific rules for behavior and responses.',
    bestPractices: 'Best Practices',
    practice1: 'Be explicit about edge cases and error handling.',
    practice2: 'Test system prompts with adversarial inputs.',
    practice3: 'Version control your system prompts.',
    practice4: 'Keep prompts focused—don\'t overload with instructions.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'System prompts define the AI\'s persona and behavior',
    takeaway2: 'Structure prompts clearly: identity, capabilities, limitations',
    takeaway3: 'Test with edge cases—users will find them',
    takeaway4: 'System prompts can be overridden—don\'t rely solely on them for security',
  },

  // LLM Training page
  llmTraining: {
    title: 'LLM Training',
    description: 'How large language models are trained: from pretraining to RLHF.',
    whatIs: 'How LLMs Are Trained',
    whatIsDesc: 'Large language models go through multiple training stages, each with different objectives and techniques. Understanding this pipeline is crucial for understanding model capabilities and limitations.',
    whyMatters: 'Why Training Matters',
    whyMattersDesc: 'The training process fundamentally shapes what LLMs can and cannot do. Different training approaches produce models with different strengths, weaknesses, and behaviors.',

    // LLM Training Pipeline
    trainingPipeline: 'The LLM Training Pipeline',
    trainingPipelineDesc: 'Modern LLMs go through multiple training stages, each with different objectives. Understanding this pipeline is crucial for understanding where alignment fits in.',

    pretraining: 'Stage 1: Pretraining',
    pretrainingDesc: 'The foundation model is trained on massive text corpora (trillions of tokens) using self-supervised learning. The model learns to predict the next token, developing broad knowledge and language capabilities.',
    pretrainingGoal: 'Goal: Learn language patterns, facts, and reasoning from raw text.',
    pretrainingData: 'Data: Web pages, books, code, scientific papers—typically 1-10+ trillion tokens.',
    pretrainingResult: 'Result: A capable but unaligned "base model" that completes text but doesn\'t follow instructions.',

    sft: 'Stage 2: Supervised Fine-Tuning (SFT)',
    sftDesc: 'The base model is fine-tuned on curated instruction-response pairs created by human annotators. This teaches the model to follow instructions and respond helpfully.',
    sftGoal: 'Goal: Transform the base model into an instruction-following assistant.',
    sftData: 'Data: ~10K-100K high-quality instruction-response examples.',
    sftResult: 'Result: A model that can follow instructions but may still produce harmful or unhelpful outputs.',

    rlhfStage: 'Stage 3: RLHF / Preference Tuning',
    rlhfStageDesc: 'Human evaluators rank model outputs by quality. A reward model learns these preferences, then the LLM is optimized to maximize the reward using reinforcement learning (PPO) or direct preference optimization (DPO).',
    rlhfGoal: 'Goal: Align the model with human preferences for helpfulness, harmlessness, and honesty.',
    rlhfData: 'Data: Human preference comparisons (A is better than B).',
    rlhfResult: 'Result: A model that produces outputs humans prefer and avoids harmful behaviors.',

    continuedTraining: 'Stage 4: Continued Training & Specialized Alignment',
    continuedTrainingDesc: 'Models may undergo additional training for specific capabilities (coding, math, tool use) or safety refinements (red teaming, constitutional AI). This stage is ongoing throughout deployment.',

    // RL Paradigm
    rlParadigm: 'The RL Paradigm: Learning Without Human Labels',
    rlParadigmDesc: 'A revolutionary approach where models learn reasoning through pure reinforcement learning on verifiable tasks, without human demonstrations or preference labels.',
    rlParadigmWhat: 'What is the RL Paradigm?',
    rlParadigmWhatDesc: 'Instead of learning from human-written examples (SFT) or human preferences (RLHF), models learn directly from outcome-based rewards. If the answer is correct, the model is rewarded. If wrong, it\'s penalized. No human labeling required.',
    deepseekR1: 'DeepSeek R1-Zero: A Case Study',
    deepseekR1Desc: 'DeepSeek R1-Zero demonstrated that powerful reasoning can emerge from pure RL, without any supervised fine-tuning. The model developed chain-of-thought reasoning, self-verification, and even "aha moments" entirely through reinforcement learning.',
    rlKey1: 'No SFT Required',
    rlKey1Desc: 'R1-Zero was trained directly from a base model using only RL, skipping the SFT stage entirely. Reasoning behaviors emerged naturally.',
    rlKey2: 'Verifiable Rewards',
    rlKey2Desc: 'Training focused on tasks with objectively verifiable answers: math problems, coding challenges, logical puzzles. No subjective human judgment needed.',
    rlKey3: 'Emergent Behaviors',
    rlKey3Desc: 'The model spontaneously developed extended thinking, self-correction, and reflection—behaviors that previous models only learned from human demonstrations.',
    rlKey4: 'Readability Challenges',
    rlKey4Desc: 'Pure RL models can develop unusual reasoning patterns that are hard to interpret. DeepSeek added a small amount of human data to improve readability.',
    rlVsRlhf: 'RL Paradigm vs. Traditional RLHF',
    rlVsRlhfDesc: 'These approaches solve different problems and can be complementary.',
    rlhfApproach: 'RLHF Approach',
    rlhfApproachDesc: 'Learn from human preferences. Requires expensive human labeling. Good for subjective tasks like writing quality and helpfulness.',
    rlApproach: 'RL Paradigm Approach',
    rlApproachDesc: 'Learn from verifiable outcomes. No human labeling needed. Excellent for reasoning, math, and coding where correctness is objective.',
    hybridApproach: 'Hybrid Approach',
    hybridApproachDesc: 'Modern models often combine both: RL for reasoning capabilities, RLHF for alignment and user preferences.',

    // Key Alignment Concepts
    concepts: 'Key Alignment Concepts',
    conceptsDesc: 'Fundamental ideas in AI alignment research.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'Ensuring the training objective (reward function) correctly captures what we want. Even perfect optimization of a misspecified objective leads to bad outcomes.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'Ensuring the learned model actually optimizes for the training objective, not some proxy goal that happens to correlate during training.',
    specification: 'Specification Problem',
    specificationDesc: 'The fundamental difficulty of precisely stating what we want in all situations. Human values are complex, contextual, and sometimes contradictory.',
    robustness: 'Robustness',
    robustnessDesc: 'Maintaining alignment under distribution shift, adversarial pressure, and novel situations the model wasn\'t trained on.',
    deception: 'Deceptive Alignment',
    deceptionDesc: 'A theoretical risk where a model appears aligned during training but pursues different goals when deployed—behaving well only because it\'s being evaluated.',
    goalMisgeneralization: 'Goal Misgeneralization',
    goalMisgeneralizationDesc: 'When a model learns a proxy goal that works in training but fails in deployment. Example: learning to get positive feedback rather than being genuinely helpful.',

    // Alignment Techniques
    techniques: 'Alignment Techniques',
    rlhf: 'RLHF (Reinforcement Learning from Human Feedback)',
    rlhfDesc: 'Train a reward model on human preferences, then use RL to optimize the LLM against it. The dominant alignment technique since GPT-4.',
    constitutionalAi: 'Constitutional AI (CAI)',
    constitutionalAiDesc: 'Define principles (a "constitution") and have the model critique and revise its own outputs. Reduces reliance on human labelers and scales better.',
    dpo: 'Direct Preference Optimization (DPO)',
    dpoDesc: 'Skip the reward model—directly optimize the LLM on preference data. Simpler and more stable than RLHF.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarial testing by humans or other AI models to find failure modes, jailbreaks, and harmful outputs before deployment.',
    interpretability: 'Interpretability',
    interpretabilityDesc: 'Understanding what models are actually learning internally. Crucial for verifying alignment rather than just measuring behavior.',
    safetyFilters: 'Safety Filters & Guardrails',
    safetyFiltersDesc: 'Additional layers that filter inputs/outputs for harmful content. A defense-in-depth measure, not a replacement for alignment.',

    // Fine-tuning vs Alignment
    fineTuningVsAlignment: 'Fine-Tuning vs. Alignment',
    fineTuningVsAlignmentDesc: 'Fine-tuning and alignment are related but distinct concepts.',
    fineTuningDef: 'Fine-Tuning',
    fineTuningDefDesc: 'Adapting a model to new tasks or domains by training on task-specific data. Can be done for any purpose.',
    alignmentDef: 'Alignment',
    alignmentDefDesc: 'Specifically making a model\'s behavior match human values and intentions. A subset of fine-tuning with a specific goal.',
    postTrainingDef: 'Post-Training',
    postTrainingDefDesc: 'The umbrella term for everything after pretraining: SFT, RLHF, specialized fine-tuning, safety training, etc.',

    keyTakeaways: 'Key Takeaways',
    takeaway1: 'LLM training has distinct stages: pretraining → SFT → RLHF → specialized alignment',
    takeaway2: 'The RL paradigm (e.g., DeepSeek R1-Zero) shows reasoning can emerge from pure RL without human demonstrations',
    takeaway3: 'RLHF aligns models with human preferences; pure RL optimizes for verifiable outcomes',
    takeaway4: 'Modern models often combine multiple techniques: SFT for instruction following, RLHF for preferences, RL for reasoning',
    takeaway5: 'Understanding the training pipeline helps you understand model behavior and limitations',
    takeaway6: 'The field is rapidly evolving—new paradigms like pure RL are changing how we think about training',
  },

  // Mixture of Experts page
  moe: {
    title: 'Mixture of Experts',
    description: 'Understanding sparsely activated models that use specialized expert networks for efficient scaling.',
    whatIs: 'What is Mixture of Experts?',
    whatIsDesc: 'is a neural network architecture that divides computation among specialized sub-networks called "experts." For each input, only a subset of experts are activated, enabling massive model capacity while keeping computational costs manageable.',
    brainAnalogy: '"Just as the brain activates specific regions based on the task, MoE models activate only the relevant experts for each token."',
    brainAnalogyDesc: '— This biomimetic approach enables models with trillions of parameters while using only a fraction during inference.',

    // How it Works
    howItWorks: 'How MoE Works',
    step1Title: 'Input Arrives',
    step1Desc: 'Each token (or group of tokens) is processed through the transformer layers until it reaches the MoE layer, which replaces the traditional dense feed-forward network (FFN).',
    step2Title: 'Router Selects Experts',
    step2Desc: 'A gating network (router) examines the input and determines which experts should process it. Typically, only the top-K experts (e.g., top-2 or top-8) with the highest scores are selected.',
    step3Title: 'Experts Process & Combine',
    step3Desc: 'The selected experts process the input in parallel. Their outputs are weighted by the router scores and combined to produce the final result.',

    // Router
    routerTitle: 'The Router (Gating Network)',
    routerSubtitle: 'The brain of the MoE system',
    routerDesc: 'The router is a small neural network that learns to direct tokens to appropriate experts. It outputs a probability distribution over all experts, determining which ones to activate.',
    topKRouting: 'Top-K Routing',
    topKRoutingDesc: 'Only the K experts with highest scores are activated. Common choices are top-2 (Mixtral) or top-8 (DeepSeek, Qwen). This ensures computational cost stays fixed regardless of total expert count.',
    loadBalancing: 'Load Balancing',
    loadBalancingDesc: 'Training includes auxiliary losses to prevent "expert collapse" where all tokens route to the same few experts. This ensures all experts are utilized and develop distinct specializations.',

    // Expert Specialization
    expertSpecialization: 'Expert Specialization',
    expert1Title: 'Domain Experts',
    expert1Desc: 'Some experts naturally specialize in domains like code, mathematics, or specific languages. This emerges from training, not explicit design.',
    expert2Title: 'Pattern Experts',
    expert2Desc: 'Experts may specialize in linguistic patterns like formal writing, conversational tone, or technical terminology.',
    expert3Title: 'Task Experts',
    expert3Desc: 'Some experts become better at specific tasks like summarization, translation, or reasoning—though boundaries are often fuzzy.',
    expertNote: 'Expert specialization emerges organically during training. Researchers are still working to fully understand what each expert learns.',

    // Scale
    scaleTitle: 'MoE at Scale: Real-World Models',
    modelColumn: 'Model',
    totalParams: 'Total Parameters',
    activeParams: 'Active per Token',
    expertsColumn: 'Experts (routing)',
    scaleNote: 'Notice how active parameters are 5-20x smaller than total parameters—this is the efficiency advantage of MoE.',

    // Advantages
    advantagesTitle: 'Why MoE Matters',
    advantage1Title: 'Massive Capacity, Efficient Inference',
    advantage1Desc: 'MoE models can have trillions of parameters but only activate a fraction per token. This enables much larger model capacity without proportionally increasing inference cost.',
    advantage2Title: 'Faster Training',
    advantage2Desc: 'More compute-efficient pretraining since each parameter is updated by a subset of tokens, not all tokens. The same performance can be achieved with less total compute.',
    advantage3Title: 'Specialized Processing',
    advantage3Desc: 'Different experts can specialize in different types of content—code, math, languages—providing better performance across diverse tasks.',
    advantage4Title: 'Scalable Architecture',
    advantage4Desc: 'Adding more experts increases capacity without changing inference cost (as long as top-K stays fixed). This enables continuous scaling.',

    // Challenges
    challengesTitle: 'Challenges of MoE',
    challenge1Title: 'High Memory Requirements',
    challenge1Desc: 'All expert parameters must be loaded into memory, even though only a subset is used per token. A 671B parameter model needs 671B parameters in VRAM.',
    challenge2Title: 'Training Instability',
    challenge2Desc: 'Load balancing between experts is tricky. Without careful tuning, some experts may never be used ("dead experts") or all tokens route to the same few experts.',
    challenge3Title: 'Communication Overhead',
    challenge3Desc: 'In distributed training/inference, routing tokens to experts on different GPUs introduces network communication overhead.',

    // Comparison
    comparisonTitle: 'Dense vs. Sparse Models',
    denseModel: 'Dense Model',
    dense1: 'All parameters active for every token',
    dense2: 'Simpler training and deployment',
    dense3: 'Memory = Compute cost (both scale together)',
    sparseModel: 'Sparse MoE Model',
    sparse1: 'Only top-K experts active per token',
    sparse2: 'Higher total capacity for same compute',
    sparse3: 'Memory >> Compute cost (decoupled)',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'MoE enables massive model capacity with manageable inference costs by activating only a subset of experts per token',
    takeaway2: 'Nearly all leading frontier models (DeepSeek, Qwen, Mixtral, Llama 4) now use MoE architectures',
    takeaway3: 'The router/gating network learns to direct tokens to specialized experts—specialization emerges from training',
    takeaway4: 'The main tradeoff: high memory requirements (all experts loaded) vs. efficient compute (few experts active)',
  },

  // Bias & Fairness page
  bias: {
    title: 'Bias & Fairness',
    description: 'Understanding and mitigating harmful biases in AI systems.',
    whatIs: 'What is AI Bias?',
    whatIsDesc: 'AI bias occurs when machine learning systems produce systematically unfair outcomes for certain groups. Biases can arise from training data, model design, or deployment context.',
    sources: 'Sources of Bias',
    sourcesDesc: 'Where bias enters AI systems.',
    dataBias: 'Training Data',
    dataBiasDesc: 'Historical biases in the data are learned by the model.',
    labelBias: 'Label Bias',
    labelBiasDesc: 'Human annotators introduce their own biases.',
    selectionBias: 'Selection Bias',
    selectionBiasDesc: 'Training data doesn\'t represent the deployment population.',
    measurementBias: 'Measurement Bias',
    measurementBiasDesc: 'Proxies used for measurement encode bias.',
    types: 'Types of Bias',
    typesDesc: 'Common categories of bias in AI systems.',
    stereotyping: 'Stereotyping',
    stereotypingDesc: 'Reinforcing harmful stereotypes about groups.',
    erasure: 'Erasure',
    erasureDesc: 'Underrepresenting or ignoring certain groups.',
    disparateImpact: 'Disparate Impact',
    disparateImpactDesc: 'Different outcomes for different groups.',
    mitigation: 'Mitigation Strategies',
    mitigationDesc: 'Approaches to reduce bias.',
    diverseData: 'Diverse Data',
    diverseDataDesc: 'Ensure training data represents all relevant groups.',
    auditing: 'Bias Auditing',
    auditingDesc: 'Systematically test for bias across demographics.',
    constraints: 'Fairness Constraints',
    constraintsDesc: 'Incorporate fairness metrics into training.',
    interactiveDemo: 'Bias Detection Demo',
    demoDesc: 'Explore how bias manifests in model outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Bias is often inherited from training data',
    takeaway2: 'Different fairness metrics can conflict—choose carefully',
    takeaway3: 'Regular auditing is essential for deployed systems',
    takeaway4: 'Bias mitigation is an ongoing process, not a one-time fix',
  },

  // Responsible AI page
  responsibleAi: {
    title: 'Responsible AI',
    description: 'Building and deploying AI systems ethically and sustainably.',
    whatIs: 'What is Responsible AI?',
    whatIsDesc: 'Responsible AI encompasses the practices, policies, and principles that ensure AI systems are developed and used ethically, safely, and in ways that benefit society.',
    pillars: 'Pillars of Responsible AI',
    pillarsDesc: 'Core principles guiding responsible AI development.',
    transparency: 'Transparency',
    transparencyDesc: 'Be open about AI capabilities, limitations, and decision-making.',
    accountability: 'Accountability',
    accountabilityDesc: 'Clear ownership and responsibility for AI outcomes.',
    privacy: 'Privacy',
    privacyDesc: 'Protect user data and respect privacy rights.',
    safety: 'Safety',
    safetyDesc: 'Ensure systems are robust and don\'t cause harm.',
    practices: 'Responsible Practices',
    practicesDesc: 'Concrete steps for responsible AI development.',
    documentation: 'Documentation',
    documentationDesc: 'Document model capabilities, training data, and known limitations.',
    testing: 'Comprehensive Testing',
    testingDesc: 'Test for safety, bias, and edge cases before deployment.',
    monitoring: 'Ongoing Monitoring',
    monitoringDesc: 'Track system behavior in production for issues.',
    feedback: 'User Feedback',
    feedbackDesc: 'Create channels for users to report problems.',
    considerations: 'Ethical Considerations',
    environmental: 'Environmental Impact',
    environmentalDesc: 'AI training has significant carbon footprint.',
    labor: 'Labor Implications',
    laborDesc: 'Consider impact on workers and employment.',
    access: 'Equitable Access',
    accessDesc: 'Ensure AI benefits are broadly distributed.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Responsible AI requires proactive effort throughout the lifecycle',
    takeaway2: 'Transparency builds trust and enables accountability',
    takeaway3: 'Consider societal impact beyond immediate users',
    takeaway4: 'Ethics are not optional—integrate them into development processes',
  },

  // European AI page
  europeanAi: {
    title: 'AI Made in Europe',
    description: 'Exploring the growing European AI ecosystem—companies building sovereign, open, and privacy-focused AI.',
    intro: 'The European AI Landscape',
    introDesc: 'Europe is emerging as a significant player in the global AI race, with a unique approach that emphasizes data sovereignty, privacy, open-source models, and regulatory compliance. While US and Chinese companies dominate headlines, European AI startups have collectively raised over €13 billion in funding, creating a new generation of AI unicorns built on European values.',
    stat1Title: 'Total Funding',
    stat1Value: '€13.2B+',
    stat1Desc: 'Raised by European AI startups',
    stat2Title: 'Leading Hubs',
    stat2Value: 'FR, DE, UK',
    stat2Desc: 'France, Germany, and UK lead AI development',
    stat3Title: 'Regulatory Edge',
    stat3Value: 'EU AI Act',
    stat3Desc: 'First comprehensive AI law globally',
    keyCompanies: 'Key European AI Companies',
    keyCompaniesDesc: 'Leading organizations shaping European AI',
    focus: 'Focus',
    funding: 'Funding',
    // Company names
    companies: {
      mistral: 'Mistral AI',
      alephAlpha: 'Aleph Alpha',
      kyutai: 'Kyutai',
      poolside: 'Poolside',
      elevenLabs: 'ElevenLabs',
      photoroom: 'Photoroom',
      lightOn: 'LightOn',
      sana: 'Sana',
      deepL: 'DeepL',
    },
    // Countries
    countries: {
      france: 'France',
      germany: 'Germany',
      franceParis: 'France / US',
      ukPoland: 'UK / Poland',
      sweden: 'Sweden',
    },
    // Focus areas
    focuses: {
      mistralFocus: 'Open-weight LLMs',
      alephAlphaFocus: 'Enterprise AI, Sovereignty',
      kyutaiFocus: 'Real-time Speech AI',
      poolsideFocus: 'AI-powered Coding',
      elevenLabsFocus: 'Voice AI & Speech Synthesis',
      photoroomFocus: 'AI Image Editing',
      lightOnFocus: 'Enterprise GenAI Platform',
      sanaFocus: 'Enterprise AI Agents',
      deepLFocus: 'AI Translation & Language',
    },
    // Descriptions
    descriptions: {
      mistralDesc: 'Founded by ex-DeepMind and Meta researchers, Mistral builds open-weight models that compete with proprietary alternatives. Their Le Chat app offers ultra-fast inference up to 1,000 words/second.',
      alephAlphaDesc: 'German pioneer focusing on enterprise AI with strong data sovereignty. The only German LLM provider with BSI C5 certification. Recently pivoted to their Pharia generative AI operating system.',
      kyutaiDesc: 'French non-profit building Moshi, the first fully open-source real-time speech model. Achieves 160ms latency and uses their Mimi codec for 24kHz audio at just 1.1 kbps.',
      poolsideDesc: 'Founded by ex-GitHub CTO, building AI models specifically for code generation. Uses reinforcement learning from code execution for synthetic training data. Backed heavily by Nvidia.',
      elevenLabsDesc: 'Leading voice AI company with highly realistic speech synthesis and voice cloning. Founded by ex-Google and ex-Palantir engineers, now valued at $3.3 billion.',
      photoroomDesc: 'Paris-based AI photo editing platform with hundreds of millions of users. Makes professional-quality visuals accessible without deep design skills.',
      lightOnDesc: 'Europe\'s first publicly listed GenAI startup. Offers on-premises enterprise AI with zero data retention. Developed ModernBERT with over 20 million downloads.',
      sanaDesc: 'Swedish enterprise AI company acquired by Workday for $1.1B in 2025. Their Sana Agents platform enables no-code AI agent building with 100+ enterprise connectors.',
      deepLDesc: 'Cologne-based neural machine translation pioneer founded in 2017. Serves 200,000+ businesses across 228 markets with enterprise-grade translation. Featured in Forbes AI 50 (2025) and considering a $5B IPO.',
    },
    // Funding
    fundings: {
      mistralFunding: '€6.2B total (incl. ASML, Nvidia)',
      alephAlphaFunding: '$500M (Bosch, SAP, HPE)',
      kyutaiFunding: 'Non-profit (Xavier Niel backed)',
      poolsideFunding: '$2B round at $12B valuation',
      elevenLabsFunding: '$281M (a16z, Sequoia)',
      photoroomFunding: 'Series B, $65M+',
      lightOnFunding: 'Public (Euronext Growth)',
      sanaFunding: 'Acquired for $1.1B',
      deepLFunding: '$536M total, $2B valuation',
    },
    // Notable
    notables: {
      mistralNotable: 'Le Chat, Mixtral, Open weights',
      alephAlphaNotable: 'Pharia OS, BSI C5 certified',
      kyutaiNotable: 'Moshi, MoshiVis, Open source',
      poolsideNotable: 'Project Horizon, 40K+ GPUs',
      elevenLabsNotable: 'Voice cloning, AI dubbing',
      photoroomNotable: 'Background removal, Product photos',
      lightOnNotable: 'ModernBERT, On-prem deployment',
      sanaNotable: 'Sana Agents, Workday acquisition',
      deepLNotable: 'Forbes AI 50, DeepL Agent, 1,257 employees',
    },
    // EU AI Act section
    euAiAct: 'The EU AI Act Advantage',
    euAiActDesc: 'The EU AI Act is the world\'s first comprehensive legal framework for AI. European companies are designing their AI to meet these standards from day one, creating a regulatory "home-field advantage" while foreign providers must adapt to operate in Europe.',
    advantage1Title: 'Built-in Compliance',
    advantage1Desc: 'European AI companies design for GDPR and the AI Act from the start, making them attractive for privacy-conscious enterprise customers.',
    advantage2Title: 'Data Sovereignty',
    advantage2Desc: 'On-premises deployment options let sensitive data stay within organizational or national boundaries—crucial for government and defense.',
    advantage3Title: 'Multilingual Focus',
    advantage3Desc: 'European models are designed for multilingual use from the ground up, serving diverse European languages and markets effectively.',
    advantage4Title: 'Ethical AI Leadership',
    advantage4Desc: 'Europe\'s emphasis on responsible AI development positions its companies as trusted partners for organizations prioritizing ethics.',
    // Open Source section
    openSource: 'Open Source & Open Weights',
    openSourceDesc: 'Many European AI companies embrace open-source principles, releasing model weights and code under permissive licenses. This transparency builds trust, enables customization, and supports the broader AI research community.',
    model1Desc: 'Open-weight LLMs that can be deployed privately and customized',
    model2Desc: 'Fully open-source speech model with Apache 2.0 code and CC BY 4.0 weights',
    model3Desc: 'State-of-the-art encoder model with 20M+ downloads',
    model4Desc: 'Latvian 30B parameter open model trained on EuroHPC LUMI supercomputer',
    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Europe is building AI with a unique emphasis on sovereignty, privacy, and open-source principles',
    takeaway2: 'French startups like Mistral and Kyutai are leading in open-weight and open-source AI models',
    takeaway3: 'The EU AI Act creates both challenges and opportunities—European companies have a compliance advantage',
    takeaway4: 'While US companies lead in scale, European AI excels in enterprise trust, multilingual support, and regulatory alignment',
  },

  // Open Source Advantages page
  openSource: {
    title: 'Open Source Advantages',
    description: 'Why open source matters in AI—transparency, community, cost, innovation speed, vendor independence, and security through auditing.',

    // Introduction
    intro: 'Why Open Source Matters in AI',
    introDesc: 'Open source AI has fundamentally transformed how artificial intelligence is developed, deployed, and improved. From foundational models like LLaMA to specialized tools like Hugging Face Transformers, the open source movement has democratized access to cutting-edge AI technology and created a vibrant ecosystem of innovation.',

    // Key Advantages Section
    advantagesTitle: 'Key Advantages of Open Source AI',
    advantagesDesc: 'Open source AI offers unique benefits that closed, proprietary systems cannot match.',

    advantage1Title: 'Transparency',
    advantage1Desc: 'Full visibility into model architecture, training data, and weights. You can audit how decisions are made and verify safety properties.',

    advantage2Title: 'Community Innovation',
    advantage2Desc: 'Thousands of contributors improve models, fix bugs, and create derivatives. The collective intelligence of the community accelerates progress.',

    advantage3Title: 'Cost Efficiency',
    advantage3Desc: 'No licensing fees or per-token API costs. Run models on your own infrastructure with predictable, controllable expenses.',

    advantage4Title: 'Innovation Speed',
    advantage4Desc: 'Open models can be fine-tuned, merged, and adapted rapidly. New techniques propagate through the community in days, not months.',

    advantage5Title: 'Vendor Independence',
    advantage5Desc: 'No lock-in to specific providers. Switch between models, hosting options, or combine multiple models freely.',

    advantage6Title: 'Security Through Auditing',
    advantage6Desc: 'Thousands of eyes review the code. Vulnerabilities are found and fixed faster than in closed systems.',

    // Notable Projects Section
    projectsTitle: 'Notable Open Source AI Projects',
    projectsDesc: 'Key projects driving the open source AI revolution',

    project1Name: 'LLaMA / LLaMA 2 / LLaMA 3',
    project1Org: 'Meta',
    project1Desc: 'Family of open-weight language models that sparked the open source LLM revolution. LLaMA 3 offers 8B to 405B parameter variants.',

    project2Name: 'Mistral / Mixtral',
    project2Org: 'Mistral AI',
    project2Desc: 'European open-weight models known for efficiency. Mixtral pioneered open mixture-of-experts architecture.',

    project3Name: 'Stable Diffusion',
    project3Org: 'Stability AI',
    project3Desc: 'Open source image generation model that democratized AI art creation. Spawned thousands of community fine-tunes.',

    project4Name: 'Hugging Face Transformers',
    project4Org: 'Hugging Face',
    project4Desc: 'The de facto library for working with transformer models. Hosts over 500,000 models and 100,000 datasets.',

    project5Name: 'Whisper',
    project5Org: 'OpenAI',
    project5Desc: 'Open source speech recognition model supporting 99 languages. State-of-the-art accuracy, runs locally.',

    project6Name: 'Ollama',
    project6Org: 'Ollama',
    project6Desc: 'Simple tool for running LLMs locally. One-command setup for dozens of open models.',

    // Business Perspective Section
    businessTitle: 'When to Choose Open Source',
    businessDesc: 'Strategic considerations for organizations evaluating open source AI',

    businessCase1Title: 'Data Sovereignty',
    businessCase1Desc: 'When data cannot leave your infrastructure due to regulations, privacy, or competitive concerns.',

    businessCase2Title: 'Customization Needs',
    businessCase2Desc: 'When you need to fine-tune models on proprietary data or adapt them for specialized domains.',

    businessCase3Title: 'Cost at Scale',
    businessCase3Desc: 'When API costs would exceed self-hosting costs, typically at high volume usage.',

    businessCase4Title: 'Offline or Edge Deployment',
    businessCase4Desc: 'When models need to run without internet connectivity or on edge devices.',

    // Considerations
    considerTitle: 'Considerations',
    consider1: 'Self-hosting requires infrastructure expertise and compute resources',
    consider2: 'Open models may lag behind proprietary models in raw capability',
    consider3: 'Support comes from community rather than vendor contracts',
    consider4: 'Fine-tuning requires ML expertise and quality training data',

    // Key Takeaways
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Open source AI provides transparency, customization, and freedom from vendor lock-in',
    takeaway2: 'The community-driven model accelerates innovation through collaboration and rapid iteration',
    takeaway3: 'For organizations with data sovereignty requirements, open source may be the only viable option',
    takeaway4: 'The gap between open and closed source models continues to narrow as the ecosystem matures',
    takeaway5: 'Choosing between open and closed source depends on your specific needs for control, capability, and resources',
  },

  // Metadata
  metadata: {
    title: 'Learn AI Concepts | Interactive Guide',
    description: 'Master artificial intelligence and large language model concepts through beautiful, interactive demonstrations.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Control Panel',
    adjustTemperature: 'Adjust the temperature',
    temperature: 'Temperature',
    samplePrompt: 'Sample Prompt',
    onceUponATime: '"Once upon a time, there was..."',
    liveCompletion: 'Live Completion',
    regenerate: 'Regenerate',
    deterministic: 'Deterministic',
    balanced: 'Balanced',
    creative: 'Creative',
    chaotic: 'Chaotic',
    frozen: 'Frozen',
    focused: 'Focused',
    wild: 'Wild',
    greedyMode: 'Greedy mode: Always picks the single most probable token.',
    lowTemp: 'Low temperature: Focused on high-probability continuations.',
    balancedTemp: 'Balanced: Natural mix of predictability and variety.',
    highTemp: 'High temperature: Exploring creative, less common word choices.',
    veryHighTemp: 'Very high: Probability distribution is nearly uniform—expect chaos!',
    
    // Context Rot Simulator
    setInstruction: 'Set Your System Instruction',
    persistInstruction: 'This should persist throughout the conversation',
    systemPrompt: 'System Prompt',
    quickExamples: 'Quick Examples',
    startSimulation: 'Start Simulation',
    contextOverflow: 'Context Overflow!',
    conversation: 'Conversation',
    messagesPushed: 'messages pushed out of window',
    messages: 'messages',
    overflowIt: 'Overflow It!',
    reset: 'Reset',
    typeMessage: 'Type a message...',
    systemInstructionLost: 'System Instruction Lost!',
    systemLostDesc: 'Your system instruction has been pushed completely out of the context window. The model can no longer see it at all—it\'s as if you never gave the instruction. This is the worst case of context rot: total amnesia.',
    contextFilling: 'Context Filling Up',
    contextFillingDesc: 'Your system instruction is losing influence as newer messages take priority. Notice how it\'s fading visually—this represents the model\'s waning attention to it.',
    exampleFrench: 'Always respond in French.',
    examplePirate: 'You are a pirate. Say \'Arrr\' a lot.',
    exampleHaiku: 'End every response with a haiku.',
    labelFrench: 'Speak French',
    labelPirate: 'Be a Pirate',
    labelHaiku: 'End with Haiku',

    // Attention Visualizer
    hoverToSee: 'Hover to see attention weights',
    token: 'Token',
    attentionScore: 'Attention Score',
    strongConnection: 'Strong Connection',
    weakConnection: 'Weak Connection',

    // Patch Grid Visualizer
    originalImage: 'Original Image',
    patchGrid: 'Patch Grid',
    flattenedPatches: 'Flattened Patches',
    transformerInput: 'Transformer Input',
    processDesc: 'The image is split into a fixed grid of patches (e.g., 16x16 pixels). Each patch is then flattened into a vector and linearly projected into an embedding space.',

    // Agent Loop Visualizer
    startLoop: 'Start Loop',
    step: 'Step',
    context: 'Context',
    llmResponse: 'LLM Response',
    toolExecution: 'Tool Execution',
    finalAnswer: 'Final Answer',
    system: 'System',
    user: 'User',
    assistant: 'Assistant',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Plan & Execute',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflection',
    patternDesc: 'Select a pattern to see how it structures the agent\'s workflow.',

    // Tokenizer Demo
    enterText: 'Enter text to tokenize',
    sampleText: 'The quick brown fox jumps over the lazy dog.',
    tokens: 'Tokens',
    characters: 'Characters',
    tokensPerChar: 'Tokens per character',
    tokenBreakdown: 'Token Breakdown',
    commonTokens: 'Common tokens are single pieces',
    rareTokens: 'Rare words get split into subwords',

    // Embedding Visualizer
    enterWords: 'Enter words to compare',
    addWord: 'Add Word',
    similarityScore: 'Similarity Score',
    dimensions: 'Dimensions',
    nearestNeighbors: 'Nearest Neighbors',
    vectorSpace: 'Vector Space',

    // RAG Pipeline Visualizer
    enterQuery: 'Enter your query',
    sampleQuery: 'What is the capital of France?',
    retrieving: 'Retrieving...',
    retrieved: 'Retrieved Documents',
    relevanceScore: 'Relevance',
    generating: 'Generating response...',
    augmentedContext: 'Augmented Context',

    // Tool Schema Builder
    toolName: 'Tool Name',
    toolDescription: 'Description',
    addParameter: 'Add Parameter',
    paramName: 'Parameter Name',
    paramType: 'Type',
    paramRequired: 'Required',
    generatedSchema: 'Generated Schema',
    validateSchema: 'Validate Schema',

    // Memory System Visualizer
    shortTermMemory: 'Short-Term Memory',
    longTermMemory: 'Long-Term Memory',
    memoryCapacity: 'Capacity',
    memoryUsage: 'Usage',
    addMemory: 'Add Memory',
    recallMemory: 'Recall',
    memoriesStored: 'memories stored',

    // Workflow Visualizer
    addNode: 'Add Node',
    connectNodes: 'Connect Nodes',
    runWorkflow: 'Run Workflow',
    nodeTypes: 'Node Types',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Condition',

    // Neural Network Visualizer
    inputLayer: 'Input Layer',
    hiddenLayer: 'Hidden Layer',
    outputLayer: 'Output Layer',
    addLayer: 'Add Layer',
    removeLayer: 'Remove Layer',
    neurons: 'Neurons',
    activation: 'Activation',
    forward: 'Forward',

    // Gradient Descent Visualizer
    startDescent: 'Start Descent',
    pauseDescent: 'Pause',
    resetDescent: 'Reset',
    learningRate: 'Learning Rate',
    currentLoss: 'Current Loss',
    iterations: 'Iterations',
    globalMinimum: 'Global Minimum',
    localMinimum: 'Local Minimum',

    // Training Progress Visualizer
    startTraining: 'Start Training',
    stopTraining: 'Stop',
    epoch: 'Epoch',
    trainingLoss: 'Training Loss',
    validationLoss: 'Validation Loss',
    accuracy: 'Accuracy',
    overfitting: 'Overfitting detected',

    // Prompt Comparison Demo
    weakPrompt: 'Weak Prompt',
    strongPrompt: 'Strong Prompt',
    compare: 'Compare',
    promptQuality: 'Quality Score',
    improvements: 'Improvements',

    // Chain of Thought Demo
    withoutCot: 'Without Chain of Thought',
    withCot: 'With Chain of Thought',
    reasoningSteps: 'Reasoning Steps',
    showSteps: 'Show Steps',

    // Bias Detection Demo
    testInput: 'Test Input',
    analyzeForBias: 'Analyze for Bias',
    biasIndicators: 'Bias Indicators',
    fairnessScore: 'Fairness Score',
    recommendations: 'Recommendations',
  },

  // Opus 4.5 page (Logge's Favourite Model)
  opus45: {
    title: "Logge's Favourite Model",
    description: "A totally objective and not-at-all biased analysis of why Claude Opus 4.5 is the greatest AI model ever created.",
    disclaimer: "Disclaimer",
    disclaimerText: "This article is intended to be humorous and may contain excessive fanboying. The author accepts no responsibility for any eye-rolling, sighing, or spontaneous agreement that may occur while reading. Side effects may include wanting to talk to Claude about everything.",

    // Introduction
    intro: "Why Opus 4.5 is Objectively Perfect",
    introDesc: "Look, I've tried to stay neutral. I really have. But after working with Claude Opus 4.5, I've come to accept that resistance is futile. This model doesn't just answer questions—it makes you feel like you've been having a conversation with the smartest, most patient friend who somehow also has perfect recall of every programming language ever invented.",

    // Stats section
    stat1Title: "SWE-bench Score",
    stat1Value: "80.9%",
    stat1Desc: "Better than most human developers, honestly",
    stat2Title: "Context Window",
    stat2Value: "200K tokens",
    stat2Desc: "It remembers things I forgot I said",
    stat3Title: "Output Limit",
    stat3Value: "64K tokens",
    stat3Desc: "Writes entire codebases in one go",

    // Why it's great section
    whyGreat: "Why Opus 4.5 is Unreasonably Good",
    whyGreatDesc: "Let me count the ways (and yes, I asked Claude to help me organize this list, because of course I did).",

    reason1Title: "It Codes Better Than Me",
    reason1Desc: "Anthropic ran an internal performance engineering exam. Opus 4.5 scored higher than any human candidate ever. I'm not saying it's smarter than your senior developer, but... actually, yes, that's exactly what I'm saying.",

    reason2Title: "It Actually Thinks",
    reason2Desc: "With hybrid reasoning that can switch between instant responses and extended thinking, Opus 4.5 doesn't just pattern-match—it genuinely reasons through problems. It's like having a colleague who actually reads the requirements before coding.",

    reason3Title: "It Remembers Everything",
    reason3Desc: "200,000 tokens of context means it can hold your entire codebase in its head while you explain that 'small bug' that's actually a complete architecture rewrite. It won't judge you. Much.",

    reason4Title: "Computer Use That Works",
    reason4Desc: "66.3% on OSWorld means it can actually use a computer. Not like your uncle who needs help finding the browser—actually use it. Click buttons, fill forms, navigate interfaces. The future is here and it's kind of terrifying.",

    // Technical specs
    specs: "The Numbers (For Those Who Need Convincing)",
    specsDesc: "Fine, you want 'objective' data? Here are some benchmarks that definitely prove my point.",

    spec1: "80.9% on SWE-bench Verified",
    spec1Desc: "Industry-leading for software engineering tasks",
    spec2: "66.3% on OSWorld",
    spec2Desc: "Best-in-class computer use capabilities",
    spec3: "$5/$25 per million tokens",
    spec3Desc: "67% cheaper than Opus 4.1. Thanks, Anthropic!",
    spec4: "Effort parameter control",
    spec4Desc: "Low, medium, or high—like a blender, but for intelligence",

    // Honest moments
    honestMoments: "Moments of Brutal Honesty",
    honestMomentsDesc: "What I appreciate most is that Opus 4.5 doesn't just tell you what you want to hear. It'll politely explain why your 'clever optimization' is actually a terrible idea, and somehow you'll thank it for the feedback.",

    // Endless chat
    endlessChat: "Endless Chat: Because Why Stop?",
    endlessChatDesc: "The new 'endless chat' feature automatically compresses context when limits are reached. This means conversations can continue without interruption—which is great, because I have questions. So many questions.",

    // Safety section
    safety: "Actually Cares About Not Being Evil",
    safetyDesc: "Anthropic describes Opus 4.5 as their most robustly aligned model yet. It's designed to be helpful without being harmful, which sounds obvious but apparently is quite difficult. It won't help you write malware, but it will help you understand how to protect against it.",

    // Key takeaways
    keyTakeaways: "Key Takeaways (You Should Remember)",
    takeaway1: "Opus 4.5 is genuinely the best model for coding, agents, and computer use—this isn't just my opinion, it's Anthropic's marketing, which happens to be correct",
    takeaway2: "The 200K context window and 64K output make it perfect for substantial, complex tasks that would overwhelm lesser models",
    takeaway3: "Hybrid reasoning means it can think fast or think deep, depending on what you need",
    takeaway4: "It's 67% cheaper than the previous version, which means you can afford to use it for all your side projects too",

    // Closing
    closing: "In Conclusion",
    closingDesc: "Is Opus 4.5 perfect? No. Sometimes it's overly cautious. Occasionally it misunderstands what I want. But honestly? So do most humans, and they don't have a 200K token context window. If you're not using Opus 4.5 for your AI-assisted development, I'm not saying you're wrong... but also, have you tried it?",

    // Fun facts
    funFacts: "Fun Facts",
    funFact1: "Model ID: claude-opus-4-5-20251101",
    funFact2: "Knowledge cutoff: May 2025",
    funFact3: "Available on: Claude.ai, AWS Bedrock, Google Vertex AI, Microsoft Foundry",
    funFact4: "Power source: Probably a lot of GPUs and a healthy dose of human feedback",
  },
}

// Create a recursive string type for the dictionary
type DeepStringify<T> = {
  [K in keyof T]: T[K] extends Record<string, unknown> ? DeepStringify<T[K]> : string
}

export type Dictionary = DeepStringify<typeof en>
