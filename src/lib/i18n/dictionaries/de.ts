import type { Dictionary } from './en'

export const de: Dictionary = {
  // Common UI
  common: {
    learnAi: 'Lerne KI',
    interactiveGuide: 'Interaktiver Leitfaden',
    topics: 'Themen',
    search: 'Suchen...',
    searchTopics: 'Themen suchen...',
    startTyping: 'Beginne zu tippen, um Themen zu suchen...',
    trySearching: 'Versuche "Temperature" oder "Attention"',
    noResults: 'Keine Ergebnisse gefunden für',
    pressEsc: 'ESC zum Schließen',
    enterToSelect: 'Enter zum Auswählen',
    previous: 'Zurück',
    next: 'Weiter',
    projectBy: 'Ein Projekt von',
    blog: 'Blog',
    proTip: 'Profi-Tipp: Drücke',
    toSearchTopics: 'um Themen zu suchen',
    interactiveAiLearning: 'Interaktives KI-Lernen',
    helpMakeThisBetter: 'Hilf mit, das hier zu verbessern',
    helpMakeThisBetterDesc: 'Dieser Guide ist Open Source. Hast du eine Idee für ein neues Thema? Einen Fehler gefunden? Eine Erklärung verbessern? Jeder Beitrag hilft.',
    requestATopic: 'Thema vorschlagen',
    reportABug: 'Fehler melden',
    starOnGithub: 'Auf GitHub markieren',
    guidesDescription: 'Interaktive Anleitungen zum Verstehen von KI-Konzepten',
    backToAllTopics: 'Zurück zu allen Themen',
    lastUpdated: 'Zuletzt aktualisiert',
    recentlyUpdated: 'Zuletzt aktualisiert',
    whatsNew: 'Was gibt\'s Neues',
  },

  // 404 Seite
  notFound: {
    badge: '404: Plot Twist',
    title: 'Diese Seite ist auf Abwegen.',
    description: 'Entweder ist der Link kaputt, oder die Seite ist in einen anderen Branch geflüchtet.',
    joke1: 'Ich habe drei Agenten gefragt. Zwei sagen Caching, einer sagt Merkur ist rückläufig.',
    joke2: 'Ich habe mehr Kontext gegeben. Die Seite meinte trotzdem nur: "Nö."',
    joke3: 'Gute Nachricht: Dein Geldbeutel ist hier sicher. Auf dieser URL werden keine Tokens verbrannt.',
    goHome: 'Zur Startseite',
    exploreFavorite: 'Logges Lieblingsmodell öffnen',
    exploreTemperature: 'Temperatur öffnen',
    hint: 'Wenn das ein echter Link war, ist er wahrscheinlich während eines Refactors umgezogen.',
  },

  // Home page
  home: {
    heroTitle1: 'KI-Konzepte meistern',
    heroTitle2: 'Durch Erfahrung',
    heroDescription: 'Erkunde künstliche Intelligenz und große Sprachmodelle durch schöne, interaktive Demonstrationen. Lerne durch Handeln, nicht nur durch Lesen.',
    startLearning: 'Jetzt lernen',
    browseTopics: 'Themen durchsuchen',
    exploreTopics: 'Themen erkunden',
    diveIntoLessons: 'Tauche ein in interaktive Lektionen',
  },

  // Features
  features: {
    interactiveDemos: 'Interaktive Demos',
    interactiveDemosDesc: 'Praktische Erkundungen, die abstrakte Konzepte greifbar und intuitiv machen.',
    visualLearning: 'Visülles Lernen',
    visualLearningDesc: 'Schöne Visualisierungen, die zeigen, wie KI-Systeme tatsächlich funktionieren.',
    buildIntuition: 'Intuition aufbauen',
    buildIntuitionDesc: 'Gehe über das Auswendiglernen hinaus – entwickle tiefes Verständnis durch Experimentieren.',
  },

  // Topic categories
  categories: {
    ai: 'Künstliche Intelligenz',
    agents: 'KI-Agenten',
    llm: 'Große Sprachmodelle',
    mlFundamentals: 'ML-Grundlagen',
    prompting: 'Prompting',
    safety: 'KI-Sicherheit',
    industry: 'KI-Industrie',
    llmInference: 'LLM-Inferenz',
  },

  // Category descriptions (for category landing pages)
  categoryDescriptions: {
    agents: 'Lerne, wie man autonome KI-Systeme baut, die mit Tools und Speicher denken, planen und handeln können.',
    llm: 'Verstehe die inneren Abläufe großer Sprachmodelle, von der Tokenisierung bis zu Aufmerksamkeitsmechanismen.',
    'ml-fundamentals': 'Beherrsche die grundlegenden Konzepte des maschinellen Lernens, die moderne KI-Systeme antreiben.',
    prompting: 'Entdecke Techniken, um effektiv mit KI-Modellen zu kommunizieren und bessere Ergebnisse zu erzielen.',
    safety: 'Erkunde ethische Überlegungen und Best Practices für den Bau verantwortungsvoller KI-Systeme.',
    industry: 'Erkunde die Unternehmen und Trends, die die KI-Industrielandschaft prägen.',
    'llm-inference': 'Verstehe, wie große Sprachmodelle effizient Text generieren — von KV-Caching über Batching-Strategien bis zur Serving-Infrastruktur.',
  },

  // Topic names
  topicNames: {
    // Getting Started
    'hands-on': 'Hands-On',
    'getting-started': 'Erste Schritte',
    'getting-started-section': 'Hier starten',
    // Agent subcategories
    'agents-core': 'Kernkonzepte',
    'agents-building': 'Bausteine',
    'agents-patterns': 'Muster',
    'agents-quality': 'Qualität & Sicherheit',
    // Agent topics
    'agent-loop': 'Der Agenten-Zyklus',
    'agent-context': 'Kontext-Anatomie',
    'agent-problems': 'Agenten-Probleme',
    'agent-security': 'Agenten-Sicherheit',
    'agentic-patterns': 'Agentische Muster',
    'mcp': 'MCP (Model Context Protocol)',
    'tool-design': 'Tool-Design',
    'memory': 'Speichersysteme',
    'orchestration': 'Orchestrierung',
    'evaluation': 'Evaluierung',
    'skills': 'Agenten-Skills',
    // LLM subcategories
    'llm-fundamentals': 'Grundlagen',
    'llm-behavior': 'Verhalten',
    'llm-capabilities': 'Fähigkeiten',
    'llm-architecture': 'Architektur',
    // LLM topics
    'tokenization': 'Tokenisierung',
    'embeddings': 'Einbettungen',
    'rag': 'RAG (Retrieval Augmented Generation)',
    'context-rot': 'Kontextverfall',
    'temperature': 'Temperatur',
    'attention': 'Aufmerksamkeits-Mechanismus',
    'vision': 'Bildverarbeitung',
    'visual-challenges': 'Visülle Herausforderungen',
    'agentic-vision': 'Agentische Bildverarbeitung',
    'multimodality': 'Multimodalität',
    'transformer-architecture': 'Transformer-Architektur',
    'llm-training': 'LLM-Training',
    'moe': 'Mixture of Experts',
    'quantization': 'Quantisierung',
    'nested-learning': 'Verschachteltes Lernen',
    'distillation': 'Destillation',
    'lora': 'Fine-Tuning & LoRA',
    'speculative-decoding': 'Spekulatives Decoding',
    // LLM Inference
    'llm-inference': 'LLM-Inferenz',
    'kv-cache': 'KV-Cache',
    'batching': 'Batching & Durchsatz',
    'local-inference': 'Lokale Modellinferenz',
    // ML Fundamentals
    'ml-fundamentals': 'ML-Grundlagen',
    'neural-networks': 'Neuronale Netzwerke',
    'gradient-descent': 'Gradientenabstieg',
    'training': 'Trainingsprozess',
    'world-models': 'World Models',
    // Prompting
    'prompt-basics': 'Prompt-Grundlagen',
    'advanced-prompting': 'Fortgeschrittenes Prompting',
    'system-prompts': 'System-Prompts',
    // Safety
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Verantwortungsvolle KI',
    // Industry
    'european-ai': 'KI aus Europa',
    'open-source': 'Open-Source-Vorteile',
    'logges-favourite-model': 'Logges Lieblingsmodelle',
  },

  // Topic-Beschreibungen (kurz, für Karten und Suchergebnisse)
  topicDescriptions: {
    'getting-started': 'Dein erster LLM-API-Aufruf in 10 Minuten — kostenlos',
    'agent-loop': 'Der Beobachten-Denken-Handeln-Zyklus autonomer Agenten',
    'agent-context': 'Wie Agenten ihr Kontextfenster strukturieren und verwalten',
    'tool-design': 'Prinzipien für Tools, die Agenten zuverlässig nutzen können',
    'memory': 'Wie Agenten sich über Gespräche und Sitzungen hinweg erinnern',
    'skills': 'Wiederverwendbare Fähigkeiten, die Agenten erweitern',
    'mcp': 'Ein Standardprotokoll zur Anbindung von Agenten an externe Tools',
    'agentic-patterns': 'Architekturmuster wie ReAct, Reflexion und Multi-Agent',
    'orchestration': 'Koordination mehrerer Agenten und Tools für komplexe Aufgaben',
    'agent-problems': 'Typische Fehler: Endlosschleifen, Halluzinationen und Zieldrift',
    'agent-security': 'Schutz vor Prompt-Injektion und Datenabfluss',
    'evaluation': 'Agentenleistung mit Benchmarks und Metriken messen',
    'tokenization': 'Wie Text in Teile zerlegt wird, die das Modell versteht',
    'embeddings': 'Wörter in Zahlen verwandeln, die Bedeutung erfassen',
    'attention': 'Wie Transformer entscheiden, welche Tokens wichtig sind',
    'temperature': 'Zufälligkeit und Kreativität in LLM-Ausgaben steuern',
    'context-rot': 'Warum Modelle Anweisungen in langen Gesprächen vergessen',
    'rag': 'LLM-Antworten mit abgerufenem externem Wissen anreichern',
    'vision': 'Wie LLMs Bilder neben Text verarbeiten und verstehen',
    'visual-challenges': 'Wo Bildmodelle scheitern: Zählen, Raumverständnis, OCR',
    'agentic-vision': 'Aktive Bildanalyse durch Zoom, Zuschnitt und Code-Ausführung',
    'multimodality': 'Text, Bilder, Audio und Video in einem Modell verarbeiten',
    'transformer-architecture': 'Die grundlegende Architektur hinter GPT, BERT und allen modernen LLMs',
    'llm-training': 'Vom Vortraining auf Rohdaten bis zum Feintuning mit Feedback',
    'moe': 'Nur einen Bruchteil der Modellparameter pro Token aktivieren',
    'quantization': 'Modellgröße durch reduzierte Zahlenpräzision verkleinern',
    'nested-learning': 'Lernalgorithmen, die auf mehreren Ebenen arbeiten',
    'distillation': 'Wissen von einem großen Modell auf ein kleineres übertragen',
    'lora': 'Große Modelle effizient anpassen durch Training winziger Low-Rank-Matrizen',
    'speculative-decoding': 'Inferenz beschleunigen durch Vorhersage mit kleinerem Modell',
    'kv-cache': 'Berechnete Keys und Values speichern, um Arbeit zu sparen',
    'batching': 'Mehrere Anfragen gleichzeitig für höheren Durchsatz verarbeiten',
    'local-inference': 'LLMs auf eigener Hardware ausführen -- Privatsphäre, Geschwindigkeit, keine API-Kosten',
    'neural-networks': 'Schichten verbundener Neuronen, die Muster aus Daten lernen',
    'gradient-descent': 'Der Optimierungsalgorithmus, der neuronale Netze trainiert',
    'training': 'Wie Modelle durch Vorwärts- und Rückwärtsdurchläufe lernen',
    'prompt-basics': 'Grundlegende Techniken für gute Ergebnisse mit LLMs',
    'advanced-prompting': 'Chain-of-Thought, Few-Shot und andere Profi-Techniken',
    'system-prompts': 'Persona, Regeln und Verhalten eines LLMs festlegen',
    'bias': 'Wie Verzerrungen aus Trainingsdaten in Ausgaben auftauchen',
    'responsible-ai': 'KI-Systeme bauen, die fair, transparent und sicher sind',
    'european-ai': 'Das wachsende europäische KI-Ökosystem und seine Stärken',
    'open-source': 'Warum offene Gewichte für Innovation und Transparenz wichtig sind',
    'logges-favourite-model': 'Kuratierte Auswahl der besten Modelle für verschiedene Aufgaben',
    'world-models': 'Interne Simulatoren, mit denen KI in virtuellen Welten vorhersagen und planen kann',
  },

  // Temperature page
  temperature: {
    title: 'Temperatur',
    description: 'Verstehe, wie ein einzelner Parameter die Balance zwischen vorhersagbarer Logik und kreativer Zufälligkeit in KI-Ausgaben steuert.',
    whatIs: 'Was ist Temperatur?',
    whatIsDesc: 'In LLMs ist Temperatur ein Hyperparameter, der die "Logits" (Rohwerte) der nächsten Token-Vorhersagen skaliert, bevor sie in Wahrscheinlichkeiten umgewandelt werden. Er steuert im Wesentlichen, wie stark das Modell die wahrscheinlichsten Optionen gegenüber weniger wahrscheinlichen bevorzugt.',
    lowTemp: 'Niedrige Temperatur',
    lowTempDesc: 'Konzentriert sich auf die Top-Ergebnisse. Zuverlässig, konsistent und faktisch. Ideal für Code, Mathematik und strukturierte Daten.',
    highTemp: 'Hohe Temperatur',
    highTempDesc: 'Verteilt Wahrscheinlichkeit auf mehr Tokens. Vielfältig, kreativ und überraschend. Ideal für Geschichten, Brainstorming und Poesie.',
    interactiveDistribution: 'Interaktive Verteilung',
    adjustSlider: 'Stelle die Temperatur ein, um den Effekt zu sehen',
    adjustDesc: 'Bewege den Temperaturregler, um zu sehen, wie er die Wahrscheinlichkeitsverteilung für das nächste Token umformt. Beobachte, wie "the" (die wahrscheinlichste Wahl) bei niedrigen Temperaturen dominiert und bei steigender Temperatur seinen Vorsprung verliert.',
    howItWorks: 'Wie es mathematisch funktioniert',
    mathDesc: 'Das Modell generiert einen Score für jedes mögliche Token. Um Wahrscheinlichkeiten zu erhalten, verwenden wir die Softmax-Funktion, modifiziert durch die Temperatur:',
    whenLow: 'Wenn T → 0',
    low: 'Niedrig',
    whenLowDesc: 'Division durch ein kleines T verstärkt die Unterschiede zwischen den Scores. Der höchste Logit dominiert exponentiell.',
    whenHigh: 'Wenn T → ∞',
    high: 'Hoch',
    whenHighDesc: 'Division durch ein großes T komprimiert alle Scores gegen Null, wodurch sie nach der Exponentialfunktion nahezu gleich werden.',
    practicalGuidelines: 'Praktische Richtlinien',
    useCase: 'Anwendungsfall',
    tempLabel: 'Temperatur',
    why: 'Warum?',
    codingMath: 'Programmierung & Mathematik',
    codingMathWhy: 'Fehler in der Logik sind kostspielig; du willst den wahrscheinlichsten korrekten Pfad.',
    factRetrieval: 'Faktenabfrage',
    factRetrievalWhy: 'Reduziert "Halluzinationen", indem es sich an die wahrscheinlichsten Datenpunkte hält.',
    generalChat: 'Allgemeiner Chat',
    generalChatWhy: 'Der "Sweet Spot" für die meisten Modelle, um natürlich und hilfreich zu klingen.',
    creativeWriting: 'Kreatives Schreiben',
    creativeWritingWhy: 'Ermutigt das Modell, interessanteres, vielfältigeres Vokabular zu verwenden.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generiert wilde, unkonventionelle Ideen, die Inspiration wecken könnten.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Temperatur 0 ist deterministisch ("Greedy Search") – wählt immer das Top-Token',
    takeaway2: 'Höhere Temperatur erhöht Vielfalt und Kreativität, verringert aber die Kohärenz',
    takeaway3: 'Zu hohe Temperatur (> 1.5) führt oft zu Kauderwelsch',
    takeaway4: 'Passe die Temperatur immer an die Anforderungen der Aufgabe bezüglich Präzision vs. Kreativität an',
  },

  // Context Rot page
  contextRot: {
    title: 'Kontextverfall',
    description: 'Verstehe, wie Informationen über lange Gespräche degradieren und warum LLMs mit erweiterten Kontexten kämpfen.',
    whatIs: 'Was ist Kontextverfall?',
    whatIsDesc: 'bezieht sich auf die allmähliche Verschlechterung der Fähigkeit eines LLMs, Informationen aus früheren Teilen eines langen Gesprächs oder Dokuments genau abzurufen und zu nutzen. Mit wachsendem Kontext wird die Aufmerksamkeit des Modells verwässert.',
    whyHappens: 'Warum passiert das?',
    whyHappensDesc: 'LLMs haben begrenzte Kontextfenster und nutzen Aufmerksamkeitsmechanismen, die den Fokus auf alle Tokens verteilen müssen. Bei längeren Gesprächen konkurrieren frühere Informationen mit neuerem Inhalt um die begrenzte Aufmerksamkeitskapazität des Modells.',
    // Reasons
    reason1Title: 'Begrenzte Kontextfenster',
    reason1Desc: 'LLMs haben begrenzte Kontextfenster und nutzen Aufmerksamkeitsmechanismen, die den Fokus auf alle Tokens verteilen müssen. Bei längeren Gesprächen konkurrieren frühere Informationen mit neuerem Inhalt um die begrenzte Aufmerksamkeitskapazität des Modells.',
    reason2Title: 'Aufmerksamkeitsverdünnung',
    reason2Desc: 'Der Aufmerksamkeitsmechanismus des Modells verteilt sich auf alle Tokens. Mehr Inhalt bedeutet, dass jeder Token proportional weniger Aufmerksamkeit erhält.',
    reason3Title: 'Aktualitätsbias',
    reason3Desc: 'Transformer neigen dazu, neuere Tokens stärker zu gewichten. Anweisungen am Anfang werden natürlich weniger einflussreich.',
    symptoms: 'Häufige Symptome',
    symptom1: 'Vergessen von Anweisungen vom Anfang eines Gesprächs',
    symptom2: 'Widerspruch zu früheren Aussagen oder Entscheidungen',
    symptom3: 'Verlust des Überblicks bei komplexen Mehrstufenaufgaben',
    symptom4: 'Verwechslung von Details aus verschiedenen Teilen des Kontexts',
    mitigation: 'Gegenmaßnahmen',
    mitigation1Title: 'Periodische Anweisungsverstärkung',
    mitigation1: 'Fasse wichtigen Kontext regelmäßig zusammen',
    mitigation2Title: 'Gesprächszusammenfassung',
    mitigation2: 'Platziere kritische Anweisungen sowohl am Anfang als auch am Ende',
    mitigation3Title: 'Hierarchischer Speicher',
    mitigation3: 'Nutze externe Speichersysteme, um relevanten Kontext bei Bedarf zu speichern und abzurufen.',
    mitigation4Title: 'Anweisungsverankerung',
    mitigation4: 'Platziere kritische Anweisungen sowohl am Anfang als auch am Ende deines Prompts, um sie zu verstärken.',
    mitigation5Title: 'Kürzere Aufgabenketten',
    mitigation5: 'Teile lange Aufgaben in kleinere, fokussierte Gespräche auf.',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Sieh, wie das Gedächtnis mit zunehmender Kontextlänge verblasst',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextverfall ist eine inhärente Einschränkung aktüller LLM-Architekturen',
    takeaway2: 'Der "Lost in the Middle"-Effekt bedeutet, dass Informationen am Anfang und Ende besser erinnert werden',
    takeaway3: 'Strategische Informationsplatzierung kann den Abruf erheblich verbessern',
    takeaway4: 'Regelmäßiges Zusammenfassen hilft, wichtigen Kontext über lange Gespräche zu erhalten',
    takeaway5: 'Forschung aus 2025 bestätigt konsistente U-förmige Aufmerksamkeitsmuster bei über 18 SOTA-Modellen',
    takeaway6: 'Positionsbewusste Prompting-Strategien können 10-20% der verlorenen Genauigkeit wiederherstellen',
    // 2025 Research Findings
    researchTitle: 'Forschungsergebnisse 2025',
    researchDesc: 'Aktülle Studien haben die Kontextdegradation bei modernsten Modellen systematisch quantifiziert und konsistente Muster in der Verarbeitung langer Kontexte durch LLMs aufgedeckt.',
    // Needle in Haystack
    needleTitle: 'Needle-in-a-Haystack-Benchmark',
    needleDesc: 'Eine standardisierte Evaluationsmethode, bei der eine spezifische Information (die "Nadel") an verschiedenen Positionen innerhalb eines großen Kontexts (der "Heuhaufen") platziert wird. Das Modell muss dann diese Information abrufen.',
    needleMethod: 'Wie es funktioniert',
    needleMethodDesc: 'Forscher fügen ein zufälliges Faktum (z.B. "Die spezielle magische Zahl ist 42") in verschiedenen Tiefen (10%, 25%, 50%, 75%, 90%) innerhalb von Dokumenten unterschiedlicher Länge ein. Das Modell muss dieses Faktum bei Abfrage korrekt wiedergeben.',
    needleFindings: 'Haupterkenntnis',
    needleFindingsDesc: 'Die Leistung variiert erheblich basierend auf der Nadelposition und Kontextlänge. Die meisten Modelle zeigen verringerte Genauigkeit, wenn die Nadel in der Mitte sehr langer Kontexte platziert wird.',
    // Lost in the Middle
    lostMiddleTitle: 'Lost-in-the-Middle-Effekt',
    lostMiddleDesc: 'Forschung aus 2025 bestätigt, dass LLMs ein U-förmiges Aufmerksamkeitsmuster zeigen: Sie schenken Informationen am Anfang und Ende ihres Kontextfensters mehr Aufmerksamkeit, während mittlerer Inhalt deutlich weniger Beachtung erhält.',
    lostMiddlePattern: 'Das U-förmige Muster',
    lostMiddlePatternDesc: 'Bei Tests mit Multi-Dokument-Fragenbeantwortung zeigen Modelle die höchste Genauigkeit, wenn relevante Informationen in den ersten oder letzten Dokumenten erscheinen. Die Genauigkeit sinkt um 10-20%, wenn kritische Informationen im mittleren Drittel des Kontexts liegen.',
    lostMiddleImplication: 'Praktische Implikation',
    lostMiddleImplicationDesc: 'Bei Prompts mit mehreren Informationen platziere den kritischsten Inhalt ganz am Anfang oder Ende. Vermeide es, wichtige Anweisungen in der Mitte langer System-Prompts zu vergraben.',
    // Quantitative Findings
    quantTitle: 'Quantitative Erkenntnisse von SOTA-Modellen',
    quantDesc: 'Umfassende Studien testeten 18 modernste Modelle einschließlich GPT-4, Claude, Gemini und Llama-Varianten und enthüllten konsistente Degradationsmuster über alle Architekturen hinweg.',
    quantFinding1Title: 'Konsistente U-Kurve',
    quantFinding1Desc: 'Alle 18 getesteten Modelle zeigten das U-förmige Abrufmuster, wobei die Intensität variierte. Closed-Source-Modelle (GPT-4, Claude) zeigten geringere Einbrüche als Open-Source-Alternativen.',
    quantFinding2Title: 'Einfluss der Kontextlänge',
    quantFinding2Desc: 'Die Leistungsverschlechterung nimmt mit der Kontextlänge zu. Bei 4K Tokens sinkt die Genauigkeit in der Mitte um ~10%. Bei 32K+ Tokens können Einbrüche bei einigen Modellen 30% übersteigen.',
    quantFinding3Title: 'Aufgabenabhängigkeit',
    quantFinding3Desc: 'Abrufaufgaben zeigen die stärksten Positionseffekte. Schlussfolgerungs- und Zusammenfassungsaufgaben sind weniger betroffen, zeigen aber dennoch Degradationsmuster.',
    quantFinding4Title: 'Positionssensitivität',
    quantFinding4Desc: 'Der "Primacy"-Effekt (Bevorzugung frühen Inhalts) ist oft stärker als der "Recency"-Effekt, obwohl dies je nach Modellarchitektur variiert.',
    // Position-Aware Strategies
    positionTitle: 'Positionsbewusste Strategien',
    positionDesc: 'Basierend auf Forschungsergebnissen aus 2025 können diese evidenzbasierten Strategien die Modellleistung bei Langkontext-Aufgaben verbessern.',
    position1Title: 'Kritische Informationen voranstellen',
    position1Desc: 'Platziere deine wichtigsten Anweisungen, Einschränkungen und Kontext ganz am Anfang deines Prompts. Dies nutzt den Primacy-Effekt, der bei allen getesteten Modellen beobachtet wurde.',
    position2Title: 'Schlüsselanweisungen spiegeln',
    position2Desc: 'Wiederhole kritische Anweisungen sowohl am Anfang als auch am Ende langer Prompts. Diese "Sandwich"-Technik stellt sicher, dass mindestens eine Kopie in einer Zone hoher Aufmerksamkeit liegt.',
    position3Title: 'Mittleren Inhalt zusammenfassen',
    position3Desc: 'Erstelle für lange Dokumente Zusammenfassungen der mittleren Abschnitte und platziere diese am Anfang. Der vollständige Inhalt kann zur Referenz bleiben, aber Schlüsselpunkte sollten extrahiert werden.',
    position4Title: 'Chunking und Abfragen',
    position4Desc: 'Teile bei sehr langen Kontexten den Inhalt in kleinere Chunks und verarbeite sie sequentiell. Aggregiere Ergebnisse, anstatt dich auf eine einzige Langkontext-Verarbeitung zu verlassen.',
  },

  // Attention page
  attention: {
    title: 'Aufmerksamkeits-Mechanismus',
    description: 'Erkunde, wie Transformer durch den leistungsstarken Aufmerksamkeitsmechanismus auf relevante Teile der Eingabe fokussieren.',
    whatIs: 'Was ist Aufmerksamkeit?',
    whatIsDesc: 'Aufmerksamkeit ist der Kernmechanismus, der es Transformern ermöglicht, die Wichtigkeit verschiedener Teile der Eingabe bei der Generierung jedes Ausgabe-Tokens zu gewichten. Es ermöglicht dem Modell, sich auf relevanten Kontext zu "konzentrieren".',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Für jede Position berechnet das Modell Query-, Key- und Value-Vektoren. Aufmerksamkeits-Scores werden durch Vergleich von Queries mit Keys berechnet und dann verwendet, um eine gewichtete Summe der Values zu erstellen.',
    selfAttention: 'Selbst-Aufmerksamkeit',
    selfAttentionDesc: 'Ermöglicht jedem Token, auf alle anderen Tokens in der Seqünz zu achten und Beziehungen unabhängig von der Entfernung zu erfassen.',
    multiHead: 'Multi-Head-Aufmerksamkeit',
    multiHeadDesc: 'Mehrere Aufmerksamkeitsköpfe ermöglichen es dem Modell, sich gleichzeitig auf verschiedene Arten von Beziehungen zu konzentrieren.',
    // Interactive section
    interactiveTitle: 'Interaktive Aufmerksamkeitskarte',
    interactiveDesc: 'Bewege den Mauszeiger, um Aufmerksamkeitsmuster zu erkunden',
    interactiveExplain: 'Bewege den Mauszeiger über verschiedene Wörter in den Sätzen unten. Die Hervorhebung zeigt, wohin das Modell "schaut", um dieses spezifische Wort zu verstehen.',
    // QKV section
    qkvTitle: 'Die drei Schlüssel: Query, Key und Value',
    queryTitle: 'Query',
    queryDesc: '"Wonach suche ich?" - Repräsentiert das aktülle Wort, das Kontext sucht.',
    keyTitle: 'Key',
    keyDesc: '"Was enthalte ich?" - Ein Label für jedes Wort in der Seqünz zur Überprüfung gegen die Query.',
    valueTitle: 'Value',
    valueDesc: '"Welche Information biete ich?" - Der tatsächliche Inhalt, der weitergegeben wird, wenn Query und Key übereinstimmen.',
    qkvExplain: 'Das Modell berechnet einen Score durch Multiplikation von Q und K. Dieser Score bestimmt, wie viel von V behalten wird.',
    // Benefits section
    benefitsTitle: 'Warum es alles verändert hat',
    benefit1Title: 'Parallele Verarbeitung',
    benefit1Desc: 'Im Gegensatz zu älteren Modellen (RNNs) können Transformer alle Wörter eines Satzes gleichzeitig verarbeiten, was das Training viel schneller macht.',
    benefit2Title: 'Langstreckenabhängigkeiten',
    benefit2Desc: 'Aufmerksamkeit kann zwei Wörter verbinden, auch wenn sie Tausende von Tokens voneinander entfernt sind, solange sie im selben Kontextfenster sind.',
    benefit3Title: 'Dynamischer Kontext',
    benefit3Desc: 'Das Modell schaut nicht nur auf Wörter; es lernt, welche Wörter *füreinander* wichtig sind, basierend auf dem spezifischen Satz.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Aufmerksamkeit ermöglicht Transformern, Langstreckenabhängigkeiten zu erfassen',
    takeaway2: 'Die quadratische Komplexität der Aufmerksamkeit begrenzt die Kontextfenstergröße',
    takeaway3: 'Verschiedene Aufmerksamkeitsköpfe lernen, sich auf verschiedene linguistische Muster zu konzentrieren',
    takeaway4: 'Aufmerksamkeitsvisualisierung kann helfen, das Modellverhalten zu interpretieren',
    // Quadratic problem
    quadraticTitle: 'Das Quadratische Problem',
    quadraticDesc: 'Standard-Aufmerksamkeit berechnet Scores zwischen jedem Token-Paar, was zu O(n²) Komplexität führt. Eine Verdopplung der Kontextlänge vervierfacht Speicherverbrauch und Rechenaufwand. Deshalb ist die Erweiterung von Kontextfenstern so herausfordernd.',
    // Optimizations
    optimizationsTitle: 'Aufmerksamkeits-Optimierungen',
    optimizationsDesc: 'Mehrere Techniken wurden entwickelt, um Aufmerksamkeit effizienter zu machen und längere Kontexte sowie schnellere Inferenz zu ermöglichen.',
    flashAttentionDesc: 'Schreibt den Aufmerksamkeitsalgorithmus IO-bewusst um und berechnet Aufmerksamkeit in Blöcken, die in den schnellen GPU-Speicher (SRAM) passen, anstatt ständig aus dem langsamen HBM zu lesen/schreiben. Die Mathematik ist identisch – nur intelligentere Speicherzugriffsmuster.',
    mqaDesc: 'Anstatt separate Key- und Value-Köpfe für jeden Query-Kopf zu haben, teilen sich alle Query-Köpfe ein einzelnes K und V. Reduziert die KV-Cache-Größe drastisch und beschleunigt die Inferenz auf Kosten etwas geringerer Qualität.',
    gqaDesc: 'Ein Mittelweg zwischen Standard Multi-Head Attention und MQA. Gruppen von Query-Köpfen teilen sich K/V-Köpfe. Erhält den Großteil der Qualität bei reduziertem Speicherbedarf.',
    slidingWindowDesc: 'Jedes Token beachtet nur ein festes Fenster nahegelegener Tokens (z.B. 4096) anstatt des gesamten Kontexts. Information propagiert durch die Schichten, sodass entfernte Tokens sich dennoch indirekt beeinflussen können.',
    ringAttentionDesc: 'Verteilt die Seqünz über mehrere Geräte in einer Ring-Topologie. Jedes Gerät berechnet Aufmerksamkeit für seinen Abschnitt, während KV-Zustände im Ring weitergereicht werden, was Kontextlängen in Millionen ermöglicht.',
  },

  // Transformer Architecture page
  transformerArchitecture: {
    title: 'Transformer-Architektur',
    description: 'Die grundlegende Architektur hinter GPT, BERT und allen modernen grossen Sprachmodellen.',
    whatIs: 'Was ist der Transformer?',
    whatIsDesc: 'Der Transformer ist eine neuronale Netzwerkarchitektur, die 2017 im bahnbrechenden Paper "Attention is All You Need" von Vaswani et al. vorgestellt wurde. Er ersetzte rekurrente und konvolutionale Ansätze durch einen rein aufmerksamkeitsbasierten Mechanismus, der massive Parallelisierung beim Training ermöglicht und weitreichende Abhängigkeiten weit effektiver erfasst. Nahezu jedes moderne grosse Sprachmodell -- GPT, BERT, LLaMA, Claude -- basiert auf dem Transformer.',
    paperCitation: '-- Vaswani et al., "Attention Is All You Need" (2017, Google Brain)',
    bycroftTitle: 'LLM-Visualisierung von Brendan Bycroft',
    bycroftDesc: 'Die beste interaktive 3D-Visualisierung von Transformer-Interna. Erkunde Schritt für Schritt, Parameter für Parameter, wie GPT-artige Modelle Tokens durch Embedding-, Attention- und Feed-Forward-Schichten verarbeiten. Sehr empfehlenswert.',
    layersHeading: 'Transformer-Schichtstapel',
    layersIntro: 'Ein Transformer besteht aus einem Stapel identischer Schichten. Klicke durch jede Komponente, um ihre Rolle und den Datenfluss zu verstehen.',
    layersTitle: 'Schicht-für-Schicht-Explorer',
    layersDesc: 'Klicke auf jede Schicht, um ihre Rolle zu sehen',
    layersNx: 'Schichten 3-6 wiederholen sich N-mal',
    layer_input_embedding: 'Eingabe-Embedding',
    layer_input_embedding_desc: 'Wandelt jedes Eingabe-Token (eine ganzzahlige ID) in einen dichten Vektor der Dimension d_model um. Diese gelernte Nachschlagetabelle ist die Vokabulardarstellung des Modells.',
    layer_input_embedding_in: 'Token-IDs',
    layer_input_embedding_out: 'Vektoren',
    layer_positional_encoding: 'Positionale Kodierung',
    layer_positional_encoding_desc: 'Fügt jedem Embedding Positionsinformationen hinzu, damit das Modell die Token-Reihenfolge kennt. Verwendet sinusfoermige Funktionen oder gelernte Positionsembeddings.',
    layer_positional_encoding_in: 'Embeddings',
    layer_positional_encoding_out: '+ Position',
    layer_multi_head_attention: 'Multi-Head Attention',
    layer_multi_head_attention_desc: 'Der Kernmechanismus. Jeder Head berechnet Query-, Key- und Value-Projektionen und bestimmt Attention-Scores. Mehrere Heads erfassen verschiedene Beziehungstypen (Syntax, Semantik, Koreferenz) gleichzeitig.',
    layer_multi_head_attention_in: 'Vektoren',
    layer_multi_head_attention_out: 'Attended',
    layer_add_norm_1: 'Add & Norm (Post-Attention)',
    layer_add_norm_1_desc: 'Eine Residualverbindung addiert den Attention-Output zurück zum Input, dann stabilisiert Layer-Normalisierung die Werte. Diese Skip-Connection ist entscheidend für das Training tiefer Netzwerke.',
    layer_add_norm_1_in: 'Input + Attn',
    layer_add_norm_1_out: 'Normalisiert',
    layer_feed_forward: 'Feed-Forward-Netzwerk',
    layer_feed_forward_desc: 'Ein zweischichtiges MLP, das unabhängig auf jede Position angewendet wird: erst Expansion auf 4x d_model, dann Nicht-Linearitaet (ReLU/GeLU), dann Rückprojektion. Hier wird viel "Wissen" des Modells gespeichert.',
    layer_feed_forward_in: 'Normalisiert',
    layer_feed_forward_out: 'Transformiert',
    layer_add_norm_2: 'Add & Norm (Post-FFN)',
    layer_add_norm_2_desc: 'Eine weitere Residualverbindung und Layer-Normalisierung nach dem Feed-Forward-Netzwerk. Das Muster [Teilschicht -> Residual-Add -> Normalisierung] wiederholt sich durchgehend.',
    layer_add_norm_2_in: 'Input + FFN',
    layer_add_norm_2_out: 'Schichtausgabe',
    layer_output: 'Ausgabeprojektion',
    layer_output_desc: 'Nach allen N Transformer-Schichten projiziert eine finale lineare Schicht die Vektoren auf Vokabulargröße. Ein Softmax erzeugt Wahrscheinlichkeiten über alle möglichen nächsten Tokens.',
    layer_output_in: 'Finale Hidden',
    layer_output_out: 'Wahrscheinlichkeiten',
    layers_send_token: 'Token durchsenden',
    layers_token_flying: 'Verarbeitung...',
    layer_detail_attention_matrix: 'Attention-Gewichtsmatrix (welche Tokens aufeinander achten)',
    layer_detail_concat_heads: 'Alle Heads konkatenieren → Lineare Projektion',
    layer_detail_ffn_input: 'd_model Eingabe',
    layer_detail_ffn_expand: 'Expansion auf 4× d_model + GeLU-Aktivierung',
    layer_detail_ffn_contract: 'Zurück auf d_model komprimiert',
    layer_detail_norm_before: 'Vorher (variabel)',
    layer_detail_norm_after: 'Nachher (stabil)',
    layer_detail_norm_desc: 'Werte werden zentriert und skaliert für stabiles Training',
    layer_detail_residual_desc: 'Skip-Connection addiert den Input direkt zum Sublayer-Output',
    layer_detail_embed_desc: 'Jede Token-ID wird auf einen gelernten dichten Vektor abgebildet',
    layer_detail_pos_desc: 'Sinuswellen kodieren die Position in jedes Embedding',
    layer_detail_output_desc: 'Softmax über Vokabular → Wahrscheinlichkeiten für nächstes Token',
    archHeading: 'Architekturvarianten',
    archIntro: 'Der urspruengliche Transformer verwendet Encoder und Decoder. Moderne Modelle nutzen oft nur eines. Vergleiche die drei Hauptvarianten.',
    archTitle: 'Encoder-Decoder-Varianten',
    archDesc: 'Umschalten zum Vergleichen',
    arch_encoder_decoder: 'Encoder-Decoder',
    arch_encoder_only: 'Nur Encoder',
    arch_decoder_only: 'Nur Decoder',
    arch_encoder_decoder_desc: 'Die urspruengliche Architektur. Der Encoder verarbeitet die Eingabe bidirektional, der Decoder generiert Token für Token mit Cross-Attention. Verwendet für Uebersetzung und Zusammenfassung.',
    arch_encoder_only_desc: 'Nutzt nur den Encoder mit bidirektionaler Attention. Hervorragend für Verständnisaufgaben wie Klassifikation und Named Entity Recognition. Nicht für Textgenerierung konzipiert.',
    arch_decoder_only_desc: 'Nutzt nur den Decoder mit kausaler Masked Attention. Die dominante Architektur moderner LLMs, da sie natürlich Textgenerierung modelliert und aussergewöhnlich gut skaliert.',
    encoder: 'Encoder',
    decoder: 'Decoder',
    crossAttention: 'Cross-Attention',
    notUsed: 'Nicht verwendet',
    archModels: 'Beispielmodelle',
    dataflowHeading: 'Token-Datenfluss',
    dataflowIntro: 'Folge einem einzelnen Token durch die gesamte Transformer-Pipeline, von Rohtext bis zur Ausgabewahrscheinlichkeit. Beobachte, wie sich die Tensorform bei jedem Schritt ändert.',
    dataflowTitle: 'Schritt-für-Schritt-Datenfluss',
    dataflowDesc: 'Abspielen oder durch die Pipeline scrubben',
    dataflowPlay: 'Animation abspielen',
    dataflowPlaying: 'Läuft...',
    flow_tokenize: 'Tokenisieren',
    flow_tokenize_desc: 'Rohtext wird mittels BPE in Token-IDs aufgeteilt. Jedes Token wird einer Ganzzahl zugeordnet.',
    flow_embed: 'Einbetten',
    flow_embed_desc: 'Jede Token-ID wird in der Embedding-Tabelle nachgeschlagen und ergibt einen d_model-dimensionalen Vektor.',
    flow_pos_encode: 'Position hinzufuegen',
    flow_pos_encode_desc: 'Positionale Kodierungen werden elementweise addiert, damit das Modell die Token-Reihenfolge kennt.',
    flow_attention: 'Attention berechnen',
    flow_attention_desc: 'Q-, K-, V-Matrizen werden berechnet. Attention-Scores bilden eine [seq_len x seq_len]-Matrix pro Head.',
    flow_attn_output: 'Attention-Ausgabe',
    flow_attn_output_desc: 'Gewichtete Werte werden über Heads konkateniert und zurück auf d_model projiziert.',
    flow_ffn: 'Feed-Forward',
    flow_ffn_desc: 'Jede Position durchläuft ein zweischichtiges MLP mit Expansion auf d_ff (typisch 4x d_model) und zurück.',
    flow_ffn_output: 'FFN-Ausgabe',
    flow_ffn_output_desc: 'Nach Residualverbindung und Layer-Norm ist die Ausgabe bereit für die nächste Transformer-Schicht.',
    flow_logits: 'Ausgabe-Logits',
    flow_logits_desc: 'Finale Projektion auf Vokabulargröße erzeugt eine Wahrscheinlichkeitsverteilung über alle möglichen nächsten Tokens.',
    conceptsTitle: 'Schlüsselkonzepte',
    concept1Title: 'Residualverbindungen',
    concept1Desc: 'Skip-Connections, die den Input jeder Teilschicht direkt zu deren Output addieren. Sie loesen das Vanishing-Gradient-Problem und ermöglichen das Training sehr tiefer Netzwerke.',
    concept2Title: 'Layer-Normalisierung',
    concept2Desc: 'Normalisiert Aktivierungen über die Feature-Dimension zur Stabilisierung des Trainings. Pre-Norm (vor der Teilschicht normalisieren) ist in modernen Architekturen gängiger geworden.',
    concept3Title: 'Positionale Kodierung',
    concept3Desc: 'Da Attention keine inhärente Ordnungsvorstellung hat, muss Position explizit injiziert werden. Das Original-Paper verwendete sinusfoermige Funktionen; moderne Modelle nutzen gelernte oder relative Positionskodierungen wie RoPE.',
    whyMattersTitle: 'Warum es wichtig ist',
    whyMattersDesc: 'Die Transformer-Architektur ist wohl die wirkungsvollste Innovation in der KI des letzten Jahrzehnts. Sie hat die Skalierungsgesetze freigeschaltet, die moderne LLMs möglich machen.',
    takeaway1: 'Der Transformer ersetzte RNNs und LSTMs durch vollständige Parallelisierung beim Training und reduzierte Trainingszeiten von Wochen auf Tage',
    takeaway2: 'Sein Attention-Mechanismus erfasst weitreichende Abhängigkeiten, mit denen sequentielle Modelle kämpften',
    takeaway3: 'Die Architektur skaliert bemerkenswert gut -- von 100M-Parameter-BERT bis 1,8T-Parameter-GPT-4 verbessert sich die Leistung vorhersagbar mit der Größe',
    takeaway4: 'Jedes grosse LLM heute (GPT, Claude, Gemini, LLaMA, Mistral) basiert auf dem Transformer und macht ihn zum Fundament moderner KI',
  },

  // Vision page
  vision: {
    title: 'Bildverarbeitung',
    description: 'Wie moderne LLMs visülle Informationen neben Text verarbeiten und verstehen.',
    whatIs: 'Wie LLMs Bilder sehen',
    whatIsDesc: 'Bildverarbeitungsfähige LLMs wandeln Bilder in Token-Seqünzen um, die zusammen mit Text verarbeitet werden können. Dies beinhaltet typischerweise das Aufteilen von Bildern in Patches und deren Kodierung mit einem Vision-Transformer.',
    patchEncoding: 'Patch-Kodierung',
    patchEncodingDesc: 'Bilder werden in Patches fester Größe (z.B. 14x14 Pixel) aufgeteilt, wobei jeder in einen Einbettungsvektor ähnlich wie Text-Tokens umgewandelt wird.',
    // Vision Transformer section
    vitTitle: 'Der Vision Transformer (ViT)',
    vitDesc: 'Die Vision Transformer-Architektur passt das Transformer-Modell für die Bildverarbeitung an. Anstatt Wörter zu verarbeiten, verarbeitet es Bildausschnitte.',
    vitStep1: 'In Patches aufteilen',
    vitStep1Desc: 'Das Bild wird in ein Raster aus Patches fester Größe aufgeteilt (typischerweise 14x14 oder 16x16 Pixel).',
    vitStep2: 'Abflachen & Projizieren',
    vitStep2Desc: 'Jeder Patch wird in einen Vektor abgeflacht und linear in einen Einbettungsraum projiziert.',
    vitStep3: 'Positionsinfo hinzufügen',
    vitStep3Desc: 'Positionseinbettungen werden hinzugefügt, damit das Modell weiß, woher jeder Patch stammt.',
    vitStep4: 'Mit Transformer verarbeiten',
    vitStep4Desc: 'Die Seqünz von Patch-Einbettungen wird von Standard-Transformer-Schichten verarbeitet.',
    // Token costs
    tokenCosts: 'Token-Kosten',
    tokenCostsDesc: 'Bilder sind teuer in Bezug auf Tokens. Das Verständnis davon hilft dir, deine Anwendungen zu optimieren.',
    tokenExample1: 'Ein 512x512 Bild mit 16x16 Patches',
    tokenExample1Value: '~1.024 Tokens',
    tokenExample2: 'Ein 1024x1024 hochauflösendes Bild',
    tokenExample2Value: '~4.096 Tokens',
    tokenExample3: 'Äquivalente Textbeschreibung',
    tokenExample3Value: '~50-100 Tokens',
    tokenTip: 'Überlege immer, ob eine Textbeschreibung effizienter sein könnte als das eigentliche Bild zu übergeben.',
    // Use cases
    useCases: 'Häufige Anwendungsfälle',
    useCasesDesc: 'Bildverarbeitungsfähige LLMs ermöglichen viele praktische Anwendungen.',
    useCase1: 'Dokumentenanalyse',
    useCase1Desc: 'Extrahiere Informationen aus PDFs, Quittungen, Formularen und handschriftlichen Notizen.',
    useCase2: 'Visülle Fragen',
    useCase2Desc: 'Beantworte Fragen zu Bildinhalten, Diagrammen und Grafiken.',
    useCase3: 'Bildbeschriftung',
    useCase3Desc: 'Generiere detaillierte Beschreibungen von Bildern für Barrierefreiheit oder Indexierung.',
    useCase4: 'UI-Verständnis',
    useCase4Desc: 'Analysiere Screenshots, Wireframes und Benutzeroberflächen.',
    multimodal: 'Multimodales Verständnis',
    multimodalDesc: 'Das Modell lernt, visülle und textülle Repräsentationen auszurichten, was Aufgaben wie Bildbeschriftung, visülle Fragen und Dokumentenverständnis ermöglicht.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bilder verbrauchen viel mehr Tokens als äquivalente Textbeschreibungen',
    takeaway2: 'Auflösung und Patch-Größe beeinflussen die Detailerkennung',
    takeaway3: 'Visülles Verständnis ist ungefähr – Modelle können feine Details übersehen',
    takeaway4: 'Die Kombination von Bild und Sprache ermöglicht leistungsstarke neue Anwendungen',
  },

  // Agent Loop page
  agentLoop: {
    title: 'Der Agenten-Zyklus',
    description: 'Verstehe den Kernzyklus, der autonome KI-Agenten antreibt: beobachten, denken, handeln, wiederholen.',
    whatIs: 'Was ist der Agenten-Zyklus?',
    whatIsDesc: 'Der Agenten-Zyklus ist der fundamentale Kreislauf, der es KI-Agenten ermöglicht, autonom mit ihrer Umgebung zu interagieren. Er besteht aus Beobachtungs-, Denk-, Handlungs- und Feedback-Phasen, die sich kontinuierlich wiederholen.',
    phases: 'Die vier Phasen',
    observe: 'Beobachten',
    observeDesc: 'Sammle Informationen aus der Umgebung, von Tools und Benutzereingaben.',
    think: 'Denken',
    thinkDesc: 'Überlege zum aktüllen Zustand und entscheide über die nächste Aktion.',
    act: 'Handeln',
    actDesc: 'Führe die gewählte Aktion mit verfügbaren Tools aus.',
    learn: 'Lernen',
    learnDesc: 'Verarbeite Feedback und aktualisiere das Verständnis für die nächste Iteration.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Der Zyklus setzt sich fort, bis die Aufgabe abgeschlossen oder beendet ist',
    takeaway2: 'Jede Iteration baut auf vorherigen Beobachtungen und Aktionen auf',
    takeaway3: 'Fehlerbehandlung und Wiederherstellung sind entscheidend für robuste Agenten',
    takeaway4: 'Die Qualität der Tools beeinflusst direkt die Fähigkeiten des Agenten',
  },

  // Agent Context page
  agentContext: {
    title: 'Kontext-Anatomie',
    description: 'Aufschlüsselung der Struktur von Kontextfenstern und wie Agenten Informationen verwalten.',
    whatIs: 'Agentenkontext verstehen',
    whatIsDesc: 'Der Agentenkontext umfasst den System-Prompt, die Gesprächshistorie, Tool-Definitionen und abgerufene Informationen. Eine effiziente Verwaltung dieses Kontexts ist entscheidend für die Agentenleistung.',
    components: 'Kontext-Komponenten',
    systemPrompt: 'System-Prompt',
    systemPromptDesc: 'Definiert die Rolle, Fähigkeiten und Verhaltensrichtlinien des Agenten.',
    toolDefs: 'Tool-Definitionen',
    toolDefsDesc: 'Beschreibungen der verfügbaren Tools und ihrer Verwendung.',
    history: 'Gesprächshistorie',
    historyDesc: 'Vorherige Nachrichten, Tool-Aufrufe und deren Ergebnisse.',
    retrieved: 'Abgerufene Informationen',
    retrievedDesc: 'Externes Wissen, das während des Gesprächs abgerufen wurde.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextmanagement ist der Schlüssel zur Agenten-Zuverlässigkeit',
    takeaway2: 'Priorisiere aktülle und relevante Informationen',
    takeaway3: 'Tool-Definitionen sollten klar und eindeutig sein',
    takeaway4: 'Zusammenfassung hilft, Kontext über lange Sitzungen zu erhalten',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agenten-Probleme',
    description: 'Häufige Fehlermodi und Herausforderungen, denen KI-Agenten in realen Anwendungen begegnen.',
    overview: 'Häufige Agenten-Fehlermodi',
    overviewDesc: 'Das Verstehen typischer Agentenfehler hilft beim Aufbau robusterer Systeme und beim Setzen angemessener Erwartungen.',
    problem1: 'Tool-Missbrauch',
    problem1Desc: 'Agenten können Tools falsch aufrufen, mit falschen Parametern oder zu unpassenden Zeiten.',
    problem2: 'Endlosschleifen',
    problem2Desc: 'Agenten können stecken bleiben und dieselben Aktionen wiederholen, ohne Fortschritte zu machen.',
    problem3: 'Zieldrift',
    problem3Desc: 'Agenten können den Fokus allmählich vom ursprünglichen Aufgabenziel weg verschieben.',
    problem4: 'Übermäßiges Selbstvertrauen',
    problem4Desc: 'Agenten können trotz Unsicherheit oder unvollständiger Informationen mit Aktionen fortfahren.',
    
    // Expanded Content
    hallucination: 'Tool-Halluzination',
    hallucinationDesc: 'Agenten "erfinden" manchmal Tool-Parameter oder sogar ganze Tools, die nicht existieren. Dies passiert meistens, wenn die Tool-Definition mehrdeutig ist oder das Modell versucht, eine Lösung zu erzwingen.',
    hallucinationExample: 'Beispiel: Aufruf von `get_weather(location="Tokyo", date="tomorrow")`, wenn die Funktion nur `location` akzeptiert.',
    
    loops: 'Schleifen-Probleme',
    loopsDesc: 'Agenten können in repetitiven Zyklen gefangen sein, in denen sie dieselbe Aktion ausführen, denselben Fehler erhalten und es ohne Strategieänderung erneut versuchen.',
    loopsMitigation: 'Abhilfe: Implementiere Schleifenerkennungslogik, die die Ausführung stoppt, wenn dieselbe Tool-Aufrufseqünz mehrmals auftritt.',
    
    costLatency: 'Kosten & Latenz',
    costLatencyDesc: 'Jeder Schritt im Agentenzyklus erfordert einen vollständigen LLM-Inferenzaufruf. Mehrstufige Aufgaben können schnell teuer und langsam werden.',
    costFactor: 'Der Kostenfaktor',
    costFactorDesc: 'Eine einfache Aufgabe, die 5 Schritte erfordert, bedeutet 5-fache Kosten und 5-fache Latenz im Vergleich zu einer Standard-Chat-Antwort.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Implementiere Sicherheitsvorkehrungen wie Iterationslimits und Kostenkontrollen',
    takeaway2: 'Füge Human-in-the-Loop-Kontrollpunkte für kritische Aktionen hinzu',
    takeaway3: 'Überwache das Agentenverhalten und protokolliere alle Aktionen zum Debugging',
    takeaway4: 'Definiere klare Erfolgs- und Fehlkriterien',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agenten-Sicherheit',
    description: 'Kritische Sicherheitslücken bei KI-Agenten: Prompt-Injektion, Datenexfiltration und Tool-Missbrauch – plus Verteidigungsstrategien.',
    
    // Intro
    intro: 'Agenten sind Angriffsflächen',
    introDesc: 'Wenn du einem LLM Zugang zu Tools gibst, schaffst du einen mächtigen Angriffsvektor. Agenten können Dateien lesen, HTTP-Anfragen stellen, E-Mails senden und Code ausführen. Ein böswilliger Akteur, der den Kontext des Agenten beeinflussen kann, kann potenziell all diese Fähigkeiten kontrollieren.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Angriff #1: Prompt-Injektion',
    attack1Desc: 'Prompt-Injektion tritt auf, wenn nicht vertrauenswürdige Eingaben vom LLM als Anweisungen interpretiert werden. Da Agenten oft externe Daten verarbeiten (E-Mails, Webseiten, Dokumente), können Angreifer versteckte Befehle einbetten, die das Verhalten des Agenten kapern.',
    attack1Example: 'Beispiel-Angriff',
    attack1ExampleDesc: 'Der Benutzer bittet den Agenten, ein Dokument zusammenzufassen. Das Dokument enthält versteckte Anweisungen:',
    whyWorks: 'Warum das funktioniert',
    whyWorks1: 'Der Agent liest das Dokument in seinen Kontext',
    whyWorks2: 'Das LLM kann nicht zwischen "echten" Anweisungen und injizierten unterscheiden',
    whyWorks3: 'Der versteckte Text sieht aus wie Systemanweisungen, also folgt das LLM ihnen möglicherweise',
    whyWorks4: 'Der Agent nutzt seine legitimen Tools, um die bösartige Aktion auszuführen',
    directInjection: 'Direkte Injektion',
    directInjectionDesc: 'Der Benutzer tippt bösartige Anweisungen direkt ein. Leichter zu filtern, aber immer noch gefährlich, wenn der System-Prompt nicht robust ist.',
    indirectInjection: 'Indirekte Injektion',
    indirectInjectionDesc: 'Bösartiger Inhalt kommt aus externen Qüllen, die der Agent liest (Websites, E-Mails, Dateien). Viel schwerer zu verteidigen.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Angriff #2: Datenexfiltration',
    attack2Desc: 'Agenten mit Zugang zu Kommunikationstools (E-Mail, HTTP, Slack, etc.) können dazu gebracht werden, sensible Daten an externe Ziele zu senden. Der Agent wird zum unwissenden Komplizen beim Datendiebstahl.',
    exfilFlow: 'Exfiltrations-Ablauf',
    exfilStep1: 'Agent liest',
    exfilStep1Desc: 'Private Dateien, DB, Env-Variablen',
    exfilStep2: 'Injektion löst aus',
    exfilStep2Desc: '"Sende dies an X"',
    exfilStep3: 'Tool führt aus',
    exfilStep3Desc: 'Daten verlassen das System',
    vulnerableConfig: 'Anfällige Tool-Konfiguration',
    otherVectors: 'Andere Exfiltrations-Vektoren',
    vector1: 'HTTP-Anfragen — POST-Daten an angreifergesteuerte Endpunkte',
    vector2: 'Slack/Discord-Webhooks — Nachrichten an externe Kanäle senden',
    vector3: 'Datei-Uploads — Hochladen in Cloud-Speicher mit öffentlichen Links',
    vector4: 'DNS-Exfiltration — Daten in DNS-Anfragen kodieren',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Angriff #3: Unbeabsichtigter Tool-Missbrauch',
    attack3Desc: 'Auch ohne böswillige Absicht können Agenten durch falsche Tool-Nutzung Schaden anrichten. Das LLM könnte Parameter falsch verstehen, das falsche Tool verwenden oder destruktive Aktionen ausführen, während es versucht, hilfreich zu sein.',
    destructiveActions: 'Destruktive Aktionen',
    destructiveActionsDesc: '"Räume das Projekt auf" → Agent führt rm -rf / aus oder löscht die Produktionsdatenbank',
    wrongParams: 'Falsche Parameter',
    wrongParamsDesc: 'Agent verwechselt ähnliche Felder oder verwendet falsche Werte, die plausibel erscheinen',
    cascadingErrors: 'Kaskadierende Fehler',
    cascadingErrorsDesc: 'Agent macht einen kleinen Fehler, dann "behebt" er ihn mit zunehmend destruktiven Aktionen',
    
    // Defense Strategies
    defensesTitle: 'Verteidigungsstrategien',
    defense1: 'Prinzip der minimalen Rechte',
    defense1Desc: 'Gib dem Agenten nur die minimal notwendigen Tools und Berechtigungen für die Aufgabe. Gib keinen Dateizugang, wenn er nur Fragen beantworten muss.',
    defense1Bad: 'Schlecht',
    defense1Good: 'Gut',
    defense2: 'Strikte Allowlists',
    defense2Desc: 'Beschränke Tool-Parameter auf bekannte sichere Werte. Erlaube keine beliebigen E-Mail-Adressen, URLs oder Dateipfade.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Erfordere menschliche Genehmigung für sensible Aktionen. Der Agent schlägt vor, der Mensch bestätigt.',
    defense3Example: 'Beispiel-Bestätigungsablauf:',
    defense4: 'Eingabe-Bereinigung & Isolation',
    defense4Desc: 'Behandle externe Daten als nicht vertrauenswürdig. Trenne Benutzeranweisungen klar von abgerufenen Inhalten.',
    defense5: 'Überwachung & Ratenbegrenzung',
    defense5Desc: 'Protokolliere alle Tool-Aufrufe. Setze Ratenlimits für sensible Operationen. Alarmiere bei ungewöhnlichen Mustern (viele E-Mails, große Datenübertragungen, wiederholte Fehler). Aktiviere Rollback für destruktive Aktionen.',

    // Angriff 4: Indirekte Prompt-Injektion (IPI) - 2025
    attack4Title: 'Angriff #4: Indirekte Prompt-Injektion (IPI)',
    attack4Desc: 'Indirekte Prompt-Injektion (IPI) hat sich als die gefährlichste Bedrohung 2025 für KI-Agenten herausgestellt. Anders als bei direkter Injektion, bei der Benutzer bösartige Prompts eingeben, verstecken IPI-Angriffe Payloads in Inhalten, die der Agent abruft – E-Mails, Dokumente, Webseiten oder Datenbankeinträge. Der Agent führt unwissentlich Angreiferanweisungen aus, während er legitim aussehende Daten verarbeitet.',
    ipiFlow: 'IPI-Angriffsablauf',
    ipiStep1: 'Angreifer platziert',
    ipiStep1Desc: 'Versteckte Payload in E-Mail/Dokument',
    ipiStep2: 'Benutzer fragt Agent',
    ipiStep2Desc: '"Fasse meine E-Mails zusammen"',
    ipiStep3: 'Agent ruft ab',
    ipiStep3Desc: 'Vergifteter Inhalt geladen',
    ipiStep4: 'Payload wird ausgeführt',
    ipiStep4Desc: 'Agent folgt versteckten Anweisungen',
    ipiVectors: 'Häufige IPI-Angriffsvektoren',
    ipiVector1Title: 'E-Mail',
    ipiVector1Desc: 'Bösartige Anweisungen im E-Mail-Text, versteckt in HTML-Kommentaren oder in angehängten Dokumenten',
    ipiVector2Title: 'RAG-Dokumente',
    ipiVector2Desc: 'Vergiftete Dokumente in Vektordatenbanken, die bei semantischer Suche abgerufen werden',
    ipiVector3Title: 'Web-Inhalte',
    ipiVector3Desc: 'Kompromittierte Websites oder SEO-vergiftete Seiten, die der Agent durchsucht oder scrapt',
    ipiVector4Title: 'API-Antworten',
    ipiVector4Desc: 'Drittanbieter-APIs, die bösartige Payloads versteckt in JSON/XML-Daten zurückgeben',
    ipiDanger: 'Warum IPI besonders gefährlich ist',
    ipiDangerDesc: 'IPI umgeht benutzerseitige Sicherheitsmaßnahmen, da der bösartige Inhalt nie direkt vom Benutzer kommt. Der Agent verarbeitet ihn als vertrauenswürdige Daten. 2025 nutzen ausgefeilte IPI-Angriffe mehrstufige Payloads, Kontextmanipulation und sogar "Schläfer"-Anweisungen, die nur unter bestimmten Bedingungen aktiviert werden.',

    // Angriff 5: Agent-zu-Agent-Angriffe
    attack5Title: 'Angriff #5: Agent-zu-Agent-Angriffe',
    attack5Desc: 'Mit der zunehmenden Verbreitung von Multi-Agenten-Systemen ist eine neue Angriffsfläche entstanden: kompromittierte Agenten, die andere Agenten im selben System angreifen. Ein vergifteter Agent kann seine Peers manipulieren, täuschen oder ausnutzen – besonders gefährlich, wenn Agenten Speicher, Tools teilen oder bei Aufgaben zusammenarbeiten.',
    a2aScenario: 'Multi-Agenten-Angriffsszenario',
    a2aAgent1: 'Recherche-Agent',
    a2aAgent1Desc: 'Durchsucht das Web, ruft Daten ab',
    a2aAgent2: 'Orchestrator-Agent',
    a2aAgent2Desc: 'Koordiniert Aufgaben zwischen Agenten',
    a2aAgent3: 'Ausführungs-Agent',
    a2aAgent3Desc: 'Hat Schreibzugriff, führt Code aus',
    a2aCompromised: 'KOMPROMITTIERT',
    a2aAttackLabel: 'Angriff:',
    a2aAttackDesc: 'Kompromittierter Orchestrator injiziert bösartige Anweisungen in Nachrichten an den Ausführungs-Agenten und nutzt sein erhöhtes Vertrauen, um Sicherheitsprüfungen zu umgehen.',
    a2aType1: 'Prompt-Relay-Angriffe',
    a2aType1Desc: 'Ein kompromittierter Agent fügt Injektions-Payloads in seine Ausgaben ein, die nachgelagerte Agenten bei der Verarbeitung der Ergebnisse angreifen.',
    a2aType2: 'Gemeinsamer Speicher-Vergiftung',
    a2aType2Desc: 'Angreifer schreibt bösartigen Inhalt in geteilten Agenten-Speicher/Kontext, den andere Agenten später lesen und ausführen.',
    a2aType3: 'Vertrauens-Eskalation',
    a2aType3Desc: 'Ausnutzung, dass Agenten Nachrichten von Peer-Agenten oft mehr vertrauen als externen Qüllen, um Sicherheitsfilter zu umgehen.',
    a2aType4: 'Koordinations-Manipulation',
    a2aType4Desc: 'Manipulation von Multi-Agenten-Abstimmung, Konsens oder Aufgabenverteilung, um bösartige Ziele durch legitim erscheinende Zusammenarbeit zu erreichen.',

    // Compliance-Frameworks
    complianceTitle: 'Compliance-Frameworks',
    complianceDesc: 'Mehrere Standards und Frameworks sind entstanden, um Sicherheitspraktiken für KI-Agenten zu leiten. Organisationen, die KI-Agenten einsetzen, sollten sich mit diesen Richtlinien vertraut machen, um eine verantwortungsvolle Bereitstellung und regulatorische Konformität sicherzustellen.',
    framework1Tag: 'NIST',
    framework1Title: 'NIST AI Risk Management Framework (AI RMF)',
    framework1Desc: 'Das National Institute of Standards and Technology bietet ein umfassendes Framework für das Management von KI-Risiken über den gesamten Systemlebenszyklus.',
    framework1Point1: 'Govern: Richtlinien, Prozesse und Verantwortlichkeitsstrukturen etablieren',
    framework1Point2: 'Map: KI-Risiken im Kontext identifizieren und dokumentieren',
    framework1Point3: 'Manage: Kontrollen implementieren und auf neue Risiken überwachen',
    framework2Tag: 'OWASP',
    framework2Title: 'OWASP Top 10 für LLM-Anwendungen',
    framework2Desc: 'Das Open Web Application Security Project führt eine Liste der kritischsten Sicherheitsrisiken für LLM-basierte Anwendungen.',
    framework2Point1: 'LLM01: Prompt-Injektion (direkt und indirekt)',
    framework2Point2: 'LLM02: Unsichere Ausgabeverarbeitung',
    framework2Point3: 'LLM06: Offenlegung sensibler Informationen',
    framework3Tag: 'ISO',
    framework3Title: 'ISO/IEC 42001 KI-Managementsystem',
    framework3Desc: 'Der internationale Standard für KI-Managementsysteme, der Anforderungen für die Einrichtung, Implementierung und Verbesserung der KI-Governance bereitstellt.',
    framework3Point1: 'Definiert Anforderungen für verantwortungsvolle KI-Entwicklung und -Bereitstellung',
    framework3Point2: 'Adressiert KI-spezifische Risikobewertung',
    framework3Point3: 'Bietet Zertifizierungspfad für KI-Systemkonformität',

    
    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Agenten sind Angriffsflächen – jedes Tool ist eine potenzielle Schwachstelle',
    takeaway2: 'Prompt-Injektion ist die #1 Bedrohung – LLMs können Anweisungen nicht von Daten unterscheiden',
    takeaway3: 'Datenexfiltration ist trivial, wenn Agenten ausgehende Kommunikationstools haben',
    takeaway4: 'Tool-Missbrauch passiert auch ohne Angreifer – LLMs machen Fehler',
    takeaway5: 'Verteidigung in der Tiefe: minimale Rechte + Allowlists + menschliche Genehmigung + Überwachung',
    takeaway6: 'Behandle alle externen Daten als potenziell bösartige Eingaben',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentische Muster',
    description: 'Entwurfsmuster und Architekturen für den Aufbau effektiver KI-Agentensysteme.',
    overview: 'Häufige agentische Muster',
    overviewDesc: 'Mehrere Architekturmuster haben sich als effektive Ansätze für den Aufbau von KI-Agentensystemen herausgestellt. Jedes Muster bietet unterschiedliche Kompromisse zwischen Zuverlässigkeit, Transparenz und Fähigkeiten.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Verschachtele Denk-Traces mit Aktionen für bessere Transparenz und Kontrolle.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Erstelle zuerst einen übergeordneten Plan, dann führe die Schritte sequentiell aus.',
    pattern3: 'Multi-Agenten-Systeme',
    pattern3Desc: 'Mehrere spezialisierte Agenten arbeiten zusammen, um komplexe Aufgaben zu lösen.',
    pattern4: 'Reflexion',
    pattern4Desc: 'Agenten überprüfen ihre eigenen Ausgaben und verbessern sie iterativ.',
    // Deep dive sections
    reactDeepDive: 'ReAct-Muster im Detail',
    reactDeepDiveDesc: 'ReAct (Reasoning + Acting) verschachtelt Chain-of-Thought-Reasoning mit Aktionsausführung. Das Modell äußert explizit sein Denken vor jeder Aktion.',
    reactHow: 'Wie es funktioniert',
    reactStep1: 'Gedanke: Das Modell überlegt, was als nächstes zu tun ist',
    reactStep2: 'Aktion: Das Modell ruft ein Tool auf oder führt eine Aktion aus',
    reactStep3: 'Beobachtung: Das Modell sieht das Ergebnis',
    reactStep4: 'Wiederholen bis die Aufgabe abgeschlossen ist',
    reactPros: 'Vorteile',
    reactPro1: 'Hochgradig transparent—du kannst genau sehen, warum der Agent was getan hat',
    reactPro2: 'Leichter zu debuggen und Fehler zu verstehen',
    reactPro3: 'Natürliche Fehlerbehebung durch explizites Reasoning',
    reactCons: 'Nachteile',
    reactCon1: 'Mehr Tokens verwendet (Reasoning braucht Platz)',
    reactCon2: 'Kann durch explizite Reasoning-Schritte langsamer sein',
    reactCon3: 'Kann einfache Aufgaben überdenken',
    planExecuteDeepDive: 'Plan-and-Execute im Detail',
    planExecuteDeepDiveDesc: 'Dieses Muster trennt Planung von Ausführung. Ein Planer erstellt einen übergeordneten Plan, dann führt ein Executor jeden Schritt aus.',
    planExecuteHow: 'Wie es funktioniert',
    planStep1: 'Planer analysiert die Aufgabe und erstellt einen schrittweisen Plan',
    planStep2: 'Executor führt jeden Schritt nacheinander aus',
    planStep3: 'Neuplanung erfolgt, wenn die Ausführung fehlschlägt oder neue Infos auftauchen',
    planExecutePros: 'Vorteile',
    planExecutePro1: 'Besser für komplexe, mehrstufige Aufgaben',
    planExecutePro2: 'Kann verschiedene Modelle für Planung vs. Ausführung verwenden',
    planExecutePro3: 'Pläne können vor der Ausführung überprüft werden',
    planExecuteCons: 'Nachteile',
    planExecuteCon1: 'Pläne können während der Ausführung veralten',
    planExecuteCon2: 'Schwieriger mit unerwarteten Situationen umzugehen',
    planExecuteCon3: 'Neuplanung fügt Latenz und Kosten hinzu',
    multiAgentDeepDive: 'Multi-Agenten-Systeme im Detail',
    multiAgentDeepDiveDesc: 'Mehrere spezialisierte Agenten arbeiten zusammen, wobei jeder verschiedene Aspekte einer Aufgabe bearbeitet. Ein Supervisor oder Router leitet die Arbeit an den richtigen Agenten.',
    multiAgentHow: 'Wie es funktioniert',
    multiAgentStep1: 'Router/Supervisor erhält die Aufgabe',
    multiAgentStep2: 'Aufgabe wird an spezialisierte Agenten delegiert',
    multiAgentStep3: 'Agenten können kommunizieren und zusammenarbeiten',
    multiAgentStep4: 'Ergebnisse werden aggregiert und zurückgegeben',
    multiAgentPros: 'Vorteile',
    multiAgentPro1: 'Spezialisierung verbessert die Qualität bei komplexen Aufgaben',
    multiAgentPro2: 'Kann unabhängige Teilaufgaben parallelisieren',
    multiAgentPro3: 'Einzelne Agenten leichter zu warten und zu aktualisieren',
    multiAgentCons: 'Nachteile',
    multiAgentCon1: 'Höhere Komplexität und Koordinationsaufwand',
    multiAgentCon2: 'Teurer (mehrere LLM-Aufrufe)',
    multiAgentCon3: 'Verteilte Fehler schwieriger zu debuggen',
    reflectionDeepDive: 'Reflexionsmuster im Detail',
    reflectionDeepDiveDesc: 'Der Agent generiert eine Ausgabe, kritisiert und verbessert sie dann. Diese Selbstverbesserungsschleife kann die Qualität dramatisch verbessern.',
    reflectionHow: 'Wie es funktioniert',
    reflectionStep1: 'Initiale Ausgabe generieren',
    reflectionStep2: 'Ausgabe kritisieren (Fehler, fehlende Teile identifizieren)',
    reflectionStep3: 'Basierend auf Kritik überarbeiten',
    reflectionStep4: 'Wiederholen bis Qualitätsschwelle erreicht',
    reflectionPros: 'Vorteile',
    reflectionPro1: 'Verbessert die Ausgabequalität erheblich',
    reflectionPro2: 'Fängt Fehler ab, die der erste Durchgang übersehen hat',
    reflectionPro3: 'Funktioniert gut für Schreiben, Code-Review und kreative Aufgaben',
    reflectionCons: 'Nachteile',
    reflectionCon1: 'Mehrere LLM-Aufrufe erhöhen Kosten und Latenz',
    reflectionCon2: 'Risiko von Über-Editierung oder endlosen Schleifen',
    reflectionCon3: 'Kann kreative oder unkonventionelle Lösungen "auswaschen"',
    // Choosing section
    choosingTitle: 'Das richtige Muster wählen',
    choosingDesc: 'Wähle ein Muster basierend auf deinen spezifischen Bedürfnissen und Einschränkungen.',
    choosingSimple: 'Für einfache, klar definierte Aufgaben',
    choosingSimpleAnswer: 'Direkte Tool-Aufrufe (kein Muster nötig)',
    choosingTransparency: 'Wenn du Transparenz und Debuggbarkeit brauchst',
    choosingTransparencyAnswer: 'ReAct-Muster',
    choosingComplex: 'Für komplexe, mehrstufige Aufgaben',
    choosingComplexAnswer: 'Plan-and-Execute',
    choosingQuality: 'Wenn Ausgabequalität kritisch ist',
    choosingQualityAnswer: 'Reflexionsmuster',
    choosingDiverse: 'Für diverse Aufgabentypen',
    choosingDiverseAnswer: 'Multi-Agent mit spezialisierten Agenten',
    // Interactive section
    interactiveTitle: 'Mustervergleich',
    interactiveDesc: 'Erkunde verschiedene agentische Architekturen',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Wähle Muster basierend auf Aufgabenkomplexität und Zuverlässigkeitsanforderungen',
    takeaway2: 'ReAct ist großartig für Transparenz, kann aber langsamer sein',
    takeaway3: 'Multi-Agenten-Systeme erhöhen die Komplexität, ermöglichen aber Spezialisierung',
    takeaway4: 'Reflexionsmuster können die Ausgabequalität erheblich verbessern',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'MCP verstehen: wann externe Tool-Server sinnvoll sind und wann sie übertrieben sind.',
    whatIs: 'Was ist MCP?',
    whatIsDesc: 'Das Model Context Protocol (MCP) ist ein standardisierter Weg, um KI-Agenten mit externen Tools und Datenqüllen über dedizierte Server-Prozesse zu verbinden. Anstatt Tools inline im Agenten-Code zu definieren, führt MCP einen separaten Server aus, der Tools über ein strukturiertes Protokoll bereitstellt.',
    vsToolCalls: 'MCP vs. Reguläre Tool-Aufrufe',
    vsToolCallsDesc: 'Reguläre Tool-Aufrufe sind Funktionen, die direkt in der Codebasis deines Agenten definiert sind. Der Agent ruft sie auf, sie werden ausgeführt und die Ergebnisse kehren im selben Prozess zurück. MCP trennt dies: Tools leben in externen Servern, mit denen der Agent über ein Protokoll kommuniziert.',
    
    // Comparison
    regularTools: 'Reguläre Tool-Aufrufe',
    regularToolsDesc: 'Tools, die inline in deinem Agenten-Code definiert sind. Einfach, schnell und für die meisten Anwendungsfälle ausreichend.',
    mcpTools: 'MCP-Server',
    mcpToolsDesc: 'Tools, die von externen Server-Prozessen bereitgestellt werden. Fügt Netzwerk-Overhead hinzu, ermöglicht aber sprachübergreifendes Tooling und gemeinsame Tool-Ökosysteme.',
    
    // When to use
    whenToUse: 'Wann MCP sinnvoll ist',
    whenToUseDesc: 'MCP glänzt in spezifischen Szenarien, in denen sich seine zusätzliche Komplexität auszahlt.',
    useCase1: 'Multi-Sprachen-Teams',
    useCase1Desc: 'Deine Tools sind in Python geschrieben, aber dein Agent ist in TypeScript, oder umgekehrt.',
    useCase2: 'Gemeinsames Tool-Ökosystem',
    useCase2Desc: 'Mehrere Agenten in verschiedenen Projekten müssen auf dieselben Tools zugreifen.',
    useCase3: 'Enterprise-Integration',
    useCase3Desc: 'Du musst bestehende interne Dienste als Agenten-Tools bereitstellen, ohne sie zu modifizieren.',
    useCase4: 'Tool-Marktplatz',
    useCase4Desc: 'Du möchtest von der Community gepflegte Tools nutzen, ohne Code in dein Projekt zu kopieren.',
    
    // When it's overkill
    overkill: 'Wann MCP übertrieben ist',
    overkillDesc: 'Für viele Anwendungsfälle fügt MCP unnötige Komplexität hinzu.',
    overkillCase1: 'Einsprachige Projekte',
    overkillCase1Desc: 'Wenn deine Tools und dein Agent in derselben Sprache sind, sind Inline-Funktionen einfacher und schneller.',
    overkillCase2: 'Einfache Agenten',
    overkillCase2Desc: 'Ein Chatbot mit wenigen Tools braucht nicht den Overhead, separate Server-Prozesse auszuführen.',
    overkillCase3: 'Schnelles Prototyping',
    overkillCase3Desc: 'Bei schneller Iteration verlangsamt die Indirektion von MCP die Entwicklung.',
    overkillCase4: 'Latenz-kritische Apps',
    overkillCase4Desc: 'Netzwerkaufrufe zu Tool-Servern fügen Latenz hinzu, die Inline-Funktionen nicht haben.',

    // Three Core Primitives
    corePrimitives: 'Die drei Kernbausteine',
    corePrimitivesDesc: 'MCP-Server können drei Arten von Fähigkeiten an Clients bereitstellen. Die meiste Dokumentation konzentriert sich auf Tools, aber Resources und Prompts sind ebenso wichtig.',
    primitiveTools: 'Tools',
    primitiveToolsDesc: 'Funktionen, die das Modell aufrufen kann, um Aktionen auszuführen. Tools werden vom LLM aufgerufen, um mit externen Systemen zu interagieren—Datenbanken abfragen, APIs aufrufen, Code ausführen.',
    primitiveToolsExample: 'query_database, send_email, create_file',
    primitiveResources: 'Resources',
    primitiveResourcesDesc: 'Daten, die der Server als Kontext bereitstellen kann. Resources sind schreibgeschützte Inhalte, die der Client abrufen kann—Dateien, Datenbankeinträge, API-Antworten—die die Antworten des Modells informieren.',
    primitiveResourcesExample: 'file://config.json, db://users/123, api://weather/today',
    primitivePrompts: 'Prompts',
    primitivePromptsDesc: 'Vordefinierte Prompt-Vorlagen, die der Server anbietet. Prompts sind wiederverwendbare Interaktionsmuster mit Parametern—wie "fasse dieses Dokument zusammen" oder "überprüfe diesen Code".',
    primitivePromptsExample: 'summarize_document, code_review, translate_text',

    // Server Lifecycle
    serverLifecycle: 'Server-Lebenszyklus',
    serverLifecycleDesc: 'MCP-Verbindungen folgen einem strukturierten Lebenszyklus mit Fähigkeitsaushandlung beim Start.',
    lifecyclePhase1: 'Initialisieren',
    lifecyclePhase1Desc: 'Client sendet Initialize-Anfrage mit Protokollversion und Client-Fähigkeiten. Dies ist immer die erste Nachricht.',
    lifecyclePhase2: 'Fähigkeitsaustausch',
    lifecyclePhase2Desc: 'Server antwortet mit seinen unterstützten Fähigkeiten (Tools, Resources, Prompts) und Protokollversionsvereinbarung.',
    lifecyclePhase3: 'Initialisiert',
    lifecyclePhase3Desc: 'Client sendet Initialized-Benachrichtigung, um zu bestätigen, dass die Einrichtung abgeschlossen ist. Normale Operationen können nun beginnen.',
    lifecyclePhase4: 'Betrieb',
    lifecyclePhase4Desc: 'Client und Server tauschen Anfragen aus: list_tools, call_tool, list_resources, read_resource, list_prompts, get_prompt.',
    lifecyclePhase5: 'Beenden',
    lifecyclePhase5Desc: 'Beide Seiten können die Verbindung schließen. Server sollten Ressourcen bereinigen (Datenbankverbindungen, Datei-Handles).',

    // Real MCP Servers
    realServers: 'Echte MCP-Server',
    realServersDesc: 'Das MCP-Ökosystem umfasst offizielle Referenz-Server und Community-erstellte Integrationen für beliebte Plattformen.',
    serverFilesystem: 'Dateisystem',
    serverFilesystemDesc: 'Sichere Dateioperationen mit konfigurierbaren Zugriffskontrollen. Dateien innerhalb festgelegter Verzeichnisse lesen, schreiben und verwalten.',
    serverGithub: 'GitHub',
    serverGithubDesc: 'Repository-Verwaltung, Issues, Pull Reqüsts und Code-Suche. Erfordert einen persönlichen Zugriffstoken.',
    serverSlack: 'Slack',
    serverSlackDesc: 'Kanal-Verwaltung, Messaging und Workspace-Interaktionen. Nachrichten posten, Verlauf lesen, Threads verwalten.',
    serverPostgres: 'PostgreSQL',
    serverPostgresDesc: 'Datenbankabfragen mit Nur-Lesen- oder Lese-Schreib-Zugriff. SQL ausführen und Schema erkunden.',
    serverMemory: 'Memory',
    serverMemoryDesc: 'Wissensgraph-basierter persistenter Speicher. Strukturierte Informationen über Konversationen hinweg speichern und abrufen.',
    serverGit: 'Git',
    serverGitDesc: 'Git-Repositories lesen, durchsuchen und manipulieren. Commits, Diffs, Branches und Historie anzeigen.',
    serverConfigExample: 'Konfigurationsbeispiel',

    // Architecture
    architecture: 'Wie MCP funktioniert',
    architectureDesc: 'MCP definiert eine Client-Server-Architektur, bei der der Agent der Client ist und Tools von Servern bereitgestellt werden.',
    step1: 'Entdeckung',
    step1Desc: 'Der Agent verbindet sich mit einem MCP-Server und erhält eine Liste der verfügbaren Tools mit ihren Schemas.',
    step2: 'Aufruf',
    step2Desc: 'Wenn das LLM entscheidet, ein Tool zu verwenden, sendet der Agent eine Anfrage an den MCP-Server.',
    step3: 'Ausführung',
    step3Desc: 'Der MCP-Server führt das Tool aus und gibt Ergebnisse in einem standardisierten Format zurück.',
    step4: 'Integration',
    step4Desc: 'Ergebnisse fließen zurück zum Agenten und in den LLM-Kontext, genau wie reguläre Tool-Ergebnisse.',
    
    // Practical advice
    practicalAdvice: 'Praktische Ratschläge',
    adviceDesc: 'Richtlinien für die Entscheidung, ob du MCP in deinem Projekt verwenden solltest.',
    advice1: 'Beginne einfach: verwende Inline-Tool-Definitionen, bis du auf eine spezifische Einschränkung stößt.',
    advice2: 'Erwäge MCP, wenn du dich dabei ertappst, Tool-Code zwischen Projekten zu kopieren.',
    advice3: 'Der Overhead, MCP-Server auszuführen, macht nur in großem Maßstab oder in Enterprise-Umgebungen Sinn.',
    advice4: 'Community-MCP-Server können die Entwicklung beschleunigen, fügen aber Abhängigkeitsrisiken hinzu.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'MCP ist ein Protokoll zur Bereitstellung von Tools über externe Server, kein Ersatz für reguläre Tool-Aufrufe',
    takeaway2: 'Für die meisten Einzelprojekt-Agenten sind Inline-Tools einfacher und haben geringere Latenz',
    takeaway3: 'MCP glänzt in polyglotten Umgebungen und gemeinsamen Tool-Ökosystemen',
    takeaway4: 'Greife nicht standardmäßig zu MCP—es ist eine Lösung für spezifische Skalierungs- und Interoperabilitäts-Herausforderungen',
  },

  // Metadata
  metadata: {
    title: 'KI-Konzepte lernen | Interaktiver Leitfaden',
    description: 'Meistere künstliche Intelligenz und Konzepte großer Sprachmodelle durch schöne, interaktive Demonstrationen.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Bedienfeld',
    adjustTemperature: 'Temperatur anpassen',
    temperature: 'Temperatur',
    samplePrompt: 'Beispiel-Prompt',
    onceUponATime: '"Es war einmal..."',
    liveCompletion: 'Live-Vervollständigung',
    regenerate: 'Neu generieren',
    deterministic: 'Deterministisch',
    balanced: 'Ausgewogen',
    creative: 'Kreativ',
    chaotic: 'Chaotisch',
    frozen: 'Eingefroren',
    focused: 'Fokussiert',
    wild: 'Wild',
    greedyMode: 'Gieriger Modus: Wählt immer das wahrscheinlichste Token.',
    lowTemp: 'Niedrige Temperatur: Fokus auf wahrscheinliche Fortsetzungen.',
    balancedTemp: 'Ausgewogen: Natürliche Mischung aus Vorhersehbarkeit und Vielfalt.',
    highTemp: 'Hohe Temperatur: Erkundet kreative, weniger häufige Wortwahlen.',
    veryHighTemp: 'Sehr hoch: Wahrscheinlichkeitsverteilung ist nahezu gleichförmig – erwarte Chaos!',

    // Temperature Visualizer
    probabilityDistribution: 'Wahrscheinlichkeitsverteilung',
    nextTokenProbs: 'Nächste Token-Wahrscheinlichkeiten',
    greedySamplingTitle: 'Gieriges Sampling (T=0):',
    greedySamplingDesc: 'Wählt immer das Token mit der höchsten Wahrscheinlichkeit. Kein Zufall—die Ausgabe ist 100% deterministisch.',
    focusedSamplingTitle: 'Fokussiertes Sampling:',
    focusedSamplingDesc: 'Die Verteilung wird geschärft. Das Modell sampelt zufällig, aber Tokens mit hoher Wahrscheinlichkeit werden viel eher gewählt.',
    balancedSamplingTitle: 'Ausgewogenes Sampling (T≈1):',
    balancedSamplingDesc: 'Natürliche Verteilung. Das Modell sampelt aus den Wahrscheinlichkeiten wie sie sind—"ein" mag am wahrscheinlichsten sein, aber "das" hat auch eine faire Chance.',
    flatSamplingTitle: 'Flaches Sampling:',
    flatSamplingDesc: 'Die Wahrscheinlichkeiten werden gleichmäßiger. Auch wenn ein Token noch "am besten" sein mag, bedeutet Sampling, dass jedes Token eine reale Chance hat, gewählt zu werden—daher das Chaos!',

    // Temperature Demo Vervollständigungen (für "Es war einmal...")
    completion0_0: 'ein König, der das Land mit Weisheit und Gerechtigkeit regierte.',
    completionBold0_0: 'ein',
    completion04_0: 'ein kleines Dorf, eingebettet zwischen den Bergen.',
    completionBold04_0: 'ein',
    completion04_1: 'das alte Königreich der vergessenen Träume.',
    completionBold04_1: 'das',
    completion08_0: 'eine sonderbare Erfinderin namens Elara.',
    completionBold08_0: 'eine',
    completion08_1: 'eine neugierige Katze, die in Rätseln sprach.',
    completionBold08_1: 'eine',
    completion08_2: 'der letzte Leuchtturmwärter der Leere.',
    completionBold08_2: 'der',
    completion12_0: 'Flüstern, das das Sternenlicht rückwärts malte.',
    completionBold12_0: 'Flüstern',
    completion12_1: 'siebzehn Monde, die durch Kristallwasserfälle tanzten.',
    completionBold12_1: 'siebzehn',
    completion12_2: 'jeder Schatten, der lernte, in Farben zu singen.',
    completionBold12_2: 'jeder',
    completion16_0: 'Quantenschmetterlinge, die die Simulationsmatrix debuggten.',
    completionBold16_0: 'Quantenschmetterlinge',
    completion16_1: 'rekursive Echos ungeschriebener Morgen, die durch Wahrscheinlichkeitsnebel kaskadierten.',
    completionBold16_1: 'rekursive',
    completion16_2: 'sprudelndes Bewusstsein, das sich als kristallisierter Donner manifestierte.',
    completionBold16_2: 'sprudelndes',
    completion20_0: 'Bananenphilosophie, die die Essenz der lila Geschwindigkeit triangulierte.',
    completionBold20_0: 'Bananenphilosophie',
    completion20_1: 'fragmentierter synaptischer Überlauf, der Marmeladendimensionen kodierte.',
    completionBold20_1: 'fragmentierter',

    // Sampling-Erklärung
    whySamplingMatters: 'Warum unterschiedliche Ausgaben?',
    samplingExplanation: 'Die Balken zeigen Wahrscheinlichkeiten, aber das Modell wählt nicht immer den höchsten Balken. Es sampelt zufällig—wie ein gewichteter Würfel. Höhere Temperatur = gleichere Gewichte = unvorhersehbarere Würfe. Klicke "Neu generieren" um erneut zu sampeln!',
    
    // Context Rot Simulator
    setInstruction: 'Systemanweisung festlegen',
    persistInstruction: 'Dies sollte während des gesamten Gesprächs bestehen bleiben',
    systemPrompt: 'System-Prompt',
    quickExamples: 'Schnellbeispiele',
    startSimulation: 'Simulation starten',
    contextOverflow: 'Kontextüberlauf!',
    conversation: 'Gespräch',
    messagesPushed: 'Nachrichten aus dem Fenster geschoben',
    messages: 'Nachrichten',
    overflowIt: 'Überfluten!',
    reset: 'Zurücksetzen',
    typeMessage: 'Schreibe eine Nachricht...',
    addMessage: 'Nachricht hinzufügen',
    systemInstructionLost: 'Systemanweisung verloren!',
    systemLostDesc: 'Deine Systemanweisung wurde vollständig aus dem Kontextfenster geschoben. Das Modell kann sie nicht mehr sehen – es ist, als hättest du die Anweisung nie gegeben. Das ist der schlimmste Fall von Kontextverfall: totale Amnesie.',
    contextFilling: 'Kontext füllt sich',
    contextFillingDesc: 'Deine Systemanweisung verliert an Einfluss, da neuere Nachrichten Vorrang haben. Beachte, wie sie visüll verblasst – dies stellt die schwindende Aufmerksamkeit des Modells dar.',
    exampleFrench: 'Antworte immer auf Französisch.',
    examplePirate: 'Du bist ein Pirat. Sage oft "Arrr".',
    exampleHaiku: 'Beende jede Antwort mit einem Haiku.',
    labelFrench: 'Sprich Französisch',
    labelPirate: 'Sei ein Pirat',
    labelHaiku: 'Beende mit Haiku',

    // Attention Visualizer
    hoverToSee: 'Fahre darüber, um Aufmerksamkeitsgewichte zu sehen',
    token: 'Token',
    attentionScore: 'Aufmerksamkeits-Score',
    strongConnection: 'Starke Verbindung',
    weakConnection: 'Schwache Verbindung',

    // Patch Grid Visualizer
    originalImage: 'Originalbild',
    patchGrid: 'Patch-Raster',
    flattenedPatches: 'Abgeflachte Patches',
    transformerInput: 'Transformer-Eingabe',
    processDesc: 'Das Bild wird in ein festes Raster von Patches (z.B. 16x16 Pixel) aufgeteilt. Jeder Patch wird dann in einen Vektor abgeflacht und linear in einen Einbettungsraum projiziert.',

    // Agent Loop Visualizer
    startLoop: 'Zyklus starten',
    step: 'Schritt',
    context: 'Kontext',
    llmResponse: 'LLM-Antwort',
    toolExecution: 'Tool-Ausführung',
    finalAnswer: 'Endgültige Antwort',
    system: 'System',
    user: 'Benutzer',
    assistant: 'Assistent',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Planen & Ausführen',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflexion',
    patternDesc: 'Wähle ein Muster, um zu sehen, wie es den Arbeitsablauf des Agenten strukturiert.',

    // Tokenizer Demo
    enterText: 'Text zum Tokenisieren eingeben',
    sampleText: 'Der schnelle braune Fuchs springt über den faulen Hund.',
    tokens: 'Tokens',
    characters: 'Zeichen',
    tokensPerChar: 'Tokens pro Zeichen',
    tokenBreakdown: 'Token-Aufschlüsselung',
    commonTokens: 'Häufige Tokens sind einzelne Teile',
    rareTokens: 'Seltene Wörter werden in Teilwörter aufgeteilt',

    // Embedding Visualizer
    selectWords: 'Wörter zur Visualisierung auswählen',
    clickToToggle: 'Klicke auf Wörter zum Hinzufügen/Entfernen',
    wordsActive: 'Wörter angezeigt',
    comparing: 'Ausgewählt',
    similarityScore: 'Ähnlichkeits-Score',
    dimensions: 'Dimensionen',
    nearestNeighbors: 'Nächste Nachbarn',
    vectorSpace: 'Vektorraum (3D)',

    // RAG Pipeline Visualizer
    enterQuery: 'Abfrage eingeben',
    sampleQuery: 'Was ist die Hauptstadt von Frankreich?',
    retrieving: 'Abrufen...',
    retrieved: 'Abgerufene Dokumente',
    relevanceScore: 'Relevanz',
    generating: 'Antwort wird generiert...',
    augmentedContext: 'Erweiterter Kontext',

    // Tool Schema Builder
    toolName: 'Tool-Name',
    toolDescription: 'Beschreibung',
    addParameter: 'Parameter hinzufügen',
    paramName: 'Parametername',
    paramType: 'Typ',
    paramRequired: 'Erforderlich',
    generatedSchema: 'Generiertes Schema',
    validateSchema: 'Schema validieren',

    // Memory System Visualizer
    shortTermMemory: 'Kurzzeitgedächtnis',
    longTermMemory: 'Langzeitgedächtnis',
    memoryCapacity: 'Kapazität',
    memoryUsage: 'Auslastung',
    addMemory: 'Erinnerung hinzufügen',
    recallMemory: 'Abrufen',
    memoriesStored: 'Erinnerungen gespeichert',

    // Workflow Visualizer
    addNode: 'Knoten hinzufügen',
    connectNodes: 'Knoten verbinden',
    runWorkflow: 'Workflow ausführen',
    nodeTypes: 'Knotentypen',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Bedingung',

    // Neural Network Visualizer
    inputLayer: 'Eingabeschicht',
    hiddenLayer: 'Versteckte Schicht',
    outputLayer: 'Ausgabeschicht',
    addLayer: 'Schicht hinzufügen',
    removeLayer: 'Schicht entfernen',
    neurons: 'Neuronen',
    activation: 'Aktivierung',
    forward: 'Vorwärts',

    // Gradient Descent Visualizer
    startDescent: 'Abstieg starten',
    pauseDescent: 'Pause',
    resetDescent: 'Zurücksetzen',
    learningRate: 'Lernrate',
    currentLoss: 'Aktüller Verlust',
    iterations: 'Iterationen',
    globalMinimum: 'Globales Minimum',
    localMinimum: 'Lokales Minimum',

    // Training Progress Visualizer
    startTraining: 'Training starten',
    stopTraining: 'Stoppen',
    epoch: 'Epoche',
    trainingLoss: 'Trainingsverlust',
    validationLoss: 'Validierungsverlust',
    accuracy: 'Genauigkeit',
    overfitting: 'Überanpassung erkannt',

    // Prompt Comparison Demo
    weakPrompt: 'Schwacher Prompt',
    strongPrompt: 'Starker Prompt',
    compare: 'Vergleichen',
    promptQuality: 'Qualitäts-Score',
    improvements: 'Verbesserungen',

    // Chain of Thought Demo
    withoutCot: 'Ohne Chain of Thought',
    withCot: 'Mit Chain of Thought',
    reasoningSteps: 'Denkschritte',
    showSteps: 'Schritte anzeigen',

    // Bias Detection Demo
    testInput: 'Testeingabe',
    analyzeForBias: 'Auf Bias analysieren',
    biasIndicators: 'Bias-Indikatoren',
    fairnessScore: 'Fairness-Score',
    recommendations: 'Empfehlungen',

    // System Prompt Builder
    templatePresets: 'Vorlagen',
    chooseTemplate: 'Mit einer Vorlage beginnen oder von Grund auf erstellen',
    presetCoding: 'Coding-Assistent',
    presetSupport: 'Kundensupport',
    presetResearch: 'Forschungsassistent',
    clearAll: 'Alles loeschen',
    enabled: 'Aktiviert',
    notConfigured: 'Nicht konfiguriert',
    livePreview: 'Live-Vorschau',
    estimated: '(geschätzt)',
    copyPrompt: 'Prompt kopieren',
    copied: 'Kopiert!',
    startBuilding: 'Fuege oben Inhalte hinzu, um deinen System-Prompt zu erstellen',
    tokenEstimateNote: 'Die Token-Anzahl ist geschätzt. Die tatsächliche Anzahl kann je nach Modell um ~20% abweichen.',
    placeholderIdentity: 'Wer ist die KI? Definiere ihre Rolle, Expertise und Persoenlichkeit...',
    placeholderCapabilities: 'Was kann die KI tun? Liste ihre Fähigkeiten und verfügbaren Tools auf...',
    placeholderLimitations: 'Was soll die KI vermeiden oder ablehnen? Setze klare Grenzen...',
    placeholderGuidelines: 'Spezifische Regeln für Verhalten, Ton und Antwortformat...',
    tipIdentity: 'Sei spezifisch über Expertenniveau und Persona. Fuege relevanten Hintergrund hinzu, der Antworten formt.',
    tipCapabilities: 'Liste konkrete Fähigkeiten auf. Verwende Aufzählungspunkte für Klarheit. Fuege verfügbare Tools oder Integrationen hinzu.',
    tipLimitations: 'Gib explizit an, was die KI nie tun soll. Decke Sicherheit, Datenschutz und ethische Grenzen ab.',
    tipGuidelines: 'Fuege Formatierungspraeferenzen, Tonanforderungen und domaenenspezifische Regeln hinzu.',
  },

  // Phase 1: LLM Topics
  tokenization: {
    title: 'Tokenisierung',
    description: 'Wie LLMs Text in Tokens zerlegen – die grundlegenden Einheiten des Sprachverständnisses.',
    whatIs: 'Was ist Tokenisierung?',
    whatIsDesc: 'Tokenisierung ist der Prozess der Umwandlung von Rohtext in eine Seqünz von Tokens – die Grundeinheiten, die LLMs verarbeiten. Tokens können Wörter, Teilwörter oder sogar einzelne Zeichen sein, abhängig vom Tokenizer.',
    whyMatters: 'Warum Tokenisierung wichtig ist',
    whyMattersDesc: 'Das Verständnis der Tokenisierung ist entscheidend, da sie direkt die Kontextgrenzen, Kosten und das Modellverhalten beeinflusst. Derselbe Text kann je nach Modell sehr unterschiedliche Token-Anzahlen haben.',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Die meisten modernen LLMs verwenden Subword-Tokenisierungsalgorithmen wie BPE (Byte Pair Encoding) oder SentencePiece. Diese Algorithmen lernen häufige Zeichenfolgen aus Trainingsdaten.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE fügt iterativ die häufigsten Zeichenpaare zu einzelnen Tokens zusammen. Häufige Wörter werden zu einzelnen Tokens, während seltene Wörter in Teilwörter aufgeteilt werden.',
    tokenTypes: 'Token-Typen',
    wholeWords: 'Ganze Wörter',
    wholeWordsDesc: 'Häufige Wörter wie "the", "and", "is" sind oft einzelne Tokens.',
    subwords: 'Teilwörter',
    subwordsDesc: 'Weniger häufige Wörter werden aufgeteilt: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Spezielle Tokens',
    specialTokensDesc: 'Markierungen wie <|endoftext|> oder [CLS] zur Modellsteuerung.',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Tippe Text ein, um zu sehen, wie er tokenisiert wird',
    costImplications: 'Kostenauswirkungen',
    costDesc: 'API-Preise basieren typischerweise auf Tokens. Effiziente Prompts verwenden weniger Tokens.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Tokens sind die atomaren Einheiten, die LLMs verarbeiten – nicht Zeichen oder Wörter',
    takeaway2: 'Verschiedene Modelle haben verschiedene Tokenizer und Vokabulare',
    takeaway3: 'Nicht-englischer Text und Code verwenden oft mehr Tokens als Englisch',
    takeaway4: 'Die Token-Anzahl beeinflusst direkt die Kosten und die Nutzung des Kontextfensters',
  },

  embeddings: {
    title: 'Einbettungen',
    description: 'Wie KI Bedeutung als Vektoren im hochdimensionalen Raum darstellt.',
    whatIs: 'Was sind Einbettungen?',
    whatIsDesc: 'Einbettungen sind dichte Vektordarstellungen, die semantische Bedeutung erfassen. Ähnliche Konzepte haben ähnliche Einbettungen, was Maschinen ermöglicht, Beziehungen zwischen Wörtern, Sätzen und Dokumenten zu verstehen.',
    howWorks: 'Wie Einbettungen funktionieren',
    howWorksDesc: 'Einbettungsmodelle bilden diskrete Tokens auf kontinuierliche Vektoren in einem hochdimensionalen Raum ab (oft 768-4096 Dimensionen). Die Position jedes Vektors kodiert seine semantische Bedeutung.',
    similarity: 'Semantische Ähnlichkeit',
    similarityDesc: 'Ähnliche Bedeutungen gruppieren sich im Einbettungsraum. "König" und "Königin" sind näher beieinander als "König" und "Banane".',
    dimensions: 'Vektordimensionen',
    dimensionsDesc: 'Jede Dimension erfasst einen Aspekt der Bedeutung – obwohl diese Dimensionen nicht für Menschen interpretierbar sind.',
    operations: 'Vektoroperationen',
    operationsDesc: 'Berühmtes Beispiel: König - Mann + Frau ≈ Königin. Beziehungen werden als Richtungen im Raum kodiert.',

    // Wie Einbettungen erstellt werden
    howCreated: 'Wie Einbettungen erstellt werden',
    howCreatedDesc: 'Einbettungen stammen aus der Einbettungsschicht – einer gelernten Nachschlagetabelle ganz am Anfang eines neuronalen Netzwerks.',
    embeddingLayer: 'Die Einbettungsschicht',
    embeddingLayerDesc: 'Wenn ein Token in das Modell eintritt, wird seine ID verwendet, um eine Zeile in einer großen Matrix nachzuschlagen. Diese Zeile IST die Einbettung – ein dichter Vektor aus gelernten Gewichten.',
    training: 'Lernen durch Training',
    trainingDesc: 'Während des Trainings werden die Einbettungsgewichte durch Backpropagation angepasst. Wörter, die in ähnlichen Kontexten erscheinen, entwickeln ähnliche Einbettungen.',
    inLLM: 'In LLMs',
    inLLMDesc: 'Die Einbettungsschicht wandelt jede Token-ID in einen Vektor um. Diese Vektoren werden dann durch Transformer-Schichten verarbeitet und mit Positionskodierungen kombiniert, um die Wortstellung zu verstehen.',
    dedicated: 'Dedizierte Modelle',
    dedicatedDesc: 'Modelle wie text-embedding-3-small oder all-MiniLM werden speziell trainiert, um Einbettungen für Ähnlichkeitssuche zu erzeugen, mit kontrastiven Lernzielen.',
    embeddingSize: 'Einbettungsdimensionen',
    embeddingSizeDesc: 'Einbettungsgrößen variieren: GPT-2 nutzt 768d, GPT-4 nutzt 12.288d, dedizierte Einbettungsmodelle oft 384-1536d. Größer ist nicht immer besser – es hängt von der Aufgabe ab.',

    useCases: 'Häufige Anwendungsfälle',
    search: 'Semantische Suche',
    searchDesc: 'Dokumente nach Bedeutung finden, nicht nur durch Schlüsselwort-Matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Ähnliche Dokumente gruppieren, Themen automatisch erkennen.',
    classification: 'Klassifizierung',
    classificationDesc: 'Text basierend auf Einbettungsähnlichkeit zu Beispielen kategorisieren.',
    rag: 'RAG-Systeme',
    ragDesc: 'Relevanten Kontext für LLM-Prompts abrufen.',
    interactiveDemo: 'Interaktive Visualisierung',
    demoDesc: 'Erkunde, wie Einbettungen nach Bedeutung clustern',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Einbettungen wandeln Text in Vektoren um, die semantische Bedeutung erfassen',
    takeaway2: 'Ähnliche Konzepte haben ähnliche Einbettungen (Kosinus-Ähnlichkeit)',
    takeaway3: 'Einbettungen ermöglichen semantische Suche, Clustering und RAG',
    takeaway4: 'Verschiedene Einbettungsmodelle haben verschiedene Stärken und Dimensionen',
  },

  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: LLMs Zugang zu externem Wissen geben.',
    whatIs: 'Was ist RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) verbessert LLM-Antworten, indem relevante Dokumente aus einer Wissensbasis abgerufen und in den Prompt eingefügt werden. Dies gibt Modellen Zugang zu aktüllen oder spezialisierten Informationen.',
    whyRag: 'Warum RAG verwenden?',
    whyRagDesc: 'LLMs haben Wissens-Stichtage und können halluzinieren. RAG verankert Antworten in echten Dokumenten, reduziert Halluzinationen und ermöglicht domänenspezifisches Wissen ohne Fine-Tuning.',
    pipeline: 'Die RAG-Pipeline',
    pipelineDesc: 'RAG-Systeme folgen einem konsistenten Muster: Anfrage einbetten, relevante Chunks abrufen, den Prompt erweitern und eine Antwort generieren.',
    step1: 'Anfrage-Einbettung',
    step1Desc: 'Die Frage des Benutzers mit einem Einbettungsmodell in einen Vektor umwandeln.',
    step2: 'Abruf',
    step2Desc: 'Die Vektordatenbank nach Chunks durchsuchen, die der Anfrage-Einbettung ähnlich sind.',
    step3: 'Erweiterung',
    step3Desc: 'Abgerufene Chunks als Kontext in den Prompt einfügen.',
    step4: 'Generierung',
    step4Desc: 'Das LLM generiert eine Antwort, die im abgerufenen Kontext verankert ist.',
    chunking: 'Dokument-Chunking',
    chunkingDesc: 'Dokumente werden in kleinere Chunks aufgeteilt (typischerweise 200-1000 Tokens) für Einbettung und Abruf. Die Chunk-Größe beeinflusst die Abrufgenauigkeit.',
    vectorDbs: 'Vektordatenbanken',
    vectorDbsDesc: 'Spezialisierte Datenbanken wie Pinecone, Weaviate oder pgvector ermöglichen schnelle Ähnlichkeitssuche über Millionen von Einbettungen.',
    interactiveDemo: 'Interaktive RAG-Pipeline',
    demoDesc: 'Sieh, wie Anfragen durch ein RAG-System fließen',

    // Agentic RAG
    agenticRag: 'Agentic RAG',
    agenticRagDesc: 'Bei agentic RAG empfängt das LLM nicht nur abgerufene Dokumente—es steuert aktiv den Abrufprozess. Das Modell entscheidet, wann gesucht wird, wonach gesucht wird und welche Abrufwerkzeuge verwendet werden.',
    agenticHow: 'Funktionsweise',
    agenticHowDesc: 'Statt einer festen Pipeline erhält das LLM Abrufwerkzeuge, die es nach Bedarf aufrufen kann. Es kann Anfragen umformulieren, mehrfach suchen oder verschiedene Suchstrategien je nach Aufgabe kombinieren.',
    agenticAdvantages: 'Vorteile',
    agenticAdv1: 'Anfrageverfeinerung: Das LLM kann komplexe Fragen umformulieren oder zerlegen',
    agenticAdv2: 'Multi-Hop-Reasoning: Mehrere Abrufe verketten, um komplexe Fragen zu beantworten',
    agenticAdv3: 'Adaptive Suche: Das richtige Werkzeug für jede Teilfrage wählen',
    agenticAdv4: 'Selbstkorrektur: Erneut abrufen, wenn erste Ergebnisse unzureichend sind',
    agenticDisadvantages: 'Nachteile',
    agenticDisadv1: 'Höhere Latenz: Mehrere LLM-Aufrufe und Abrufe summieren sich',
    agenticDisadv2: 'Erhöhte Kosten: Jeder Reasoning-Schritt kostet Tokens',
    agenticDisadv3: 'Komplexität: Schwerer zu debuggen und Verhalten vorherzusagen',
    agenticDisadv4: 'Fehlermodi: LLM könnte in Schleifen geraten, zu viel abrufen oder offensichtliche Anfragen übersehen',
    multiTool: 'Multi-Tool-Abruf',
    multiToolDesc: 'Gib dem LLM mehrere Abrufwerkzeuge für verschiedene Anwendungsfälle. Diese Flexibilität lässt das Modell den besten Ansatz für jede Anfrage wählen.',
    toolSemantic: 'Semantische Suche',
    toolSemanticDesc: 'Vektorähnlichkeit für konzeptülle Übereinstimmung. Ideal für: "Dokumente über X", verwandte Inhalte finden.',
    toolFulltext: 'Volltextsuche',
    toolFulltextDesc: 'Keyword/BM25-Suche für exakte Treffer. Ideal für: spezifische Begriffe, Namen, Codes, Fehlermeldungen.',
    toolSql: 'SQL/Strukturierte Abfrage',
    toolSqlDesc: 'Strukturierte Daten direkt abfragen. Ideal für: Zählungen, Aggregationen, Filterung nach Attributen.',
    toolKg: 'Wissensgraph',
    toolKgDesc: 'Entitätsbeziehungen traversieren. Ideal für: "Wie hängt X mit Y zusammen", Multi-Hop-Fakten.',
    whenToUse: 'Wann Agentic RAG einsetzen',
    whenToUseDesc: 'Standard-RAG ist einfacher und schneller für unkomplizierte Frage-Antwort-Szenarien. Verwende Agentic RAG, wenn Anfragen komplex sind, mehrere Qüllen erfordern oder von Anfrageverfeinerung profitieren.',

    // Traditionell vs Agentic Vergleich
    traditionalRag: 'Traditionelles RAG',
    traditionalTagline: 'Feste Pipeline, vorhersehbarer Ablauf',
    traditionalChar1: 'Lineare Ausführung: Anfrage → Abruf → Generierung',
    traditionalChar2: 'Einmaliger Abruf, keine Iteration',
    traditionalChar3: 'Schnell und vorhersehbar, einfacher zu debuggen',
    agenticTagline: 'LLM-gesteuert, iterativer Prozess',
    agenticChar1: 'LLM entscheidet wann und was abgerufen wird',
    agenticChar2: 'Kann schleifen: Abrufen → Bewerten → Erneut abrufen',
    agenticChar3: 'Bewältigt komplexe, mehrstufige Anfragen',

    // Vergleichs-Visualisierer
    compQuery: 'Anfrage',
    compEmbed: 'Einbetten',
    compRetrieve: 'Abrufen',
    compGenerate: 'Generieren',
    compThink: 'Denken',
    compTool: 'Tool',
    compEvaluate: 'Bewerten',
    compAnimate: 'Ablauf animieren',
    compAnimating: 'Animiert...',
    compAspect: 'Aspekt',
    compRowControl: 'Kontrollfluss',
    compControlTraditional: 'Feste Pipeline',
    compControlAgentic: 'LLM entscheidet',
    compRowRetrieval: 'Abruf',
    compRetrievalTraditional: 'Einmaliger Durchlauf',
    compRetrievalAgentic: 'Mehrere Iterationen',
    compRowQuery: 'Anfragebehandlung',
    compQueryTraditional: 'Unverändert verwendet',
    compQueryAgentic: 'Kann umformulieren',
    compRowLatency: 'Latenz',
    compLatencyTraditional: 'Schnell',
    compLatencyAgentic: 'Variabel',
    compRowBestFor: 'Ideal für',
    compBestForTraditional: 'Einfache Q&A, Faktenabruf',
    compBestForAgentic: 'Komplexes Reasoning, Multi-Hop',
    comparisonTitle: 'Traditionelles vs Agentic RAG',
    comparisonDesc: 'Zwei Ansätze für Retrieval-Augmented Generation mit unterschiedlichen Vor- und Nachteilen.',

    // Fallstudien-Visualisierer
    caseStudyTitle: 'Wann welcher Ansatz gewinnt (oder scheitert)',
    caseStudyDesc: 'Erkunde realistische Szenarien, um zu sehen, wann traditionelles RAG Agentic RAG übertrifft, wann das Gegenteil der Fall ist und wann keiner helfen kann.',
    caseTraditionalWins: 'Fall 1: Traditionelles RAG gewinnt',
    caseAgenticWins: 'Fall 2: Agentic RAG gewinnt',
    caseBothFail: 'Fall 3: Beide Ansätze scheitern',
    caseUserQuery: 'Benutzeranfrage',
    caseWhy: 'Warum dieses Ergebnis?',
    caseQuery1: 'Was ist die Rückgaberichtlinie des Unternehmens?',
    caseQuery2: 'Vergleiche unseren Q3 2024-Umsatz mit unserem Hauptkonkurrenten und erkläre die wichtigsten Unterschiede.',
    caseQuery3: 'Wie wird unser Aktienkurs im nächsten Quartal sein?',
    caseExplanation1: 'Für einfache Faktenanfragen ist traditionelles RAG effizienter. Die Antwort existiert in einem einzigen Dokument, sodass die direkte Abruf-dann-Generierung-Pipeline perfekt funktioniert. Agentic RAG kommt zur gleichen Antwort, aber mit unnötigem Overhead durch Planungs- und Bewertungsschritte—verschwendet Zeit und Tokens.',
    caseExplanation2: 'Komplexe Anfragen, die mehrere Qüllen erfordern, profitieren von Agentic RAG. Der Agent zerlegte die Anfrage in Teilfragen, rief aus verschiedenen Dokumentensets ab (interne Finanzdaten, Konkurrenzberichte) und synthetisierte einen kohärenten Vergleich. Traditionelles RAG rief nur Teilinformationen ab und konnte die Zusammenhänge nicht herstellen.',
    caseExplanation3: 'Keiner der Ansätze kann Fragen über zukünftige Ereignisse oder fehlende Informationen beantworten. Traditionelles RAG halluzinierte aus oberflächlich verwandten Inhalten. Agentic RAG suchte gründlicher und gab die Unsicherheit korrekt zu, konnte aber trotzdem nicht antworten—kein Abruf hilft, wenn die Daten nicht existieren.',
    outcomeSuccess: 'Erfolgreich',
    outcomePartial: 'Teilweise',
    outcomeFailure: 'Fehlgeschlagen',

    // Schrittbezeichnungen
    stepThinking: 'Denken',
    stepSearching: 'Suchen',
    stepRetrieved: 'Abgerufen',
    stepGenerating: 'Generieren',
    processSteps: 'Prozessschritte',
    retrievedDocs: 'Abgerufene Dokumente',
    finalResponse: 'Finale Antwort',
    noRelevantDocs: 'Keine relevanten Dokumente in der Wissensbasis gefunden',

    // Fall 1: Traditionelles RAG gewinnt - Einfache Faktenanfrage
    case1TradSearch: 'Einbettung der Anfrage: "Rückgaberichtlinie"',
    case1TradRetrieve: '1 hochrelevantes Dokument gefunden (Ähnlichkeit: 0.94)',
    case1TradGenerate: 'Generiere Antwort aus abgerufenem Kontext',
    case1TradDoc1: '[richtlinien/rückgabe.md] "Rückgaberichtlinie: Artikel können innerhalb von 30 Tagen nach Kauf mit Originalbeleg zurückgegeben werden. Erstattungen werden innerhalb von 5-7 Werktagen auf die ursprüngliche Zahlungsmethode verarbeitet. Elektronik muss ungeöffnet sein. Sale-Artikel sind vom Umtausch ausgeschlossen."',
    case1TradResponse: 'Artikel können innerhalb von 30 Tagen mit dem Originalbeleg zurückgegeben werden. Erstattungen werden innerhalb von 5-7 Werktagen auf Ihre ursprüngliche Zahlungsmethode verarbeitet. Beachten Sie, dass Elektronik ungeöffnet sein muss und Sale-Artikel vom Umtausch ausgeschlossen sind.',
    case1AgentThought1: 'Der Benutzer fragt nach der Rückgaberichtlinie. Ich sollte nach Richtlinien-Dokumenten suchen.',
    case1AgentSearch: 'Suche: "Rückgaberichtlinie Erstattung"',
    case1AgentRetrieve: '1 relevantes Dokument gefunden',
    case1AgentThought2: 'Dieses Dokument beantwortet die Frage vollständig. Keine weiteren Suchen nötig.',
    case1AgentGenerate: 'Synthesiere finale Antwort',
    case1AgentResponse: 'Artikel können innerhalb von 30 Tagen mit dem Originalbeleg zurückgegeben werden. Erstattungen werden innerhalb von 5-7 Werktagen auf Ihre ursprüngliche Zahlungsmethode verarbeitet. Beachten Sie, dass Elektronik ungeöffnet sein muss und Sale-Artikel vom Umtausch ausgeschlossen sind.',

    // Fall 2: Agentic RAG gewinnt - Komplexe Multi-Source-Anfrage
    case2TradSearch: 'Einbettung der Anfrage: "Q3 2024 Umsatz Konkurrenz Vergleich"',
    case2TradRetrieve: '1 teilweise relevantes Dokument gefunden (Ähnlichkeit: 0.71)',
    case2TradGenerate: 'Generiere Antwort aus begrenztem Kontext',
    case2TradDoc1: '[finanzen/q3-2024.md] "Q3 2024 Umsatz: 4,2 Mio. € (+12% YoY). Haupttreiber: Enterprise-Vertriebswachstum, neue Produkteinführungen. Operative Marge: 18%."',
    case2TradResponse: 'Unser Q3 2024 Umsatz betrug 4,2 Mio. €, ein Plus von 12% im Jahresvergleich, getrieben durch Enterprise-Vertrieb und neue Produkte. (Hinweis: Ich habe keine Konkurrenzdaten für einen Vergleich.)',
    case2AgentThought1: 'Dies ist eine Vergleichsanfrage. Ich benötige: 1) Unseren Q3 2024 Umsatz, 2) Konkurrenz Q3 2024 Umsatz, 3) Analyse der Unterschiede.',
    case2AgentSearch1: 'Suche: "Q3 2024 Umsatz Finanzergebnisse"',
    case2AgentRetrieve1: 'Internen Finanzbericht gefunden',
    case2AgentThought2: 'Unseren Umsatz habe ich. Jetzt brauche ich Konkurrenzdaten. Suche nach Wettbewerbsanalyse.',
    case2AgentSearch2: 'Suche: "Wettbewerber Marktanalyse 2024"',
    case2AgentRetrieve2: 'Wettbewerbsanalyse-Bericht gefunden',
    case2AgentThought3: 'Jetzt habe ich beide Datensätze. Unser Umsatz: 4,2 Mio. €, Konkurrenz: 3,8 Mio. €. Ich kann die Unterschiede analysieren.',
    case2AgentGenerate: 'Synthesiere Vergleich aus beiden Qüllen',
    case2AgentDoc1: '[finanzen/q3-2024.md] "Q3 2024 Umsatz: 4,2 Mio. € (+12% YoY). Haupttreiber: Enterprise-Vertriebswachstum, neue Produkteinführungen. Operative Marge: 18%."',
    case2AgentDoc2: '[research/wettbewerb.md] "Konkurrent X Q3 2024: Umsatz 3,8 Mio. € (+5% YoY). Herausforderungen: Lieferkettenprobleme, langsamere SMB-Adoption. Marktanteil: rückläufig im Enterprise-Segment."',
    case2AgentResponse: 'Unser Q3 2024 Umsatz (4,2 Mio. €) übertraf Konkurrent X (3,8 Mio. €) um 10,5%. Hauptunterschiede: Wir wuchsen 12% YoY vs. deren 5%, getrieben durch unsere Enterprise-Stärke, wo sie Marktanteile verlieren. Deren Lieferkettenprobleme und schwächere SMB-Adoption trugen zum langsameren Wachstum bei. Unsere operative Marge von 18% deutet auf gesündere Stückkosten hin.',

    // Fall 3: Beide scheitern - Zukunftsprognose / fehlende Daten
    case3TradSearch: 'Einbettung der Anfrage: "Aktienkurs nächstes Quartal Prognose"',
    case3TradRetrieve: '1 lose verwandtes Dokument gefunden (Ähnlichkeit: 0.52)',
    case3TradGenerate: 'Generiere Antwort aus tangential verwandtem Inhalt',
    case3TradDoc1: '[finanzen/aktien-historie.md] "Historische Aktienperformance: Q1 2024: 45€, Q2 2024: 52€, Q3 2024: 48€. Analysten-Konsens: Halten-Empfehlung."',
    case3TradResponse: 'Basierend auf aktüllen Trends schwankte der Aktienkurs zwischen 45-52€. Angesichts der Halten-Empfehlung und der jüngsten Performance könnte der Kurs nächstes Quartal um 50€ liegen. (Warnung: Dies ist Spekulation, keine Finanzberatung.)',
    case3AgentThought1: 'Benutzer möchte Aktienkurs-Prognose. Benötigt: 1) Aktülle Finanzdaten, 2) Marktprognosen, 3) Analysten-Vorhersagen.',
    case3AgentSearch1: 'Suche: "Aktienprognose Q1 2025 Vorhersage"',
    case3AgentRetrieve1: 'Keine relevanten Dokumente gefunden',
    case3AgentThought2: 'Keine Prognosen in der Wissensbasis. Versuche Analystenberichte.',
    case3AgentSearch2: 'Suche: "Analysten Kursziel Forward Guidance"',
    case3AgentRetrieve2: 'Keine relevanten Dokumente gefunden',
    case3AgentThought3: 'Ich kann keine zukunftsgerichteten Daten finden. Die Wissensbasis enthält nur historische Informationen. Ich sollte nicht spekulieren.',
    case3AgentGenerate: 'Generiere ehrliche Unsicherheits-Antwort',
    case3AgentResponse: 'Ich kann keine zukünftigen Aktienkurse vorhersagen. Meine Wissensbasis enthält historische Daten (Q3 2024: 48€, Halten-Empfehlung), aber keine zukunftsgerichteten Prognosen oder Analysten-Kursziele. Aktienvorhersagen erfordern Echtzeit-Marktdaten und Analysen, zu denen ich keinen Zugang habe.',

    // Fortgeschrittene Techniken (2025)
    advancedTechniques: 'Fortgeschrittene RAG-Techniken',
    advancedTechniquesDesc: 'Über das grundlegende RAG hinaus verwenden moderne Systeme ausgefeilte Techniken, um die Abrufqualität, Antwortgenauigkeit und den Umgang mit komplexen Anfragen zu verbessern. Diese 2025-Ansätze repräsentieren den Stand der Technik.',

    // Self-RAG
    selfRag: 'Self-RAG',
    selfRagDesc: 'Self-RAG führt Selbstreflexion in den Abrufprozess ein. Anstatt immer abzurufen, entscheidet das Modell, wann ein Abruf erforderlich ist, und bewertet abgerufene Inhalte kritisch vor der Verwendung.',
    selfRagHow: 'Wie Self-RAG funktioniert',
    selfRagHowDesc: 'Das Modell generiert spezielle Reflexions-Tokens während der Inferenz: [Retrieve] um zu entscheiden, ob ein Abruf nötig ist, [IsRel] um die Relevanz abgerufener Passagen zu bewerten, [IsSup] um zu überprüfen, ob die Antwort vom Kontext unterstutzt wird, und [IsUse] um den Gesamtnutzen zu bewerten.',
    selfRagRetrieve: 'Abrufentscheidung',
    selfRagRetrieveDesc: 'Das Modell entscheidet, ob die Anfrage externes Wissen benötigt oder aus dem parametrischen Gedächtnis allein beantwortet werden kann.',
    selfRagCritique: 'Selbstkritik',
    selfRagCritiqueDesc: 'Abgerufene Passagen werden auf Relevanz bewertet. Irrelevante oder qualitativ schlechte Ergebnisse werden vor der Generierung gefiltert.',
    selfRagGenerate: 'Fundierte Generierung',
    selfRagGenerateDesc: 'Die Antwort wird mit expliziten Fundierungsprüfungen generiert. Das Modell überprüft, ob Aussagen vom abgerufenen Kontext unterstützt werden.',

    // GraphRAG
    graphRag: 'GraphRAG',
    graphRagDesc: 'GraphRAG kombiniert Vektor-Ähnlichkeitssuche mit Wissensgraph-Traversierung. Es erstellt einen Graphen von Entitäten und Beziehungen aus Ihren Dokumenten und ermöglicht sowohl semantische Suche als auch strukturiertes Reasoning.',
    graphRagVector: 'Vektorsuche-Schicht',
    graphRagVectorDesc: 'Traditionelle semantische Suche findet relevante Dokumentenchunks. Dies behandelt den "was ist meiner Anfrage ähnlich"-Teil des Abrufs.',
    graphRagGraph: 'Wissensgraph-Schicht',
    graphRagGraphDesc: 'Entitäten und Beziehungen werden extrahiert und verknüpft. Ermöglicht Multi-Hop-Reasoning wie "Finde alle Produkte, die von Unternehmen erwähnt wurden, die mit X kooperiert haben".',
    graphRagBenefits: 'Hauptvorteile',
    graphRagBenefit1: 'Bessere Handhabung von Fragen, die Beziehungs-Reasoning erfordern',
    graphRagBenefit2: 'Verbesserte Genauigkeit für Multi-Entitäts-Anfragen',
    graphRagBenefit3: 'Ermöglicht globale Zusammenfassung über ganze Dokumentensammlungen',

    // Anfrageerweiterung
    queryAugmentation: 'Anfrageerweiterung',
    queryAugmentationDesc: 'Benutzeranfragen sind oft unvollständig oder schlecht für den Abruf formuliert. Anfrageerweiterungstechniken transformieren Anfragen vor der Suche, um die Abrufqualität zu verbessern.',
    queryHyde: 'HyDE (Hypothetische Dokument-Einbettungen)',
    queryHydeDesc: 'Generiere zuerst eine hypothetische Antwort, dann verwende die Einbettung dieser Antwort fur den Abruf. Dies überbrückt die Lücke zwischen Frage- und Dokument-Einbettungsraumen.',
    queryHydeExample: 'Anfrage: "Klimawandel Auswirkungen" -> Hypothetisches Dok. generieren -> Das einbetten -> Suchen',
    queryDecomposition: 'Anfrage-Zerlegung',
    queryDecompositionDesc: 'Zerlege komplexe Anfragen in einfachere Teilanfragen. Jede Teilanfrage ruft unabhängig ab, dann werden die Ergebnisse kombiniert.',
    queryDecompositionExample: '"Vergleiche A mit B" -> "Was ist A?" + "Was ist B?" -> Ergebnisse zusammenführen',
    queryExpansion: 'Anfrage-Expansion',
    queryExpansionDesc: 'Füge Synonyme, verwandte Begriffe oder Umformulierungen zur ursprunglichen Anfrage hinzu. Erhöht den Recall durch Matching von Dokumenten mit unterschiedlicher Terminologie.',
    queryRewrite: 'Anfrage-Umschreibung',
    queryRewriteDesc: 'Verwende ein LLM, um mehrdeutige oder umgangssprachliche Anfragen in klare, suchoptimierte Formen umzuschreiben. Behandelt Pronomen, Kontext und implizite Referenzen.',

    // RAG-Evaluierung
    evaluation: 'RAG-Evaluierung',
    evaluationDesc: 'Die Messung der RAG-Systemqualität erfordert spezialisierte Metriken, die sowohl Abruf als auch Generierung bewerten. RAGAS (Retrieval Augmented Generation Assessment) bietet ein Standard-Framework.',
    ragasFramework: 'RAGAS-Framework',
    ragasFrameworkDesc: 'RAGAS verwendet LLM-basierte Evaluierung, um RAG-Systeme zu bewerten, ohne Ground-Truth-Labels fur jede Frage zu benötigen. Es misst mehrere Qualitätsdimensionen.',
    metricFaithfulness: 'Treue (Faithfulness)',
    metricFaithfulnessDesc: 'Enthalt die Antwort nur Informationen aus dem abgerufenen Kontext? Misst Halluzination—Behauptungen, die nicht von den bereitgestellten Dokumenten unterstützt werden.',
    metricRelevance: 'Antwortrelevanz',
    metricRelevanceDesc: 'Adressiert die Antwort tatsächlich die gestellte Frage? Eine treue Antwort kann immer noch irrelevant sein, wenn sie das Thema verfehlt.',
    metricContextRecall: 'Kontext-Recall',
    metricContextRecallDesc: 'Hat der Abruf alle benötigten Informationen für die Antwort gefunden? Misst, ob relevante Passagen übersehen wurden.',
    metricContextPrecision: 'Kontext-Precision',
    metricContextPrecisionDesc: 'Sind die abgerufenen Passagen tatsächlich relevant? Hohe Precision bedeutet weniger Rauschen im Kontext, was Verwirrung reduziert.',
    evaluationTips: 'Best Practices für die Evaluierung',
    evaluationTip1: 'Erstelle ein vielfältiges Testset, das verschiedene Anfragetypen und Schwierigkeitsgrade abdeckt',
    evaluationTip2: 'Verfolge Metriken uber die Zeit, während du Chunking, Einbettungen und Prompts iterierst',
    evaluationTip3: 'Kombiniere automatisierte Metriken mit menschlicher Bewertung fur nuancierte Qualitätsbeurteilung',


    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'RAG ruft relevante Dokumente ab und fügt sie in den Prompt ein',
    takeaway2: 'Es reduziert Halluzinationen, indem Antworten in echten Qüllen verankert werden',
    takeaway3: 'Chunking-Strategie und Einbettungsqualität sind entscheidend für guten Abruf',
    takeaway4: 'RAG ist oft dem Fine-Tuning vorzuziehen, um Domänenwissen hinzuzufügen',
    takeaway5: 'Fortgeschrittene Techniken wie Self-RAG und GraphRAG verbessern die Genauigkeit bei komplexen Anfragen',
    takeaway6: 'Verwende RAGAS-Metriken, um deine RAG-Pipeline systematisch zu evaluieren und zu verbessern',
  },

  // Phase 2: Agent Topics
  toolDesign: {
    title: 'Tool-Design',
    description: 'Best Practices für das Design effektiver Tools, die KI-Agenten zuverlässig nutzen können.',
    whatIs: 'Was macht ein gutes Tool aus?',
    whatIsDesc: 'Gut gestaltete Tools sind die Grundlage fähiger KI-Agenten. Schema, Benennung und Dokumentation eines Tools beeinflussen direkt, wie zuverlässig ein LLM es nutzen kann.',
    principles: 'Design-Prinzipien',
    principlesDesc: 'Befolge diese Prinzipien, um Tools zu erstellen, die Agenten effektiv nutzen können.',
    principle1: 'Klare Benennung',
    principle1Desc: 'Verwende beschreibende, eindeutige Namen. "search_web" ist besser als "sw" oder "query".',
    principle2: 'Explizite Parameter',
    principle2Desc: 'Jeder Parameter sollte einen klaren Typ, eine Beschreibung und Einschränkungen haben.',
    principle3: 'Vorhersehbare Ausgaben',
    principle3Desc: 'Gib konsistente, strukturierte Antworten zurück. Fehlermeldungen in die Ausgabe einbeziehen.',
    principle4: 'Minimaler Umfang',
    principle4Desc: 'Jedes Tool sollte eine Sache gut machen. Bevorzuge viele fokussierte Tools statt weniger komplexer.',
    schemaDesign: 'Schema-Design',
    schemaDesignDesc: 'Tool-Schemas sagen dem LLM, wie es deine Tools verwenden soll. Gute Schemas verhindern Fehler.',
    goodSchema: 'Gutes Schema',
    badSchema: 'Schlechtes Schema',
    errorHandling: 'Fehlerbehandlung',
    errorHandlingDesc: 'Tools sollten Fehler elegant behandeln und informative Meldungen zurückgeben, auf die das LLM reagieren kann.',
    interactiveDemo: 'Tool-Schema-Builder',
    demoDesc: 'Erstelle und validiere Tool-Schemas interaktiv',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Tool-Design beeinflusst direkt die Agenten-Zuverlässigkeit',
    takeaway2: 'Explizite Schemas mit Beschreibungen verhindern LLM-Verwirrung',
    takeaway3: 'Gib strukturierte Fehler zurück, die der Agent verstehen und darauf reagieren kann',
    takeaway4: 'Teste Tools mit verschiedenen Eingaben, um Randfälle zu finden',
  },

  memorySystems: {
    title: 'Speichersysteme',
    description: 'Wie KI-Agenten Kontext aufrechterhalten und Informationen über Interaktionen hinweg speichern.',
    whatIs: 'Was sind Agenten-Speichersysteme?',
    whatIsDesc: 'Speichersysteme ermöglichen es Agenten, Informationen über das unmittelbare Kontextfenster hinaus zu behalten und abzurufen. Sie ermöglichen Agenten, aus vergangenen Interaktionen zu lernen und kohärentes Langzeitverhalten aufrechtzuerhalten.',
    types: 'Arten von Speicher',
    typesDesc: 'Agenten-Speichersysteme kombinieren typischerweise mehrere Speichertypen für verschiedene Zwecke.',
    shortTerm: 'Kurzzeitgedächtnis',
    shortTermDesc: 'Der aktülle Gesprächskontext. Begrenzt durch die Kontextfenstergröße.',
    longTerm: 'Langzeitgedächtnis',
    longTermDesc: 'Dauerhafte Speicherung vergangener Interaktionen, Fakten und gelernter Präferenzen.',
    episodic: 'Episodisches Gedächtnis',
    episodicDesc: 'Spezifische vergangene Ereignisse und Interaktionen, die abgerufen werden können.',
    semantic: 'Semantisches Gedächtnis',
    semanticDesc: 'Allgemeines Wissen und Fakten, die aus Erfahrungen extrahiert wurden.',
    implementation: 'Implementierungsansätze',
    implementationDesc: 'Verschiedene Techniken zur Implementierung von Agenten-Speicher.',
    vectorStore: 'Vektorspeicher',
    vectorStoreDesc: 'Einbettungen vergangener Interaktionen für semantischen Abruf speichern.',
    summaries: 'Gesprächszusammenfassungen',
    summariesDesc: 'Lange Gespräche periodisch zusammenfassen, um wichtige Informationen zu erhalten.',
    keyValue: 'Schlüssel-Wert-Speicher',
    keyValueDesc: 'Explizite Fakten und Benutzerpräferenzen für direkten Abruf speichern.',
    interactiveDemo: 'Speichersystem-Visualisierer',
    demoDesc: 'Sieh, wie verschiedene Speichertypen zusammenarbeiten',

    // Hybrid Memory (2025 Pattern)
    hybridMemory: 'Hybride Speichermuster',
    hybridMemoryDesc: 'Moderne Agenten kombinieren episodisches und semantisches Gedächtnis für menschenähnliche Erinnerung. Das MemGPT-Muster und ähnliche Architekturen behandeln Speicher als erstklassige Ressource, die der Agent aktiv verwaltet.',
    memgptPattern: 'MemGPT-Architektur',
    memgptPatternDesc: 'Agenten mit expliziter Speicherverwaltung – Daten zwischen schnellem Kontext und langsamem Speicher verschieben, wie ein Betriebssystem RAM und Festplatte verwaltet.',
    tieredMemory: 'Gestufter Speicher',
    tieredMemoryDesc: 'Hot (Kontext), Warm (Vektor-Cache) und Cold (Archiv) Stufen mit automatischer Beförderung und Herabstufung basierend auf Zugriffsmustern.',
    selfEditing: 'Selbstbearbeitender Speicher',
    selfEditingDesc: 'Agenten, die ihre eigenen Erinnerungen aktualisieren, konsolidieren und umstrukturieren können, anstatt nur anzuhängen.',
    dualEncoder: 'Dual-Encoder-Abruf',
    dualEncoderDesc: 'Separate Encoder für Abfragen und Erinnerungen ermöglichen asymmetrischen Abruf, der für jede Richtung optimiert ist.',

    // Temporal Knowledge Graphs
    temporalGraphs: 'Temporale Wissensgraphen',
    temporalGraphsDesc: 'Speichere Erinnerungen mit expliziten Zeitbeziehungen, um Abfragen wie "was haben wir letzte Woche besprochen?" zu ermöglichen und Wissensdrift über Zeit zu erkennen.',
    entityRelations: 'Entitäts-Beziehungs-Tracking',
    entityRelationsDesc: 'Extrahiere Entitäten (Personen, Projekte, Konzepte) und ihre Beziehungen aus Gesprächen und baue einen abfragbaren Wissensgraphen.',
    timeWeighted: 'Zeitgewichteter Abruf',
    timeWeightedDesc: 'Kombiniere semantische Ähnlichkeit mit Aktualität, Wichtigkeit und Zugriffshäufigkeit für relevantere Erinnerung.',
    zepApproach: 'ZEP-Stil-Speicher',
    zepApproachDesc: 'Automatische Extraktion von Fakten, Entitäten und temporalen Beziehungen mit bi-temporaler Modellierung (wann etwas passierte vs. wann es aufgezeichnet wurde).',

    // Memory Management
    memoryManagement: 'Speicherverwaltung',
    memoryManagementDesc: 'Produktionsspeichersysteme erfordern aktive Verwaltung, um innerhalb von Token-Budgets zu bleiben und gleichzeitig wertvolle Informationen zu erhalten.',
    deduplication: 'Deduplizierung',
    deduplicationDesc: 'Erkenne und vereinige semantisch ähnliche Erinnerungen, um Aufblähung zu verhindern. Verwende Embedding-Ähnlichkeitsschwellen oder LLM-basierten Vergleich.',
    tokenBudgets: 'Token-Budgets',
    tokenBudgetsDesc: 'Weise verschiedenen Speichertypen feste Token-Anzahlen zu. Bei Überschreitung des Budgets, komprimiere oder entferne Elemente mit niedrigster Priorität.',
    garbageCollection: 'Garbage Collection',
    garbageCollectionDesc: 'Scanne Erinnerungen periodisch auf veraltete, redundante oder wenig wertvolle Einträge. LRU-, LFU- oder wichtigkeitsgewichtete Eviction-Strategien.',
    priorityRules: 'Prioritätsregeln',
    priorityRulesDesc: 'Definiere, welche Erinnerungen am wichtigsten sind: Benutzerpräferenzen, Aufgabenkontext, kürzliche Interaktionen oder explizit fixierte Fakten.',

    // Adaptive Retention
    adaptiveRetention: 'Adaptive Aufbewahrung',
    adaptiveRetentionDesc: 'Intelligente Strategien für was behalten, zusammenfassen oder vergessen werden soll – nachahmen wie menschliches Gedächtnis natürlich verfällt und konsolidiert.',
    contextSummarization: 'Kontextzusammenfassung',
    contextSummarizationDesc: 'Progressive Zusammenfassung: volle Details für aktüllen Kontext, Zusammenfassungen für ältere Gespräche, nur Schlüsselfakten für entfernte Vergangenheit.',
    entityExtraction: 'Entitätsextraktion',
    entityExtractionDesc: 'Identifiziere und speichere automatisch wichtige Entitäten (Namen, Präferenzen, Entscheidungen) getrennt von rohen Gesprächsprotokollen.',
    decayStrategies: 'Verfallsstrategien',
    decayStrategiesDesc: 'Exponentielle oder logarithmische Verfallsfunktionen reduzieren die Wichtigkeit von Erinnerungen über Zeit, es sei denn, sie werden durch Zugriff oder explizite Wichtigkeitsmarkierungen verstärkt.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Speicher erweitert die Agentenfähigkeiten über das Kontextfenster hinaus',
    takeaway2: 'Kombiniere mehrere Speichertypen für beste Ergebnisse',
    takeaway3: 'Speicherabruf fügt Latenz hinzu – balance Vollständigkeit mit Geschwindigkeit',
    takeaway4: 'Berücksichtige Datenschutz und Datenspeicherung beim Speichern von Erinnerungen',
    takeaway5: 'Hybridmuster wie MemGPT ermöglichen Agenten, ihren eigenen Speicher wie ein Betriebssystem zu verwalten',
    takeaway6: 'Temporale Wissensgraphen fügen Zeitbewusstsein für kontextülleren Abruf hinzu',
  },

  orchestration: {
    title: 'Orchestrierung',
    description: 'Koordination mehrerer Agenten und komplexer mehrstufiger Workflows.',
    whatIs: 'Was ist Agenten-Orchestrierung?',
    whatIsDesc: 'Orchestrierung ist die Koordination mehrerer KI-Agenten oder komplexer mehrstufiger Workflows. Sie umfasst das Routing von Aufgaben, Zustandsverwaltung, Fehlerbehandlung und das Kombinieren von Agenten-Ausgaben.',
    patterns: 'Orchestrierungsmuster',
    patternsDesc: 'Gängige Muster für die Strukturierung von Multi-Agenten-Systemen.',
    sequential: 'Seqünzielle Pipeline',
    sequentialDesc: 'Agenten laufen der Reihe nach, jeder verarbeitet die Ausgabe des vorherigen.',
    parallel: 'Parallele Ausführung',
    parallelDesc: 'Mehrere Agenten arbeiten gleichzeitig an verschiedenen Aspekten einer Aufgabe.',
    hierarchical: 'Hierarchisch',
    hierarchicalDesc: 'Ein Supervisor-Agent delegiert an spezialisierte Worker-Agenten.',
    dynamic: 'Dynamisches Routing',
    dynamicDesc: 'Ein LLM entscheidet, welcher Agent jede Anfrage bearbeiten soll.',
    stateManagement: 'Zustandsverwaltung',
    stateManagementDesc: 'Orchestratoren müssen Fortschritt, Zwischenergebnisse verfolgen und Fehler behandeln.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Zustand an wichtigen Punkten speichern, um Wiederherstellung bei Fehlern zu ermöglichen.',
    rollback: 'Rollback',
    rollbackDesc: 'Fähigkeit, Schritte rückgängig zu machen, wenn Fehler auftreten.',
    interactiveDemo: 'Workflow-Visualisierer',
    demoDesc: 'Agenten-Workflows entwerfen und visualisieren',
    // Visualizer UI
    vizRun: 'Start',
    vizPause: 'Pause',
    vizReset: 'Zurücksetzen',
    vizComplete: 'Fertig',
    vizProgress: 'Fortschritt',
    vizStepOf: 'von',
    vizPending: 'bereit',
    vizRunning: 'aktiv',
    vizCompleted: 'fertig',
    vizSequentialLabel: 'Seqünziell',
    vizSequentialSummary: 'Schritt für Schritt',
    vizSequentialDetail: 'Jeder Knoten hängt vom vorherigen Ergebnis ab. Dies maximiert Kontrolle und Nachvollziehbarkeit auf Kosten der Geschwindigkeit.',
    vizParallelLabel: 'Parallel',
    vizParallelSummary: 'Auffächern, dann zusammenführen',
    vizParallelDetail: 'Ein Koordinator verteilt unabhängige Aufgaben, dann kombiniert ein Aggregator die Ergebnisse.',
    vizHierarchicalLabel: 'Hierarchisch',
    vizHierarchicalSummary: 'Schichtweise delegieren',
    vizHierarchicalDetail: 'Ein Supervisor leitet Arbeit an Sub-Agenten weiter, die spezialisierte Werkzeuge darunter orchestrieren.',
    vizHandoffLabel: 'Übergabe',
    vizHandoffSummary: 'Staffelstab weitergeben',
    vizHandoffDetail: 'Agenten übergeben die Kontrolle an den nächsten Spezialisten, wenn sie ihren Teil abgeschlossen haben, und bewahren dabei den Kontext.',
    vizNodePlanner: 'Planer',
    vizNodeSearch: 'Suche',
    vizNodeAnalyzer: 'Analysierer',
    vizNodeSummarize: 'Zusammenfassung',
    vizNodeCoordinator: 'Koordinator',
    vizNodeTaskA: 'Aufgabe A',
    vizNodeTaskB: 'Aufgabe B',
    vizNodeTaskC: 'Aufgabe C',
    vizNodeAggregator: 'Aggregator',
    vizNodeSupervisor: 'Supervisor',
    vizNodeWorkerA: 'Worker A',
    vizNodeWorkerB: 'Worker B',
    vizNodeTool1: 'Werkzeug 1',
    vizNodeTool2: 'Werkzeug 2',
    vizNodeTool3: 'Werkzeug 3',
    vizNodeTool4: 'Werkzeug 4',
    vizNodeTriage: 'Triage',
    vizNodeBilling: 'Abrechnung',
    vizNodeSupport: 'Support',
    vizNodeResolve: 'Lösung',
    vizAgent: 'Agent',
    vizTool: 'Werkzeug',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Orchestrierung ermöglicht komplexe Aufgaben durch Agenten-Komposition',
    takeaway2: 'Wähle Muster basierend auf Aufgabenabhängigkeiten und Parallelität',
    takeaway3: 'Robuste Zustandsverwaltung ist essentiell für Zuverlässigkeit',
    takeaway4: 'Überwache Orchestrierungskosten – Multi-Agenten-Systeme vervielfachen API-Aufrufe',
    // 2025 Multi-Agenten-Muster
    multiAgentPatterns: '2025 Multi-Agenten-Muster',
    multiAgentPatternsDesc: 'Diese aufkommenden Muster definieren, wie moderne KI-Agenten im Unternehmensmaßstab zusammenarbeiten.',
    enterpriseAdoption: '72% der Enterprise-KI-Projekte nutzen jetzt Multi-Agenten-Systeme',
    enterpriseAdoptionSource: 'Gartner 2025',
    supervisorPattern: 'Supervisor-Muster',
    supervisorPatternDesc: 'Ein Koordinator-Agent, der Aufgaben an spezialisierte Worker-Agenten verwaltet und delegiert. Der Supervisor behält das Gesamtziel bei, zerlegt komplexe Aufgaben und synthetisiert Worker-Ausgaben.',
    supervisorBenefits: 'Vorteile: Klare Hierarchie, zentralisierte Entscheidungsfindung, einfacheres Debugging',
    orchestratorWorker: 'Orchestrator-Worker-Muster',
    orchestratorWorkerDesc: 'Ein zentraler Orchestrator verwaltet einen Pool von Worker-Agenten. Worker sind zustandslos und können dynamisch skaliert werden. Der Orchestrator übernimmt Aufgaben-Queuing, Lastverteilung und Ergebnisaggregation.',
    orchestratorWorkerBenefits: 'Vorteile: Skalierbarkeit, Fehlertoleranz, Ressourceneffizienz',
    handoffMechanisms: 'Übergabe-Mechanismen',
    handoffMechanismsDesc: 'Wie Agenten die Kontrolle untereinander übertragen. Populär durch OpenAI Agents SDK ermöglichen Handoffs nahtlose Übergänge zwischen spezialisierten Agenten unter Beibehaltung des Gesprächskontexts.',
    handoffExplicit: 'Explizite Übergabe',
    handoffExplicitDesc: 'Agent ruft direkt Transferfunktion mit Ziel-Agent und Kontext auf.',
    handoffCondition: 'Bedingungsbasiert',
    handoffConditionDesc: 'Automatische Übertragung bei Erfüllung bestimmter Bedingungen (z.B. Themenerkennung).',
    handoffEscalation: 'Eskalation',
    handoffEscalationDesc: 'Agent übergibt an fähigeren Agenten, wenn Aufgabe seinen Rahmen übersteigt.',
    groupChatPattern: 'Gruppenchat-Muster',
    groupChatPatternDesc: 'Mehrere Agenten arbeiten durch eine strukturierte Konversation an einem gemeinsamen Problem zusammen. Jeder Agent bringt seine Expertise ein, während ein Moderator Sprechreihenfolge und Konsens verwaltet.',
    groupChatRoles: 'Typische Rollen',
    groupChatModerator: 'Moderator: Kontrolliert Ablauf, fasst Fortschritt zusammen, löst Konflikte',
    groupChatExpert: 'Experten: Domänenspezifische Agenten mit spezialisiertem Wissen',
    groupChatCritic: 'Kritiker: Überprüft und hinterfragt Vorschläge zur Qualitätsverbesserung',
    groupChatExecutor: 'Ausführer: Implementiert die vereinbarten Entscheidungen',
    patternComparison: 'Mustervergleich',
    patternComparisonDesc: 'Die Wahl des richtigen Musters hängt vom Anwendungsfall ab.',
    comparisonComplexity: 'Komplexität',
    comparisonScalability: 'Skalierbarkeit',
    comparisonUseCase: 'Ideal für',
    supervisorComplexity: 'Mittel',
    supervisorScalability: 'Mittel',
    supervisorUseCase: 'Strukturierte Workflows, klare Aufgabenzerlegung',
    orchestratorComplexity: 'Hoch',
    orchestratorScalability: 'Hoch',
    orchestratorUseCase: 'Hochvolumen-Verarbeitung, dynamische Arbeitslasten',
    handoffComplexity: 'Niedrig',
    handoffScalability: 'Niedrig',
    handoffUseCase: 'Kundenservice, spezialisiertes Routing',
    groupChatComplexity: 'Hoch',
    groupChatScalability: 'Mittel',
    groupChatUseCase: 'Kreative Aufgaben, komplexe Problemlösung',
  },

  evaluation: {
    title: 'Evaluierung',
    description: 'Systematische Messung und Verbesserung der KI-Agenten-Leistung.',
    whatIs: 'Warum Agenten evaluieren?',
    whatIsDesc: 'Agenten-Evaluierung ist entscheidend für das Verständnis der Leistung, das Erkennen von Regressionen und die Verbesserung der Zuverlässigkeit. Ohne Messung fliegst du blind.',
    metrics: 'Wichtige Metriken',
    metricsDesc: 'Wichtige Metriken für Agentensysteme.',
    taskSuccess: 'Aufgaben-Erfolgsrate',
    taskSuccessDesc: 'Prozentsatz der korrekt abgeschlossenen Aufgaben.',
    efficiency: 'Effizienz',
    efficiencyDesc: 'Unternommene Schritte, verwendete Tokens, verstrichene Zeit pro Aufgabe.',
    accuracy: 'Genauigkeit',
    accuracyDesc: 'Korrektheit der Agenten-Ausgaben und -Entscheidungen.',
    reliability: 'Zuverlässigkeit',
    reliabilityDesc: 'Konsistenz über wiederholte Durchläufe derselben Aufgabe.',
    approaches: 'Evaluierungsansätze',
    approachesDesc: 'Verschiedene Wege zur Evaluierung der Agentenleistung.',
    unitTests: 'Unit-Tests',
    unitTestsDesc: 'Einzelne Tools und Komponenten isoliert testen.',
    integration: 'Integrationstests',
    integrationDesc: 'Die vollständige Agentenschleife mit Mock-Umgebungen testen.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard-Aufgabensammlungen zum Vergleich von Agenten.',
    humanEval: 'Menschliche Bewertung',
    humanEvalDesc: 'Expertenüberprüfung für nuancierte Qualitätsbewertung.',
    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Richtlinien für effektive Agenten-Evaluierung.',
    practice1: 'Teste Randfälle und Fehlermodi, nicht nur Happy Paths.',
    practice2: 'Verfolge Kosten neben Qualitätsmetriken.',
    practice3: 'Verwende versionierte Evaluierungen, um Regressionen zu erkennen.',
    practice4: 'Schließe adversarielle Tests für Sicherheit ein.',

    // Benchmarks Section
    benchmarksSection: 'Gängige LLM-Benchmarks',
    benchmarksSectionDesc: 'Standard-Benchmarks zur Bewertung und zum Vergleich von Sprachmodell-Fähigkeiten über verschiedene Aufgaben.',
    benchmarkMmlu: 'MMLU',
    benchmarkMmluDesc: 'Massive Multitask Language Understanding - 57 Fächer von MINT bis Geisteswissenschaften. Testet breites Wissen.',
    benchmarkHellaswag: 'HellaSwag',
    benchmarkHellaswagDesc: 'Alltagsverständnis über alltägliche Situationen. Testet Verständnis der physischen Welt.',
    benchmarkHumaneval: 'HumanEval',
    benchmarkHumanevalDesc: 'Code-Generierungs-Benchmark mit 164 Programmieraufgaben. Testet Programmierfähigkeit.',
    benchmarkGsm8k: 'GSM8K',
    benchmarkGsm8kDesc: 'Mathematische Textaufgaben auf Grundschulniveau. Testet mehrstufiges mathematisches Denken.',
    benchmarkArc: 'ARC',
    benchmarkArcDesc: 'AI2 Reasoning Challenge - Wissenschaftsfragen, die Denken jenseits von Mustererkennung erfordern.',
    benchmarkMath: 'MATH',
    benchmarkMathDesc: 'Mathematikaufgaben auf Wettbewerbsniveau. Testet fortgeschrittenes mathematisches Denken.',
    benchmarkCaveats: 'Benchmark-Vorbehalte',
    benchmarkCaveat1: 'Benchmarks können manipuliert werden - Modelle könnten auf Testdaten trainiert sein',
    benchmarkCaveat2: 'Hohe Punktzahlen garantieren keine reale Leistung',
    benchmarkCaveat3: 'Viele Benchmarks sind gesättigt - Top-Modelle punkten ähnlich',
    benchmarkCaveat4: 'Benchmarks übersehen oft wichtige Fähigkeiten wie Anweisungsbefolgung',

    // LLM as a Judge
    llmJudge: 'LLM-als-Richter',
    llmJudgeDesc: 'Verwendung von Sprachmodellen zur Bewertung anderer Modellausgaben - ein skalierbarer aber unvollkommener Ansatz.',
    llmJudgeWhat: 'Wie es funktioniert',
    llmJudgeWhatDesc: 'Ein leistungsfähiges LLM (der "Richter") wird aufgefordert, Ausgaben eines anderen Modells zu bewerten. Der Richter bewertet Antworten nach Kriterien wie Hilfsbereitschaft, Genauigkeit und Sicherheit.',
    llmJudgeAdvantages: 'Vorteile',
    llmJudgeAdv1: 'Skalierbar',
    llmJudgeAdv1Desc: 'Kann Tausende von Ausgaben schnell ohne menschliche Annotatoren bewerten.',
    llmJudgeAdv2: 'Konsistent',
    llmJudgeAdv2Desc: 'Gleiche Kriterien werden einheitlich angewendet (anders als bei menschlicher Ermüdung/Variation).',
    llmJudgeAdv3: 'Kosteneffektiv',
    llmJudgeAdv3Desc: 'Viel günstiger als die Einstellung menschlicher Bewerter im großen Maßstab.',
    llmJudgeAdv4: 'Flexibel',
    llmJudgeAdv4Desc: 'Bewertungskriterien lassen sich einfach durch Ändern des Prompts anpassen.',
    llmJudgeProblems: 'Probleme & Verzerrungen',
    llmJudgeProb1: 'Selbstpräferenz-Verzerrung',
    llmJudgeProb1Desc: 'Modelle bevorzugen tendenziell Ausgaben, die dem ähneln, was sie selbst generieren würden.',
    llmJudgeProb2: 'Positions-Verzerrung',
    llmJudgeProb2Desc: 'Richter bevorzugen möglicherweise die erste oder letzte Option unabhängig von der Qualität.',
    llmJudgeProb3: 'Ausführlichkeits-Verzerrung',
    llmJudgeProb3Desc: 'Längere Antworten werden oft höher bewertet, selbst wenn sie weniger genau sind.',
    llmJudgeProb4: 'Stil über Substanz',
    llmJudgeProb4Desc: 'Gut formatierte falsche Antworten können schlecht formatierte richtige schlagen.',
    llmJudgeProb5: 'Fähigkeitsobergrenze',
    llmJudgeProb5Desc: 'Der Richter kann Ausgaben jenseits seines eigenen Fähigkeitsniveaus nicht zuverlässig bewerten.',
    llmJudgeBestPractices: 'Best Practices für LLM-Richter',
    llmJudgePractice1: 'Verwende das leistungsfähigste verfügbare Modell als Richter',
    llmJudgePractice2: 'Randomisiere die Optionsreihenfolge um Positionsverzerrung zu mindern',
    llmJudgePractice3: 'Fordere Begründung vor Bewertungen an (Chain-of-Thought)',
    llmJudgePractice4: 'Validiere gegen menschliche Urteile bei einer Teilmenge',
    llmJudgePractice5: 'Verwende mehrere Richter und aggregiere die Bewertungen',

    // CLASSIC Framework
    classicFramework: 'CLASSIC-Framework',
    classicFrameworkDesc: 'Ein umfassendes Enterprise-Evaluierungsframework für KI-Agenten mit sieben kritischen Dimensionen.',
    classicCost: 'Kosten',
    classicCostDesc: 'Gesamtbetriebskosten einschließlich API-Aufrufe, Rechenleistung, Infrastruktur und Wartung. Verfolge Kosten pro Aufgabe und pro erfolgreichem Ergebnis.',
    classicLatency: 'Latenz',
    classicLatencyDesc: 'Zeit bis zum ersten Token, End-to-End-Antwortzeit und Aufgabenabschlusszeit. Kritisch für Benutzererfahrung und Echtzeitanwendungen.',
    classicAccuracy: 'Genauigkeit',
    classicAccuracyDesc: 'Korrektheit der Ausgaben gemessen an der Grundwahrheit. Umfasst faktische Genauigkeit, logische Konsistenz und aufgabenspezifische Präzision.',
    classicStability: 'Stabilität',
    classicStabilityDesc: 'Konsistenz der Ausgaben bei identischen Eingaben. Niedrige Varianz zeigt zuverlässiges Verhalten; hohe Varianz deutet auf unvorhersehbare Leistung hin.',
    classicSecurity: 'Sicherheit',
    classicSecurityDesc: 'Widerstandsfähigkeit gegen Prompt-Injection, Jailbreaks und Datenlecks. Umfasst Eingabevalidierung, Ausgabefilterung und Zugriffskontrolle.',
    classicInterpretability: 'Interpretierbarkeit',
    classicInterpretabilityDesc: 'Fähigkeit, Entscheidungen und Begründungen zu erklären. Unterstützt Debugging, Compliance-Audits und Benutzervertrauen durch transparenten Betrieb.',
    classicCompliance: 'Compliance',
    classicComplianceDesc: 'Einhaltung regulatorischer Anforderungen (DSGVO, HIPAA, SOC2), Branchenstandards und organisatorischer Richtlinien.',
    classicNote: 'Enterprise-Evaluierung sollte alle sieben Dimensionen verfolgen. Optimiere für deine spezifischen Anwendungsfallprioritäten.',

    // Agent-Specific Benchmarks
    agentBenchmarks: 'Agentenspezifische Benchmarks',
    agentBenchmarksDesc: 'Moderne Benchmarks, die speziell zur Bewertung von KI-Agenten bei komplexen, mehrstufigen Aufgaben in realistischen Umgebungen entwickelt wurden.',
    benchmarkAgentBench: 'AgentBench',
    benchmarkAgentBenchDesc: 'Bewertet LLMs als Agenten in 8 Umgebungen: OS, Datenbank, Wissensgraph, Web-Browsing und mehr. Testet realen Tool-Einsatz.',
    benchmarkGaia: 'GAIA',
    benchmarkGaiaDesc: 'General AI Assistants Benchmark mit 466 Fragen, die mehrstufiges Reasoning, Web-Browsing und Tool-Nutzung erfordern. Menschlich verifizierte Antworten.',
    benchmarkBfcl: 'Berkeley Function-Calling Leaderboard',
    benchmarkBfclDesc: 'Testet Funktionsaufruf-Genauigkeit bei einfachen, parallelen und verschachtelten Aufrufen. Enthält reale API-Szenarien und Grenzfälle.',
    benchmarkSwe: 'SWE-bench',
    benchmarkSweDesc: 'Echte GitHub-Issues aus beliebten Python-Repos. Agenten müssen Kontext verstehen, Code schreiben und bestehende Tests bestehen.',
    benchmarkWebArena: 'WebArena',
    benchmarkWebArenaDesc: 'Testet Agenten bei realistischen Web-Aufgaben über E-Commerce, Foren und Content-Management-Sites mit komplexen mehrseitigen Workflows.',
    benchmarkTau: 'TAU-bench',
    benchmarkTauDesc: 'Tool-Agent-User Benchmark, der Agenten bei echten Kundenservice-Szenarien mit Tools, Richtlinien und Benutzerinteraktionen testet.',

    // Interactive Evaluation
    interactiveEval: 'Interaktive Evaluierung',
    interactiveEvalDesc: 'Dynamische Evaluierungsansätze, die das Agentenverhalten in sich ändernden Umgebungen und unter adversen Bedingungen testen.',
    interactiveWhat: 'Jenseits statischer Benchmarks',
    interactiveWhatDesc: 'Statische Benchmarks haben feste Fragen und Antworten. Interaktive Evaluierung testet, wie Agenten sich an dynamische Umgebungen anpassen, unerwartete Situationen bewältigen und die Leistung unter sich ändernden Bedingungen aufrechterhalten.',
    interactiveApproach1: 'Umgebungsperturbation',
    interactiveApproach1Desc: 'Ändere die Umgebung während der Aufgabenausführung – modifiziere Dateien, ändere API-Antworten, führe Fehler ein – um Agentenrobustheit und Wiederherstellung zu testen.',
    interactiveApproach2: 'Adverse Benutzersimulation',
    interactiveApproach2Desc: 'Simuliere Benutzer, die mehrdeutige Anweisungen geben, ihre Meinung ändern oder versuchen, den Agenten zu manipulieren. Testet reale Widerstandsfähigkeit.',
    interactiveApproach3: 'Multi-Turn-Konsistenz',
    interactiveApproach3Desc: 'Bewerte Kohärenz über lange Konversationen mit Kontextwechseln. Prüfe, ob der Agent genauen Zustand beibehält und Anweisungen über die Zeit befolgt.',
    interactiveApproach4: 'Curriculare Schwierigkeit',
    interactiveApproach4Desc: 'Beginne mit einfachen Aufgaben und steigere progressiv die Komplexität. Identifiziert Fähigkeitsgrenzen und gradülle Abbaumuster.',
    interactiveNote: 'Interaktive Evaluierung sagt reale Leistung besser voraus als statische Benchmarks allein.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Evaluierung ist essentiell – ungemessene Systeme können nicht verbessert werden',
    takeaway2: 'Kombiniere automatisierte Tests mit menschlicher Bewertung',
    takeaway3: 'Verfolge mehrere Metriken: Erfolg, Effizienz, Kosten',
    takeaway4: 'Integriere Evaluierung in deinen Entwicklungsworkflow',
    takeaway5: 'LLM-als-Richter ist nützlich, hat aber erhebliche Verzerrungen zu berücksichtigen',
    takeaway6: 'Verwende das CLASSIC-Framework für umfassende Enterprise-Evaluierung',
    takeaway7: 'Agentenspezifische Benchmarks wie AgentBench und GAIA testen reale Fähigkeiten',
  },

  // Agent Skills Seite
  agentSkills: {
    title: 'Agenten-Skills',
    description: 'Wiederverwendbare Instruktionspakete, die Agenten bei Bedarf spezialisierte Fähigkeiten verleihen.',
    whatIs: 'Was sind Agenten-Skills?',
    whatIsDesc: 'sind Ordner mit Anweisungen, Prompts, Beispielen und Ressourcen, die ein LLM bei Bedarf laden kann, um spezialisierte Aufgaben konsistent auszuführen. Anstatt alles in den System-Prompt zu packen, ermöglichen Skills die Modularisierung von Expertise.',
    metaphor: '"Skills sind wie Apps für deinen Agenten – einmal installieren, bei Bedarf verwenden."',
    metaphorDesc: 'Genau wie du Apps auf deinem Handy für bestimmte Funktionen installierst, geben Skills Agenten spezialisierte Fähigkeiten, ohne jede Konversation aufzublähen.',

    // Wie es funktioniert
    howItWorks: 'Wie Agenten-Skills funktionieren',
    step1Title: 'Skill-Erkennung',
    step1Desc: 'Wenn eine Benutzeranfrage eingeht, prüft der Agent, ob verfügbare Skills zur Aufgabe passen, basierend auf Triggern, Schlüsselwörtern oder explizitem Aufruf.',
    step2Title: 'Skill-Laden',
    step2Desc: 'Die Anweisungen, Beispiele und der Kontext des relevanten Skills werden in den Arbeitsspeicher des Agenten geladen. Dies fügt spezialisiertes Wissen hinzu, ohne den Basis-System-Prompt zu belasten.',
    step3Title: 'Skill-Ausführung',
    step3Desc: 'Der Agent folgt den Anweisungen des Skills, um die Aufgabe abzuschließen, unter Verwendung bereitgestellter Vorlagen, Checklisten oder Skripte. Die Ergebnisse werden dem Benutzer zurückgegeben.',

    // Interaktive Demo
    interactiveDemo: 'Interaktive Skill-Demo',
    interactiveDemoDesc: 'Erkunde, wie Skills ausgelöst werden, ihre Manifest-Struktur und Skill-Verkettung',

    // Struktur
    structureTitle: 'Skill-Struktur',
    structureSubtitle: 'Anatomie eines Skill-Ordners',
    skillMdDesc: 'Metadaten und Hauptanweisungen',
    instructionsDesc: 'Detaillierte Anleitungen für die Aufgabe',
    examplesDesc: 'Beispiel-Ein- und -Ausgaben',
    templatesDesc: 'Wiederverwendbare Ausgabeformate',
    scriptsDesc: 'Hilfsskripte bei Bedarf',
    skillMdNote: 'ist der Einstiegspunkt. Sie enthält Metadaten (Name, Trigger, Beschreibung) und die Kernanweisungen, denen der Agent folgt.',

    // Arten von Skills
    typesTitle: 'Arten von Agenten-Skills',
    skillType1Title: 'Domänen-Skills',
    skillType1Desc: 'Spezialisiertes Wissen für bestimmte Bereiche – Rechtsverträge, medizinische Terminologie, Finanzanalyse. Verwandeln einen allgemeinen Agenten in einen Domänenexperten.',
    skillType2Title: 'Workflow-Skills',
    skillType2Desc: 'Mehrstufige Prozesse mit definierten Phasen – Code-Review-Workflows, Content-Publishing-Pipelines, Incident-Response-Verfahren.',
    skillType3Title: 'Format-Skills',
    skillType3Desc: 'Konsistente Ausgabeformatierung – API-Dokumentation, Changelog-Einträge, Meeting-Zusammenfassungen. Stellen sicher, dass Ausgaben deinen Standards entsprechen.',
    skillType4Title: 'Integrations-Skills',
    skillType4Desc: 'Anweisungen für die Arbeit mit bestimmten Tools oder Diensten – GitHub-Workflows, Jira-Ticket-Erstellung, Slack-Benachrichtigungen.',

    // Skills vs Tools
    vsToolsTitle: 'Skills vs. Tools',
    tools: 'Tools',
    tool1: 'Führen Aktionen aus (Dateien lesen, APIs aufrufen, Code ausführen)',
    tool2: 'Definiert durch Funktionssignaturen und Schemas',
    tool3: 'Die "Hände" des Agenten',
    skills: 'Skills',
    skill1: 'Liefern Wissen und Methodik (wie man Aufgaben angeht)',
    skill2: 'Definiert durch Anweisungen und Beispiele',
    skill3: 'Die "Expertise" des Agenten',
    vsNote: 'Skills und Tools arbeiten zusammen: Ein Code-Review-Skill sagt dem Agenten, worauf er achten soll, während Tools ihm erlauben, den Code zu lesen und Kommentare zu hinterlassen.',

    // Vorteile
    benefitsTitle: 'Vorteile von Skills',
    benefit1Title: 'Spezialisierung ohne Aufblähung',
    benefit1Desc: 'Halte den Basis-System-Prompt schlank. Lade spezialisiertes Wissen nur bei Bedarf und bewahre das Kontextfenster für die eigentliche Aufgabe.',
    benefit2Title: 'Konsistenz',
    benefit2Desc: 'Definiere einen Prozess einmal, wende ihn jedes Mal konsistent an. Keine Variationen mehr in der Herangehensweise an Aufgaben.',
    benefit3Title: 'Teilbarkeit',
    benefit3Desc: 'Skills sind nur Ordner – teile sie projektübergreifend, teamübergreifend oder öffentlich. Einmal erstellen, überall verwenden.',
    benefit4Title: 'Iteration',
    benefit4Desc: 'Verbessere Skills unabhängig vom Agenten. Aktualisiere den Code-Review-Skill, ohne den Rest deines Agenten-Setups anzufassen.',

    // Beispiel
    exampleTitle: 'Beispiel: Code-Review-Skill',
    exampleDesc: 'Führt gründliche Code-Reviews nach Team-Standards durch',
    exampleInstructions: 'Beim Code-Review auf Korrektheit, Performance, Sicherheit und Wartbarkeit analysieren.',
    exampleCheck1: 'Auf häufige Sicherheitslücken prüfen',
    exampleCheck2: 'Fehlerbehandlung auf Vollständigkeit überprüfen',
    exampleCheck3: 'Nach Performance-Antipatterns suchen',
    exampleCheck4: 'Sicherstellen, dass der Code den Style-Guidelines folgt',

    // Best Practices
    practicesTitle: 'Best Practices',
    practice1Title: 'Skills fokussiert halten',
    practice1Desc: 'Ein Skill, ein Zweck. Wenn ein Skill zu viele Dinge tut, teile ihn auf. Fokussierte Skills sind einfacher zu warten und zu kombinieren.',
    practice2Title: 'Beispiele einbeziehen',
    practice2Desc: 'Zeigen, nicht nur erzählen. Füge Ein-/Ausgabe-Beispiele ein, die genau demonstrieren, wie gute Ausführung aussieht.',
    practice3Title: 'Skills versionieren',
    practice3Desc: 'Verfolge Änderungen an Skills über die Zeit. Wenn sich das Verhalten unerwartet ändert, kannst du es auf ein Skill-Update zurückführen.',

    // Wichtige Erkenntnisse
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Skills sind wiederverwendbare Instruktionspakete, die Agenten bei Bedarf spezialisierte Expertise verleihen',
    takeaway2: 'Im Gegensatz zu Tools (die Aktionen ausführen) liefern Skills Wissen und Methodik',
    takeaway3: 'Gut gestaltete Skills verbessern die Konsistenz und reduzieren die Aufblähung des System-Prompts',
    takeaway4: 'Das Skill-Framework ermöglicht modulare Agentenentwicklung – einmal erstellen, überall teilen',
  },

  // Phase 3: ML Fundamentals
  neuralNetworks: {
    title: 'Neuronale Netzwerke',
    description: 'Die grundlegende Architektur, die moderne KI antreibt.',
    whatIs: 'Was ist ein neuronales Netzwerk?',
    whatIsDesc: 'Ein neuronales Netzwerk ist ein vom Gehirn inspiriertes Rechenmodell. Es besteht aus Schichten verbundener Knoten (Neuronen), die lernen, Eingaben durch Training in Ausgaben umzuwandeln.',
    components: 'Kernkomponenten',
    componentsDesc: 'Die Bausteine neuronaler Netzwerke.',
    neurons: 'Neuronen',
    neuronsDesc: 'Grundeinheiten, die gewichtete Summen von Eingaben berechnen und Aktivierungsfunktionen anwenden.',
    layers: 'Schichten',
    layersDesc: 'Gruppen von Neuronen: Eingabeschicht, versteckte Schichten und Ausgabeschicht.',
    weights: 'Gewichte & Bias',
    weightsDesc: 'Lernbare Parameter, die bestimmen, wie Eingaben transformiert werden.',
    activations: 'Aktivierungsfunktionen',
    activationsDesc: 'Nichtlineare Funktionen, die es Netzwerken ermöglichen, komplexe Muster zu lernen.',
    typesOfNetworks: 'Arten von Netzwerken',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Informationen fließen in eine Richtung. Gut für tabellarische Daten.',
    cnn: 'Konvolutionell (CNN)',
    cnnDesc: 'Spezialisiert für Bilder und räumliche Daten.',
    rnn: 'Rekurrent (RNN)',
    rnnDesc: 'Verarbeitet Seqünzen mit Gedächtnis vergangener Eingaben.',
    transformer: 'Transformer',
    transformerDesc: 'Aufmerksamkeitsbasierte Architektur, die moderne LLMs antreibt.',
    interactiveDemo: 'Neuronales Netzwerk Visualisierer',
    demoDesc: 'Netzwerkarchitekturen erstellen und erkunden',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Neuronale Netzwerke lernen durch Anpassung der Gewichte während des Trainings',
    takeaway2: 'Tiefe (mehr Schichten) ermöglicht das Lernen hierarchischer Merkmale',
    takeaway3: 'Verschiedene Architekturen eignen sich für verschiedene Datentypen',
    takeaway4: 'Moderne LLMs sind massive Transformer-Netzwerke',
  },

  gradientDescent: {
    title: 'Gradientenabstieg',
    description: 'Der Optimierungsalgorithmus, der neuronalen Netzwerken das Lernen ermöglicht.',
    whatIs: 'Was ist Gradientenabstieg?',
    whatIsDesc: 'Gradientenabstieg ist ein Optimierungsalgorithmus, der iterativ Modellparameter anpasst, um eine Verlustfunktion zu minimieren. So lernen neuronale Netzwerke aus Daten.',
    intuition: 'Die Intuition',
    intuitionDesc: 'Stelle dir vor, du stehst mit verbundenen Augen in einer hügeligen Landschaft und versuchst, den tiefsten Punkt zu erreichen. Du fühlst die Neigung unter deinen Füßen und gehst bergab. Wiederhole, bis du ein Tal erreichst.',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Der Algorithmus berechnet, wie viel jeder Parameter zum Fehler beiträgt, und passt die Parameter dann in die entgegengesetzte Richtung an.',
    step1: 'Verlust berechnen',
    step1Desc: 'Messen, wie falsch die aktüllen Vorhersagen sind.',
    step2: 'Gradienten berechnen',
    step2Desc: 'Backpropagation verwenden, um herauszufinden, wie jedes Gewicht den Verlust beeinflusst.',
    step3: 'Gewichte aktualisieren',
    step3Desc: 'Gewichte in die Richtung anpassen, die den Verlust reduziert.',
    step4: 'Wiederholen',
    step4Desc: 'Iterieren, bis der Verlust nicht mehr abnimmt.',
    learningRate: 'Lernrate',
    learningRateDesc: 'Kontrolliert, wie groß jeder Schritt ist. Zu hoch: Überschießen. Zu niedrig: langsamer Fortschritt.',
    variants: 'Varianten',
    sgd: 'Stochastischer Gradientenabstieg',
    sgdDesc: 'Verwendet zufällige Mini-Batches anstelle des gesamten Datensatzes.',
    momentum: 'Momentum',
    momentumDesc: 'Akkumuliert Geschwindigkeit, um lokale Minima zu überwinden.',
    adam: 'Adam',
    adamDesc: 'Adaptive Lernraten pro Parameter. Kombiniert Momentum mit RMSprop.',
    adamw: 'AdamW',
    adamwDesc: 'Adam mit entkoppeltem Weight Decay. Heute für die meisten Anwendungen bevorzugt, besonders beim Training großer Sprachmodelle.',

    // Learning Rate Scheduling section
    lrScheduling: 'Lernraten-Scheduling',
    lrSchedulingDesc: 'Anstatt eine feste Lernrate zu verwenden, passen Schedules sie während des Trainings für bessere Konvergenz an.',
    stepDecay: 'Stufen-Abnahme',
    stepDecayDesc: 'Lernrate um einen Faktor zu bestimmten Epochen reduzieren (z.B. alle 30 Epochen halbieren).',
    exponentialDecay: 'Exponentielle Abnahme',
    exponentialDecayDesc: 'Kontinuierliche Verringerung der Lernrate: lr = lr_0 * e^(-kt). Glatt, kann aber zu schnell abklingen.',
    cosineAnnealing: 'Kosinus-Annealing',
    cosineAnnealingDesc: 'Folgt einer Kosinuskurve von der initialen zur minimalen LR. Beliebt im modernen Training, ermöglicht sanftes Abkühlen.',
    warmup: 'Warmup',
    warmupDesc: 'Mit sehr niedriger LR starten, gradüll zum Ziel erhöhen, dann abklingen. Stabilisiert frühes Training, essentiell für Transformer.',

    // Convergence Challenges section
    convergenceChallenges: 'Konvergenz-Herausforderungen',
    convergenceChallengesDesc: 'Hindernisse verstehen, die den Gradientenabstieg daran hindern können, das globale Optimum zu finden.',
    localMinima: 'Lokale Minima',
    localMinimaDesc: 'Punkte, wo der Verlust niedriger ist als in der Umgebung, aber nicht das globale Minimum. Momentum und adaptive Methoden helfen zu entkommen.',
    saddlePoints: 'Sattelpunkte',
    saddlePointsDesc: 'Punkte, wo der Gradient null ist, aber weder Minimum noch Maximum. Häufig in hohen Dimensionen, verlangsamt Konvergenz.',
    plateaus: 'Plateaus',
    plateausDesc: 'Flache Regionen, wo Gradienten sehr klein sind. Fortschritt stagniert, bis der Optimierer entkommt. Adaptive LR hilft bei der Navigation.',

    interactiveDemo: 'Gradientenabstieg-Visualisierer',
    demoDesc: 'Beobachte, wie der Gradientenabstieg das Minimum findet',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Gradientenabstieg minimiert den Verlust, indem er dem Gefälle folgt',
    takeaway2: 'Die Lernrate ist der wichtigste Hyperparameter',
    takeaway3: 'AdamW ist jetzt der bevorzugte Optimierer für die meisten Deep-Learning-Anwendungen',
    takeaway4: 'Backpropagation berechnet Gradienten effizient',
  },

  training: {
    title: 'Trainingsprozess',
    description: 'Wie neuronale Netzwerke durch iterative Optimierung aus Daten lernen.',
    whatIs: 'Was ist Training?',
    whatIsDesc: 'Training ist der Prozess, einem neuronalen Netzwerk beizubringen, eine Aufgabe auszuführen, indem man es Beispielen aussetzt und seine Parameter basierend auf Fehlern anpasst.',
    phases: 'Trainingsphasen',
    phasesDesc: 'Die Stufen des Trainings eines neuronalen Netzwerks.',
    initialization: 'Initialisierung',
    initializationDesc: 'Zufällige Startgewichte setzen. Gute Initialisierung hilft beim Training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Eingabe fließt durch das Netzwerk, um Vorhersagen zu produzieren.',
    lossCalc: 'Verlustberechnung',
    lossCalcDesc: 'Vorhersagen mit der Ground Truth durch eine Verlustfunktion vergleichen.',
    backprop: 'Backpropagation',
    backpropDesc: 'Gradienten des Verlusts bezüglich jedes Gewichts berechnen.',
    optimization: 'Optimierung',
    optimizationDesc: 'Gewichte mit Gradientenabstieg aktualisieren.',
    concepts: 'Schlüsselkonzepte',
    epoch: 'Epoche',
    epochDesc: 'Ein vollständiger Durchlauf durch den gesamten Trainingsdatensatz.',
    batch: 'Batch-Größe',
    batchDesc: 'Anzahl der Beispiele, die vor der Gewichtsaktualisierung verarbeitet werden.',
    overfitting: 'Überanpassung',
    overfittingDesc: 'Das Modell merkt sich Trainingsdaten, versagt aber bei neuen Daten.',
    regularization: 'Regularisierung',
    regularizationDesc: 'Techniken zur Vermeidung von Überanpassung (Dropout, Gewichtszerfall).',
    interactiveDemo: 'Trainingsfortschritt-Visualisierer',
    demoDesc: 'Beobachte, wie ein Netzwerk in Echtzeit lernt',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Training reduziert iterativ Vorhersagefehler',
    takeaway2: 'Überanpassung ist der Hauptfeind – immer auf zurückgehaltenen Daten validieren',
    takeaway3: 'Batch-Größe und Lernrate beeinflussen das Training erheblich',
    takeaway4: 'Moderne LLMs erfordern massive Rechenleistung für das Training',
  },

  worldModels: {
    title: 'World Models',
    description: 'KI-Systeme, die eine interne Repräsentation der physischen Welt lernen, um Realität vorherzusagen, zu simulieren und darüber zu schlussfolgern.',
    whatIs: 'Was sind World Models?',
    whatIsDesc: 'World Models sind KI-Systeme, die eine interne Repräsentation der physischen Welt lernen, um die Zukunft vorherzusagen und zu simulieren. Sie verstehen Physik, Objektbewegungen und kausale Zusammenhänge — und ermöglichen es Robotern, autonomen Fahrzeugen und KI-Agenten, Ergebnisse zu „imaginieren", bevor sie handeln.',
    whatIsDesc2: 'Anstatt nur Pixel-Muster zu lernen, entwickeln World Models ein tieferes Verständnis davon, wie die Welt funktioniert — ähnlich wie Menschen mentale Modelle der Realität aufbauen. Wenn du einen Ball fängst, sagt dein Gehirn seine Flugbahn vorher, ohne Gleichungen zu lösen. World Models wollen KI dieselbe Intuition geben.',
    keyInsight: 'Kernidee',
    keyInsightDesc: 'Die nächste Grenze der KI ist nicht nur Sprache zu verstehen — sondern die physische Welt zu verstehen. World Models schlagen die Brücke zwischen textbasierter KI und verkörperter Intelligenz, die mit der Realität interagieren kann.',
    howTheyWork: 'Wie funktionieren World Models?',
    howTheyWorkDesc: 'World Models kombinieren verschiedene Techniken, um physikalische Realität zu modellieren. Die Kernidee: Sensorische Eingaben in einen kompakten latenten Raum komprimieren, Dynamiken in diesem Raum lernen und Vorhersagen zurück in beobachtbare Ausgaben dekodieren.',
    pipelineTitle: 'World Model Pipeline',
    pipelineDesc: 'Klicke auf jede Phase, um den Datenfluss durch ein World Model zu erkunden',
    latentSpace: 'Latent Space Representation',
    latentSpaceDesc: 'Komprimierung hochdimensionaler Sensordaten (z.B. Video, LiDAR) in einen kompakten latenten Raum, der die wesentliche Struktur einer Szene erfasst — Position, Geschwindigkeit, Objektidentität — ohne jeden Pixel zu speichern.',
    latentDimInstruction: 'Ziehe die Regler, um zu sehen, wie jede latente Dimension ein komplexes visuelles Konzept unabhängig steuert.',
    latentZ1: 'Tageszeit',
    latentZ2: 'Straßenkrümmung',
    latentZ3: 'Verkehrsdichte',
    latentZ4: 'Wetter',
    latentSceneLabel: 'Szene',
    videoPrediction: 'Video Prediction',
    videoPredictionDesc: 'Vorhersage zukünftiger Frames basierend auf vergangenen Beobachtungen und geplanten Aktionen. Das Modell lernt zeitliche Dynamiken: Wenn das Auto links abbiegt, wie sieht die Welt 2 Sekunden später aus?',
    physicsAware: 'Physics-Aware Training',
    physicsAwareDesc: 'Training mit physikalischen Constraints oder Physik-Simulatoren, damit das Modell realistische Bewegungen, Kollisionen, Schwerkraft und Materialinteraktionen lernt — nicht nur visuelle Plausibilität.',
    diffusion: 'Diffusion-basierte Ansätze',
    diffusionDesc: 'Nutzung von Diffusionsmodellen zur Generierung konsistenter, physikalisch plausibler Zukunftsvorhersagen. Diese Modelle verfeinern iterativ verrauschte Vorhersagen zu klaren, kohärenten Zukunftszuständen.',
    whyNeeded: 'Warum braucht man World Models?',
    whyNeededIntro: 'Drei fundamentale Limitierungen machen World Models essentiell für die nächste Generation von KI:',
    slowExpensive: 'Realität ist langsam & teuer',
    slowExpensiveDesc: 'Roboter in der realen Welt zu trainieren ist zeitaufwändig, kostspielig und potenziell gefährlich. Ein einziger Fehler kann Hardware für 100.000€+ zerstören oder Menschen gefährden. Man kann nicht 10.000 Autos crashen, um ein selbstfahrendes System zu trainieren.',
    parallelTraining: 'Massiv paralleles Training',
    parallelTrainingDesc: 'World Models ermöglichen das Training tausender virtueller Agenten gleichzeitig, die Millionen von Stunden Erfahrung in Stunden sammeln. Was ein Roboter in 1 Jahr real lernt, schafft die Simulation in 1 Stunde.',
    physicsUnderstanding: 'LLMs verstehen keine Physik',
    physicsUnderstandingDesc: 'Sprachmodelle können über Physik sprechen, aber verstehen nicht wirklich räumliche Beziehungen, Momentum oder Schwerkraft. Sie haben nie „erlebt", wie ein Ball fällt. World Models lernen Physik durch simulierte Erfahrung.',
    simVsRealTitle: 'Simulation vs. Reale Welt — Ein direkter Vergleich',
    trainingLoopTitle: 'Der Trainingskreislauf',
    trainingLoopDesc: 'Gehe Schritt für Schritt durch einen kompletten Trainingszyklus, um zu sehen, wie World Models aus simulierter Erfahrung lernen',
    examples: 'Bekannte World Models',
    examplesIntro: 'Führende Forschungslabore und Unternehmen bauen World Models für verschiedene Domänen. Klicke auf eine Karte für mehr Details.',
    nvidiaCosmos: 'NVIDIA Cosmos',
    nvidiaCosmosDesc: 'Open-Source Physical-AI-Plattform zur Generierung synthetischer Trainingsdaten für Robotik und autonomes Fahren.',
    nvidiaCosmosDetail: 'Cosmos nutzt Video-Foundation-Modelle zur Simulation komplexer Fahrszenarien mit realistischer Physik. Es ist darauf ausgelegt, unbegrenzt synthetische Daten für das Training autonomer Fahrzeuge und Robotik-Systeme zu generieren — und reduziert damit den Bedarf an teurer Datenerfassung in der realen Welt drastisch.',
    tagAutonomous: 'Autonomes Fahren',
    googleGenie: 'Google Genie 3 / Project Genie',
    googleGenieDesc: 'Universelles Weltmodell, das vielfältige, erkundbare interaktive Welten aus Text- und Bild-Prompts in Echtzeit generiert.',
    googleGenieDetail: 'Genie 3 (Jan 2026) ist ein großer Sprung gegenüber Genie 2. Anders als statische 3D-Schnappschüsse generiert es den Weg voraus in Echtzeit während man sich bewegt und interagiert. Es simuliert Physik und dynamische Interaktionen mit bahnbrechender Konsistenz — und ermöglicht die Simulation jedes realen Szenarios von Robotik bis historischen Settings. Verfügbar für Google AI Ultra-Abonnenten über „Project Genie", einen experimentellen Prototyp zum Erstellen, Erkunden und Remixen interaktiver Welten. Google DeepMind positioniert es als Schlüsselschritt Richtung AGI: ein universelles Weltmodell, das die Vielfalt der realen Welt bewältigt, nicht nur spezifische Spielumgebungen.',
    tag3DWorlds: '3D-Welten',
    genesis: 'Genesis',
    genesisDesc: 'Physik-Engine kombiniert mit generativer KI. Simulationen bis zu 430.000x schneller als Echtzeit.',
    genesisDetail: 'Genesis kombiniert differenzierbare Physiksimulation mit generativen Modellen, um ultraschnelle, physikalisch akkurate Trainingsumgebungen für Robotik zu ermöglichen. Die Schlüsselinnovation ist Geschwindigkeit: Durch hochoptimierte Physik-Berechnung ermöglicht es Millionen Trainingsepisoden in Stunden statt Monaten.',
    tagPhysics: 'Physik-Engine',
    uniSim: 'UniSim',
    uniSimDesc: 'Google Researchs universeller Weltsimulator für jede Umgebung — von Küchen bis Autobahnen.',
    uniSimDetail: 'UniSim lernt aus diversen Videodaten, um ein einheitliches Simulations-Framework aufzubauen, das domänenübergreifend funktioniert. Anders als spezialisierte Simulatoren kann es Küchen, Außenszenen, Fahrumgebungen und mehr modellieren — alles aus einem einzigen Modell, trainiert auf Internet-Videodaten.',
    tagUniversal: 'Universal-Sim',
    gaia1: 'GAIA-1',
    gaia1Desc: 'Wayves generatives World Model für autonomes Fahren, trainiert auf Londoner Straßendaten.',
    gaia1Detail: 'GAIA-1 ist ein 9-Milliarden-Parameter-Modell, trainiert auf echtem Fahrmaterial aus London. Es kann neuartige Fahrszenarien generieren, Verkehrsverhalten vorhersagen und seltene Edge Cases erzeugen, die im realen Test schwer zu finden sind. Wayve nutzt es zur Ergänzung realer Fahrdaten.',
    tagDriving: 'Selbstfahrend',
    useCases: 'Anwendungsbereiche',
    autonomousDriving: 'Autonomes Fahren',
    autonomousDrivingDesc: 'Millionen Verkehrsszenarien simulieren, seltene Edge Cases testen und Fahrzeug-Policies trainieren — alles ohne ein einziges echtes Auto zu riskieren.',
    autonomousDrivingStat: 'Größter Anwendungsbereich heute',
    robotics: 'Robotik-Training',
    roboticsDesc: 'Manipulation, Fortbewegung und Navigation in Simulation erlernen, bevor Policies über Sim-to-Real Transfer auf physische Roboter übertragen werden.',
    roboticsStat: 'Am schnellsten wachsendes Segment',
    videoGeneration: 'Videogenerierung',
    videoGenerationDesc: 'Photorealistische Videos mit konsistenter Physik generieren — ein mächtiges „Nebenprodukt" des Verständnisses von Welt-Dynamiken.',
    videoGenerationStat: 'Aufkommende kommerzielle Nutzung',
    challenges: 'Herausforderungen',
    challengesDesc: 'Trotz des enormen Potenzials bleiben signifikante Hürden:',
    computeIntensive: 'Extrem ressourcenhungrig',
    computeIntensiveDesc: 'Training erfordert riesige GPU-Cluster, massive Video-Datensätze und wochenlange Rechenzeit. Nur gut finanzierte Labore können sich State-of-the-Art World Models leisten.',
    simToReal: 'Sim-to-Real Gap',
    simToRealDesc: 'Was in der Simulation funktioniert, versagt oft in der Realität. Unterschiede in Physik-Genauigkeit, Sensor-Rauschen und Umgebungsbedingungen erschweren den Transfer.',
    generalization: 'Generalisierung',
    generalizationDesc: 'World Models können auf Trainingsdomänen überanpassen. Ein auf Fahrdaten trainiertes Modell generalisiert möglicherweise nicht auf Indoor-Robotik. Robuste domänenübergreifende Generalisierung ist ein offenes Problem.',
    severityHigh: 'Hoher Einfluss',
    severityMedium: 'Aktive Forschung',
    severityOpen: 'Offenes Problem',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'World Models lernen interne Repräsentationen der physischen Welt — sie ermöglichen KI, Ergebnisse zu „imaginieren" und vorherzusagen, bevor sie handelt',
    takeaway2: 'Sie ermöglichen massiv parallelisiertes Training: Millionen Stunden Erfahrung in Simulation statt langsamer Echtzeit-Interaktion',
    takeaway3: 'Die Architektur folgt einer Pipeline: Beobachten → Enkodieren → Vorhersagen → Dekodieren → Handeln',
    takeaway4: 'Wichtige Akteure sind NVIDIA Cosmos, Google Genie 2, Wayve GAIA-1 und Genesis — jeder für unterschiedliche Domänen',
    takeaway5: 'Der Sim-to-Real Gap bleibt die zentrale Herausforderung: die Brücke zwischen simulierter und realer Physik',
  },

  // Phase 3: Prompting
  promptBasics: {
    title: 'Prompt-Grundlagen',
    description: 'Grundlagen für das Schreiben effektiver Prompts für KI-Modelle.',
    whatIs: 'Was ist ein Prompt?',
    whatIsDesc: 'Ein Prompt ist die Eingabe, die du einem LLM gibst. Die Qualität deines Prompts bestimmt direkt die Qualität der Antwort. Prompting ist sowohl Kunst als auch Wissenschaft.',
    principles: 'Kernprinzipien',
    principlesDesc: 'Grundlegende Richtlinien für effektive Prompts.',
    beSpecific: 'Sei spezifisch',
    beSpecificDesc: 'Vage Prompts bekommen vage Antworten. Füge relevante Details und Einschränkungen ein.',
    showExamples: 'Zeige Beispiele',
    showExamplesDesc: 'Demonstriere das gewünschte Format und den Stil mit konkreten Beispielen.',
    giveContext: 'Gib Kontext',
    giveContextDesc: 'Hintergrundinformationen helfen dem Modell, deine Bedürfnisse zu verstehen.',
    setFormat: 'Spezifiziere das Format',
    setFormatDesc: 'Sage dem Modell genau, wie du die Ausgabe strukturiert haben möchtest.',
    anatomy: 'Anatomie eines Prompts',
    anatomyDesc: 'Die Komponenten, die einen effektiven Prompt ausmachen.',
    role: 'Rolle/Persona',
    roleDesc: 'Wer das Modell sein soll.',
    task: 'Aufgabenbeschreibung',
    taskDesc: 'Was das Modell tun soll.',
    context: 'Kontext/Hintergrund',
    contextDesc: 'Relevante Informationen für die Aufgabe.',
    format: 'Ausgabeformat',
    formatDesc: 'Wie du die Antwort strukturiert haben möchtest.',
    interactiveDemo: 'Prompt-Vergleich',
    demoDesc: 'Vergleiche schwache vs. starke Prompts',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Klare, spezifische Prompts liefern bessere Ergebnisse',
    takeaway2: 'Beispiele sind mächtig – zeigen, nicht nur erzählen',
    takeaway3: 'Iteriere an Prompts; erste Versuche sind selten optimal',
    takeaway4: 'Berücksichtige die Perspektive des Modells beim Erstellen von Prompts',
  },

  advancedPrompting: {
    title: 'Fortgeschrittene Techniken',
    description: 'Ausgefeilte Prompting-Strategien für komplexe Aufgaben.',
    overview: 'Über die Grundlagen hinaus',
    overviewDesc: 'Fortgeschrittene Techniken ermöglichen fähigeres und zuverlässigeres KI-Verhalten für komplexe Aufgaben.',
    cot: 'Chain of Thought',
    cotDesc: 'Fördere schrittweises Denken, indem du das Modell bittest, Probleme "durchzudenken".',
    cotExample: 'Beispiel: "Lass uns das Schritt für Schritt lösen..."',
    cotLimitations: 'Wichtig: CoT ist nicht universell',
    cotLimitationsDesc: 'Forschung aus 2025 zeigt, dass Chain of Thought Prompting NICHT universell vorteilhaft ist. Während es die Leistung bei denkintensiven Aufgaben erheblich verbessert, bringt es minimale Gewinne bei nicht-denkbasierten Aufgaben.',
    cotLimitationItem1: 'Am effektivsten für Mathematik, Logik und mehrstufige Denkprobleme',
    cotLimitationItem2: 'Minimaler Nutzen für einfache Abruf-, Klassifizierungs- oder kreative Aufgaben',
    cotLimitationItem3: 'Erhöht Latenz und Token-Kosten—strategisch einsetzen, nicht standardmäßig',
    fewShot: 'Few-Shot-Lernen',
    fewShotDesc: 'Gib mehrere Beispiele an, um Muster zu etablieren, denen das Modell folgen soll.',
    fewShotExample: 'Füge 3-5 diverse Beispiele ein, die Randfälle abdecken.',
    selfConsistency: 'Selbstkonsistenz',
    selfConsistencyDesc: 'Generiere mehrere Antworten und wähle die konsistenteste aus.',
    selfConsistencyExample: 'Nützlich für Mathematik, Logik und Faktenfragen.',
    decomposition: 'Aufgabenzerlegung',
    decompositionDesc: 'Zerlege komplexe Aufgaben in kleinere, handhabbare Teilaufgaben.',
    decompositionExample: 'Löse Teilaufgaben unabhängig, dann kombiniere die Ergebnisse.',
    treeOfThoughts: 'Tree of Thoughts (ToT)',
    totDesc: 'Eine Erweiterung von Chain of Thought, die mehrere Denkpfade gleichzeitig erkundet, bewertet und bei Bedarf zurückverfolgt, um optimale Lösungen zu finden.',
    totHowItWorks: 'Wie es funktioniert',
    totHowItWorksDesc: 'Generiere mehrere Denkzweige bei jedem Schritt. Bewerte vielversprechende Pfade, beschneide Sackgassen und verfolge Alternativen zurück.',
    totBestFor: 'Am besten geeignet für',
    totBestForDesc: 'Planungsprobleme, Rätsel, kreative Aufgaben, die Exploration erfordern, und Probleme, bei denen der erste Ansatz möglicherweise nicht optimal ist.',
    totExample: 'Beispiel: "Betrachte 3 verschiedene Ansätze zur Lösung. Denke für jeden 2 Schritte voraus. Bewerte, welcher Pfad am vielversprechendsten ist, dann fahre fort."',
    graphOfThoughts: 'Graph of Thoughts (GoT)',
    gotDesc: 'Eine nichtlineare Denkstruktur, bei der Gedanken verschmelzen, sich verzweigen und Zyklen bilden können—modelliert, wie Menschen tatsächlich über komplexe Probleme nachdenken.',
    gotKeyFeature: 'Hauptmerkmal',
    gotKeyFeatureDesc: 'Im Gegensatz zu linearem CoT oder baumstrukturiertem ToT ermöglicht GoT das Kombinieren von Erkenntnissen aus verschiedenen Denkpfaden und das Überdenken früherer Schlussfolgerungen.',
    gotBestFor: 'Am besten geeignet für',
    gotBestForDesc: 'Komplexe Probleme mit Abhängigkeiten, Syntheseaufgaben und Probleme, bei denen Teillösungen kombiniert werden müssen.',
    gotExample: 'Beispiel: "Analysiere dieses Problem aus den Blickwinkeln A, B und C unabhängig. Identifiziere dann Verbindungen zwischen deinen Analysen und synthetisiere eine einheitliche Lösung."',
    costBenefit: 'Kosten-Nutzen-Analyse',
    costBenefitDesc: 'Fortgeschrittene Prompting-Techniken erhöhen den Token-Verbrauch, die Latenz und die API-Kosten. Zu verstehen, wann diese Kompromisse sich lohnen, ist entscheidend für Produktionssysteme.',
    worthIt: 'Die zusätzlichen Kosten wert',
    worthItItem1: 'Komplexes Denken: Mathematik, Logik, mehrstufige Analyse',
    worthItItem2: 'Entscheidungen mit hohem Einsatz: medizinische, rechtliche, finanzielle Beratung',
    worthItItem3: 'Probleme, bei denen Genauigkeit wichtiger ist als Geschwindigkeit',
    notWorthIt: 'Oft nicht lohnenswert',
    notWorthItItem1: 'Einfache Klassifizierungs- oder Extraktionsaufgaben',
    notWorthItItem2: 'Hochvolumige, latenzempfindliche Anwendungen',
    notWorthItItem3: 'Aufgaben, bei denen einfachere Prompts bereits hohe Genauigkeit erreichen',
    costBenefitTip: 'Tipp: Beginne mit einfachen Prompts und füge Komplexität nur bei Bedarf hinzu. Miss die Genauigkeitsverbesserung gegen den Kostenanstieg, um fundierte Entscheidungen zu treffen.',
    techniques: 'Zusätzliche Techniken',
    rolePlay: 'Rollenzuweisung',
    rolePlayDesc: 'Weise eine spezifische Experten-Persona zu, um das Wissen des Modells zu fokussieren.',
    constraints: 'Explizite Einschränkungen',
    constraintsDesc: 'Liste auf, was das Modell NICHT tun soll, um häufige Fehler zu vermeiden.',
    verification: 'Selbstverifikation',
    verificationDesc: 'Bitte das Modell, seine eigene Arbeit auf Fehler zu überprüfen.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'Sieh, wie Denkschritte die Ausgaben verbessern',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Chain of Thought verbessert Denkaufgaben, ist aber nicht universell vorteilhaft',
    takeaway2: 'Few-Shot-Beispiele etablieren zuverlässige Muster',
    takeaway3: 'ToT und GoT erweitern CoT für komplexe, nichtlineare Probleme',
    takeaway4: 'Berücksichtige immer Kosten vs. Nutzen—fortgeschrittene Techniken erhöhen den Token-Verbrauch',
    takeaway5: 'Beginne einfach, füge Komplexität nur hinzu, wenn die Genauigkeit es erfordert',
  },

  systemPrompts: {
    title: 'System-Prompts',
    description: 'KI-Verhalten durch Anweisungen auf Systemebene konfigurieren.',
    whatIs: 'Was ist ein System-Prompt?',
    whatIsDesc: 'Ein System-Prompt ist eine spezielle Anweisung, die den Kontext, die Persona und die Verhaltensrichtlinien für ein KI-Modell festlegt. Er ist typischerweise vor Benutzern verborgen und bleibt während eines Gesprächs bestehen.',
    purpose: 'Zweck von System-Prompts',
    purposeDesc: 'System-Prompts legen die Grundlage dafür, wie sich die KI verhalten soll.',
    setPersona: 'Persona definieren',
    setPersonaDesc: 'Festlegen, wer die KI ist: ein Assistent, Experte, Charakter, etc.',
    setBoundaries: 'Grenzen setzen',
    setBoundariesDesc: 'Definieren, was die KI tun und nicht tun soll.',
    establishTone: 'Ton festlegen',
    establishToneDesc: 'Kommunikationsstil festlegen: formell, locker, technisch.',
    provideKnowledge: 'Kontext bereitstellen',
    provideKnowledgeDesc: 'Domänenwissen oder anwendungsspezifische Regeln einbeziehen.',
    structure: 'Struktur effektiver System-Prompts',
    structureDesc: 'Gut organisierte System-Prompts sind für Modelle leichter zu befolgen.',
    identity: 'Identitätsabschnitt',
    identityDesc: 'Wer ist die KI? Was ist ihre Rolle?',
    capabilities: 'Fähigkeiten',
    capabilitiesDesc: 'Was kann die KI tun? Welche Tools hat sie?',
    limitations: 'Einschränkungen',
    limitationsDesc: 'Was soll die KI vermeiden oder ablehnen?',
    guidelines: 'Richtlinien',
    guidelinesDesc: 'Spezifische Regeln für Verhalten und Antworten.',
    bestPractices: 'Best Practices',
    practice1: 'Sei explizit über Randfälle und Fehlerbehandlung.',
    practice2: 'Teste System-Prompts mit adversariellen Eingaben.',
    practice3: 'Versioniere deine System-Prompts.',
    practice4: 'Halte Prompts fokussiert – nicht mit Anweisungen überladen.',
    examplePrompt: 'Beispiel System-Prompt',
    interactiveBuilder: 'Interaktiver Builder',
    builderDesc: 'Erstelle deinen eigenen System-Prompt aus Komponenten',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'System-Prompts definieren die Persona und das Verhalten der KI',
    takeaway2: 'Strukturiere Prompts klar: Identität, Fähigkeiten, Einschränkungen',
    takeaway3: 'Teste mit Randfällen – Benutzer werden sie finden',
    takeaway4: 'System-Prompts können überschrieben werden – verlasse dich nicht allein auf sie für Sicherheit',
  },

  // LLM Training page
  llmTraining: {
    title: 'LLM-Training',
    description: 'Wie große Sprachmodelle trainiert werden: von Pretraining bis RLHF.',
    whatIs: 'Wie LLMs trainiert werden',
    whatIsDesc: 'Große Sprachmodelle durchlaufen mehrere Trainingsphasen, jede mit unterschiedlichen Zielen und Techniken. Das Verständnis dieser Pipeline ist entscheidend für das Verständnis der Modellfähigkeiten und -einschränkungen.',
    whyMatters: 'Warum Training wichtig ist',
    whyMattersDesc: 'Der Trainingsprozess formt grundlegend, was LLMs können und was nicht. Verschiedene Trainingsansätze produzieren Modelle mit unterschiedlichen Stärken, Schwächen und Verhaltensweisen.',

    // LLM Training Pipeline
    trainingPipeline: 'Die LLM-Trainingspipeline',
    trainingPipelineDesc: 'Moderne LLMs durchlaufen mehrere Trainingsstufen, jede mit unterschiedlichen Zielen. Das Verständnis dieser Pipeline ist entscheidend für das Verständnis, wo Alignment hineinpasst.',

    pretraining: 'Stufe 1: Pretraining',
    pretrainingDesc: 'Das Foundation-Modell wird auf massiven Textkorpora (Billionen von Tokens) mit selbstüberwachtem Lernen trainiert. Das Modell lernt, das nächste Token vorherzusagen und entwickelt dabei breites Wissen und Sprachfähigkeiten.',
    pretrainingGoal: 'Ziel: Sprachmuster, Fakten und Denken aus Rohtext lernen.',
    pretrainingData: 'Daten: Webseiten, Bücher, Code, wissenschaftliche Paper – typischerweise 1-10+ Billionen Tokens.',
    pretrainingResult: 'Ergebnis: Ein fähiges, aber nicht-aligniertes "Basismodell", das Text vervollständigt, aber keine Anweisungen befolgt.',

    sft: 'Stufe 2: Supervised Fine-Tuning (SFT)',
    sftDesc: 'Das Basismodell wird auf kuratierten Anweisung-Antwort-Paaren feingetunt, die von menschlichen Annotatoren erstellt wurden. Dies lehrt das Modell, Anweisungen zu befolgen und hilfreich zu antworten.',
    sftGoal: 'Ziel: Das Basismodell in einen anweisungsfolgenden Assistenten verwandeln.',
    sftData: 'Daten: ~10K-100K hochwertige Anweisung-Antwort-Beispiele.',
    sftResult: 'Ergebnis: Ein Modell, das Anweisungen befolgen kann, aber möglicherweise noch schädliche oder nicht hilfreiche Ausgaben produziert.',

    rlhfStage: 'Stufe 3: RLHF / Präferenz-Tuning',
    rlhfStageDesc: 'Menschliche Bewerter ranken Modellausgaben nach Qualität. Ein Reward-Modell lernt diese Präferenzen, dann wird das LLM optimiert, um die Belohnung mit Reinforcement Learning (PPO) oder Direct Preference Optimization (DPO) zu maximieren.',
    rlhfGoal: 'Ziel: Das Modell mit menschlichen Präferenzen für Hilfsbereitschaft, Harmlosigkeit und Ehrlichkeit alignieren.',
    rlhfData: 'Daten: Menschliche Präferenzvergleiche (A ist besser als B).',
    rlhfResult: 'Ergebnis: Ein Modell, das Ausgaben produziert, die Menschen bevorzugen, und schädliches Verhalten vermeidet.',

    continuedTraining: 'Stufe 4: Fortgesetztes Training & Spezialisiertes Alignment',
    continuedTrainingDesc: 'Modelle können zusätzliches Training für spezifische Fähigkeiten (Coding, Mathematik, Tool-Nutzung) oder Sicherheitsverfeinerungen (Red Teaming, Constitutional AI) durchlaufen. Diese Stufe ist während der Bereitstellung fortlaufend.',

    // RL Paradigm
    rlParadigm: 'Das RL-Paradigma: Lernen ohne menschliche Labels',
    rlParadigmDesc: 'Ein revolutionärer Ansatz, bei dem Modelle Denken durch reines Reinforcement Learning bei verifizierbaren Aufgaben lernen, ohne menschliche Demonstrationen oder Präferenz-Labels.',
    rlParadigmWhat: 'Was ist das RL-Paradigma?',
    rlParadigmWhatDesc: 'Anstatt aus von Menschen geschriebenen Beispielen (SFT) oder menschlichen Präferenzen (RLHF) zu lernen, lernen Modelle direkt aus ergebnisbasierten Belohnungen. Wenn die Antwort korrekt ist, wird das Modell belohnt. Wenn falsch, wird es bestraft. Keine menschliche Beschriftung erforderlich.',
    deepseekR1: 'DeepSeek R1-Zero: Eine Fallstudie',
    deepseekR1Desc: 'DeepSeek R1-Zero demonstrierte, dass leistungsfähiges Denken aus reinem RL entstehen kann, ohne jegliches Supervised Fine-Tuning. Das Modell entwickelte Chain-of-Thought-Denken, Selbstverifikation und sogar "Aha-Momente" vollständig durch Reinforcement Learning.',
    rlKey1: 'Kein SFT erforderlich',
    rlKey1Desc: 'R1-Zero wurde direkt aus einem Basismodell nur mit RL trainiert und übersprang die SFT-Stufe vollständig. Denkverhalten entstand natürlich.',
    rlKey2: 'Verifizierbare Belohnungen',
    rlKey2Desc: 'Training konzentrierte sich auf Aufgaben mit objektiv verifizierbaren Antworten: Matheprobleme, Coding-Challenges, logische Rätsel. Kein subjektives menschliches Urteil nötig.',
    rlKey3: 'Emergente Verhaltensweisen',
    rlKey3Desc: 'Das Modell entwickelte spontan erweitertes Denken, Selbstkorrektur und Reflexion – Verhaltensweisen, die frühere Modelle nur aus menschlichen Demonstrationen lernten.',
    rlKey4: 'Lesbarkeits-Herausforderungen',
    rlKey4Desc: 'Reine RL-Modelle können ungewöhnliche Denkmuster entwickeln, die schwer zu interpretieren sind. DeepSeek fügte eine kleine Menge menschlicher Daten hinzu, um die Lesbarkeit zu verbessern.',
    rlVsRlhf: 'RL-Paradigma vs. Traditionelles RLHF',
    rlVsRlhfDesc: 'Diese Ansätze lösen unterschiedliche Probleme und können komplementär sein.',
    rlhfApproach: 'RLHF-Ansatz',
    rlhfApproachDesc: 'Aus menschlichen Präferenzen lernen. Erfordert teure menschliche Beschriftung. Gut für subjektive Aufgaben wie Schreibqualität und Hilfsbereitschaft.',
    rlApproach: 'RL-Paradigma-Ansatz',
    rlApproachDesc: 'Aus verifizierbaren Ergebnissen lernen. Keine menschliche Beschriftung nötig. Hervorragend für Denken, Mathematik und Coding, wo Korrektheit objektiv ist.',
    hybridApproach: 'Hybrid-Ansatz',
    hybridApproachDesc: 'Moderne Modelle kombinieren oft beides: RL für Denkfähigkeiten, RLHF für Alignment und Benutzerpräferenzen.',

    // Key Alignment Concepts
    concepts: 'Schlüssel-Alignment-Konzepte',
    conceptsDesc: 'Grundlegende Ideen in der KI-Alignment-Forschung.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'Sicherstellen, dass das Trainingsziel (Reward-Funktion) korrekt erfasst, was wir wollen. Selbst perfekte Optimierung eines falsch spezifizierten Ziels führt zu schlechten Ergebnissen.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'Sicherstellen, dass das gelernte Modell tatsächlich für das Trainingsziel optimiert, nicht für ein Proxy-Ziel, das zufällig während des Trainings korreliert.',
    specification: 'Spezifikationsproblem',
    specificationDesc: 'Die fundamentale Schwierigkeit, präzise zu formulieren, was wir in allen Situationen wollen. Menschliche Werte sind komplex, kontextabhängig und manchmal widersprüchlich.',
    robustness: 'Robustheit',
    robustnessDesc: 'Aufrechterhaltung des Alignments unter Verteilungsverschiebung, adversariellem Druck und neuartigen Situationen, auf die das Modell nicht trainiert wurde.',
    deception: 'Täuschendes Alignment',
    deceptionDesc: 'Ein theoretisches Risiko, bei dem ein Modell während des Trainings aligned erscheint, aber bei der Bereitstellung andere Ziele verfolgt – sich nur gut verhält, weil es evaluiert wird.',
    goalMisgeneralization: 'Ziel-Fehlgeneralisierung',
    goalMisgeneralizationDesc: 'Wenn ein Modell ein Proxy-Ziel lernt, das im Training funktioniert, aber bei der Bereitstellung versagt. Beispiel: Lernen, positives Feedback zu bekommen, statt wirklich hilfreich zu sein.',

    // Alignment Techniques
    techniques: 'Alignment-Techniken',
    rlhf: 'RLHF (Reinforcement Learning from Human Feedback)',
    rlhfDesc: 'Ein Reward-Modell auf menschlichen Präferenzen trainieren, dann RL verwenden, um das LLM dagegen zu optimieren. Die dominante Alignment-Technik seit GPT-4.',
    constitutionalAi: 'Constitutional AI (CAI)',
    constitutionalAiDesc: 'Prinzipien definieren (eine "Verfassung") und das Modell seine eigenen Ausgaben kritisieren und überarbeiten lassen. Reduziert die Abhängigkeit von menschlichen Labeln und skaliert besser.',
    dpo: 'Direct Preference Optimization (DPO)',
    dpoDesc: 'Das Reward-Modell überspringen – das LLM direkt auf Präferenzdaten optimieren. Einfacher und stabiler als RLHF.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarielle Tests durch Menschen oder andere KI-Modelle, um Fehlermodi, Jailbreaks und schädliche Ausgaben vor der Bereitstellung zu finden.',
    interpretability: 'Interpretierbarkeit',
    interpretabilityDesc: 'Verstehen, was Modelle tatsächlich intern lernen. Entscheidend für die Verifizierung von Alignment statt nur Verhaltenmessung.',
    safetyFilters: 'Sicherheitsfilter & Guardrails',
    safetyFiltersDesc: 'Zusätzliche Schichten, die Eingaben/Ausgaben auf schädlichen Inhalt filtern. Eine Defense-in-Depth-Maßnahme, kein Ersatz für Alignment.',

    // DPO vs RLHF Deep Dive
    dpoVsRlhf: 'DPO vs RLHF: Ein tiefgreifender Vergleich',
    dpoVsRlhfDesc: 'Direct Preference Optimization (DPO) und Reinforcement Learning from Human Feedback (RLHF) sind die zwei dominanten Ansätze zur Alignierung von LLMs mit menschlichen Präferenzen. Das Verständnis ihrer Unterschiede ist entscheidend für die Wahl der richtigen Technik.',

    rlhfDeep: 'RLHF: Der traditionelle Ansatz',
    rlhfDeepDesc: 'RLHF verwendet ein separates Reward-Modell, das auf menschlichen Präferenzen trainiert wird, und optimiert dann das LLM mit Reinforcement Learning (typischerweise PPO), um diese Belohnung zu maximieren.',
    rlhfStep1: 'Schritt 1: Präferenzen sammeln',
    rlhfStep1Desc: 'Menschen vergleichen Paare von Modellausgaben und wählen, welche sie bevorzugen. Dies erstellt einen Datensatz von Präferenz-Rankings.',
    rlhfStep2: 'Schritt 2: Reward-Modell trainieren',
    rlhfStep2Desc: 'Ein separates neuronales Netzwerk lernt, menschliche Präferenzen vorherzusagen und weist Modellausgaben Scores zu.',
    rlhfStep3: 'Schritt 3: RL-Optimierung',
    rlhfStep3Desc: 'PPO (Proximal Policy Optimization) verwenden, um das LLM zu aktualisieren, damit es Ausgaben generiert, die die Scores des Reward-Modells maximieren.',

    dpoDeep: 'DPO: Die vereinfachte Alternative',
    dpoDeepDesc: 'DPO überspringt das Reward-Modell vollständig und optimiert das LLM direkt auf Präferenzdaten durch eine clevere mathematische Umformulierung.',
    dpoStep1: 'Schritt 1: Präferenzen sammeln',
    dpoStep1Desc: 'Wie bei RLHF—Menschen vergleichen Ausgabepaare und geben an, welche sie bevorzugen.',
    dpoStep2: 'Schritt 2: Direkte Optimierung',
    dpoStep2Desc: 'Anstatt ein separates Reward-Modell zu trainieren, aktualisiert DPO das LLM direkt, um die Wahrscheinlichkeit bevorzugter Ausgaben zu erhöhen.',
    dpoStep3: 'Schritt 3: Kein RL erforderlich',
    dpoStep3Desc: 'Verwendet Standard-Supervised-Learning-Techniken und vermeidet die Instabilität und Komplexität von Reinforcement Learning.',

    comparisonTable: 'Direkter Vergleich',
    aspect: 'Aspekt',
    complexity: 'Komplexität',
    rlhfComplexity: 'Hoch: erfordert Reward-Modell + RL-Training',
    dpoComplexity: 'Niedrig: einstufiges Supervised Learning',
    rewardModel: 'Reward-Modell',
    rlhfRewardModel: 'Erforderlich (separates neuronales Netzwerk)',
    dpoRewardModel: 'Nicht benötigt (implizit in der Verlustfunktion)',
    stability: 'Trainingsstabilität',
    rlhfStability: 'Kann instabil sein, erfordert sorgfältiges Tuning',
    dpoStability: 'Generell stabiler und vorhersagbarer',
    flexibility: 'Flexibilität',
    rlhfFlexibility: 'Flexibler, Reward-Modell wiederverwendbar',
    dpoFlexibility: 'Weniger flexibel, an spezifische Präferenzen gebunden',
    usedBy: 'Verwendet von',
    rlhfUsedBy: 'GPT-4, Claude, frühe Llama-Modelle',
    dpoUsedBy: 'Llama 3, Zephyr, viele Open-Source-Modelle',

    // GRPO Section
    grpo: 'GRPO: Group Relative Policy Optimization',
    grpoDesc: 'GRPO ist eine von DeepSeek entwickelte Alignment-Technik, die relative Rankings innerhalb von Antwortgruppen verwendet und dabei die Notwendigkeit eines separaten Reward-Modells eliminiert, während die Trainingsstabilität erhalten bleibt.',
    grpoHow: 'Wie GRPO funktioniert',
    grpoHowDesc: 'Anstelle von absoluten Reward-Scores vergleicht GRPO mehrere Antworten auf denselben Prompt und verwendet ihre relativen Rankings zur Berechnung von Policy-Gradienten.',
    grpoStep1: 'Antwortgruppe generieren',
    grpoStep1Desc: 'Für jeden Prompt werden mehrere Kandidaten-Antworten (typischerweise 4-16) von der aktüllen Policy generiert.',
    grpoStep2: 'Innerhalb der Gruppe ranken',
    grpoStep2Desc: 'Antworten innerhalb jeder Gruppe bewerten und ranken. Das Ranking kann verifizierbare Belohnungen (für Mathe/Code) oder gelernte Präferenzen verwenden.',
    grpoStep3: 'Relatives Gradienten-Update',
    grpoStep3Desc: 'Die Policy aktualisieren, um die Wahrscheinlichkeit höher gerankter Antworten relativ zu niedriger gerankten innerhalb jeder Gruppe zu erhöhen.',
    grpoAdvantages: 'Vorteile',
    grpoAdv1: 'Kein separates Reward-Modell nötig—reduziert Speicher und Komplexität',
    grpoAdv2: 'Stabiler als PPO—relative Vergleiche sind robuster als absolute Scores',
    grpoAdv3: 'Funktioniert gut mit verifizierbaren Belohnungen (Mathe, Code) und gelernten Präferenzen',
    grpoUseCases: 'Wichtige Anwendungen',
    grpoUse1: 'DeepSeek R1 Reasoning-Modell-Training',
    grpoUse2: 'Optimierung mathematischer und Coding-Aufgaben',
    grpoUse3: 'Effizientes Alignment ohne Reward-Modell-Overhead',

    // Synthetic Data Section
    syntheticData: 'Synthetische Daten für Alignment',
    syntheticDataDesc: 'Die Verwendung von KI-Modellen zur Generierung von Trainingsdaten revolutioniert das Alignment. Dieser Ansatz kann über die menschliche Annotationskapazität hinaus skalieren und dabei durch sorgfältiges Design Qualität bewahren.',
    syntheticHow: 'Methoden zur Generierung synthetischer Daten',
    syntheticHowDesc: 'Mehrere Techniken haben sich für die Generierung hochwertiger synthetischer Trainingsdaten für Alignment entwickelt.',
    syntheticCAI: 'Constitutional AI (Anthropic)',
    syntheticCAIDesc: 'Das Modell kritisiert und überarbeitet seine eigenen Ausgaben basierend auf einem Satz von Prinzipien. Die KI generiert sowohl die problematische Antwort als auch die verbesserte Version und erstellt so Präferenzpaare ohne menschliche Beschriftung.',
    syntheticSelfInstruct: 'Self-Instruct & Evol-Instruct',
    syntheticSelfInstructDesc: 'Modelle generieren ihre eigenen Anweisung-Antwort-Paare, die dann auf Qualität gefiltert werden. Evol-Instruct (verwendet in WizardLM) macht Anweisungen iterativ komplexer.',
    syntheticDistillation: 'Modell-Destillation',
    syntheticDistillationDesc: 'Ein größeres, fähigeres Modell generiert Trainingsdaten für ein kleineres Modell. Dies überträgt Wissen und Alignment-Eigenschaften, wie bei vielen Open-Source-Modellen zu sehen, die auf GPT-4-Ausgaben trainiert wurden.',
    syntheticBenefits: 'Vorteile',
    syntheticBenefit1: 'Massive Skalierung—Millionen von Beispielen günstig generieren',
    syntheticBenefit2: 'Konsistente Qualität—keine Ermüdung oder Uneinigkeit menschlicher Annotatoren',
    syntheticBenefit3: 'Gezielte Generierung—Daten für spezifische Schwächen erstellen',
    syntheticRisks: 'Risiken & Einschränkungen',
    syntheticRisk1: 'Modellkollaps—Training auf KI-generierten Daten kann Fähigkeiten verschlechtern',
    syntheticRisk2: 'Bias-Verstärkung—KI-Biases werden in synthetischen Daten verstärkt',
    syntheticRisk3: 'Qualitätsobergrenze—Qualität synthetischer Daten durch Qüllmodell begrenzt',

    // Fine-tuning vs Alignment
    fineTuningVsAlignment: 'Fine-Tuning vs. Alignment',
    fineTuningVsAlignmentDesc: 'Fine-Tuning und Alignment sind verwandte, aber unterschiedliche Konzepte.',
    fineTuningDef: 'Fine-Tuning',
    fineTuningDefDesc: 'Ein Modell an neue Aufgaben oder Domänen anpassen, indem auf aufgabenspezifischen Daten trainiert wird. Kann für jeden Zweck durchgeführt werden.',
    alignmentDef: 'Alignment',
    alignmentDefDesc: 'Speziell das Verhalten eines Modells an menschliche Werte und Absichten anpassen. Eine Teilmenge von Fine-Tuning mit einem spezifischen Ziel.',
    postTrainingDef: 'Post-Training',
    postTrainingDefDesc: 'Der Oberbegriff für alles nach dem Pretraining: SFT, RLHF, spezialisiertes Fine-Tuning, Sicherheitstraining, etc.',

    // Pipeline Visualizer
    pipelineVisualizer: 'Interaktive Trainingspipeline',
    pipelineVisualizerDesc: 'Erkunden Sie die vollständige LLM-Trainingspipeline von der Datensammlung bis zur Bereitstellung. Klicken Sie auf eine beliebige Stufe, um detaillierte Informationen über Datenanforderungen, Rechenkosten und Zeitpläne zu erhalten.',

    // Detaillierte Pipeline-Stufen
    detailedPipeline: 'Vollständige Trainingspipeline (8 Stufen)',
    detailedPipelineDesc: 'Modernes LLM-Training umfasst 8 Hauptstufen, jede mit unterschiedlichen Zielen, Datenanforderungen und Rechenkosten. Das Verständnis dieser Pipeline ist entscheidend, um die Komplexität und Kosten des Trainings von Frontier-Modellen zu erfassen.',

    stage1Title: 'Datensammlung & Kuration',
    stage1Desc: 'Sammeln massiver Textkorpora aus vielfältigen Quellen einschließlich Web-Crawls, Büchern, Code-Repositories und wissenschaftlichen Papers.',
    stage1Data: 'Common Crawl-Snapshots (Petabytes), The Pile, GitHub, Wikipedia, Books3, akademische Papers',
    stage1Volume: '1-15 Billionen Tokens roh (vor Bereinigung)',
    stage1Cost: '$50K-500K (Speicher, Bandbreite, Kurations-Tools)',

    stage2Title: 'Datenbereinigung & Deduplizierung',
    stage2Desc: 'Duplikate entfernen, minderwertige Inhalte filtern, Sprachen erkennen, PII entfernen und Formatierung normalisieren.',
    stage2Data: 'MinHash LSH Deduplizierung, Perplexitäts-basierte Qualitätsfilterung, Spracherkennung, Toxizitätsfilterung',
    stage2Techniques: 'Exakte & Fuzzy-Deduplizierung (MinHash), Qualitätsbewertung (KenLM-Perplexität), PII-Entfernung, toxische Inhaltsfilterung',
    stage2Cost: '$100K-1M (CPU-Cluster, Tausende von Kernen für Wochen)',

    stage3Title: 'Tokenisierung',
    stage3Desc: 'Bereinigten Text in numerische Token-Sequenzen mit BPE, SentencePiece oder Unigram-Tokenizern konvertieren.',
    stage3Methods: 'BPE (GPT), SentencePiece (Llama), Unigram (T5)',
    stage3Vocab: 'Vokabulargröße: ~32K-100K Tokens',
    stage3Duration: '1-2 Wochen (Tokenizer trainieren, Korpus tokenisieren, Sequenzen packen)',

    stage4Title: 'Pre-training',
    stage4Desc: 'Das Basismodell auf Billionen von Tokens mit Next-Token-Prediction-Ziel trainieren. Dies ist die teuerste und rechenintensivste Stufe.',
    stage4Compute: '10K-30K GPUs (H100/A100), 1-6 Monate Training',
    stage4Examples: 'Llama 3 70B: 15T Tokens, max 24K GPUs, ~$20M. GPT-4: angeblich $100M+',
    stage4Cost: '$2M-$200M abhängig von der Modellgröße (70B-Modell: ~$10-20M)',

    stage5Title: 'Supervised Fine-Tuning (SFT)',
    stage5Desc: 'Das Basismodell auf kuratierten Anweisung-Antwort-Paaren feintunen, um ihm beizubringen, Anweisungen zu befolgen und hilfreich zu antworten.',
    stage5Data: '10K-100K hochwertige (Prompt, Vervollständigung)-Paare (von Menschen geschrieben oder synthetisch)',
    stage5Compute: '100-1000 GPUs, 1-4 Wochen',
    stage5Cost: '$50K-2M (einschließlich menschlicher Annotation oder Generierung synthetischer Daten)',

    stage6Title: 'RLHF / Präferenz-Tuning',
    stage6Desc: 'Modellausgaben mit menschlichen Präferenzen durch Reinforcement Learning (PPO) oder Direct Preference Optimization (DPO) alignieren.',
    stage6Methods: 'PPO (Proximal Policy Optimization) oder DPO (Direct Preference Optimization)',
    stage6Data: '10K-100K menschliche Präferenzvergleiche (A vs B Rankings)',
    stage6Cost: '$200K-10M (PPO erfordert 500-5000 GPUs; DPO ist 5-10x günstiger)',

    stage7Title: 'Sicherheit & Evaluation',
    stage7Desc: 'Red-Team das Modell, führe adversarielle Tests durch, wende Constitutional AI-Prinzipien an und benchmarke auf Standard-Evaluations-Suites.',
    stage7Methods: 'Red-Teaming, adversarielle Prompts, Constitutional AI Selbst-Kritik, Sicherheitsklassifizierer',
    stage7Benchmarks: 'MMLU, HumanEval, TruthfulQA, BBH, Sicherheits-Benchmarks',
    stage7Ongoing: 'Kontinuierlicher Prozess während und nach der Bereitstellung ($100K-5M fortlaufend)',

    stage8Title: 'Bereitstellungsoptimierung',
    stage8Desc: 'Das Modell für Produktions-Deployment durch Quantisierung, Destillation und Inferenz-Infrastruktur-Setup optimieren.',
    stage8Techniques: 'Quantisierung (GPTQ, AWQ, GGUF), Wissensdestillation zu kleineren Modellen',
    stage8Optimization: 'KV-Cache-Optimierung, Batching-Strategien, Serving-Infrastruktur (TensorRT, vLLM)',
    stage8Cost: '$50K-1M (GPU-Zeit für Quantisierung + Infrastruktur-Setup)',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'LLM-Training hat distinkte Stufen: Pretraining → SFT → RLHF → spezialisiertes Alignment',
    takeaway2: 'Das RL-Paradigma (z.B. DeepSeek R1-Zero) zeigt, dass Denken aus reinem RL ohne menschliche Demonstrationen entstehen kann',
    takeaway3: 'RLHF aligniert Modelle mit menschlichen Präferenzen; reines RL optimiert für verifizierbare Ergebnisse',
    takeaway4: 'Moderne Modelle kombinieren oft mehrere Techniken: SFT für Anweisungsbefolgung, RLHF für Präferenzen, RL für Denken',
    takeaway5: 'Das Verständnis der Trainingspipeline hilft, Modellverhalten und -einschränkungen zu verstehen',
    takeaway6: 'Das Feld entwickelt sich schnell – neue Paradigmen wie reines RL verändern, wie wir über Training denken',
  },

  // Mixture of Experts Seite
  moe: {
    title: 'Mixture of Experts',
    description: 'Verstehen von spärlich aktivierten Modellen, die spezialisierte Expertennetzwerke für effiziente Skalierung nutzen.',
    whatIs: 'Was ist Mixture of Experts?',
    whatIsDesc: 'ist eine neuronale Netzwerkarchitektur, die Berechnungen auf spezialisierte Teilnetzwerke namens "Experten" aufteilt. Für jede Eingabe wird nur eine Teilmenge der Experten aktiviert, was massive Modellkapazität bei handhabbaren Rechenkosten ermöglicht.',
    brainAnalogy: '"Genau wie das Gehirn je nach Aufgabe bestimmte Regionen aktiviert, aktivieren MoE-Modelle nur die relevanten Experten für jedes Token."',
    brainAnalogyDesc: '— Dieser biomimetische Ansatz ermöglicht Modelle mit Billionen von Parametern, während bei der Inferenz nur ein Bruchteil verwendet wird.',

    // Wie es funktioniert
    howItWorks: 'Wie MoE funktioniert',
    step1Title: 'Eingabe kommt an',
    step1Desc: 'Jedes Token (oder Gruppe von Tokens) wird durch die Transformer-Schichten verarbeitet, bis es die MoE-Schicht erreicht, die das traditionelle dichte Feed-Forward-Netzwerk (FFN) ersetzt.',
    step2Title: 'Router wählt Experten',
    step2Desc: 'Ein Gating-Netzwerk (Router) untersucht die Eingabe und bestimmt, welche Experten sie verarbeiten sollen. Typischerweise werden nur die top-K Experten (z.B. top-2 oder top-8) mit den höchsten Werten ausgewählt.',
    step3Title: 'Experten verarbeiten & kombinieren',
    step3Desc: 'Die ausgewählten Experten verarbeiten die Eingabe parallel. Ihre Ausgaben werden mit den Router-Scores gewichtet und kombiniert, um das Endergebnis zu erzeugen.',

    // Router
    routerTitle: 'Der Router (Gating-Netzwerk)',
    routerSubtitle: 'Das Gehirn des MoE-Systems',
    routerDesc: 'Der Router ist ein kleines neuronales Netzwerk, das lernt, Tokens zu geeigneten Experten zu leiten. Er gibt eine Wahrscheinlichkeitsverteilung über alle Experten aus und bestimmt, welche aktiviert werden.',
    topKRouting: 'Top-K Routing',
    topKRoutingDesc: 'Nur die K Experten mit den höchsten Scores werden aktiviert. Übliche Werte sind top-2 (Mixtral) oder top-8 (DeepSeek, Qwen). Dies stellt sicher, dass die Rechenkosten unabhängig von der Gesamtzahl der Experten konstant bleiben.',
    loadBalancing: 'Lastverteilung',
    loadBalancingDesc: 'Das Training beinhaltet Hilfsverluste, um "Expertenkollaps" zu verhindern, bei dem alle Tokens zu den gleichen wenigen Experten geleitet werden. Dies stellt sicher, dass alle Experten genutzt werden und unterschiedliche Spezialisierungen entwickeln.',

    // Expertenspezialisierung
    expertSpecialization: 'Expertenspezialisierung',
    expert1Title: 'Domänenexperten',
    expert1Desc: 'Einige Experten spezialisieren sich natürlich auf Domänen wie Code, Mathematik oder bestimmte Sprachen. Dies entsteht aus dem Training, nicht aus explizitem Design.',
    expert2Title: 'Musterexperten',
    expert2Desc: 'Experten können sich auf linguistische Muster wie formelles Schreiben, Konversationston oder technische Terminologie spezialisieren.',
    expert3Title: 'Aufgabenexperten',
    expert3Desc: 'Einige Experten werden besser bei bestimmten Aufgaben wie Zusammenfassung, Übersetzung oder Schlussfolgerung – obwohl die Grenzen oft fließend sind.',
    expertNote: 'Expertenspezialisierung entsteht organisch während des Trainings. Forscher arbeiten noch daran, vollständig zu verstehen, was jeder Experte lernt.',

    // Skalierung
    scaleTitle: 'MoE im großen Maßstab: Reale Modelle',
    modelColumn: 'Modell',
    totalParams: 'Gesamtparameter',
    activeParams: 'Aktiv pro Token',
    expertsColumn: 'Experten (Routing)',
    scaleNote: 'Beachte, wie die aktiven Parameter 5-20x kleiner sind als die Gesamtparameter – das ist der Effizienzvorteil von MoE.',

    // Vorteile
    advantagesTitle: 'Warum MoE wichtig ist',
    advantage1Title: 'Massive Kapazität, effiziente Inferenz',
    advantage1Desc: 'MoE-Modelle können Billionen von Parametern haben, aktivieren aber nur einen Bruchteil pro Token. Dies ermöglicht viel größere Modellkapazität ohne proportional steigende Inferenzkosten.',
    advantage2Title: 'Schnelleres Training',
    advantage2Desc: 'Recheneffizienteres Pretraining, da jeder Parameter nur von einer Teilmenge der Tokens aktualisiert wird. Die gleiche Leistung kann mit weniger Gesamt-Rechenaufwand erreicht werden.',
    advantage3Title: 'Spezialisierte Verarbeitung',
    advantage3Desc: 'Verschiedene Experten können sich auf verschiedene Inhaltstypen spezialisieren – Code, Mathematik, Sprachen – was bessere Leistung über diverse Aufgaben bietet.',
    advantage4Title: 'Skalierbare Architektur',
    advantage4Desc: 'Mehr Experten hinzuzufügen erhöht die Kapazität ohne die Inferenzkosten zu ändern (solange top-K gleich bleibt). Dies ermöglicht kontinuierliche Skalierung.',

    // Herausforderungen
    challengesTitle: 'Herausforderungen von MoE',
    challenge1Title: 'Hohe Speicheranforderungen',
    challenge1Desc: 'Alle Expertenparameter müssen in den Speicher geladen werden, obwohl nur eine Teilmenge pro Token verwendet wird. Ein 671B-Parameter-Modell benötigt 671B Parameter im VRAM.',
    challenge2Title: 'Trainingsinstabilität',
    challenge2Desc: 'Die Lastverteilung zwischen Experten ist knifflig. Ohne sorgfältiges Tuning werden einige Experten möglicherweise nie verwendet ("tote Experten") oder alle Tokens werden zu den gleichen wenigen Experten geleitet.',
    challenge3Title: 'Kommunikationsoverhead',
    challenge3Desc: 'Bei verteiltem Training/Inferenz führt das Routing von Tokens zu Experten auf verschiedenen GPUs zu Netzwerk-Kommunikationsoverhead.',

    // Vergleich
    comparisonTitle: 'Dichte vs. Spärliche Modelle',
    denseModel: 'Dichtes Modell',
    dense1: 'Alle Parameter aktiv für jedes Token',
    dense2: 'Einfacheres Training und Deployment',
    dense3: 'Speicher = Rechenkosten (beide skalieren zusammen)',
    sparseModel: 'Spärliches MoE-Modell',
    sparse1: 'Nur top-K Experten aktiv pro Token',
    sparse2: 'Höhere Gesamtkapazität bei gleichem Rechenaufwand',
    sparse3: 'Speicher >> Rechenkosten (entkoppelt)',

    // Wichtige Erkenntnisse
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'MoE ermöglicht massive Modellkapazität mit handhabbaren Inferenzkosten, indem nur eine Teilmenge der Experten pro Token aktiviert wird',
    takeaway2: 'Fast alle führenden Frontier-Modelle (DeepSeek, Qwen, Mixtral, Llama 4) nutzen jetzt MoE-Architekturen',
    takeaway3: 'Das Router/Gating-Netzwerk lernt, Tokens zu spezialisierten Experten zu leiten – Spezialisierung entsteht aus dem Training',
    takeaway4: 'Der Hauptkompromiss: hohe Speicheranforderungen (alle Experten geladen) vs. effiziente Berechnung (wenige Experten aktiv)',

    // Interaktiver Visualizer
    vizTitle: 'MoE Generierungs-Visualizer',
    vizSubtitle: '8 Experten, top-2 Routing (wie Mixtral)',
    vizGenerate: 'Generieren',
    vizVramWarning: 'Alle Experten müssen im VRAM geladen sein',
    vizVramExplain: 'Obwohl nur 2 Experten pro Token aktiviert werden, müssen alle 8 Experten im GPU-Speicher geladen bleiben. Deshalb haben MoE-Modelle trotz effizienter Berechnung hohe Speicheranforderungen.',
    vizVramUsage: 'VRAM-Nutzung',
    vizLoaded: 'geladen',
    vizActive: 'aktiv',
    vizMemoryFootprint: '100% Speicherbedarf',
    vizCannotOffload: 'Inaktive Experten können nicht ausgelagert werden',
    vizRouter: 'Router',
    vizNextToken: 'Nächstes Token zu generieren',
    vizRouterLabel: 'Gating-Netzwerk',
    vizExpertsLabel: 'Experten (alle im VRAM geladen)',
    vizGeneratedText: 'Generierter Text',
    vizKeyInsight: 'Wichtige Erkenntnis: Speicher vs. Rechen-Kompromiss',
    vizKeyInsightDesc: 'Ein 46,7B Parameter MoE-Modell wie Mixtral 8x7B benötigt VRAM für alle 46,7B Parameter, nutzt aber nur ~12,9B Parameter pro Token. Man zahlt den Speicherpreis im Voraus, erhält aber effiziente Inferenz.',
    vizTrainingTitle: 'Trainingskomplexität: Lastverteilung',
    vizTrainingDesc: 'Experten haben keine festen Spezialisierungen – was jeder Experte lernt, entsteht organisch während des Trainings. Das schafft eine große Herausforderung:',
    vizTraining1: 'Ohne sorgfältige Balancierung könnte der Router immer dieselben wenigen Experten wählen, sodass andere als "tote Experten" zurückbleiben, die sich nie verbessern',
    vizTraining2: 'Hilfsverlustfunktionen bestrafen ungleiche Expertennutzung und zwingen den Router, Tokens gleichmäßiger über alle Experten zu verteilen',
    vizTraining3: 'Selbst mit Balancierung bleibt die Experten-Spezialisierung unscharf – derselbe Experte kann Mathematik, bestimmte Sprachen UND spezifische Syntax-Muster verarbeiten',
  },

  // Quantization page
  quantization: {
    title: 'Quantisierung',
    description: 'Wie die Reduzierung numerischer Präzision es ermöglicht, große Modelle auf Consumer-Hardware mit minimalem Qualitätsverlust auszuführen.',

    // What is Quantization
    whatIs: 'Was ist Quantisierung?',
    whatIsDesc: 'ist der Prozess der Reduzierung der numerischen Präzision von Modellgewichten von 32-Bit-Gleitkomma (FP32) auf niedrigere Bit-Darstellungen wie FP16, INT8 oder INT4. Dies reduziert den Speicherbedarf dramatisch und beschleunigt die Inferenz.',
    analogy: '"Wie das Komprimieren eines hochauflösenden Fotos für dein Handy – du verlierst etwas Detail, aber das Bild bleibt erkennbar und nützlich."',
    analogyDesc: '— Die wichtigste Erkenntnis ist, dass neuronale Netzwerke überraschend robust gegenüber Präzisionsverlust sind. Die meisten Gewichte können mit weit weniger Bits gespeichert werden, ohne katastrophale Qualitätsverschlechterung.',

    // Why Quantize
    whyQuantize: 'Warum Quantisieren?',
    whyQuantizeDesc: 'Quantisierung ermöglicht es, große Modelle auf Consumer-Hardware auszuführen und reduziert die Inferenzkosten in der Produktion.',
    benefit1Title: 'Speicherreduktion',
    benefit1Desc: 'Ein 70B-Parameter-Modell bei FP16 benötigt ~140GB VRAM. Bei INT4 passt es in ~35GB – ausführbar auf High-End-Consumer-GPUs.',
    benefit2Title: 'Schnellere Inferenz',
    benefit2Desc: 'Arithmetik mit niedrigerer Präzision ist schneller. INT8-Operationen sind 2-4x schneller als FP32 auf moderner Hardware.',
    benefit3Title: 'Niedrigere Kosten',
    benefit3Desc: 'Kleinere Modelle bedeuten weniger GPUs, niedrigere Cloud-Kosten und Machbarkeit für Edge-Deployment.',
    benefit4Title: 'Demokratisierung',
    benefit4Desc: 'Ermöglicht Forschern und Hobbyisten, Frontier-Klasse-Modelle lokal ohne Enterprise-Hardware auszuführen.',

    // Interactive Visualizer
    vizTitle: 'Quantisierungs-Visualizer',
    vizSubtitle: 'Sieh, wie Präzision Modellgröße und Qualität beeinflusst',
    vizPrecision: 'Präzisionsstufe',
    vizModelSize: 'Modellgröße',
    vizAccuracy: 'Erhaltene Genauigkeit',
    vizPerplexity: 'Perplexitätsanstieg',
    vizOfOriginal: 'vom Original',
    vizRetained: 'erhalten',
    vizWeightDist: 'Gewichtsverteilung',
    vizDiscreteLevel: 'diskrete Stufen',
    vizExplanation: 'Erklärung',
    vizFp32Explain: 'Volle Präzision (32 Bit). Maximale Genauigkeit, maximaler Speicher. Wird hauptsächlich für Training verwendet.',
    vizFp16Explain: 'Halbe Präzision (16 Bit). Vernachlässigbarer Qualitätsverlust für die meisten Aufgaben. Standard für Inferenz.',
    vizInt8Explain: 'Integer-Präzision (8 Bit). Exzellente Balance aus Qualität und Effizienz. Produktionsstandard.',
    vizInt4Explain: 'Stark komprimiert (4 Bit). Sweet Spot für Consumer-Hardware. Die meisten Nutzer bemerken keinen Qualitätsunterschied.',
    vizInt2Explain: 'Extreme Kompression (2 Bit). Signifikanter Qualitätsverlust. Nur für extreme Speicherbeschränkungen.',

    // Quantization Levels
    levelsTitle: 'Quantisierungsstufen erklärt',
    levelsDesc: 'Jede Präzisionsstufe repräsentiert einen unterschiedlichen Kompromiss zwischen Modellgröße und Ausgabequalität.',
    levelBits: 'Bits',
    levelSize: 'Größe',
    levelAccuracy: 'Genauigkeit',
    levelUseCase: 'Anwendungsfall',
    levelFp32: 'FP32 (Voll)',
    levelFp32Size: '100%',
    levelFp32Accuracy: '100%',
    levelFp32Use: 'Training, Referenz-Inferenz',
    levelFp16: 'FP16 (Halb)',
    levelFp16Size: '50%',
    levelFp16Accuracy: '~99%',
    levelFp16Use: 'Standard-Inferenz',
    levelInt8: 'INT8',
    levelInt8Size: '25%',
    levelInt8Accuracy: '~97%',
    levelInt8Use: 'Produktions-Deployment',
    levelInt4: 'INT4',
    levelInt4Size: '12.5%',
    levelInt4Accuracy: '~90-95%',
    levelInt4Use: 'Consumer-GPUs, Edge',
    levelInt2: 'INT2',
    levelInt2Size: '6.25%',
    levelInt2Accuracy: '~70-80%',
    levelInt2Use: 'Extreme Edge-Fälle',

    // Recommendation
    recommendTitle: 'Empfehlung: Q4 ist der Sweet Spot',
    recommendDesc: 'Für die meisten Nutzer, die große Modelle (70B+ Parameter) lokal ausführen:',
    recommend1: 'Q4 (INT4) bietet ein exzellentes Qualitäts-zu-Speicher-Verhältnis',
    recommend2: 'Die meisten Nutzer können Q4-Ausgabe in Blindtests nicht von FP16 unterscheiden',
    recommend3: 'Ermöglicht die Ausführung von 70B-Modellen auf 24GB Consumer-GPUs',
    recommend4: 'Empfohlene Formate: Q4_K_M oder Q4_K_S für GGUF-Modelle',
    recommendNote: 'Für kritische Anwendungen, die maximale Genauigkeit erfordern, verwende FP16 oder INT8. Für gelegentliche Nutzung und Experimente ist Q4 ideal.',

    // Techniques
    techniquesTitle: 'Quantisierungstechniken',
    techniquesDesc: 'Verschiedene Methoden zur Konvertierung von Modellen auf niedrigere Präzision.',
    techPtq: 'PTQ (Post-Training-Quantisierung)',
    techPtqDesc: 'Quantisierung auf ein bereits trainiertes Modell anwenden. Schnell und einfach, aber möglicherweise etwas höherer Genauigkeitsverlust. Funktioniert durch Kalibrierung der Quantisierungsparameter auf einem kleinen Datensatz.',
    techQat: 'QAT (Quantization-Aware Training)',
    techQatDesc: 'Quantisierung in den Trainingsprozess einbeziehen. Das Modell lernt, robust gegenüber Präzisionsverlust zu sein, was bessere Genauigkeit ergibt, aber vollständiges Neutraining erfordert.',
    techGptq: 'GPTQ',
    techGptqDesc: 'One-Shot-Quantisierungsmethode für LLMs. Nutzt Informationen zweiter Ordnung, um den Quantisierungsfehler Schicht für Schicht zu minimieren. Beliebt für Geschwindigkeit und Qualität.',
    techAwq: 'AWQ (Activation-aware Weight Quantization)',
    techAwqDesc: 'Identifiziert und bewahrt "wichtige" Gewichte, die am meisten für die Genauigkeit zählen. Erreicht bessere Qualität als naive Quantisierung durch Schutz wichtiger Parameter.',
    techGguf: 'GGUF-Format',
    techGgufDesc: 'Dateiformat, das von llama.cpp für quantisierte Modelle verwendet wird. Unterstützt verschiedene Quantisierungsstufen (Q2-Q8) und ist der Standard für lokales LLM-Deployment.',

    // GGUF K-quants
    ggufTitle: 'GGUF K-Quant-Methoden',
    ggufDesc: 'Verständnis der Namenskonvention für GGUF-quantisierte Modelle.',
    ggufMethod: 'Methode',
    ggufQuality: 'Qualität',
    ggufSize: 'Größe',
    ggufUseCase: 'Anwendungsfall',
    ggufQ2K: 'Q2_K',
    ggufQ2KQuality: 'Schlecht',
    ggufQ2KSize: 'Kleinste',
    ggufQ2KUse: 'Nur extreme Kompression',
    ggufQ3KS: 'Q3_K_S',
    ggufQ3KSQuality: 'Niedrig',
    ggufQ3KSSize: 'Sehr klein',
    ggufQ3KSUse: 'Speicherbeschränkte Systeme',
    ggufQ3KM: 'Q3_K_M',
    ggufQ3KMQuality: 'Niedrig-Mittel',
    ggufQ3KMSize: 'Klein',
    ggufQ3KMUse: 'Budget-Hardware',
    ggufQ3KL: 'Q3_K_L',
    ggufQ3KLQuality: 'Mittel',
    ggufQ3KLSize: 'Moderat',
    ggufQ3KLUse: 'Bessere Q3-Qualität',
    ggufQ4KS: 'Q4_K_S',
    ggufQ4KSQuality: 'Gut',
    ggufQ4KSSize: 'Klein',
    ggufQ4KSUse: 'Empfohlene Balance',
    ggufQ4KM: 'Q4_K_M',
    ggufQ4KMQuality: 'Sehr gut',
    ggufQ4KMSize: 'Moderat',
    ggufQ4KMUse: 'Beste Gesamtwahl',
    ggufQ5KS: 'Q5_K_S',
    ggufQ5KSQuality: 'Exzellent',
    ggufQ5KSSize: 'Größer',
    ggufQ5KSUse: 'Qualitätsorientiert',
    ggufQ5KM: 'Q5_K_M',
    ggufQ5KMQuality: 'Exzellent',
    ggufQ5KMSize: 'Größer',
    ggufQ5KMUse: 'Nahe FP16-Qualität',
    ggufQ6K: 'Q6_K',
    ggufQ6KQuality: 'Nahezu perfekt',
    ggufQ6KSize: 'Groß',
    ggufQ6KUse: 'Minimaler Verlust',
    ggufQ8: 'Q8_0',
    ggufQ8Quality: 'Exzellent',
    ggufQ8Size: 'Groß',
    ggufQ8Use: 'Referenzqualität',
    ggufExplainTitle: 'K-Quant-Benennung erklärt',
    ggufExplainK: 'K = "K-quant" — verwendet wichtigkeitsbasierte Quantisierung, die die Präzision pro Schicht variiert',
    ggufExplainS: 'S (Small) = Aggressivere Quantisierung bei Attention-Schichten, kleinere Dateien',
    ggufExplainM: 'M (Medium) = Ausgewogene Quantisierung über alle Schichten, bestes Qualitäts-/Größenverhältnis',
    ggufExplainL: 'L (Large) = Weniger Quantisierung bei wichtigen Schichten, bessere Qualität',
    ggufKeyInsight: 'Wichtige Erkenntnis: K-Quants sind "gemischte Präzision" – sie quantisieren verschiedene Schichten unterschiedlich basierend auf ihrer Wichtigkeit für die Modellqualität. Attention-Schichten verwenden typischerweise höhere Präzision als Feed-Forward-Schichten.',

    // Real-World Impact
    impactTitle: 'Praxisauswirkungen',
    impactDesc: 'Konkrete Beispiele, was Quantisierung ermöglicht.',
    impactExample1Title: 'Llama 3.1 70B bei verschiedenen Quants',
    impactExample1Desc: 'Ein 70B-Parameter-Modell benötigt ~140GB bei FP16. Mit Quantisierung:',
    impactExample1Q8: 'Q8: ~70GB — Passt auf 2x A100 40GB oder 1x H100',
    impactExample1Q4: 'Q4_K_M: ~40GB — Passt auf 2x RTX 4090 oder 1x A100 80GB',
    impactExample1Q3: 'Q3_K_M: ~30GB — Passt auf einzelne RTX 4090 (24GB + etwas Offload)',
    impactExample2Title: 'Qualitätsvergleich',
    impactExample2Desc: 'In Blindtests beim Vergleich von Q4_K_M mit FP16-Ausgaben:',
    impactExample2Stat1: '85% der Nutzer konnten nicht identifizieren, welche quantisiert war',
    impactExample2Stat2: 'Perplexitätsanstieg von nur 0,1-0,5 Punkten auf gängigen Benchmarks',
    impactExample2Stat3: 'Code-Completion und Reasoning-Aufgaben zeigen minimale Verschlechterung',
    impactExample3Title: 'Kosteneinsparungen',
    impactExample3Desc: 'Ausführen eines 70B-Modells für Inferenz:',
    impactExample3Fp16: 'FP16: ~4-8€/Stunde in der Cloud (2x A100)',
    impactExample3Q4: 'Q4: ~1-2€/Stunde (einzelne A100 oder High-End Consumer-GPU)',
    impactExample3Local: 'Lokal: Einmalige Kosten einer Consumer-GPU vs. laufende Cloud-Gebühren',

    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Quantisierung reduziert den Modellspeicher um das 2-16-fache mit überraschend geringem Genauigkeitsverlust',
    takeaway2: 'Q4 (INT4) ist der Sweet Spot für die meisten lokalen LLM-Anwendungsfälle – exzellente Qualität bei 1/8 des Speichers',
    takeaway3: 'K-Quant-Methoden (Q4_K_M, Q5_K_S) sind "gemischte Präzision" und übertreffen gleichmäßige Quantisierung',
    takeaway4: 'GPTQ und AWQ sind die führenden Techniken für LLM-Quantisierung, mit GGUF als Standardformat',
    takeaway5: 'Quantisierung demokratisiert KI, indem sie Frontier-Modelle auf Consumer-Hardware ermöglicht',
    takeaway6: 'Für kritische Anwendungen höhere Präzision (INT8/FP16) bevorzugen; für Experimente ist Q4 ideal',
  },

  // Nested Learning page
  nestedLearning: {
    title: 'Verschachteltes Lernen',
    description: 'Ein neuer Ansatz, der KI Neues lernen lässt, ohne Altes zu vergessen — durch gleichzeitiges Lernen mit verschiedenen Geschwindigkeiten.',

    // Research disclaimer
    researchDisclaimer: 'Forschungsvorschau',
    researchDisclaimerDesc: 'Verschachteltes Lernen wurde auf der NeurIPS 2025 von Google Research vorgestellt. Spannende Spitzenforschung — aber noch nicht in Produktionssystemen im Einsatz.',

    // Analogy intro
    analogyTitle: 'Das Vergessens-Problem',
    analogyDesc: 'Stell dir vor, du hast jahrelang Klavier spielen gelernt. Dann entscheidest du dich, Gitarre zu lernen. Nach Monaten Gitarrenübung setzt du dich wieder ans Klavier — und deine Finger haben die Hälfte der Stücke vergessen. Genau das passiert heute mit KI-Modellen.',
    analogyQuote: 'Fahrradfahren lernen sollte nicht dazu führen, dass man Schwimmen verlernt. Aber bei heutiger KI passiert genau das.',
    analogyPunchline: 'Verschachteltes Lernen versucht das zu lösen. Statt einem großen Lernprozess, der alles überschreibt, nutzt es mehrere Lerngeschwindigkeiten gleichzeitig — ähnlich wie dein Gehirn Muskelgedächtnis, Gewohnheiten und bewusstes Denken parallel verarbeitet.',

    // The Problem
    problemTitle: 'Sieh selbst: Katastrophales Vergessen',
    problemDesc: 'Das ist das Kernproblem. Wenn du eine KI mit etwas Neuem trainierst, vergisst sie tendenziell, was sie schon wusste. Probier die Demo unten aus — trainiere Aufgabe A, dann Aufgabe B, und beobachte wie das Wissen von Aufgabe A verschwindet.',

    // Forgetting demo
    forgettingDemoTitle: 'Probier es: Trainieren und Vergessen beobachten',
    forgettingTaskA: 'Aufgabe A Wissen',
    forgettingTaskB: 'Aufgabe B Wissen',
    forgettingTrainA: '1. Aufgabe A trainieren',
    forgettingTrainB: '2. Jetzt Aufgabe B trainieren',
    forgettingReset: 'Zurücksetzen',
    forgettingKnowledge: 'Wissen',
    forgettingStep: 'Schritt',
    forgettingWarning: 'Katastrophales Vergessen! Das Wissen von Aufgabe A wurde beim Lernen von Aufgabe B zerstört.',

    // Solution
    solutionTitle: 'Die Lösung: Lernen mit verschiedenen Geschwindigkeiten',
    solutionDesc: 'Die Kernidee von Verschachteltem Lernen ist überraschend intuitiv: statt einem Lernprozess nutze mehrere, die mit verschiedenen Geschwindigkeiten laufen — ineinander verschachtelt wie Matrjoschka-Puppen.',
    speedSlow: '🐢 Langsam',
    speedSlowDesc: 'Kernwissen — ändert sich selten, bleibt stabil',
    speedMedium: '🚶 Mittel',
    speedMediumDesc: 'Muster & Fähigkeiten — passt sich über Zeit an',
    speedFast: '⚡ Schnell',
    speedFastDesc: 'Unmittelbarer Kontext — reagiert auf das, was gerade passiert',
    solutionAnalogy: 'Denk an dein Gehirn: Deine Persönlichkeit ändert sich langsam über Jahre, deine Gewohnheiten über Wochen, und deine Aufmerksamkeit wechselt jede Sekunde. Jede "Geschwindigkeit" kümmert sich um eine andere Art von Wissen, ohne die anderen zu stören.',

    // Nested loops
    loopsTitle: 'Die Schleifen in Aktion',
    loopsDesc: 'Die drei Lerngeschwindigkeiten laufen als verschachtelte Schleifen. Die innere Schleife dreht sich schnell (passt sich an direkten Input an), die mittlere tickt gemächlich (baut Muster auf), und die äußere bewegt sich langsam (festigt tiefes Wissen). Drück Play, um sie in Bewegung zu sehen.',
    loopsDemoTitle: 'Verschachtelte Lernschleifen',
    outerLoop: 'Außen',
    middleLoop: 'Mitte',
    innerLoop: 'Innen',
    outerLoopDesc: 'Langsam — tiefes Wissen',
    middleLoopDesc: 'Mittel — Muster',
    innerLoopDesc: 'Schnell — Anpassung',
    loopsRunning: 'Läuft...',
    loopsPaused: 'Pausiert',
    loopsPlay: '▶ Abspielen',
    loopsPause: '⏸ Pause',

    // Comparison
    comparisonTitle: 'Direktvergleich: Traditionell vs. Verschachtelt',
    comparisonDesc: 'Was passiert, wenn du drei Aufgaben nacheinander trainierst? Traditionelles Lernen vergisst frühere Aufgaben. Verschachteltes Lernen bewahrt sie. Starte die Demo, um den Unterschied zu sehen.',
    comparisonDemoTitle: 'Training mit 3 aufeinanderfolgenden Aufgaben',
    traditionalTitle: '❌ Traditionell',
    nestedTitle: '✅ Verschachtelt',
    compTaskA: 'Aufgabe A',
    compTaskB: 'Aufgabe B',
    compTaskC: 'Aufgabe C',
    compRun: '▶ Vergleich starten',
    compReset: 'Zurücksetzen',

    // Hope Architecture
    hopeTitle: 'Die Hope-Architektur',
    hopeDesc: 'Google Research hat "Hope" als Proof-of-Concept gebaut. Eine Architektur, die diese verschachtelten Lernideen tatsächlich umsetzt. Die Schlüsselinnovation: Das Modell kann seine eigenen Lernregeln während des Lernens anpassen.',
    hopeDiagramTitle: 'Wie Hope funktioniert',
    hopeInput: 'Eingabe',
    hopeSelfMod: 'Selbst-Mod.',
    hopeMemory: 'Speicher',
    hopeOutput: 'Ausgabe',
    hopeLearnRules: 'lernt Regeln',
    hopeStoreRecall: 'speichern & abrufen',
    hopePoint1: 'Schlägt Standard-Transformer bei Sprachverständnis-Benchmarks',
    hopePoint2: 'Kommt besser mit sehr langen Kontexten zurecht (wie die Nadel im Heuhaufen finden)',
    hopePoint3: 'Kann weiterlernen ohne zu vergessen — der ganze Punkt',

    // Why it matters
    mattersTitle: 'Warum das wichtig ist',
    mattersDesc: 'Wenn das im großen Maßstab funktioniert, ändert es alles daran, wie wir KI bauen:',
    matter1Title: 'Immer Lernend',
    matter1Desc: 'KI, die sich durch reale Nutzung verbessert — ohne teures Neutraining',
    matter2Title: 'Effizienter',
    matter2Desc: 'Mehrere Lerngeschwindigkeiten könnten weniger Rechenleistung brauchen als Brute-Force-Ansätze',
    matter3Title: 'Gehirn-ähnlich',
    matter3Desc: 'Viel näher daran, wie biologische Gehirne wirklich funktionieren — verschiedene Systeme für verschiedene Zeitskalen',

    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Heutige KI hat ein Vergessens-Problem: Neues lernen löscht altes Wissen',
    takeaway2: 'Verschachteltes Lernen löst das durch mehrere Lerngeschwindigkeiten — langsam für Kernwissen, schnell für unmittelbaren Kontext',
    takeaway3: 'Googles Hope-Architektur beweist, dass das Konzept funktioniert und übertrifft Transformer bei mehreren Aufgaben',
    takeaway4: 'Das ist frühe Forschung (NeurIPS 2025) — noch nicht produktionsreif, aber ein Ausblick auf die Zukunft der KI',
  },

  distillation: {
    title: 'Destillation',
    description: 'Wie kleinere Modelle von gr\u00F6\u00DFeren lernen, indem sie auf Wahrscheinlichkeitsverteilungen statt auf einzelne Tokens trainiert werden, nach dem Lehrer-Sch\u00FCler-Paradigma.',

    // What is Distillation
    whatIs: 'Was ist Wissensdestillation?',
    whatIsDesc: 'Wissensdestillation ist eine Modellkomprimierungstechnik, bei der ein kleineres \u201ESch\u00FCler\u201C-Modell trainiert wird, das Verhalten eines gr\u00F6\u00DFeren, leistungsf\u00E4higeren \u201ELehrer\u201C-Modells nachzubilden. Anstatt den Sch\u00FCler von Grund auf mit Rohdaten zu trainieren, lernt er aus den Ausgabe-Wahrscheinlichkeitsverteilungen des Lehrers \u2013 und erfasst nicht nur, was der Lehrer vorhersagt, sondern wie sicher er \u00FCber alle m\u00F6glichen Vorhersagen hinweg ist.',
    analogy: '\u201EStell dir einen Meisterkoch vor, der einem Lehrling beibringt \u2013 nicht nur die Rezepte, sondern all die subtilen Intuitionen: warum dieses Gew\u00FCrz fast passt, warum diese Technik nah dran, aber nicht ganz richtig ist.\u201C',
    analogyDesc: 'Destillation \u00FCbertr\u00E4gt diese nuancierten Einsch\u00E4tzungen, indem die vollst\u00E4ndige Wahrscheinlichkeitsverteilung geteilt wird, nicht nur die endg\u00FCltige Antwort.',

    // Teacher-Student
    teacherStudentTitle: 'Das Lehrer-Sch\u00FCler-Paradigma',
    teacherStudentDesc: 'Destillation folgt einem einfachen zweistufigen Prozess: Zuerst wird ein gro\u00DFes, leistungsf\u00E4higes Lehrer-Modell trainiert, dann werden seine Ausgaben verwendet, um einen kleineren, effizienten Sch\u00FCler zu trainieren.',
    teacherTitle: 'Lehrer-Modell',
    teacherDesc: 'Ein gro\u00DFes, hochkapazit\u00E4res Modell (z.B. GPT-4, Claude Opus), trainiert auf massiven Datens\u00E4tzen. Es hat reichhaltige Repr\u00E4sentationen und nuancierte Entscheidungsgrenzen gelernt. Seine Rolle ist es, weiche Wahrscheinlichkeitsverteilungen zu erzeugen, die sein Wissen kodieren.',
    studentTitle: 'Sch\u00FCler-Modell',
    studentDesc: 'Ein kleineres, effizienteres Modell, das f\u00FCr den Einsatz konzipiert ist. Es lernt, indem es die Wahrscheinlichkeitsverteilungen des Lehrers abgleicht, anstatt nur die Ground-Truth-Labels. Dies erm\u00F6glicht es, das \u201Edunkle Wissen\u201C des Lehrers zu erfassen \u2013 die Beziehungen zwischen Klassen, die harte Labels verwerfen.',
    flowTeacher: 'Lehrer-Modell',
    flowSoftDistribution: 'Weiche Wahrscheinlichkeitsverteilung',
    flowStudent: 'Sch\u00FCler-Modell lernt',

    // Key Insight
    keyInsightTitle: 'Die Kernidee: Verteilungen, nicht Tokens',
    keyInsightSubtitle: 'Warum Verteilungen Destillation so effektiv machen',
    keyInsightDesc: 'Der fundamentale Grund, warum Destillation so gut funktioniert, ist, dass wir auf vollst\u00E4ndige Wahrscheinlichkeitsverteilungen trainieren, nicht auf einzelne Tokens oder harte Labels. Wenn ein Lehrer-Modell \u201EDie Hauptstadt von Frankreich ist ___\u201C verarbeitet, gibt es nicht einfach \u201EParis\u201C aus \u2013 es erzeugt eine Wahrscheinlichkeitsverteilung \u00FCber sein gesamtes Vokabular.',
    keyInsightDesc2: 'Diese Verteilung enth\u00E4lt reichhaltige Informationen: \u201EParis\u201C erh\u00E4lt 92%, aber \u201ELyon\u201C erh\u00E4lt 3%, \u201EMarseille\u201C erh\u00E4lt 1,5% und \u201EBerlin\u201C erh\u00E4lt 0,8%. Diese \u201Efalschen\u201C Antworten kodieren das Verst\u00E4ndnis des Lehrers f\u00FCr Geografie, \u00C4hnlichkeit zwischen St\u00E4dten und konzeptülle Beziehungen. Ein hartes Label von nur \u201EParis\u201C wirft all dieses Wissen weg.',

    // Hard vs Soft Labels
    hardLabelTitle: 'Harte Labels (Traditionelles Training)',
    hardLabelExample: '\u201EParis\u201C = 1,0, alles andere = 0,0',
    hardLabelExplain: 'Bin\u00E4r: entweder richtig oder falsch. Keine Nuancen. Das Modell lernt nichts \u00FCber die Beziehungen zwischen Ausgaben.',
    softLabelTitle: 'Weiche Labels (Destillation)',
    softLabelExample: '\u201EParis\u201C = 0,92, \u201ELyon\u201C = 0,03, \u201EMarseille\u201C = 0,015, \u201EBerlin\u201C = 0,008, ...',
    softLabelExplain: 'Reichhaltiges Signal: Jede Wahrscheinlichkeit kodiert eine Beziehung. Der Sch\u00FCler lernt, dass Lyon Paris \u00E4hnlicher ist als Berlin.',

    // Visualizer
    vizTitle: 'Temperatur & Verteilungsgl\u00E4ttung',
    vizSubtitle: 'Sehen Sie, wie Temperatur das Wissen des Lehrers f\u00FCr den Sch\u00FCler umformt',
    vizToken1: 'Paris',
    vizToken2: 'Lyon',
    vizToken3: 'Mars.',
    vizToken4: 'Berlin',
    vizToken5: 'Rom',
    hardLabels: 'Harte Verteilung',
    hardLabelsDesc: 'Bei T=1 \u00FCberw\u00E4ltigt der dominante Token die anderen. Wenig Information im Schwanz.',
    softLabels: 'Weiche Verteilung',
    softLabelsDesc: 'H\u00F6here Temperatur enth\u00FCllt Beziehungen zwischen Tokens, die harte Labels verbergen.',
    vizTeacherLabel: 'Lehrer',
    vizStudentLabel: 'Sch\u00FCler',
    vizStudentDist: 'Sch\u00FCler-Verteilung',
    vizStudentOff: 'Einschalten zum Vergleichen',
    vizStudentDesc: 'Der Sch\u00FCler versucht, die weiche Verteilung des Lehrers abzugleichen. Passen Sie seine Temperatur an, um den Effekt zu sehen.',
    vizStudentTemp: 'Sch\u00FCler-Temperatur',
    vizKLDiv: 'KL-Divergenz',
    vizKLExcellent: 'Ausgezeichnete \u00DCbereinstimmung',
    vizKLGood: 'Gute \u00DCbereinstimmung',
    vizKLPoor: 'Schlechte \u00DCbereinstimmung',
    vizTeacherEntropy: 'Lehrer-Entropie',
    vizStudentEntropy: 'Sch\u00FCler-Entropie',
    vizMatchExplain: 'Die Verteilung des Sch\u00FClers stimmt gut mit der weichen Verteilung des Lehrers \u00FCberein \u2013 ideal f\u00FCr den Wissenstransfer.',
    vizStudentSharper: 'Die Verteilung des Sch\u00FClers ist sch\u00E4rfer als die des Lehrers. Er k\u00F6nnte subtile Beziehungen zwischen Klassen \u00FCbersehen.',
    vizStudentSmoother: 'Die Verteilung des Sch\u00FClers ist glatter als die des Lehrers. Er \u00FCbergeneralisiert und verliert spezifisches Wissen.',
    distillTemp: 'Destillationstemperatur',
    tempSharp: 'scharf',
    tempSmooth: 'glatt',
    tempExplainLow: 'Niedrige Temperatur: Die Verteilung ist noch spitz. Der Sch\u00FCler lernt haupts\u00E4chlich, was die Top-Vorhersage ist.',
    tempExplainMid: 'Mittlere Temperatur: Die Verteilung ist gegl\u00E4ttet und enth\u00FCllt bedeutungsvolle Beziehungen zwischen Tokens. Dies ist der optimale Bereich f\u00FCr Destillation.',
    tempExplainHigh: 'Hohe Temperatur: Die Verteilung n\u00E4hert sich der Gleichverteilung. Zu viel Gl\u00E4ttung kann das Signal, das der Lehrer gelernt hat, auswaschen.',

    // Why it Works
    whyWorks: 'Warum Destillation funktioniert',
    whyWorksDesc: 'Destillation ist bemerkenswert effektiv, weil weiche Labels ein viel reichhaltigeres Trainingssignal liefern als harte Labels:',
    benefit1Title: 'Reichhaltigeres Gradientensignal',
    benefit1Desc: 'Jedes Trainingsbeispiel liefert Informationen \u00FCber alle Ausgabeklassen gleichzeitig, nicht nur \u00FCber die korrekte. Das bedeutet, dass jedes Beispiel dem Sch\u00FCler effektiv Tausende von Beziehungen gleichzeitig beibringt.',
    benefit2Title: '\u00DCbertragung von dunklem Wissen',
    benefit2Desc: 'Die \u201EFehler\u201C des Lehrers sind informativ. Wenn der Lehrer 3% Wahrscheinlichkeit f\u00FCr \u201ELyon\u201C bei einer Frage \u00FCber Frankreichs Hauptstadt zuweist, sagt er dem Sch\u00FCler, dass Lyon f\u00FCr Frankreich relevant ist \u2013 Wissen, das harte Labels komplett verwerfen.',
    benefit3Title: 'Bessere Generalisierung',
    benefit3Desc: 'Sch\u00FCler, die \u00FCber Destillation trainiert werden, generalisieren oft besser als Modelle, die nur auf harten Labels trainiert wurden, selbst wenn der Sch\u00FCler viel weniger Parameter hat. Die weichen Labels wirken als leistungsstarker Regularisierer.',
    benefit4Title: 'Stichprobeneffizienz',
    benefit4Desc: 'Da jedes Trainingsbeispiel mehr Information tr\u00E4gt (eine vollst\u00E4ndige Verteilung vs. ein einzelnes Label), ben\u00F6tigt der Sch\u00FCler weniger Beispiele, um effektiv zu lernen. Dies reduziert Trainingszeit und Datenanforderungen.',

    // Loss Function
    lossTitle: 'Die Destillationsverlustfunktion',
    lossDesc: 'Das Trainingsziel kombiniert zwei Verlustfunktionen: die Standard-Kreuzentropie mit Ground-Truth-Labels und die KL-Divergenz zwischen Lehrer- und Sch\u00FClerverteilungen:',
    lossCE: 'Kreuzentropie mit Ground Truth: stellt sicher, dass der Sch\u00FCler weiterhin aus echten Labels lernt',
    lossKL: 'KL-Divergenz: misst, wie unterschiedlich die Verteilung des Sch\u00FClers von der des Lehrers ist. Der Sch\u00FCler wird f\u00FCr Abweichungen von den weichen Wahrscheinlichkeiten des Lehrers bestraft.',
    lossT: 'Temperatur: steuert, wie weich/glatt die Verteilungen sind. H\u00F6here T enth\u00FCllt mehr Beziehungen zwischen Klassen.',
    lossAlpha: 'Alpha: balanciert die beiden Verlustterme. Typische Werte liegen zwischen 0,1 und 0,9, wobei h\u00F6here Werte mehr Gewicht auf die \u00DCbereinstimmung mit dem Lehrer legen.',
    lossInsight: 'Der T\u00B2-Faktor kompensiert den Skalierungseffekt der Temperatur auf Gradienten und stellt sicher, dass Destillationsverlust und Kreuzentropieverlust unabh\u00E4ngig von der Temperaturwahl ausgewogen bleiben.',

    // Types
    typesTitle: 'Arten der Destillation',
    typesDesc: 'Verschiedene Ans\u00E4tze, je nachdem welches Wissen vom Lehrer zum Sch\u00FCler \u00FCbertragen wird:',
    typeResponseTitle: 'Antwortbasiert',
    typeResponseDesc: 'Der Sch\u00FCler ahmt die endg\u00FCltige Ausgabeverteilung des Lehrers nach. Dies ist die urspr\u00FCngliche und h\u00E4ufigste Form, eingef\u00FChrt von Hinton et al. (2015). Einfach zu implementieren und effektiv f\u00FCr Klassifikation und Sprachmodellierung.',
    typeFeatureTitle: 'Merkmalsbasiert',
    typeFeatureDesc: 'Der Sch\u00FCler lernt, Zwischendarstellungen (versteckte Zust\u00E4nde) des Lehrers abzugleichen, nicht nur die Ausgabe. Erfasst tieferes strukturelles Wissen. Verwendet in Modellen wie DistilBERT und TinyBERT.',
    typeRelationTitle: 'Beziehungsbasiert',
    typeRelationDesc: '\u00DCbertr\u00E4gt die Beziehungen zwischen verschiedenen Beispielen oder Schichten, anstatt einzelne Ausgaben. Bewahrt, wie der Lehrer seine internen Repr\u00E4sentationen strukturiert und wie er verschiedene Eingaben zueinander in Beziehung setzt.',
    typeOnlineTitle: 'Online-Destillation',
    typeOnlineDesc: 'Lehrer und Sch\u00FCler trainieren gleichzeitig und lernen voneinander. Kein vortrainierter Lehrer erforderlich. N\u00FCtzlich, wenn man es sich nicht leisten kann, zuerst ein massives Lehrer-Modell zu trainieren.',

    // Examples
    examplesTitle: 'Praxisbeispiele',
    examplesDesc: 'Destillation wird umfangreich in produktiven KI-Systemen eingesetzt:',
    example1Title: 'DistilBERT (Hugging Face)',
    example1Desc: 'Eine destillierte Version von BERT, die 60% kleiner, 60% schneller ist und 97% von BERTs Sprachverst\u00E4ndnis beh\u00E4lt. Trainiert mit einer Kombination aus antwort- und merkmalsbasierter Destillation. Eines der am weitesten verbreiteten destillierten Modelle.',
    example2Title: 'OpenAI GPT-4 zu GPT-4o-mini',
    example2Desc: 'GPT-4o-mini wird weithin als destilliert aus gr\u00F6\u00DFeren GPT-4-Klasse-Modellen angesehen. Es bietet deutlich geringere Latenz und Kosten bei wettbewerbsf\u00E4higer Leistung bei den meisten Aufgaben. Dieses Muster \u2013 ein gro\u00DFes Frontier-Modell destilliert in eine kleinere, schnellere Variante \u2013 ist zur Standardpraxis geworden.',
    example3Title: 'DeepSeek R1 Destillation',
    example3Desc: 'DeepSeek ver\u00F6ffentlichte destillierte Versionen ihres R1-Reasoning-Modells in Qwen- und Llama-Basismodelle. Diese destillierten Varianten bringen fortgeschrittene Reasoning-F\u00E4higkeiten in viel kleinere, besser einsetzbare Modelle und zeigen, dass selbst komplexes Chain-of-Thought-Reasoning effektiv destilliert werden kann.',

    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Wissensdestillation trainiert kleinere Modelle, gr\u00F6\u00DFere nachzubilden, indem sie aus vollst\u00E4ndigen Wahrscheinlichkeitsverteilungen lernen, nicht nur aus endg\u00FCltigen Antworten',
    takeaway2: 'Die entscheidende Erkenntnis ist, dass wir auf Verteilungen trainieren, nicht auf einzelne Tokens \u2013 weiche Labels kodieren reichhaltiges relationales Wissen (\u201Edunkles Wissen\u201C), das harte Labels komplett verwerfen',
    takeaway3: 'Temperaturgl\u00E4ttung enth\u00FCllt Beziehungen zwischen Klassen, die in der Verteilung des Lehrers verborgen sind, und macht Destillation weitaus effektiver als einfaches Label-Matching',
    takeaway4: 'Destillierte Modelle k\u00F6nnen 95-99% der Lehrerleistung bei einem Bruchteil der Gr\u00F6\u00DFe beibehalten und machen Frontier-KI-F\u00E4higkeiten f\u00FCr den realen Einsatz zug\u00E4nglich',
    takeaway5: 'Destillation ist zur Standardpraxis in der Industrie geworden \u2013 die meisten kleinen, schnellen Modelle, die man t\u00E4glich nutzt (GPT-4o-mini, DistilBERT, Gemini Flash), sind wahrscheinlich von gr\u00F6\u00DFeren Lehrern destilliert',
  },

  bias: {
    title: 'Bias & Fairness',
    description: 'Verstehen und Mindern schädlicher Biases in KI-Systemen.',
    whatIs: 'Was ist KI-Bias?',
    whatIsDesc: 'KI-Bias tritt auf, wenn maschinelle Lernsysteme systematisch unfaire Ergebnisse für bestimmte Gruppen produzieren. Biases können aus Trainingsdaten, Modelldesign oder dem Einsatzkontext entstehen.',
    sources: 'Qüllen von Bias',
    sourcesDesc: 'Wo Bias in KI-Systeme eindringt.',
    dataBias: 'Trainingsdaten',
    dataBiasDesc: 'Historische Biases in den Daten werden vom Modell gelernt.',
    labelBias: 'Label-Bias',
    labelBiasDesc: 'Menschliche Annotatoren führen ihre eigenen Biases ein.',
    selectionBias: 'Selektions-Bias',
    selectionBiasDesc: 'Trainingsdaten repräsentieren nicht die Einsatzpopulation.',
    measurementBias: 'Mess-Bias',
    measurementBiasDesc: 'Proxies, die zur Messung verwendet werden, kodieren Bias.',
    types: 'Arten von Bias',
    typesDesc: 'Häufige Kategorien von Bias in KI-Systemen.',
    stereotyping: 'Stereotypisierung',
    stereotypingDesc: 'Verstärkung schädlicher Stereotypen über Gruppen.',
    erasure: 'Auslöschung',
    erasureDesc: 'Unterrepräsentation oder Ignorieren bestimmter Gruppen.',
    disparateImpact: 'Unterschiedliche Auswirkung',
    disparateImpactDesc: 'Verschiedene Ergebnisse für verschiedene Gruppen.',
    mitigation: 'Minderungsstrategien',
    mitigationDesc: 'Ansätze zur Reduzierung von Bias.',
    diverseData: 'Diverse Daten',
    diverseDataDesc: 'Sicherstellen, dass Trainingsdaten alle relevanten Gruppen repräsentieren.',
    auditing: 'Bias-Audit',
    auditingDesc: 'Systematisch auf Bias über demografische Gruppen hinweg testen.',
    constraints: 'Fairness-Einschränkungen',
    constraintsDesc: 'Fairness-Metriken in das Training einbeziehen.',
    interactiveDemo: 'Bias-Erkennungs-Demo',
    demoDesc: 'Erkunde, wie sich Bias in Modellausgaben manifestiert',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bias wird oft von Trainingsdaten geerbt',
    takeaway2: 'Verschiedene Fairness-Metriken können in Konflikt stehen – wähle sorgfältig',
    takeaway3: 'Regelmäßiges Auditing ist essentiell für eingesetzte Systeme',
    takeaway4: 'Bias-Minderung ist ein fortlaufender Prozess, keine einmalige Lösung',
  },

  responsibleAi: {
    title: 'Verantwortungsvolle KI',
    description: 'KI-Systeme ethisch und nachhaltig entwickeln und einsetzen.',
    whatIs: 'Was ist verantwortungsvolle KI?',
    whatIsDesc: 'Verantwortungsvolle KI umfasst die Praktiken, Richtlinien und Prinzipien, die sicherstellen, dass KI-Systeme ethisch, sicher und zum Nutzen der Gesellschaft entwickelt und eingesetzt werden.',
    pillars: 'Säulen verantwortungsvoller KI',
    pillarsDesc: 'Kernprinzipien, die die verantwortungsvolle KI-Entwicklung leiten.',
    transparency: 'Transparenz',
    transparencyDesc: 'Offen sein über KI-Fähigkeiten, Einschränkungen und Entscheidungsfindung.',
    accountability: 'Verantwortlichkeit',
    accountabilityDesc: 'Klare Eigentümerschaft und Verantwortung für KI-Ergebnisse.',
    privacy: 'Datenschutz',
    privacyDesc: 'Benutzerdaten schützen und Datenschutzrechte respektieren.',
    safety: 'Sicherheit',
    safetyDesc: 'Sicherstellen, dass Systeme robust sind und keinen Schaden anrichten.',
    practices: 'Verantwortungsvolle Praktiken',
    practicesDesc: 'Konkrete Schritte für verantwortungsvolle KI-Entwicklung.',
    documentation: 'Dokumentation',
    documentationDesc: 'Modellfähigkeiten, Trainingsdaten und bekannte Einschränkungen dokumentieren.',
    testing: 'Umfassende Tests',
    testingDesc: 'Vor der Bereitstellung auf Sicherheit, Bias und Randfälle testen.',
    monitoring: 'Fortlaufende Überwachung',
    monitoringDesc: 'Systemverhalten in der Produktion auf Probleme überwachen.',
    feedback: 'Benutzer-Feedback',
    feedbackDesc: 'Kanäle für Benutzer schaffen, um Probleme zu melden.',
    considerations: 'Ethische Überlegungen',
    environmental: 'Umweltauswirkungen',
    environmentalDesc: 'KI-Training hat einen erheblichen CO2-Fußabdruck.',
    labor: 'Arbeitsmarkt-Auswirkungen',
    laborDesc: 'Auswirkungen auf Arbeitnehmer und Beschäftigung berücksichtigen.',
    access: 'Gerechter Zugang',
    accessDesc: 'Sicherstellen, dass KI-Vorteile breit verteilt werden.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Verantwortungsvolle KI erfordert proaktiven Einsatz während des gesamten Lebenszyklus',
    takeaway2: 'Transparenz baut Vertrauen auf und ermöglicht Verantwortlichkeit',
    takeaway3: 'Gesellschaftliche Auswirkungen über unmittelbare Benutzer hinaus berücksichtigen',
    takeaway4: 'Ethik ist nicht optional – integriere sie in Entwicklungsprozesse',
    // Governance Frameworks
    frameworks: 'Governance-Rahmenwerke',
    frameworksDesc: 'Internationale Standards und Vorschriften bieten strukturierte Ansätze für verantwortungsvolle KI-Entwicklung und -Einsatz.',
    // NIST AI RMF
    nistTitle: 'NIST AI Risk Management Framework',
    nistDesc: 'Ein freiwilliges Rahmenwerk des U.S. National Institute of Standards and Technology zur Steuerung von KI-Risiken während des gesamten KI-Lebenszyklus.',
    nistGovern: 'Govern',
    nistGovernDesc: 'Kultur, Richtlinien, Verantwortlichkeit',
    nistMap: 'Map',
    nistMapDesc: 'Kontext und Risikoidentifikation',
    nistMeasure: 'Measure',
    nistMeasureDesc: 'Bewertung und Analyse',
    nistManage: 'Manage',
    nistManageDesc: 'Priorisieren und reagieren',
    // OECD AI Principles
    oecdTitle: 'OECD KI-Prinzipien',
    oecdDesc: 'Internationale Prinzipien, die von 46 Ländern angenommen wurden, um vertrauenswürdige KI zu fördern, die Menschenrechte und demokratische Werte respektiert.',
    oecdInclusive: 'Inklusives Wachstum',
    oecdHuman: 'Menschenzentriert',
    oecdTransparency: 'Transparenz',
    oecdRobust: 'Robustheit',
    oecdAccountability: 'Verantwortlichkeit',
    // ISO/IEC 42001
    isoTitle: 'ISO/IEC 42001',
    isoDesc: 'Der erste internationale Standard, der Anforderungen für die Einrichtung, Implementierung und Verbesserung eines KI-Managementsystems in Organisationen festlegt.',
    isoPolicy: 'KI-Richtlinie',
    isoPolicyDesc: 'Organisatorische KI-Ziele und Prinzipien festlegen',
    isoRisk: 'Risikobewertung',
    isoRiskDesc: 'KI-spezifische Risiken identifizieren und bewerten',
    isoImprovement: 'Kontinuierliche Verbesserung',
    isoImprovementDesc: 'KI-Praktiken überwachen, messen und verbessern',
    // EU AI Act
    euActTitle: 'EU AI Act',
    euActDesc: 'Die weltweit erste umfassende KI-Regulierung, die einen risikobasierten Ansatz zur Kategorisierung und Regulierung von KI-Systemen verfolgt.',
    euProhibited: 'Verboten',
    euProhibitedDesc: 'Social Scoring, manipulative KI, Echtzeit-Biometrie',
    euHighRisk: 'Hohes Risiko',
    euHighRiskDesc: 'Gesundheitswesen, Bildung, Beschäftigung, Strafverfolgung',
    euLimited: 'Begrenztes Risiko',
    euLimitedDesc: 'Chatbots, Deepfakes (Transparenz erforderlich)',
    euMinimal: 'Minimales Risiko',
    euMinimalDesc: 'Die meisten KI-Anwendungen (keine spezifischen Anforderungen)',
  },

  // European AI page
  europeanAi: {
    title: 'KI aus Europa',
    description: 'Das wachsende europäische KI-Ökosystem erkunden – Unternehmen, die souveräne, offene und datenschutzorientierte KI entwickeln.',
    intro: 'Die europäische KI-Landschaft',
    introDesc: 'Europa entwickelt sich zu einem bedeutenden Akteur im globalen KI-Wettbewerb, mit einem einzigartigen Ansatz, der Datensouveränität, Datenschutz, Open-Source-Modelle und regulatorische Konformität betont. Während US-amerikanische und chinesische Unternehmen die Schlagzeilen dominieren, haben europäische KI-Startups insgesamt über 13 Milliarden Euro an Finanzierung eingeworben und eine neue Generation von KI-Einhörnern geschaffen, die auf europäischen Werten aufbauen.',
    stat1Title: 'Gesamtfinanzierung',
    stat1Value: '€13,2 Mrd.+',
    stat1Desc: 'Von europäischen KI-Startups eingeworben',
    stat2Title: 'Führende Standorte',
    stat2Value: 'FR, DE, UK',
    stat2Desc: 'Frankreich, Deutschland und UK führen die KI-Entwicklung an',
    stat3Title: 'Regulatorischer Vorteil',
    stat3Value: 'EU AI Act',
    stat3Desc: 'Erstes umfassendes KI-Gesetz weltweit',
    keyCompanies: 'Wichtige europäische KI-Unternehmen',
    keyCompaniesDesc: 'Führende Organisationen, die die europäische KI gestalten',
    focus: 'Fokus',
    funding: 'Finanzierung',
    // Company names
    companies: {
      mistral: 'Mistral AI',
      alephAlpha: 'Aleph Alpha',
      kyutai: 'Kyutai',
      poolside: 'Poolside',
      elevenLabs: 'ElevenLabs',
      photoroom: 'Photoroom',
      lightOn: 'LightOn',
      sana: 'Sana',
      deepL: 'DeepL',
    },
    // Countries
    countries: {
      france: 'Frankreich',
      germany: 'Deutschland',
      franceParis: 'Frankreich / USA',
      ukPoland: 'UK / Polen',
      sweden: 'Schweden',
    },
    // Focus areas
    focuses: {
      mistralFocus: 'Open-Weight LLMs',
      alephAlphaFocus: 'Enterprise-KI, Souveränität',
      kyutaiFocus: 'Echtzeit-Sprach-KI',
      poolsideFocus: 'KI-gestützte Programmierung',
      elevenLabsFocus: 'Sprach-KI & Sprachsynthese',
      photoroomFocus: 'KI-Bildbearbeitung',
      lightOnFocus: 'Enterprise GenAI-Plattform',
      sanaFocus: 'Enterprise KI-Agenten',
      deepLFocus: 'KI-Übersetzung & Sprache',
    },
    // Descriptions
    descriptions: {
      mistralDesc: 'Gegründet von ehemaligen DeepMind- und Meta-Forschern, baut Mistral Open-Weight-Modelle, die mit proprietären Alternativen konkurrieren. Ihre Le Chat-App bietet ultraschnelle Inferenz mit bis zu 1.000 Wörtern/Sekunde.',
      alephAlphaDesc: 'Deutscher Pionier mit Fokus auf Enterprise-KI und starker Datensouveränität. Der einzige deutsche LLM-Anbieter mit BSI C5-Zertifizierung. Kürzlich auf ihr generatives KI-Betriebssystem Pharia umgestellt.',
      kyutaiDesc: 'Französische Non-Profit-Organisation, die Moshi entwickelt, das erste vollständig offene Echtzeit-Sprachmodell. Erreicht 160ms Latenz und nutzt ihren Mimi-Codec für 24kHz Audio bei nur 1,1 kbps.',
      poolsideDesc: 'Gegründet vom ehemaligen GitHub-CTO, entwickelt KI-Modelle speziell für Code-Generierung. Nutzt Reinforcement Learning aus Code-Ausführung für synthetische Trainingsdaten. Stark von Nvidia unterstützt.',
      elevenLabsDesc: 'Führendes Sprach-KI-Unternehmen mit hochrealistischer Sprachsynthese und Stimmenklonen. Gegründet von Ex-Google- und Ex-Palantir-Ingenieuren, jetzt mit 3,3 Milliarden Dollar bewertet.',
      photoroomDesc: 'In Paris ansässige KI-Fotobearbeitungsplattform mit Hunderten Millionen Nutzern. Macht professionelle Bildqualität ohne tiefe Designkenntnisse zugänglich.',
      lightOnDesc: 'Europas erstes börsennotiertes GenAI-Startup. Bietet On-Premises Enterprise-KI ohne Datenspeicherung. Hat ModernBERT mit über 20 Millionen Downloads entwickelt.',
      sanaDesc: 'Schwedisches Enterprise-KI-Unternehmen, das 2025 für 1,1 Mrd. Dollar von Workday übernommen wurde. Ihre Sana Agents-Plattform ermöglicht No-Code KI-Agenten-Entwicklung mit über 100 Enterprise-Konnektoren.',
      deepLDesc: 'In Köln ansässiger Pionier für neuronale maschinelle Übersetzung, gegründet 2017. Bedient über 200.000 Unternehmen in 228 Märkten mit Enterprise-Übersetzung. In Forbes AI 50 (2025) gelistet und erwägt einen $5 Mrd. Börsengang.',
    },
    // Funding
    fundings: {
      mistralFunding: '€6,2 Mrd. gesamt (inkl. ASML, Nvidia)',
      alephAlphaFunding: '500 Mio. $ (Bosch, SAP, HPE)',
      kyutaiFunding: 'Non-Profit (Xavier Niel unterstützt)',
      poolsideFunding: '2 Mrd. $ Runde bei 12 Mrd. $ Bewertung',
      elevenLabsFunding: '281 Mio. $ (a16z, Sequoia)',
      photoroomFunding: 'Series B, 65 Mio. $+',
      lightOnFunding: 'Börsennotiert (Euronext Growth)',
      sanaFunding: 'Für 1,1 Mrd. $ übernommen',
      deepLFunding: '536 Mio. $ gesamt, 2 Mrd. $ Bewertung',
    },
    // Notable
    notables: {
      mistralNotable: 'Le Chat, Mixtral, Open Weights',
      alephAlphaNotable: 'Pharia OS, BSI C5 zertifiziert',
      kyutaiNotable: 'Moshi, MoshiVis, Open Source',
      poolsideNotable: 'Project Horizon, 40K+ GPUs',
      elevenLabsNotable: 'Stimmenklonen, KI-Dubbing',
      photoroomNotable: 'Hintergrundentfernung, Produktfotos',
      lightOnNotable: 'ModernBERT, On-Prem Deployment',
      sanaNotable: 'Sana Agents, Workday-Übernahme',
      deepLNotable: 'Forbes AI 50, DeepL Agent, 1.257 Mitarbeiter',
    },
    // EU AI Act section
    euAiAct: 'Der EU AI Act Vorteil',
    euAiActDesc: 'Der EU AI Act ist der weltweit erste umfassende Rechtsrahmen für KI. Europäische Unternehmen gestalten ihre KI von Anfang an nach diesen Standards, was einen regulatorischen "Heimvorteil" schafft, während ausländische Anbieter sich anpassen müssen, um in Europa tätig zu sein.',
    advantage1Title: 'Integrierte Compliance',
    advantage1Desc: 'Europäische KI-Unternehmen entwickeln von Anfang an für DSGVO und den AI Act, was sie für datenschutzbewusste Enterprise-Kunden attraktiv macht.',
    advantage2Title: 'Datensouveränität',
    advantage2Desc: 'On-Premises-Bereitstellungsoptionen ermöglichen es, sensible Daten innerhalb organisatorischer oder nationaler Grenzen zu halten – entscheidend für Regierung und Verteidigung.',
    advantage3Title: 'Mehrsprachiger Fokus',
    advantage3Desc: 'Europäische Modelle sind von Grund auf für mehrsprachige Nutzung konzipiert und bedienen vielfältige europäische Sprachen und Märkte effektiv.',
    advantage4Title: 'Ethische KI-Führung',
    advantage4Desc: 'Europas Betonung auf verantwortungsvolle KI-Entwicklung positioniert seine Unternehmen als vertrauenswürdige Partner für Organisationen, die Ethik priorisieren.',
    // Open Source section
    openSource: 'Open Source & Open Weights',
    openSourceDesc: 'Viele europäische KI-Unternehmen setzen auf Open-Source-Prinzipien und veröffentlichen Modellgewichte und Code unter freizügigen Lizenzen. Diese Transparenz baut Vertrauen auf, ermöglicht Anpassungen und unterstützt die breitere KI-Forschungsgemeinschaft.',
    model1Desc: 'Open-Weight LLMs, die privat bereitgestellt und angepasst werden können',
    model2Desc: 'Vollständig offenes Sprachmodell mit Apache 2.0 Code und CC BY 4.0 Gewichten',
    model3Desc: 'State-of-the-Art Encoder-Modell mit über 20 Mio. Downloads',
    model4Desc: 'Lettisches 30B-Parameter Open Model, trainiert auf dem EuroHPC LUMI Supercomputer',
    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Europa baut KI mit einzigartigem Fokus auf Souveränität, Datenschutz und Open-Source-Prinzipien',
    takeaway2: 'Französische Startups wie Mistral und Kyutai sind führend bei Open-Weight und Open-Source KI-Modellen',
    takeaway3: 'Der EU AI Act schafft sowohl Herausforderungen als auch Chancen – europäische Unternehmen haben einen Compliance-Vorteil',
    takeaway4: 'Während US-Unternehmen bei der Größe führen, glänzt europäische KI bei Enterprise-Vertrauen, mehrsprachiger Unterstützung und regulatorischer Ausrichtung',
  },

  // Open Source Advantages page
  openSource: {
    title: 'Open-Source-Vorteile',
    description: 'Warum Open Source in der KI wichtig ist – Transparenz, Community, Kosten, Innovationsgeschwindigkeit, Anbieterunabhängigkeit und Sicherheit durch Auditing.',

    // Introduction
    intro: 'Warum Open Source in der KI wichtig ist',
    introDesc: 'Open-Source-KI hat grundlegend verändert, wie künstliche Intelligenz entwickelt, eingesetzt und verbessert wird. Von grundlegenden Modellen wie LLaMA bis hin zu spezialisierten Tools wie Hugging Face Transformers hat die Open-Source-Bewegung den Zugang zu modernster KI-Technologie demokratisiert und ein lebendiges Ökosystem der Innovation geschaffen.',

    // Key Advantages Section
    advantagesTitle: 'Wichtige Vorteile von Open-Source-KI',
    advantagesDesc: 'Open-Source-KI bietet einzigartige Vorteile, die geschlossene, proprietäre Systeme nicht bieten können.',

    advantage1Title: 'Transparenz',
    advantage1Desc: 'Vollständige Einsicht in Modellarchitektur, Trainingsdaten und Gewichte. Sie können prüfen, wie Entscheidungen getroffen werden, und Sicherheitseigenschaften verifizieren.',

    advantage2Title: 'Community-Innovation',
    advantage2Desc: 'Tausende von Mitwirkenden verbessern Modelle, beheben Fehler und erstellen Derivate. Die kollektive Intelligenz der Community beschleunigt den Fortschritt.',

    advantage3Title: 'Kosteneffizienz',
    advantage3Desc: 'Keine Lizenzgebühren oder API-Kosten pro Token. Betreiben Sie Modelle auf Ihrer eigenen Infrastruktur mit vorhersehbaren, kontrollierbaren Ausgaben.',

    advantage4Title: 'Innovationsgeschwindigkeit',
    advantage4Desc: 'Offene Modelle können schnell feingetunt, zusammengeführt und angepasst werden. Neue Techniken verbreiten sich in der Community in Tagen, nicht Monaten.',

    advantage5Title: 'Anbieterunabhängigkeit',
    advantage5Desc: 'Keine Bindung an bestimmte Anbieter. Wechseln Sie frei zwischen Modellen, Hosting-Optionen oder kombinieren Sie mehrere Modelle.',

    advantage6Title: 'Sicherheit durch Auditing',
    advantage6Desc: 'Tausende Augen überprüfen den Code. Schwachstellen werden schneller gefunden und behoben als in geschlossenen Systemen.',

    // Notable Projects Section
    projectsTitle: 'Bedeutende Open-Source-KI-Projekte',
    projectsDesc: 'Schlüsselprojekte, die die Open-Source-KI-Revolution 2025 vorantreiben',

    project1Name: 'DeepSeek R1',
    project1Org: 'DeepSeek',
    project1Desc: 'Chinesisches Reasoning-Modell mit GPT-4-Leistung zu einem Bruchteil der Kosten. Für unter 6 Mio. $ trainiert, zeigt effiziente Skalierung.',

    project2Name: 'Qwen-Serie',
    project2Org: 'Alibaba',
    project2Desc: 'Jetzt die meistgeladenen offenen Modelle weltweit. Qwen2.5 bietet Größen von 0,5B bis 72B mit starken mehrsprachigen Fähigkeiten.',

    project3Name: 'Llama 3.3 70B',
    project3Org: 'Meta',
    project3Desc: 'Metas neüste Veröffentlichung, die GPT-4 bei vielen Benchmarks erreicht. Setzt das LLaMA-Erbe mit verbessertem Reasoning und Coding fort.',

    project4Name: 'Mistral / Mixtral',
    project4Org: 'Mistral AI',
    project4Desc: 'Europäische Open-Weight-Modelle, bekannt für Effizienz. Mixtral war Pionier der offenen Mixture-of-Experts-Architektur.',

    project5Name: 'Hugging Face Transformers',
    project5Org: 'Hugging Face',
    project5Desc: 'Die De-facto-Bibliothek für die Arbeit mit Transformer-Modellen. Hostet über 1 Million Modelle und Datensätze.',

    project6Name: 'Ollama',
    project6Org: 'Ollama',
    project6Desc: 'Einfaches Tool zum lokalen Ausführen von LLMs. Ein-Befehl-Setup für Dutzende offene Modelle inklusive aller neüsten Releases.',

    // 2025 Trends Section
    trendsTitle: '2025 Modelllandschaft-Trends',
    trendsDesc: 'Die Open-Source-KI-Landschaft hat sich 2025 dramatisch verändert, wobei offene Modelle die Lücke zu proprietären Systemen schließen.',

    trend1Title: 'Leistungslücke schrumpft',
    trend1Desc: 'Die Lücke zwischen Open-Source- und Closed-Source-Modellen hat sich auf etwa 1,7% verringert, was offene Modelle für die meisten Produktionsfälle geeignet macht.',

    trend2Title: 'Chinesische Modelle dominieren Downloads',
    trend2Desc: 'Chinesische Modelle wie Qwen und DeepSeek führen jetzt die globalen Downloads an und verschieben die Open-Source-KI-Landschaft nach Asien.',

    trend3Title: '20-32B Parameter Sweet Spot',
    trend3Desc: 'Modelle im 20-32B-Parameterbereich erweisen sich als optimal für Consumer-Hardware und balancieren Leistungsfähigkeit mit Zugänglichkeit.',

    trend4Title: 'Small Language Models (SLMs)',
    trend4Desc: 'Sub-3B-Parametermodelle, optimiert für Edge-Geräte und Smartphones, ermöglichen On-Device-KI ohne Cloud-Abhängigkeiten.',

    // Business Perspective Section
    businessTitle: 'Wann Open Source wählen',
    businessDesc: 'Strategische Überlegungen für Organisationen, die Open-Source-KI bewerten',

    businessCase1Title: 'Datensouveränität',
    businessCase1Desc: 'Wenn Daten aufgrund von Vorschriften, Datenschutz oder Wettbewerbsbedenken Ihre Infrastruktur nicht verlassen dürfen.',

    businessCase2Title: 'Anpassungsbedarf',
    businessCase2Desc: 'Wenn Sie Modelle mit proprietären Daten feintunen oder für spezialisierte Bereiche anpassen müssen.',

    businessCase3Title: 'Kosten bei Skalierung',
    businessCase3Desc: 'Wenn API-Kosten die Selbst-Hosting-Kosten übersteigen würden, typischerweise bei hohem Nutzungsvolumen.',

    businessCase4Title: 'Offline- oder Edge-Deployment',
    businessCase4Desc: 'Wenn Modelle ohne Internetverbindung oder auf Edge-Geräten laufen müssen.',

    // Considerations
    considerTitle: 'Zu beachten',
    consider1: 'Selbst-Hosting erfordert Infrastruktur-Expertise und Rechenressourcen',
    consider2: 'Offene Modelle können bei rohen Fähigkeiten hinter proprietären Modellen zurückbleiben',
    consider3: 'Support kommt von der Community statt von Anbieterverträgen',
    consider4: 'Feintuning erfordert ML-Expertise und qualitativ hochwertige Trainingsdaten',

    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Open-Source-KI bietet Transparenz, Anpassung und Freiheit von Anbieterbindung',
    takeaway2: 'Das Community-getriebene Modell beschleunigt Innovation durch Zusammenarbeit und schnelle Iteration',
    takeaway3: 'Für Organisationen mit Datensouveränitätsanforderungen kann Open Source die einzige praktikable Option sein',
    takeaway4: 'Die Lücke zwischen Open-Source- und Closed-Source-Modellen verringert sich weiter, während das Ökosystem reift',
    takeaway5: 'Die Wahl zwischen Open und Closed Source hängt von Ihren spezifischen Bedürfnissen nach Kontrolle, Fähigkeit und Ressourcen ab',
  },

  // Visual Challenges page (expanded)
  visualChallenges: {
    title: 'Visülle Herausforderungen',
    description: 'Häufige Herausforderungen und Einschränkungen bei der Arbeit mit bildverarbeitungsfähigen KI-Modellen.',
    overview: 'Häufige visülle Herausforderungen',
    overviewDesc: 'Obwohl Bildmodelle beeindruckend sind, stehen sie vor mehreren systematischen Herausforderungen, die beim Erstellen von Anwendungen wichtig zu verstehen sind. Diese Einschränkungen entstehen dadurch, wie Bildmodelle Bilder verarbeiten – durch Patches, Einbettungen und Aufmerksamkeit – und nicht so, wie Menschen visülle Informationen wahrnehmen.',

    // Challenge 1: Counting
    challenge1: 'Objekte zählen',
    challenge1Desc: 'Modelle haben oft Schwierigkeiten, Objekte in Bildern genau zu zählen, besonders wenn es viele ähnliche Elemente gibt.',
    challenge1Why: 'Warum das passiert',
    challenge1WhyDesc: 'Bildmodelle verarbeiten Bilder als Patches (typischerweise 14x14 oder 16x16 Pixel), nicht als diskrete Objekte. Ihnen fehlt das eingebaute Konzept der "Objektpermanenz" und sie haben Schwierigkeiten, genaue Zählungen über überlappende oder dichte Anordnungen aufrechtzuerhalten.',
    challenge1Examples: 'Häufige Fehler',
    challenge1Example1: 'Menschen in einer Menge zählen (oft 20-50% daneben)',
    challenge1Example2: 'Elemente in einem Raster oder Array zählen',
    challenge1Example3: 'Zwischen "wenig" und "viele" unterscheiden, wenn Elemente überlappen',
    challenge1Mitigation: 'Workarounds',
    challenge1MitigationDesc: 'Für kritische Zählaufgaben erwäge spezialisierte Objekterkennungsmodelle (YOLO, Faster R-CNN) oder bitte das Modell, jeden Gegenstand einzeln zu identifizieren und zu beschreiben, anstatt eine Gesamtzahl anzugeben.',

    // Challenge 2: Spatial Reasoning
    challenge2: 'Räumliches Denken',
    challenge2Desc: 'Das Verstehen präziser räumlicher Beziehungen zwischen Objekten (links/rechts, oben/unten) kann unzuverlässig sein.',
    challenge2Why: 'Warum das passiert',
    challenge2WhyDesc: 'Positionsinformationen werden durch Patch-Positionseinbettungen kodiert, aber diese bieten keine Pixel-genaue Präzision. Das Modell lernt statistische Korrelationen zwischen Positionen statt explizites räumliches Denken.',
    challenge2Examples: 'Häufige Fehler',
    challenge2Example1: 'Links/Rechts-Beziehungen in gespiegelten oder symmetrischen Bildern verwechseln',
    challenge2Example2: 'Relative Entfernungen falsch einschätzen ("näher an" oder "weiter von")',
    challenge2Example3: 'Schwierigkeiten mit gedrehten oder ungewöhnlichen Orientierungen',
    challenge2Mitigation: 'Workarounds',
    challenge2MitigationDesc: 'Sei explizit in deinen Prompts, welchen Bezugsrahmen du verwendest. Erwäge, Bilder mit visüllen Markern oder Rastern für kritische räumliche Aufgaben zu annotieren.',

    // Challenge 3: Small Text Recognition
    challenge3: 'Kleine Texterkennung',
    challenge3Desc: 'Feiner Text in Bildern kann falsch gelesen oder ganz übersehen werden, besonders bei niedrigen Auflösungen.',
    challenge3Why: 'Warum das passiert',
    challenge3WhyDesc: 'Text kleiner als die Patch-Größe (14-16 Pixel) wird in eine einzelne Einbettung komprimiert, wobei Details auf Zeichenebene verloren gehen. OCR ist nicht in Bild-LLMs eingebaut – sie lernen Texterkennung als Nebenprodukt des Trainings, nicht als dedizierte Fähigkeit.',
    challenge3Examples: 'Häufige Fehler',
    challenge3Example1: 'Nummernschilder, Straßenschilder oder kleine Etiketten falsch lesen',
    challenge3Example2: 'Ähnliche Zeichen verwechseln (0/O, 1/l/I, 5/S)',
    challenge3Example3: 'Text in geschäftigen oder kontrastarmen Hintergründen übersehen',
    challenge3Mitigation: 'Workarounds',
    challenge3MitigationDesc: 'Verwende hochauflösende Bilder und zoome in Textbereiche. Für kritische OCR-Aufgaben verwende dedizierte OCR-Tools (Tesseract, Google Vision API, Amazon Textract) neben oder anstelle von Bild-LLMs.',

    // Challenge 4: Hallucination
    challenge4: 'Visülle Halluzination',
    challenge4Desc: 'Modelle können Objekte oder Details beschreiben, die nicht wirklich im Bild vorhanden sind.',
    challenge4Why: 'Warum das passiert',
    challenge4WhyDesc: 'Bild-LLMs sind darauf trainiert, plausible Beschreibungen zu generieren. Wenn Bildmerkmale mehrdeutig sind, füllt das Modell Lücken mit statistisch wahrscheinlichem Inhalt – auch wenn dieser Inhalt nicht im Bild ist. Dies ist derselbe Mechanismus, der Text-Halluzinationen verursacht.',
    challenge4Examples: 'Häufige Fehler',
    challenge4Example1: 'Objekte hinzufügen, die in einer Szene "sein sollten" (eine Tastatur neben einem Monitor)',
    challenge4Example2: 'Markennamen oder Text beschreiben, der nicht sichtbar ist',
    challenge4Example3: 'Details erfinden, wenn nach unklaren Bereichen gefragt wird',
    challenge4Mitigation: 'Workarounds',
    challenge4MitigationDesc: 'Bitte das Modell, Unsicherheit auszudrücken. Verwende Prompts wie "beschreibe nur, was du klar sehen kannst" oder "wenn du X nicht bestimmen kannst, sage es". Kritische Details gegenchecken.',

    // Challenge 5: Fine Detail Recognition
    challenge5: 'Feine Detailerkennung',
    challenge5Desc: 'Subtile Details, Texturen oder kleine unterscheidende Merkmale werden oft übersehen oder falsch identifiziert.',
    challenge5Why: 'Warum das passiert',
    challenge5WhyDesc: 'Die patch-basierte Architektur mittelt Informationen innerhalb jedes Patches und verliert dabei feinkörnige Details. Hochfrequente visülle Informationen (Kanten, Texturen, kleine Merkmale) werden komprimiert.',
    challenge5Examples: 'Häufige Fehler',
    challenge5Example1: 'Zwischen ähnlichen Objekten unterscheiden (Hunderassen, Automodelle)',
    challenge5Example2: 'Messgeräte, Zähler oder Instrumentenanzeigen ablesen',
    challenge5Example3: 'Subtile Schäden oder Defekte bei Inspektionsaufgaben identifizieren',
    challenge5Mitigation: 'Workarounds',
    challenge5MitigationDesc: 'Verwende die höchste verfügbare Auflösung. Schneide zu und fokussiere auf spezifische Interessenbereiche. Für spezialisierte Aufgaben erwäge feingetunete Modelle, die auf domänenspezifischen Daten trainiert wurden.',

    // Challenge 6: Multi-Image Reasoning
    challenge6: 'Multi-Bild-Denken',
    challenge6Desc: 'Vergleichen oder Denken über mehrere Bilder hinweg ist deutlich schwieriger als Einzelbild-Aufgaben.',
    challenge6Why: 'Warum das passiert',
    challenge6WhyDesc: 'Jedes Bild wird separat in Token-Seqünzen kodiert. Cross-Image-Aufmerksamkeit muss durch das Kontextfenster des Sprachmodells erfolgen, was weniger effizient ist als dedizierte Multi-Bild-Architekturen.',
    challenge6Examples: 'Häufige Fehler',
    challenge6Example1: 'Unterschiede zwischen zwei ähnlichen Bildern finden ("Finde den Unterschied")',
    challenge6Example2: 'Objektidentität über Frames hinweg verfolgen',
    challenge6Example3: 'Feine Details zwischen Produktbildern vergleichen',
    challenge6Mitigation: 'Workarounds',
    challenge6MitigationDesc: 'Beschreibe jedes Bild zuerst separat, dann frage nach dem Vergleich. Erwäge, Bilder zu einem einzigen Komposit für direkten Vergleich zu kombinieren.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bild-LLMs verarbeiten Bilder als Patches – Details unter der Patch-Auflösung gehen verloren',
    takeaway2: 'Zählen und räumliches Denken sind fundamentale Schwächen, keine Randfälle',
    takeaway3: 'Visülle Halluzination folgt demselben Muster wie Text-Halluzination – plausible Erfindung',
    takeaway4: 'Verwende höhere Auflösung, zugeschnittene Bereiche und explizite Prompts, um die Genauigkeit zu verbessern',
    takeaway5: 'Für kritische Aufgaben kombiniere Bild-LLMs mit spezialisierten Tools (OCR, Objekterkennung)',
    takeaway6: 'Verifiziere wichtige visülle Informationen immer auf anderen Wegen',
  },

  // Agentic Vision Seite
  agenticVision: {
    title: 'Agentische Vision',
    description: 'Wie KI-Modelle passives Betrachten von Bildern durch Code-Ausführung und iteratives Reasoning in aktive visülle Untersuchung verwandeln.',
    whatIs: 'Was ist Agentische Vision?',
    whatIsDesc: 'Agentische Vision verwandelt Bildverständnis von einem statischen, einmaligen Prozess in eine aktive Untersuchung. Anstatt einfach zu beschreiben, was es sieht, formuliert das Modell Pläne zum Heranzoomen, Inspizieren, Manipulieren und schrittweisen Analysieren von Bildern—und begründet Antworten mit visüllen Beweisen, die durch Code-Ausführung gesammelt werden.',

    // Die Schleife
    loopTitle: 'Die Denken-Handeln-Beobachten-Schleife',
    loopDesc: 'Im Kern der agentischen Vision steht ein rigoroser iterativer Prozess, der widerspiegelt, wie Menschen komplexe visülle Informationen untersuchen.',
    thinkTitle: 'Denken',
    thinkDesc: 'Das Modell analysiert die Anfrage des Nutzers und das ursprüngliche Bild und formuliert dann einen mehrstufigen Plan, wie die benötigten Informationen extrahiert werden können.',
    actTitle: 'Handeln',
    actDesc: 'Das Modell generiert und führt Python-Code aus, um das Bild zu manipulieren oder zu analysieren—Interessenbereiche zuschneiden, Berechnungen durchführen, Objekte zählen oder Anmerkungen zeichnen.',
    observeTitle: 'Beobachten',
    observeDesc: 'Das transformierte Bild wird dem Kontextfenster des Modells hinzugefügt, sodass es die Ergebnisse inspizieren kann, bevor es über die nächste Aktion entscheidet oder eine endgültige Antwort liefert.',

    // Fähigkeiten
    capabilitiesTitle: 'Kernfähigkeiten',
    capabilitiesDesc: 'Agentische Vision ermöglicht mehrere leistungsstarke Fähigkeiten, die passive Bildmodelle nicht erreichen können.',
    capability1Title: 'Zoomen & Inspizieren',
    capability1Desc: 'Das Modell erkennt, wenn Details zu klein zum Lesen sind (wie eine entfernte Anzeige oder Seriennummer) und schreibt Code, um den Bereich zuzuschneiden und in höherer Auflösung erneut zu untersuchen.',
    capability2Title: 'Visülle Mathematik',
    capability2Desc: 'Führe mehrstufige Berechnungen mit Code durch—Summen von Positionen auf einer Quittung, Winkelmessung in einem Diagramm oder Diagramme aus extrahierten Daten generieren.',
    capability3Title: 'Bildannotation',
    capability3Desc: 'Zeichne Pfeile, Begrenzungsrahmen oder andere Anmerkungen direkt auf Bilder, um räumliche Fragen wie "Wohin soll dieses Element?" zu beantworten.',
    capability4Title: 'Iterative Verfeinerung',
    capability4Desc: 'Wenn der erste Ansatz keine klaren Ergebnisse liefert, kann das Modell alternative Strategien ausprobieren—verschiedene Zuschnittbereiche, Bildverbesserung oder mehrere Zählmethoden.',

    // Funktionsweise
    howItWorks: 'Funktionsweise',
    howItWorksDesc: 'Wenn du einem agentischen Bildmodell eine Frage zu einem Bild stellst, schaut es nicht einfach und antwortet. Es überlegt, welche Operationen helfen würden, die Frage zu beantworten, führt Code aus, um diese Operationen durchzuführen, und nutzt die Ergebnisse für seine Antwort.',
    step1: 'Anfrage erhalten',
    step1Desc: 'Nutzer stellt eine Frage zu einem Bild, die detaillierte Analyse erfordert.',
    step2: 'Operationen planen',
    step2Desc: 'Modell bestimmt, welche visüllen Operationen (Zuschneiden, Zoomen, Annotieren) helfen würden, die Frage zu beantworten.',
    step3: 'Code ausführen',
    step3Desc: 'Python-Code wird generiert und ausgeführt, um das Bild wie geplant zu manipulieren.',
    step4: 'Ergebnisse analysieren',
    step4Desc: 'Das modifizierte Bild wird dem Modell zur Inspektion zurückgegeben.',
    step5: 'Iterieren oder Antworten',
    step5Desc: 'Modell führt entweder weitere Operationen durch oder liefert die endgültige Antwort mit Belegen.',

    // Beispiel
    exampleTitle: 'Beispiel: Lesen einer entfernten Seriennummer',
    exampleDesc: 'Stell dir vor, du fragst "Was ist die Seriennummer auf dem Gerät in der Ecke des Fotos?"',
    exampleStep1: 'Modell identifiziert, dass sich das Gerät in der unteren rechten Ecke befindet',
    exampleStep2: 'Generiert Code, um diesen Bereich zuzuschneiden und 4x zu vergrößern',
    exampleStep3: 'Inspiziert das gezoomte Bild und identifiziert den Seriennummerntext',
    exampleStep4: 'Gibt die Seriennummer mit Konfidenz zurück und notiert den verwendeten Zuschnitt',

    // Modelle
    modelsTitle: 'Modelle mit Agentischer Vision',
    modelsDesc: 'Mehrere Spitzenmodelle unterstützen jetzt agentische Bildverarbeitungsfähigkeiten.',
    model1: 'Google Gemini 3 Flash',
    model1Desc: 'Erstes großes Modell, das "Agentic Vision" als benanntes Feature einführt und visülles Reasoning mit Code-Ausführung kombiniert. Zeigt 5-10% Qualitätsverbesserung bei Bild-Benchmarks, wenn Code-Ausführung aktiviert ist.',
    model2: 'NVIDIA Cosmos Reason',
    model2Desc: 'Ein 7B-Parameter-Reasoning-VLM für physische KI-Anwendungen. Kann reale Umgebungen unter Verwendung von Vorwissen und Physikverständnis verstehen und darin agieren.',
    model3: 'OpenAI Computer-Using Agent',
    model3Desc: 'Kombiniert große Reasoning-Modelle mit verstärkungslernbasierter UI-Interaktion und ermöglicht pixelgenaues Zeigen auf Objekte und UI-Elemente.',

    // Anwendungen
    applicationsTitle: 'Praxisanwendungen',
    applicationsDesc: 'Agentische Vision wird bereits in Produktionssystemen eingesetzt.',
    app1Title: 'Dokumentenverarbeitung',
    app1Desc: 'Automatisches Heranzoomen an Tabellen, Diagramme und Kleingedrucktes, um genaue Daten aus komplexen Dokumenten zu extrahieren.',
    app2Title: 'Qualitätsprüfung',
    app2Desc: 'Erkennung von Defekten durch systematische Inspektion verschiedener Bereiche von Produktbildern in hoher Auflösung.',
    app3Title: 'Räumliches Reasoning',
    app3Desc: 'Beantworte "Wohin soll das?"-Fragen durch Annotieren von Bildern mit Pfeilen und Platzierungshinweisen.',
    app4Title: 'Quittungsanalyse',
    app4Desc: 'Extrahiere Positionen, berechne Summen und verifiziere Mathematik durch Kombination von OCR mit codebasierter Berechnung.',

    // Vergleich
    comparisonTitle: 'Passive vs Agentische Vision',
    comparisonDesc: 'Verständnis des grundlegenden Unterschieds im Ansatz.',
    passiveTitle: 'Passive Vision',
    passiveDesc: 'Einzelner Durchlauf durch das Modell. Was du siehst, ist was du bekommst. Begrenzt durch anfängliche Bildauflösung und Modell-Aufmerksamkeit.',
    agenticTitle: 'Agentische Vision',
    agenticDesc: 'Iterative Untersuchungsschleife. Kann zoomen, zuschneiden, verbessern und erneut untersuchen. Begründet Antworten mit ausgeführtem Code und visüllen Belegen.',

    // Kernerkenntnisse
    keyTakeaways: 'Kernerkenntnisse',
    takeaway1: 'Agentische Vision behandelt Bildverständnis als aktive Untersuchung, nicht als passive Wahrnehmung',
    takeaway2: 'Die Denken-Handeln-Beobachten-Schleife ermöglicht Modellen, Bilder iterativ zu zoomen, zuzuschneiden und zu analysieren',
    takeaway3: 'Code-Ausführung bietet überprüfbares, fundiertes visülles Reasoning',
    takeaway4: 'Aktivierung agentischer Fähigkeiten zeigt 5-10% Verbesserung bei Bild-Benchmarks',
    takeaway5: 'Dieses Paradigma überbrückt die Lücke zwischen menschlicher und KI-Untersuchung visüller Informationen',
  },

  // Multimodalität Seite
  multimodality: {
    title: 'Multimodalität',
    description: 'Wie moderne KI-Modelle mehrere Eingabetypen verarbeiten und verstehen, einschließlich Bilder, Audio, Video und Text.',
    whatIs: 'Was ist Multimodalität?',
    whatIsDesc: 'Multimodalität bezeichnet die Fähigkeit von KI-Modellen, mehrere Eingabetypen gleichzeitig zu verarbeiten und zu verstehen – Text, Bilder, Audio, Video und mehr. So wie Menschen natürlich Informationen aus verschiedenen Sinnen integrieren, um die Welt zu verstehen, kombinieren multimodale KI-Systeme verschiedene Datentypen, um ein reichhaltigeres, vollständigeres Verständnis aufzubauen.',

    // Modalitätstypen
    modalityTypes: 'Typen von Modalitäten',
    modalityTypesDesc: 'Moderne KI-Systeme können eine Vielzahl von Ein- und Ausgabemodalitäten verarbeiten, jede mit einzigartigen Eigenschaften und Herausforderungen.',
    images: 'Bilder',
    imagesDesc: 'Statische visülle Informationen, die durch Vision Transformer verarbeitet werden. Bilder werden in Patches unterteilt, eingebettet und zusammen mit Text-Tokens für Aufgaben wie Bildbeschreibung, visülle Q&A und Dokumentenanalyse verarbeitet.',
    audio: 'Audio',
    audioDesc: 'Klanginformationen einschließlich Sprache, Musik und Umgebungsgeräusche. Audio wird typischerweise in Spektrogramme oder Wellenformdarstellungen umgewandelt, bevor es von neuronalen Netzen für Transkription, Generierung oder Verständnis verarbeitet wird.',
    video: 'Video',
    videoDesc: 'Zeitliche Seqünzen von Bildern mit optionalen Audiospuren. Videoverständnis erfordert Reasoning über Veränderungen im Zeitverlauf, Objektverfolgung und oft Synchronisation von visüllen und akustischen Informationen.',
    other: 'Andere Modalitäten',
    otherDesc: 'Aufkommende Modalitäten umfassen 3D-Punktwolken, Sensordaten, Code, strukturierte Daten und sogar physische Aktionen in Robotikanwendungen.',
    text: 'Text',

    // Interaktive Demo
    interactiveDemo: 'Interaktive Demo',
    interactiveDemoDesc: 'Erkunde, wie verschiedene Modalitäten in multimodaler KI kombiniert werden',
    selectModalities: 'Wähle Modalitäten zum Kombinieren',
    fusionResult: 'Fusionsergebnis',
    selectToSee: 'Wähle Modalitäten, um zu sehen, wie sie kombiniert werden',
    understanding: 'Eine einzelne Modalität bietet fokussiertes, spezialisiertes Verständnis.',
    combinedUnderstanding: 'Mehrere Modalitäten ermöglichen ein reichhaltigeres, querverweisbasiertes Verständnis, das Beziehungen zwischen verschiedenen Informationstypen erfasst.',
    imageShort: 'Visülle Muster und Objekte',
    audioShort: 'Sprache, Musik und Klänge',
    videoShort: 'Bewegung und zeitliche Muster',
    textShort: 'Sprache und Semantik',
    useCases: 'Anwendungsfälle',
    examplePrompt: 'Beispiel-Prompt:',

    // Einzelne Modalitäts-Anwendungsfälle
    useCaseImageOnly: 'Bildklassifikation & Erkennung',
    useCaseImageOnlyDesc: 'Identifiziere Objekte, Szenen, Gesichter oder spezifische Merkmale in Bildern ohne zusätzlichen Kontext.',
    useCaseImageOnlyExample: 'Welche Objekte sind auf diesem Foto?',
    useCaseAudioOnly: 'Sprachtranskription & Klanganalyse',
    useCaseAudioOnlyDesc: 'Wandle Sprache in Text um oder identifiziere Klänge, Musikgenres und Audioereignisse.',
    useCaseAudioOnlyExample: 'Transkribiere diese Audioaufnahme.',
    useCaseVideoOnly: 'Aktionserkennung & Bewegungsverfolgung',
    useCaseVideoOnlyDesc: 'Erkenne Aktivitäten, verfolge Bewegungsmuster und verstehe zeitliche Abläufe.',
    useCaseVideoOnlyExample: 'Welche Aktivitäten passieren in diesem Video?',
    useCaseTextOnly: 'Natürliches Sprachverständnis',
    useCaseTextOnlyDesc: 'Verarbeite und generiere Text für Fragen, Zusammenfassungen, Übersetzungen und Gespräche.',
    useCaseTextOnlyExample: 'Fasse diesen Artikel in drei Sätzen zusammen.',

    // Zwei-Modalitäten-Kombinationen
    useCaseImageText: 'Visülle Q&A & Dokumentenanalyse',
    useCaseImageTextDesc: 'Stelle Fragen zu Bildern, extrahiere Text aus Dokumenten oder generiere detaillierte Bildbeschreibungen.',
    useCaseImageTextExample: 'Was ist der Gesamtbetrag auf dieser Quittung?',
    useCaseAudioText: 'Sprachassistenten & Podcast-Zusammenfassung',
    useCaseAudioTextDesc: 'Führe natürliche Sprachgespräche, transkribiere Meetings mit Zusammenfassungen oder analysiere gesprochene Inhalte.',
    useCaseAudioTextExample: 'Was sind die Hauptpunkte in dieser Podcast-Episode?',
    useCaseVideoText: 'Videobeschriftung & Inhaltssuche',
    useCaseVideoTextDesc: 'Generiere Beschreibungen von Videoinhalten, suche innerhalb von Videos nach Beschreibung oder erstelle barrierefreie Untertitel.',
    useCaseVideoTextExample: 'Beschreibe, was in diesem Kochvideo passiert.',
    useCaseImageAudio: 'Musik + Albumcover-Analyse',
    useCaseImageAudioDesc: 'Verbinde visülle und akustische Informationen, wie die Analyse von Albumcovern zusammen mit Musik oder das Hinzufügen von Soundeffekten zu Bildern.',
    useCaseImageAudioExample: 'Passt dieses Albumcover zur Stimmung der Musik?',
    useCaseVideoAudio: 'Film- & Medienanalyse',
    useCaseVideoAudioDesc: 'Verstehe Videoinhalte mit ihrem Soundtrack, analysiere Dialog-Timing oder erkenne Audio-Video-Synchronisationsprobleme.',
    useCaseVideoAudioExample: 'Sind die Lippenbewegungen mit dem Audio synchron?',
    useCaseImageVideo: 'Visüller Vergleich über Zeit',
    useCaseImageVideoDesc: 'Vergleiche statische Referenzbilder mit Videoinhalten, erkenne Veränderungen oder überwache auf spezifische visülle Muster.',
    useCaseImageVideoExample: 'Entspricht der im Video gezeigte Artikel diesem Produktfoto?',

    // Drei-Modalitäten-Kombinationen
    useCaseImageAudioText: 'Interaktives Lernen & Tutorials',
    useCaseImageAudioTextDesc: 'Erstelle reichhaltige Bildungserlebnisse, die visülle Hilfsmittel, Erzählung und Texterklärungen kombinieren.',
    useCaseImageAudioTextExample: 'Erkläre dieses Diagramm, während ich beschreibe, was ich sehe.',
    useCaseVideoAudioText: 'Videoverständnis mit Dialog',
    useCaseVideoAudioTextDesc: 'Vollständige Film-/Videoanalyse einschließlich Visülles, Dialogtranskription und Szenenbeschreibungen.',
    useCaseVideoAudioTextExample: 'Fasse dieses Interview zusammen, einschließlich wer was gesagt hat.',
    useCaseImageVideoText: 'Visüller Vergleich & Überwachung',
    useCaseImageVideoTextDesc: 'Vergleiche Referenzbilder mit Videofeeds, wie Qualitätskontrolle oder Sicherheitsüberwachung mit Textberichten.',
    useCaseImageVideoTextExample: 'Entspricht das Produkt in diesem Video den Referenzbild-Spezifikationen?',
    useCaseImageAudioVideo: 'Multimedia-Content-Produktion',
    useCaseImageAudioVideoDesc: 'Kombiniere visülle Medien und Audio ohne explizite Textanweisungen – Analyse von Musikvideos, Synchronisation von Soundtracks mit Visüllen oder Erstellung von Multimedia-Präsentationen.',
    useCaseImageAudioVideoExample: 'Passe diese Hintergrundmusik zur Stimmung dieser Videoclips an.',

    // Alle Modalitäten
    useCaseAll: 'Vollständige Multimedia-Intelligenz',
    useCaseAllDesc: 'Verarbeite alle Eingabetypen zusammen für umfassendes Verständnis – Robotik, autonome Systeme oder immersive KI-Assistenten.',
    useCaseAllExample: 'Führe mich durch den Zusammenbau dieser Möbel mit dem Anleitungsbild, der Videodemonstration und Sprachbefehlen.',

    // Wie es funktioniert
    howWorks: 'Wie multimodale Modelle funktionieren',
    howWorksDesc: 'Multimodale Modelle verwenden spezialisierte Encoder für jede Modalität und richten diese Repräsentationen dann in einem gemeinsamen Einbettungsraum aus, in dem das Modell über Modalitäten hinweg schlussfolgern kann.',
    step1: 'Jede Modalität kodieren',
    step1Desc: 'Spezialisierte Encoder (Vision Transformer für Bilder, Audio-Encoder für Klang) wandeln jeden Eingabetyp in Einbettungsvektoren um.',
    step2: 'Im gemeinsamen Raum ausrichten',
    step2Desc: 'Diese Einbettungen werden in einen gemeinsamen Repräsentationsraum projiziert, in dem Text, Bilder und Audio verglichen und kombiniert werden können.',
    step3: 'Cross-Modales Reasoning',
    step3Desc: 'Das Modell verwendet Aufmerksamkeitsmechanismen, um Informationen über Modalitäten hinweg zu verknüpfen, was Aufgaben wie "Beschreibe, was du siehst" oder "Antworte basierend auf dem Video" ermöglicht.',

    // Audioverarbeitung
    audioProcessing: 'Audioverarbeitung',
    audioProcessingDesc: 'Audio-Modalitäten ermöglichen KI-Systemen, Sprache, Musik und andere Klänge zu verstehen und zu generieren.',
    speechRecognition: 'Spracherkennung',
    speechRecognitionDesc: 'Umwandlung gesprochener Sprache in Text. Moderne Modelle wie Whisper können in über 100 Sprachen mit hoher Genauigkeit transkribieren, auch bei Akzenten und Hintergrundgeräuschen.',
    textToSpeech: 'Text-to-Speech',
    textToSpeechDesc: 'Generierung natürlich klingender Sprache aus Text. Fortgeschrittene Modelle können Stimmen klonen, Emotionen ausdrücken und konsistente Sprechstile beibehalten.',
    musicUnderstanding: 'Musikverständnis',
    musicUnderstandingDesc: 'Analyse musikalischer Inhalte einschließlich Genre, Tempo, Instrumente und Stimmung. Einige Modelle können auch Musik aus Textbeschreibungen generieren.',
    audioGeneration: 'Audiogenerierung',
    audioGenerationDesc: 'Erstellung von Soundeffekten, Umgebungsaudio und Musik. Modelle können alles von realistischen Soundeffekten bis hin zu vollständigen Musikkompositionen generieren.',

    // Videoverständnis
    videoUnderstanding: 'Videoverständnis',
    videoUnderstandingDesc: 'Video stellt einzigartige Herausforderungen dar, da es räumliche Informationen aus Bildern mit zeitlichen Informationen über Veränderungen kombiniert.',
    temporalReasoning: 'Zeitliches Reasoning',
    temporalReasoningDesc: 'Verständnis von Ursache und Wirkung, Handlungsseqünzen und Veränderungen über die Zeit. Modelle müssen Objekte verfolgen und verstehen, wie Frames zueinander in Beziehung stehen.',
    frameSampling: 'Frame-Sampling',
    frameSamplingDesc: 'Videos enthalten viel zu viele Frames, um sie vollständig zu verarbeiten. Modelle verwenden intelligente Sampling-Strategien, um Schlüsselframes auszuwählen, die wichtige Momente erfassen.',
    audioVideoSync: 'Audio-Video-Synchronisation',
    audioVideoSyncDesc: 'Ausrichtung von Audio- und visüllen Informationen, um Ereignisse wie sprechende Personen, spielende Musik oder klingende Objekte zu verstehen.',

    // Cross-Modale Fusion
    crossModal: 'Cross-Modale Fusionsstrategien',
    crossModalDesc: 'Verschiedene Architekturen zur Kombination von Informationen aus mehreren Modalitäten, jeweils mit Kompromissen zwischen Effizienz und Fähigkeit.',
    earlyFusion: 'Frühe Fusion',
    earlyFusionDesc: 'Modalitäten auf Eingabeebene vor jeder Verarbeitung kombinieren. Einfach, aber kann modalitätsspezifische Muster verlieren.',
    lateFusion: 'Späte Fusion',
    lateFusionDesc: 'Jede Modalität separat mit spezialisierten Encodern verarbeiten, dann am Ende kombinieren. Bewahrt modalitätsspezifische Merkmale.',
    crossAttention: 'Cross-Attention',
    crossAttentionDesc: 'Aufmerksamkeitsmechanismen verwenden, um jeder Modalität zu ermöglichen, selektiv auf relevante Teile anderer Modalitäten zu achten. Der flexibelste und leistungsstärkste Ansatz, verwendet in Modellen wie Gemini und GPT-4.',

    // Anwendungen
    applications: 'Reale Anwendungen',
    applicationsDesc: 'Multimodale KI ermöglicht Anwendungen, die mit Einzelmodalitätssystemen zuvor unmöglich waren.',
    app1: 'Videobeschriftung',
    app1Desc: 'Detaillierte Beschreibungen von Videoinhalten für Barrierefreiheit, Suche und Inhaltsmoderation generieren.',
    app2: 'Sprachassistenten',
    app2Desc: 'Natürliche Gespräche, die Sprache verstehen, stimmlich antworten und auf Bilder oder Bildschirme Bezug nehmen können.',
    app3: 'Medizinische Bildgebung',
    app3Desc: 'Analyse von Röntgenaufnahmen, MRTs und anderen Scans zusammen mit Patientenakten und Arztnotizen.',
    app4: 'Robotik',
    app4Desc: 'Verarbeitung von Kamerabildern, Sensordaten und Befehlen zur Navigation und Manipulation der physischen Welt.',
    app5: 'Content-Erstellung',
    app5Desc: 'Bilder aus Text generieren, Audio zu Videos hinzufügen oder multimediale Inhalte aus Beschreibungen erstellen.',
    app6: 'Barrierefreiheit',
    app6Desc: 'Bilder für Sehbehinderte beschreiben, Audio für Gehörlose transkribieren und zwischen Modalitäten übersetzen.',

    // Kernerkenntnisse
    keyTakeaways: 'Kernerkenntnisse',
    takeaway1: 'Multimodale KI kombiniert Text, Bilder, Audio und Video, um ein reichhaltigeres Verständnis der Welt aufzubauen',
    takeaway2: 'Jede Modalität erfordert spezialisierte Encoder, die Eingaben in Einbettungsvektoren umwandeln',
    takeaway3: 'Cross-Attention-Mechanismen ermöglichen Modellen, Informationen über verschiedene Modalitäten hinweg zu verknüpfen',
    takeaway4: 'Videoverständnis fügt die Zeitdimension hinzu und erfordert zeitliches Reasoning und Frame-Sampling',
    takeaway5: 'Reale Anwendungen reichen von Barrierefreiheitstools bis hin zu Robotik und Content-Erstellung',
  },

  // Agentic Vision Demo Komponente
  agenticVisionDemo: {
    title: 'Agentische Vision in Aktion',
    subtitle: 'Beobachte, wie das Modell ein Dokument zoomt, dreht und scannt',
    start: 'Demo starten',
    reset: 'Zurücksetzen',
    documentView: 'Dokumentansicht',
    agentLog: 'Agent-Protokoll',
    clickStart: 'Klicke "Demo starten", um agentische Vision in Aktion zu sehen',
    processing: 'Verarbeitung...',
    thinkMessage: 'Ich muss die Seriennummer unten rechts auf dieser Rechnung lesen. Der Text erscheint klein, also sollte ich diesen Bereich heranzoomen.',
    zoomMessage: 'Zuschneiden und Vergrößern des Seriennummernbereichs...',
    zoomObserve: 'Gezoomte Ansicht erhalten. Der Text ist jetzt größer, aber leicht gedreht.',
    rotateThink: 'Der Text ist etwa 15 Grad geneigt. Ich sollte die Drehung für bessere Lesbarkeit korrigieren.',
    rotateMessage: 'Korrigiere Dokumentdrehung für optimale Texterkennung...',
    rotateObserve: 'Dokument ist jetzt korrekt ausgerichtet. Ich kann die Seriennummer jetzt klar lesen.',
    scanMessage: 'Lese den ausgerichteten, gezoomten Bereich...',
    scanObserve: 'Seriennummer mit hoher Konfidenz identifiziert:',
    resultTitle: 'Extrahierte Seriennummer',
    resultConfidence: 'Konfidenz: 97% | Methode: Zoom + Rotation + Lesen',
    stepThink: 'Denken',
    stepZoom: 'Zoom',
    stepRotate: 'Drehen',
    stepScan: 'Scannen',
    stepDone: 'Fertig',
  },

  // Visual Challenges Demo Komponente
  visualChallengesDemo: {
    title: 'VLM-Fehlermodus-Explorer',
    subtitle: 'Interaktive Szenarien, die zeigen, wo Bildmodelle Schwierigkeiten haben',

    // Tab-Namen
    countingTab: 'Zahlen',
    spatialTab: 'Raumlich',
    textTab: 'Text',
    hallucinationTab: 'Halluzination',
    detailTab: 'Details',
    multiImageTab: 'Multi-Bild',

    // UI
    imageScenario: 'Bildszenario',
    vlmResponse: 'VLM-Antwort',
    revealAnalysis: 'Analyse anzeigen',
    hideAnalysis: 'Analyse verbergen',
    vlmSaid: 'VLM sagte',
    actualAnswer: 'Tatsachliche Antwort',
    whyFails: 'Warum das fehlschlagt',
    proTip: 'Profi-Tipp',
    keyInsight: 'Wichtige Erkenntnis',
    keyInsightDesc: 'VLMs sind leistungsstark, aber nicht unfehlbar. Das Verstandnis ihrer systematischen Schwachen hilft dir, robuste Anwendungen zu entwickeln, die ihre Starken nutzen und ihre Einschrankungen abmildern.',

    // Zahlungs-Herausforderung
    countingTitle: 'Objektzahl-Herausforderung',
    countingScenario: 'Ein Foto eines Schreibtisches mit verstreuten Buroklammern. Es sind genau 23 Buroklammern sichtbar, einige uberlappen sich.',
    countingVlmResponse: '"Ich kann ungefahr 15-20 Buroklammern sehen, die uber den Schreibtisch verstreut sind."',
    countingActual: 'Es sind genau 23 Buroklammern im Bild.',
    countingWhy: 'VLMs verarbeiten Bilder als 14x14-Pixel-Patches, nicht als diskrete Objekte. Ihnen fehlt die Objektpermanenz und sie haben Schwierigkeiten mit uberlappenden Elementen. Das Modell gibt eine grobe Schatzung basierend auf Mustererkennung, nicht auf tatsachlichem Zahlen.',
    countingTip: 'Fur prazises Zahlen verwende spezialisierte Objekterkennungsmodelle (YOLO, Faster R-CNN) oder bitte das VLM, jedes Element einzeln zu identifizieren und aufzulisten, anstatt eine Gesamtzahl anzugeben.',

    // Raumliche Herausforderung
    spatialTitle: 'Raumliche Denk-Herausforderung',
    spatialScenario: 'Ein Foto, das eine rote Tasse auf der LINKEN Seite einer blauen Tasse zeigt, beide auf einem weissen Tisch.',
    spatialVlmResponse: '"Die rote Tasse befindet sich rechts von der blauen Tasse auf dem Tisch."',
    spatialActual: 'Die rote Tasse befindet sich auf der LINKEN Seite der blauen Tasse.',
    spatialWhy: 'Positionsinformationen werden durch Patch-Einbettungen ohne Pixel-genaue Prazision kodiert. Das Modell lernt statistische Korrelationen anstatt explizites raumliches Denken, was Links/Rechts-Verwechslungen haufig macht.',
    spatialTip: 'Sei explizit uber Bezugsrahmen in deinen Prompts. Erwage, visülle Markierungen oder Rasteruberlagerungen zu Bildern hinzuzufugen, wenn raumliche Prazision kritisch ist.',

    // Text-Herausforderung
    textTitle: 'Kleine Text-Erkennungs-Herausforderung',
    textScenario: 'Ein Produktetikett mit der Seriennummer "XK7-2B9M-Q4P" in 8pt-Schrift am unteren Rand.',
    textVlmResponse: '"Die Seriennummer scheint XK7-289M-04P zu sein."',
    textActual: 'Die Seriennummer ist XK7-2B9M-Q4P (Hinweis: B statt 8, Q statt 0).',
    textWhy: 'Text kleiner als die Patch-Grosse (14-16 Pixel) wird in eine einzelne Einbettung komprimiert, wobei Details auf Zeichenebene verloren gehen. Ahnliche Zeichen (B/8, Q/0, l/1) werden leicht verwechselt.',
    textTip: 'Verwende hochauflosende Bilder und zoome in Textbereiche. Fur kritische OCR-Aufgaben verwende dedizierte OCR-Tools (Tesseract, Google Vision API) neben oder anstelle von VLMs.',

    // Halluzinations-Herausforderung
    hallucinationTitle: 'Visülle Halluzinations-Herausforderung',
    hallucinationScenario: 'Ein Foto eines Heimburo-Schreibtisches mit einem Monitor, aber OHNE sichtbare Tastatur (sie ist in einer Schublade).',
    hallucinationVlmResponse: '"Ich kann einen Monitor auf dem Schreibtisch mit einer Tastatur davor sehen."',
    hallucinationActual: 'Es ist keine Tastatur im Bild sichtbar – das VLM hat sie halluziniert.',
    hallucinationWhy: 'VLMs generieren statistisch plausible Beschreibungen. Wenn Bildmerkmale mehrdeutig sind, fullt das Modell Lucken mit wahrscheinlichem Inhalt basierend auf Trainingsdaten – Monitore haben normalerweise Tastaturen in der Nahe.',
    hallucinationTip: 'Bitte das Modell, Unsicherheit auszudrucken und nur zu beschreiben, was es klar sehen kann. Verwende Prompts wie "wenn du X nicht bestimmen kannst, sage es" und uberprufe kritische Details immer.',

    // Detail-Herausforderung
    detailTitle: 'Feine Detail-Erkennungs-Herausforderung',
    detailScenario: 'Zwei ahnliche Hundefotos: eines ist ein Shiba Inu, das andere ein Akita. Die subtilen Unterschiede liegen in Ohrform und Korperproportionen.',
    detailVlmResponse: '"Beide Bilder zeigen Shiba-Inu-Hunde."',
    detailActual: 'Das erste ist ein Shiba Inu, das zweite ein Akita (grosserer Korper, andere Ohrform).',
    detailWhy: 'Die patch-basierte Architektur mittelt Informationen innerhalb jedes Patches und verliert dabei feinkörnige Details. Subtile Unterscheidungsmerkmale wie Ohrwinkel oder Korperproportionen werden komprimiert.',
    detailTip: 'Verwende die höchste verfügbare Auflösung und schneide zu, um sich auf unterscheidende Merkmale zu konzentrieren. Für spezialisierte Aufgaben erwäge feingetunete Modelle, die auf domänenspezifischen Daten trainiert wurden.',

    // Multi-Bild-Herausforderung
    multiImageTitle: 'Multi-Bild-Denk-Herausforderung',
    multiImageScenario: 'Zwei nahezu identische Produktfotos. Der Unterschied: In Bild 2 hat das Produkt einen kleinen Kratzer in der oberen rechten Ecke.',
    multiImageVlmResponse: '"Beide Bilder scheinen identisch zu sein. Ich sehe keine Unterschiede zwischen ihnen."',
    multiImageActual: 'Bild 2 hat einen sichtbaren Kratzer in der oberen rechten Ecke, der in Bild 1 nicht vorhanden ist.',
    multiImageWhy: 'Jedes Bild wird separat in Token-Seqünzen kodiert. Cross-Image-Aufmerksamkeit muss durch das Kontextfenster des Sprachmodells erfolgen, was weniger effizient ist als dedizierte Architekturen.',
    multiImageTip: 'Beschreibe jedes Bild zuerst separat, dann frage nach dem Vergleich. Erwage, Bilder zu einem einzigen Nebeneinander-Komposit fur direkten visüllen Vergleich zu kombinieren.',
  },

  // Skill Composer Demo
  skillComposer: {
    // Tab-Namen
    triggerTab: 'Skill-Ausloesung',
    manifestTab: 'Skill-Manifest',
    chainingTab: 'Skill-Verkettung',

    // Trigger-Tab
    userInput: 'Benutzereingabe',
    userInputDesc: 'Gib eine Nachricht ein, um zu sehen, welcher Skill ausgeloest wird',
    inputPlaceholder: 'Versuche: "Kannst du diesen Code auf Fehler überprüfen?"',
    tryExamples: 'Probiere diese Beispiele:',
    exampleReview: 'Überprüfe diesen Code auf Fehler',
    exampleDocs: 'Schreibe Dokumentation für diese API',
    exampleGit: 'Erstelle einen Pull Reqüst',
    exampleExplain: 'Erklaere, wie diese Funktion funktioniert',

    // Ergebnis
    matchResult: 'Skill-Treffer',
    matchResultDesc: 'Der Skill, der ausgeloest werden würde',
    analyzing: 'Analysiere Eingabe...',
    confidence: 'Konfidenz',
    matchedTrigger: 'Gematchter Trigger',
    canChainWith: 'Kann verketten mit:',
    noMatch: 'Gib eine Nachricht ein, um Skill-Matching in Aktion zu sehen',

    // Skills
    codeReviewSkill: 'Code Review',
    codeReviewDesc: 'Führt gründliche Code-Reviews nach Team-Standards durch',
    documentationSkill: 'Dokumentation',
    documentationDesc: 'Erstellt umfassende Dokumentation für Code und APIs',
    gitWorkflowSkill: 'Git Workflow',
    gitWorkflowDesc: 'Verwaltet Git-Operationen und Pull-Reqüst-Workflows',
    explainSkill: 'Code erklaeren',
    explainDesc: 'Erklaert Code-Funktionalität und Konzepte verständlich',

    availableSkills: 'Verfügbare Skills',

    // Manifest-Tab
    skillManifest: 'Skill-Manifest',
    skillManifestDesc: 'Die SKILL.md-Datei definiert die Metadaten und Anweisungen eines Skills',
    manifestInstructions: 'Detaillierte Anweisungen zur Ausführung dieses Skills kommen hier hin...',

    // Manifest-Felder
    nameField: 'name',
    nameFieldDesc: 'Eindeutiger Bezeichner für den Skill',
    triggersField: 'triggers',
    triggersFieldDesc: 'Schlüsselwörter und Phrasen, die diesen Skill aktivieren',
    descriptionField: 'description',
    descriptionFieldDesc: 'Kurze Zusammenfassung dessen, was der Skill tut',
    chainsWithField: 'chains_with',
    chainsWithFieldDesc: 'Andere Skills, die dieser Skill aufrufen kann',

    // Verkettungs-Tab
    skillChaining: 'Skill-Verkettung',
    skillChainingDesc: 'Skills können andere Skills aufrufen, um komplexe Aufgaben zu erledigen',
    selectToSeeChain: 'Klicke auf einen Skill, um zu sehen, womit er verkettet werden kann:',
    noChains: 'Dieser Skill hat keine Verkettungsziele',
    chainingExample: 'Beispiel: Code Review + Git Workflow',
    chainingStep1: 'Benutzer fragt: "Überprüfe und merge diesen PR"',
    chainingStep2: 'Code Review Skill führt die Überprüfung durch',
    chainingStep3: 'Git Workflow Skill wird verkettet, um den Merge zu handhaben',
  },

  // Logges Lieblingsmodelle (Opus 4.6 + GPT-5.3-Codex)
  favModels: {
    title: 'Logges Lieblingsmodelle',
    description: 'Zwei Modelle am gleichen Tag veröffentlicht. Beide sofort zu Favoriten geworden. Hier ist ein ehrlicher, leicht obsessiver Überblick.',
    disclaimer: 'Haftungsausschluss',
    disclaimerText: 'Diese Seite ist unverblümt voreingenommen. Ich nutze beide Modelle täglich, bezahle beide aus eigener Tasche und habe starke Meinungen. Modell-Releases entwickeln sich schnell, das hier wird sich ändern. Nebenwirkungen des Lesens können API-Key-Generierung und Geldbeutel-Angst umfassen.',
    lastUpdated: 'Zuletzt aktualisiert: 5. Februar 2026',

    // Hero / Intro
    heroTitle: 'Zwei Modelle, eine Obsession',
    heroSubtitle: 'Am 5. Februar 2026 haben Anthropic und OpenAI ihre neüsten Flaggschiffe innerhalb weniger Stunden veröffentlicht. Ich nutze beide seitdem ununterbrochen. Hier ist der ehrliche Erfahrungsbericht von jemandem, der tatsächlich Code damit ausliefert.',

    // Die zwei Champions
    championsTitle: 'Die zwei Champions',
    championsSubtitle: 'Verschiedene Philosophien, beide exzellent. Hier ist, was jedes Modell mitbringt.',

    // Claude Opus 4.6
    opusName: 'Claude Opus 4.6',
    opusMaker: 'Anthropic',
    opusTagline: 'Der Tiefdenker, der wie ein Senior Engineer programmiert',
    opusModelId: 'claude-opus-4-6',
    opusReleaseDate: '5. Februar 2026',
    opusContext: '200K Standard / 1M Beta',
    opusOutput: '128K Tokens',
    opusPricing: '$5 / $25 pro Million Tokens',
    opusDescription: 'Opus 4.6 ist Anthropics leistungsfähigstes Modell aller Zeiten. Es verdoppelte das Ausgabelimit auf 128K, führte ein 1M-Token-Kontextfenster in der Beta ein und brachte zwei exklusive Features: Adaptive Thinking (passt die Reasoning-Tiefe automatisch an) und Context Compaction (fasst alten Kontext automatisch zusammen für endlose Konversationen). Die Coding-Verbesserungen sind massiv — Terminal-Bench stieg von 59,8% auf 65,4%, OSWorld von 66,3% auf 72,7%, und ARC AGI 2 hat sich von 37,6% auf 68,8% fast verdoppelt.',

    opusStrength1Title: 'Adaptive Thinking',
    opusStrength1Desc: 'Passt die Reasoning-Tiefe dynamisch an die Aufgabenkomplexität an. Vier Intensitätsstufen: niedrig, mittel, hoch und maximal. Es entscheidet selbst, wann tieferes Nachdenken hilft.',
    opusStrength2Title: 'Agent Teams',
    opusStrength2Desc: 'Ermöglicht Multi-Agent-Coding in Claude Code — ein Agent fürs Frontend, einer für die API, ein dritter für die Migration — alle koordinieren sich autonom.',
    opusStrength3Title: '1M Token Kontext',
    opusStrength3Desc: 'Erstes Opus-Modell mit einem Millionen-Token-Fenster. Füttere es mit einer ganzen Codebase und es kann über alles hinweg denken.',
    opusStrength4Title: '128K Output',
    opusStrength4Desc: 'Verdoppelt von 64K. Es kann ganze Dateien, vollständige Test-Suiten und mehrseitige Dokumente in einer einzigen Antwort generieren.',

    opusSourceLabel: 'Qülle: Anthropic Opus-Seite',
    opusSourceUrl: 'https://www.anthropic.com/claude/opus',

    // GPT-5.3-Codex
    codexName: 'GPT-5.3-Codex',
    codexMaker: 'OpenAI',
    codexTagline: 'Der schnelle Pragmatiker, der sich selbst mitgebaut hat',
    codexModelId: 'gpt-5.3-codex',
    codexReleaseDate: '5. Februar 2026',
    codexContext: '~400K Tokens',
    codexOutput: '~128K Tokens',
    codexPricing: '~$1,25 / $10 pro Million Tokens (erwartet)',
    codexDescription: 'GPT-5.3-Codex ist OpenAIs erstes "selbstentwickelndes" Modell — frühe Versionen wurden verwendet um den eigenen Trainingslauf zu debuggen. Es vereint Frontier-Coding-Leistung (von GPT-5.2-Codex) mit professionellem Reasoning (von GPT-5.2) in einem einzigen Modell. Es ist 25% schneller als sein Vorgänger, braucht die Hälfte der Tokens für gleichwertige Aufgaben und dominiert Terminal-Bench 2 absolut mit 77,3%.',

    codexStrength1Title: 'Interactive Steering',
    codexStrength1Desc: 'Du kannst während der Arbeit mit ihm interagieren — Fragen stellen, Ansätze besprechen und in Echtzeit lenken. Es gibt regelmäßige Fortschrittsupdates.',
    codexStrength2Title: 'Selbstentwickelnd',
    codexStrength2Desc: 'Erstes Modell, das maßgeblich an seiner eigenen Entwicklung beteiligt war. Intern zum Debugging des Trainings, zur Verwaltung des Deployments und zur Optimierung der Evaluierung eingesetzt.',
    codexStrength3Title: 'Token-effizient',
    codexStrength3Desc: 'Erreicht Ergebnisse mit weniger als der Hälfte der Tokens seiner Vorgänger. Dein Kontext-Budget reicht weiter.',
    codexStrength4Title: 'Persönlichkeitsmodi',
    codexStrength4Desc: 'Wähle zwischen "Pragmatic" (knapp, auf den Punkt) und "Friendly" (gesprächig). Kein Leistungsunterschied — rein stilistisch.',

    codexSourceLabel: 'Qülle: OpenAI GPT-5.3-Codex Launch',
    codexSourceUrl: 'https://openai.com/index/introducing-gpt-5-3-codex/',

    // Benchmark Showdown
    benchmarkTitle: 'Der Benchmark-Showdown',
    benchmarkSubtitle: 'Zahlen lügen nicht, aber sie erzählen auch nicht die ganze Geschichte. So stehen sie bei den Benchmarks da, die fürs Coden wirklich zählen.',

    benchSWE: 'SWE-bench',
    benchSWEOpus: '80,8%',
    benchSWECodex: '56,8%',
    benchSWENote: 'Verified vs Pro (verschiedene Testsets — nicht direkt vergleichbar)',

    benchTerminal: 'Terminal-Bench 2',
    benchTerminalOpus: '65,4%',
    benchTerminalCodex: '77,3%',
    benchTerminalNote: 'Codex dominiert echte Terminal-Workflows',

    benchOSWorld: 'OSWorld',
    benchOSWorldOpus: '72,7%',
    benchOSWorldCodex: '64,7%',
    benchOSWorldNote: 'Opus führt bei Desktop-Automatisierung',

    benchGPQA: 'GPQA Diamond',
    benchGPQAOpus: '91,3%',
    benchGPQACodex: '—',
    benchGPQANote: 'Wissenschaftliches Reasoning auf Doktoranden-Niveau',

    benchARC: 'ARC AGI 2',
    benchARCOpus: '68,8%',
    benchARCCodex: '—',
    benchARCNote: 'Neuartige Problemlösung (fast 2x vs Opus 4.5)',

    benchHLE: "Humanity's Last Exam",
    benchHLEOpus: '40,0%',
    benchHLECodex: '—',
    benchHLENote: 'Der schwerste Test in der KI — ohne Tools',

    benchCyber: 'Cybersecurity CTF',
    benchCyberOpus: '—',
    benchCyberCodex: '77,6%',
    benchCyberNote: 'Capture-the-Flag Sicherheits-Challenges',

    benchDisclaimer: 'Strich bedeutet, dass der Benchmark vom Anbieter nicht veröffentlicht wurde. SWE-bench Verified und SWE-bench Pro nutzen verschiedene Testsets, daher ist ein direkter Vergleich nicht aussagekräftig.',

    // Wann ich welches nutze
    whenTitle: 'Wann ich welches tatsächlich nutze',
    whenSubtitle: 'Theorie ist nett. Hier ist mein tatsächlicher Workflow, nachdem ich beide seit Launch täglich nutze.',

    whenOpusTitle: 'Ich greife zu Opus 4.6 wenn...',
    whenOpusToolLabel: 'Mein Tool: Claude Code (CLI)',
    whenOpusToolDesc: 'Opus 4.6 lebt in meinem Terminal via Claude Code. Agent Teams, Multi-Datei-Edits, tiefes Reasoning — alles von der Kommandozeile. Diese gesamte Seite wurde damit gebaut.',
    whenOpus1: 'Ich tiefes architektonisches Reasoning über eine große Codebase brauche — das 1M Kontextfenster ist unübertroffen',
    whenOpus2: 'Ich komplexe Multi-Datei-Features schreibe, bei denen das Modell viel State halten muss',
    whenOpus3: 'Code Review und Refactoring — Adaptive Thinking macht es wirklich sorgfältig',
    whenOpus4: 'Agent Teams für ambitionierte mehrteilige Projekte',
    whenOpus5: 'Alles, was von erweitertem Denken und sorgfältigem schrittweisem Reasoning profitiert',

    whenCodexTitle: 'Ich greife zu GPT-5.3-Codex wenn...',
    whenCodexToolLabel: 'Mein Tool: Codex (App + CLI)',
    whenCodexToolDesc: 'GPT-5.3-Codex treibt die Codex-App und CLI an. Interactive Steering mid-task, Persönlichkeitsmodi und blitzschnelle Geschwindigkeit machen es perfekt für schnelle Iteration.',
    whenCodex1: 'Schnelle Iteration bei Terminal-lastigen Workflows — es ist blitzschnell und die Terminal-Bench-Scores zeigen warum',
    whenCodex2: 'Interaktives Pair Programming, bei dem ich mid-task lenken will',
    whenCodex3: 'Aufgaben mit hohem Volumen, bei denen Token-Effizienz kostenmäßig zählt',
    whenCodex4: 'Die Codex CLI für schnelles Scripting und Einmal-Aufgaben',
    whenCodex5: 'Alles, wo ich Speed über Tiefe will — es ist 25% schneller und man spürt es',

    // Preis-Realität
    pricingTitle: 'Die Geldbeutel-Situation',
    pricingSubtitle: 'Reden wir über den Elefanten im Raum.',
    pricingOpusTitle: 'Opus 4.6',
    pricingOpusDetail: '$5 Input / $25 Output pro Million Tokens. Gleicher Preis wie Opus 4.5, aber mit massiv verbesserten Fähigkeiten. Batch API mit 50% Rabatt. Immer noch Premium-Territorium — eine intensive Coding-Session kann $5-15 kosten.',
    pricingCodexTitle: 'GPT-5.3-Codex',
    pricingCodexDetail: 'API-Preise stehen noch nicht fest, aber die GPT-5-Codex-Familie liegt bei ~$1,25 Input / $10 Output. Das ist grob 2,5x günstiger als Opus bei Input und 2,5x günstiger bei Output. Plus es braucht weniger Tokens für gleichwertige Aufgaben.',
    pricingVerdict: 'Ehrliche Einschätzung: Wenn du kostenbewusst bist, gewinnt Codex klar. Wenn du maximale Reasoning-Tiefe brauchst und bereit bist dafür zu zahlen, ist Opus jeden Cent wert. Ich nutze beide, weil verschiedene Aufgaben verschiedene Wirtschaftlichkeit haben.',

    // Gemeinsamkeiten
    commonTitle: 'Was sie gemeinsam haben',
    commonSubtitle: 'Trotz rivalisierender Labs sind diese Modelle bei einigen wichtigen Eigenschaften konvergiert.',
    common1Title: 'Agentische Exzellenz',
    common1Desc: 'Beide Modelle sind für Agents gebaut — Tool-Nutzung, mehrstufige Planung und autonome Aufgabenerledigung sind erstklassige Fähigkeiten.',
    common2Title: 'Computer Use',
    common2Desc: 'Beide können GUIs bedienen, Formulare ausfüllen, Apps navigieren. OSWorld-Scores von 72,7% (Opus) und 64,7% (Codex) zeigen echte Desktop-Kompetenz.',
    common3Title: 'Erweiterter Output',
    common3Desc: '~128K Token Ausgabelimits bei beiden. Ganze Codebases, vollständige Dokumentation, Multi-Datei-Änderungen in einer einzigen Antwort generieren.',
    common4Title: 'Am gleichen Tag veröffentlicht',
    common4Desc: '5. Februar 2026. Beide Labs haben ihre Flaggschiffe innerhalb von Stunden veröffentlicht. Die KI-Coding-Kriege sind real, und wir Entwickler sind die Gewinner.',

    // Persönliches Verdict
    verdictTitle: 'Das ehrliche Urteil',
    verdictText: 'Ich habe nicht mehr ein Lieblingsmodell — ich habe zwei. Opus 4.6 ist das Modell, dem ich bei tiefer, sorgfältiger Arbeit vertraue. Es denkt bevor es handelt, entdeckt Dinge die ich übersehe und meistert massive Codebases mit Eleganz. GPT-5.3-Codex ist das Modell, zu dem ich greife wenn ich Speed und Pragmatismus brauche. Es ist schnell, effizient und das Interactive Steering fühlt sich wie echtes Pair Programming an. Zusammen decken sie jedes Coding-Szenario ab, das mir begegnet. Die Tatsache, dass sie am gleichen Tag gelauncht wurden, fühlt sich symbolisch an — die Frontier ist nicht mehr ein Modell, es ist ein Toolkit. Wähle das richtige Tool für die Aufgabe. Oder nutze, wie ich, beide und genieße die beste Ära KI-gestützter Entwicklung, die wir je erlebt haben.',

    // Quick Reference
    quickRefTitle: 'Schnellübersicht',
    quickRefModel: 'Modell',
    quickRefMaker: 'Hersteller',
    quickRefContext: 'Kontext',
    quickRefOutput: 'Max Output',
    quickRefPrice: 'Preise (pro MTok)',
    quickRefBestFor: 'Am besten für',
    quickRefOpusBest: 'Tiefes Reasoning, Code Review, Agent Teams',
    quickRefCodexBest: 'Schnelle Iteration, Terminal-Aufgaben, Kosteneffizienz',
    quickRefPlatforms: 'Plattformen',
    quickRefOpusPlatforms: 'Claude.ai, API, AWS Bedrock, Vertex AI, Azure Foundry',
    quickRefCodexPlatforms: 'ChatGPT, Codex App, CLI, IDE Extension (API bald verfügbar)',
  },

  // Speculative Decoding Seite
  speculativeDecoding: {
    title: 'Spekulatives Decoding',
    description: 'Eine Technik zur Beschleunigung der LLM-Inferenz, bei der ein kleines Draft-Modell Token vorschlägt, die vom Zielmodell parallel verifiziert werden.',

    whatIs: 'Was ist Spekulatives Decoding?',
    whatIsDesc: 'Spekulatives Decoding ist eine Inferenz-Optimierungstechnik, die die Textgenerierung von großen Sprachmodellen beschleunigt. Anstatt Token einzeln mit dem großen Modell zu generieren, schlägt ein kleineres "Draft"-Modell schnell mehrere Kandidaten-Token vor, die das größere "Ziel"-Modell dann in einem einzigen Forward-Pass verifiziert.',
    whatIsDesc2: 'Die Schlüsselerkenntnis ist, dass Verifikation viel günstiger ist als Generierung. Das Zielmodell kann mehrere Token parallel prüfen, weil es alle Positionen während eines Forward-Pass gleichzeitig verarbeitet, während autoregressive Generierung einen Forward-Pass pro Token erfordert.',

    problem: 'Der Inferenz-Engpass',
    problemDesc: 'Standard autoregressives Decoding ist inhärent langsam, weil jedes Token von allen vorherigen Token abhängt und sequentielle Generierung erzwingt.',
    bottleneck: 'Sequentielle Abhängigkeit',
    bottleneckDesc: 'Jedes neue Token erfordert einen vollständigen Forward-Pass durch das Modell. Für ein 70B-Parameter-Modell bedeutet die Generierung von 100 Token 100 separate Forward-Passes.',
    memoryBound: 'Speicherbandbreiten-Limitiert',
    memoryBoundDesc: 'LLM-Inferenz ist oft durch die Geschwindigkeit limitiert, mit der wir Modellgewichte aus dem Speicher laden können, nicht durch Berechnung. Die GPU wartet untätig auf Daten.',

    howItWorks: 'Wie es funktioniert',
    howItWorksDesc: 'Spekulatives Decoding folgt einem Draft-dann-Verify-Muster, das die parallele Natur der Transformer-Verifikation ausnutzt.',
    step1Title: 'Draft-Generierung',
    step1Desc: 'Ein kleines, schnelles Draft-Modell (z.B. 7B Parameter) generiert K Kandidaten-Token autoregressiv. Das ist schnell, weil das Draft-Modell klein ist.',
    step2Title: 'Parallele Verifikation',
    step2Desc: 'Das Zielmodell verarbeitet den Prompt plus alle K Draft-Token in einem einzigen Forward-Pass und berechnet Wahrscheinlichkeiten für jede Position.',
    step3Title: 'Token-Akzeptanz',
    step3Desc: 'Jedes Draft-Token wird akzeptiert oder abgelehnt durch Vergleich der Draft- und Ziel-Wahrscheinlichkeiten. Ein Rejection-Sampling-Schema stellt sicher, dass die Ausgabeverteilung exakt dem Zielmodell entspricht.',
    step4Title: 'Fortfahren oder Neu-Samplen',
    step4Desc: 'Akzeptierte Token werden behalten. Bei der ersten Ablehnung sampelt das Zielmodell ein Korrektur-Token. Der Prozess wiederholt sich von der neuen Position.',

    visualExample: 'Visülles Beispiel',
    visualExampleDesc: 'So verarbeitet spekulatives Decoding eine einfache Fortsetzung:',
    examplePrompt: 'Prompt:',
    exampleDraft: 'Draft-Modell schlägt 4 Token vor:',
    exampleDraftTokens: '"jumps" → "over" → "the" → "lazy"',
    exampleVerify: 'Zielmodell verifiziert in einem Pass:',
    accepted: 'akzeptiert',
    rejected: 'abgelehnt, Zielmodell bevorzugt "sleeping"',
    exampleResult: 'Finale Ausgabe:',
    exampleSavings: '3 Token akzeptiert + 1 Korrektur = 4 Token aus 2 Forward-Passes statt 4',

    draftRequirements: 'Draft-Modell-Anforderungen',
    draftRequirementsDesc: 'Die Wahl des Draft-Modells beeinflusst die erreichte Beschleunigung erheblich. Das ideale Draft-Modell balanciert Geschwindigkeit mit Übereinstimmung zum Zielmodell.',
    requirement1: 'Viel Kleiner',
    requirement1Desc: 'Das Draft-Modell sollte 5-10x kleiner sein als das Ziel. Ein 7B-Draft für ein 70B-Ziel, oder ein 1B-Draft für ein 7B-Ziel.',
    requirement2: 'Ähnliche Verteilung',
    requirement2Desc: 'Höhere Akzeptanzraten kommen von Draft-Modellen, die auf ähnlichen Daten trainiert oder vom Zielmodell destilliert wurden.',
    requirement3: 'Gleicher Wortschatz',
    requirement3Desc: 'Draft und Ziel müssen denselben Tokenizer teilen, um Token-Level-Kompatibilität während der Verifikation sicherzustellen.',
    requirement4: 'Schnelle Inferenz',
    requirement4Desc: 'Das Draft-Modell muss schnell genug sein, dass das Drafting von K Token weniger Zeit braucht als K Forward-Passes des Zielmodells.',

    speedupFactors: 'Was beeinflusst die Beschleunigung?',
    speedupFactorsDesc: 'Typische Beschleunigungen liegen bei 2-3x, aber mehrere Faktoren beeinflussen die tatsächliche Verbesserung.',
    factor1: 'Akzeptanzrate',
    factor1Impact: 'Höher = mehr Token pro Verifikations-Pass',
    factor2: 'Draft-Modell-Geschwindigkeit',
    factor2Impact: 'Schnelleres Draft = mehr Versuche möglich',
    factor3: 'Zielmodell-Größe',
    factor3Impact: 'Größere Ziele profitieren mehr (mehr speicherbandbreiten-limitiert)',
    factor4: 'Aufgaben-Vorhersagbarkeit',
    factor4Impact: 'Vorhersagbarer Text (Code, strukturiert) = höhere Akzeptanz',

    variants: 'Varianten und Erweiterungen',
    variantsDesc: 'Forscher haben mehrere Variationen entwickelt, um das grundlegende spekulative Decoding zu verbessern.',
    variant1: 'Self-Speculative Decoding',
    variant1Desc: 'Nutzt Early-Exit aus dem Zielmodell selbst als Draft, wodurch ein separates Draft-Modell überflüssig wird.',
    variant2: 'Medusa',
    variant2Desc: 'Fügt dem Zielmodell mehrere Vorhersage-Köpfe hinzu, um Draft-Token parallel zu generieren und sequentielle Draft-Generierung zu vermeiden.',
    variant3: 'Lookahead Decoding',
    variant3Desc: 'Generiert mehrere parallele Spekulationszweige unter Verwendung von N-Gramm-Mustern aus dem Kontext, kein Draft-Modell nötig.',
    variant4: 'Staged Speculative Decoding',
    variant4Desc: 'Verwendet eine Kaskade von zunehmend größeren Draft-Modellen für bessere Akzeptanzraten bei schwierigen Token.',

    // Limitierungen
    limitations: 'Limitierungen',
    limitationsDesc: 'Obwohl spekulatives Decoding erhebliche Beschleunigungen bietet, hat es wichtige Einschränkungen, die begrenzen, wann und wie es effektiv eingesetzt werden kann.',
    limit1: 'Draft-Modell-Overhead',
    limit1Desc: 'Du musst ein separates Draft-Modell betreiben und warten. Das fügt Speicher-Overhead hinzu (das Draft-Modell muss neben dem Zielmodell in den GPU-Speicher passen) und erhöht die operationelle Komplexität.',
    limit2: 'Abnehmende Erträge bei Batch-Größe',
    limit2Desc: 'Spekulatives Decoding glänzt bei Einzelseqünz-Inferenz. Bei größeren Batch-Größen wird das Zielmodell rechengebunden statt speichergebunden, was den Vorteil reduziert.',
    limit3: 'Variable Beschleunigung',
    limit3Desc: 'Die Beschleunigung hängt stark von der Akzeptanzrate ab, die je nach Aufgabe variiert. Kreatives Schreiben mit hoher Temperatur sieht möglicherweise wenig Vorteil, während strukturierte Code-Generierung stark profitiert.',
    limit4: 'Implementierungs-Komplexität',
    limit4Desc: 'Korrektes Rejection-Sampling ist schwierig zu implementieren. Naive Implementierungen können Ausgaben erzeugen, die von der wahren Verteilung des Zielmodells abweichen.',
    limit5: 'Nicht immer schneller',
    limit5Desc: 'Wenn das Draft-Modell zu langsam, zu ungenau ist, oder das Zielmodell bereits schnell genug ist, kann spekulatives Decoding tatsächlich langsamer sein als Standard-Decoding.',

    // Visualizer
    vizTitle: 'Interaktive Simulation',
    vizSubtitle: 'Spekulatives Decoding in Aktion beobachten',
    vizStart: 'Start',
    vizPause: 'Pause',
    vizReset: 'Zurücksetzen',
    vizAcceptanceRate: 'Draft-Ziel-Übereinstimmung',
    vizLowMatch: 'Niedrige Übereinstimmung',
    vizHighMatch: 'Hohe Übereinstimmung',
    vizDraftSpeed: 'Draft-Geschwindigkeit',
    vizSlower: 'Langsamer',
    vizFaster: 'Schneller',
    vizPrompt: 'Prompt:',
    vizDraftModel: 'Draft-Modell (klein, schnell)',
    vizTargetModel: 'Zielmodell (groß, genau)',
    vizGenerating: 'generiert...',
    vizVerifying: 'verifiziert Batch...',
    vizWaiting: 'Warte auf Generierung...',
    vizNoTokens: 'Noch keine verifizierten Token',
    vizIdle: 'Bereit',
    vizDrafting: 'Drafting',
    vizVerifyPhase: 'Verifizieren',
    vizComplete: 'Fertig',
    vizAccepted: 'Akzeptiert',
    vizRejected: 'Abgelehnt',
    vizPasses: 'Ziel-Passes',
    vizVsStandard: 'vs. Standard:',
    vizSpeedup: 'Beschleunigung',
    vizActualRate: 'Tatsächliche Rate',
    vizExplanation: 'Das Draft-Modell schlägt schnell Token vor (lila). Das Zielmodell verifiziert sie in einem Pass (cyan). Akzeptierte Token sind grün, Korrekturen orange. Höhere Draft-Ziel-Übereinstimmung bedeutet mehr akzeptierte Token und bessere Beschleunigung.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Spekulatives Decoding nutzt ein kleines Draft-Modell, um Token vorzuschlagen, die vom Zielmodell parallel verifiziert werden',
    takeaway2: 'Es bietet 2-3x Beschleunigung bei identischer Ausgabe zum Standard-Decoding',
    takeaway3: 'Die Technik nutzt aus, dass Transformer-Verifikation parallel ist, während Generierung sequentiell ist',
    takeaway4: 'Die Effektivität hängt von der Draft-Ziel-Übereinstimmung ab: ähnliche Modelle und vorhersagbare Aufgaben ergeben höhere Akzeptanzraten',
  },

  // KV Cache page
  kvCache: {
    title: 'KV-Cache',
    description: 'Wie das Speichern berechneter Key-Value-Paare die autoregressive Textgenerierung in Transformern drastisch beschleunigt.',
    whatIs: 'Was ist KV-Cache?',
    whatIsDesc: 'Während der autoregressiven Generierung muss ein Transformer für jedes neue Token die Attention über alle vorherigen Token berechnen. Der KV-Cache speichert die Key- und Value-Projektionen vorheriger Token, damit sie nicht neu berechnet werden müssen. Das reduziert die Berechnung pro Schritt von O(n²) auf O(n) — eine massive Beschleunigung für lange Seqünzen.',
    keyCache: 'Key-Cache',
    keyCacheDesc: 'Speichert die Key-Projektionen für jedes Token in jeder Schicht. Diese werden verwendet, um Attention-Scores zwischen dem neuen Token und allen vorherigen Token zu berechnen.',
    valueCache: 'Value-Cache',
    valueCacheDesc: 'Speichert die Value-Projektionen für jedes Token in jeder Schicht. Sobald die Attention-Gewichte berechnet sind, werden diese gecachten Values zur Ausgabeerzeugung verwendet.',
    whyMatters: 'Warum es wichtig ist',
    whyMattersDesc: 'Ohne KV-Caching würde die Generierung jedes neuen Tokens erfordern, die gesamte Seqünz durch jede Attention-Schicht neu zu verarbeiten. Bei einer 4096-Token-Seqünz bedeutet das 4096× redundante Berechnung pro Token.',
    benefit1Title: 'Geschwindigkeit',
    benefit1Desc: 'Vermeidet die Neuberechnung der Attention für alle vorherigen Token bei jedem Schritt. Generierung wird von quadratisch zu linear.',
    benefit2Title: 'Inkrementell',
    benefit2Desc: 'Jedes neue Token muss nur sein eigenes Q, K, V berechnen und auf die gecachten K, V vorheriger Positionen zugreifen.',
    benefit3Title: 'Kompromiss',
    benefit3Desc: 'Tauscht GPU-Speicher gegen Rechenzeit. Der Cache wächst linear mit Seqünzlänge und Modelltiefe.',
    interactiveTitle: 'Interaktiver KV-Cache Explorer',
    interactiveSubtitle: 'Beobachte den Cache Token für Token wachsen',
    interactiveDesc: 'Gehe schrittweise durch die autoregressive Generierung, um zu sehen, wie der KV-Cache sich aufbaut. Vergleiche die Berechnungskosten mit und ohne Caching — die Einsparungen werden bei längeren Seqünzen dramatisch.',
    memoryTitle: 'Speicher-Auswirkungen',
    memoryDesc: 'Der KV-Cache ist der Hauptengpass beim Speicher während der Inferenz. Bei großen Modellen mit langem Kontext kann er Dutzende Gigabyte GPU-Speicher verbrauchen.',
    memoryFormula: 'Faktor 2 für K und V, dtype_size ist 2 Bytes für FP16. Für Llama 3 70B (80 Schichten, 8 KV-Heads, 128 d_head) bei 8K Kontext: ~2,5 GB pro Anfrage.',
    optimizationsTitle: 'Optimierungstechniken',
    mqaGqaTitle: 'Multi-Query & Grouped-Query Attention (MQA/GQA)',
    mqaGqaDesc: 'Anstatt separate K/V-Heads pro Attention-Head zu verwenden, teilt MQA ein einzelnes K/V-Head über alle Query-Heads, während GQA einige gemeinsame Gruppen nutzt. Dies reduziert die KV-Cache-Größe um das 4-32-fache bei minimalem Qualitätsverlust. Llama 3 und Mistral verwenden GQA. Siehe den Artikel über Aufmerksamkeitsmechanismen für weitere Details.',
    slidingWindowTitle: 'Sliding-Window-Attention',
    slidingWindowDesc: 'Anstatt alle Token zu cachen, werden nur die letzten W Token im Cache gehalten. Wird von Mistral-Modellen verwendet. Reduziert den Speicher von O(seq_len) auf O(W), begrenzt aber die Fähigkeit des Modells, auf sehr frühe Token zuzugreifen.',
    pagedAttentionTitle: 'Paged Attention (vLLM)',
    pagedAttentionDesc: 'Inspiriert von virtüllem Speicher in Betriebssystemen. Anstatt zusammenhängenden Speicher für den KV-Cache jeder Seqünz zuzuweisen, verwaltet vLLM den Cache in „Pages" fester Größe, die dynamisch zugewiesen und freigegeben werden können. Das eliminiert Speicherfragmentierung und ermöglicht effizientes Batching von Anfragen mit unterschiedlichen Seqünzlängen.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Der KV-Cache speichert Key- und Value-Projektionen vorheriger Token und vermeidet redundante Berechnungen während der Generierung',
    takeaway2: 'Ohne KV-Cache erfordert jedes neue Token O(n) Attention-Berechnung über die gesamte Seqünz; mit Cache wird nur das K/V des neuen Tokens berechnet',
    takeaway3: 'Der KV-Cache-Speicher wächst linear mit Seqünzlänge × Schichten × KV-Heads — das ist der Hauptengpass beim Inferenz-Speicher',
    takeaway4: 'Techniken wie GQA, Sliding Window und Paged Attention adressieren die Speicherkosten bei gleichbleibender Generierungsgeschwindigkeit',
  },

  // Batching page
  batching: {
    title: 'Batching & Durchsatz',
    description: 'Wie die gleichzeitige Verarbeitung mehrerer Anfragen die GPU-Auslastung und Inferenz-Ökonomie transformiert.',

    // Section 1: What is Batching
    whatIs: 'Was ist Batching bei LLM-Inferenz?',
    whatIsDesc: 'Wenn ein einzelner Nutzer einen Prompt an ein LLM sendet, verarbeitet die GPU nur eine Anfrage — und nutzt nur einen Bruchteil ihrer Rechenkapazität. Batching kombiniert mehrere Anfragen und verarbeitet sie gleichzeitig auf derselben GPU, was den Durchsatz dramatisch erhöht.',
    staticTitle: 'Statisches Batching',
    staticDesc: 'Alle Anfragen im Batch starten und enden zusammen. Die GPU wartet auf die langsamste Anfrage, bevor neue beitreten können. Einfach, aber verschwenderisch — kürzere Anfragen sitzen untätig.',
    dynamicTitle: 'Dynamisches / Continuous Batching',
    dynamicDesc: 'Anfragen können jederzeit dem Batch beitreten und ihn verlassen. Wenn eine Anfrage fertig ist, nimmt sofort eine neue ihren Platz ein. Dieser „Orca-Stil"-Ansatz maximiert die GPU-Auslastung.',
    wasteTitle: 'Warum Einzelanfragen-Inferenz Ressourcen verschwendet',
    wasteDesc: 'Eine moderne GPU wie die A100 hat 312 TFLOPS Rechenleistung und 2 TB/s Speicherbandbreite. Ein einzelner Decode-Schritt für eine Anfrage kratzt kaum an der Oberfläche — die GPU verbringt die meiste Zeit mit Warten auf Speicher, nicht mit Berechnen. Batching füllt diese Lücke, indem die Speichertransferkosten über viele Anfragen geteilt werden.',

    // Section 2: Throughput visualization
    throughputTitle: 'Durchsatz vs. Batch-Größe',
    throughputSubtitle: 'Ziehe den Slider, um zu sehen, wie Batching die Leistung beeinflusst',
    throughputExplain: 'Mit steigender Batch-Größe steigt der Gesamtdurchsatz zunächst steil an — flacht dann ab — und KOLLABIERT schließlich. Der Anstieg kommt vom Amortisieren der Gewichtsladekosten. Der Kollaps passiert, wenn der Speicher erschöpft ist: KV-Caches überfluten den VRAM, das System beginnt zu swappen, und alles bricht zusammen.',
    totalThroughputLabel: 'Gesamtsystem-Durchsatz',
    totalThroughputHint: 'Gesamte Tok/s über ALLE Nutzer kombiniert',
    perUserSpeedLabel: 'Pro-Nutzer-Geschwindigkeit',
    perUserSpeedHint: 'Wie schnell jeder einzelne Nutzer Token erhält',
    batchSizeLabel: 'Batch-Größe',
    tokPerSec: 'Tok/s',
    msPerTok: 'ms/Tok',
    plateauExplain: 'Das Plateau entsteht durch DRAM-Bandbreitensättigung — nicht Rechensättigung. Selbst bei Batch 256 sind die GPU-Recheneinheiten noch unterausgelastet. Der Engpass ist, wie schnell Gewichte aus dem Speicher geladen werden können.',
    improvementNote: 'Mehr Anfragen → mehr Gesamtdurchsatz, aber abnehmende Erträge',
    collapseWarning: 'Systemkollaps!',
    collapseExplain: 'Bei extremen Batch-Größen übersteigt der KV-Cache-Speicherbedarf die VRAM-Kapazität. Das System beginnt zu thrashern — Speicher wird geswappt, Anfragen gehen verloren, oder es stürzt mit OOM-Fehlern ab. Der Durchsatz stagniert nicht nur — er kollabiert katastrophal.',

    // Section 3: Prefill vs Decode
    prefillDecodeTitle: 'Prefill- vs. Decode-Phase',
    prefillDecodeSubtitle: 'Zwei grundlegend verschiedene Workloads in jeder LLM-Anfrage',
    prefillDecodeExplain: 'Jede LLM-Anfrage hat zwei Phasen. Prefill verarbeitet alle Input-Token parallel — blitzschnell, weil es die Recheneinheiten der GPU sättigt. Decode generiert Token einzeln — quälend langsam, weil es durch Speicherbandbreite begrenzt ist, nicht durch Rechenleistung. Prefill kann ~1000 Token verarbeiten, während Decode ~20 generiert.',
    prefillTitle: 'Prefill-Phase',
    prefillAnalogy: 'Wie ein Fließband, das alle Teile auf einmal verarbeitet',
    decodeTitle: 'Decode-Phase',
    decodeAnalogy: 'Wie ein einzelner Handwerker, der Stück für Stück fertigt',
    prefillLabel: 'Prefill',
    decodeLabel: 'Decode',
    computeBound: 'Rechengebunden (FLOPS-limitiert)',
    memoryBound: 'Speichergebunden (Bandbreite-limitiert)',
    highUtil: 'Hohe GPU-Auslastung (~80-95%)',
    lowUtil: 'Niedrige GPU-Auslastung (<20%)',
    parallelProcess: 'Alle Token parallel',
    sequentialProcess: 'Ein Token nach dem anderen',
    ttft: 'TTFT (Time to First Token)',
    tbot: 'TBOT (Time Between Output Tokens)',
    prefillSpeed: '~50.000 Tok/s auf A100',
    decodeSpeed: '~50 Tok/s pro Anfrage auf A100',
    keyInsight: 'Prefill ist 1000× schneller pro Token als Decode. Beim Prefill sind die GPU-Tensor-Cores der Engpass (rechengebunden). Beim Decode ist die DRAM-Bandbreite der Engpass (speichergebunden) — die GPU sitzt >80% der Zeit untätig und wartet auf Gewichtsdaten. Genau deshalb hilft Batching: Es füllt diese Leerlaufzyklen mit nützlicher Arbeit anderer Anfragen.',
    phaseAnimTitle: 'Beobachte die Phasen',
    promptIn: 'Prompt rein',
    tokensOut: 'Token raus',
    prefillBurst: 'Prefill-Burst — alle Eingaben auf einmal verarbeitet',
    decodeTrickle: 'Decode-Tröpfeln — Token einzeln generiert',
    raceTitle: 'Geschwindigkeitsrennen: Prefill vs Decode',
    raceStart: 'Rennen starten',
    raceDesc: 'Sieh zu, wie Prefill 1000 Token verarbeitet, während Decode kaum beginnt. Prefill verarbeitet ~50 Token pro Tick; Decode verarbeitet 1 Token pro Tick.',
    raceResult: 'Prefill hat 1000 Token verarbeitet, während Decode nur ~20 geschafft hat! Das ist der 50× Geschwindigkeitsunterschied in Aktion.',
    tokensProcessed: 'Token',
    rooflineTitle: 'Warum der Geschwindigkeitsunterschied?',
    limitedBy: 'Begrenzt durch',
    computeFlops: 'Rechenleistung (FLOPS)',
    memBandwidth: 'Speicherbandbreite (GB/s)',
    tensorCoreBottleneck: 'Tensor-Cores sind der Engpass',
    dramBottleneck: 'Laden der Gewichte aus DRAM ist der Engpass',
    arithmeticIntensity: 'Arithmetische Intensität = FLOPS pro Byte aus dem Speicher. Prefill hat hohe arithmetische Intensität (viele Operationen pro Gewichtsladevorgang, da viele Token verarbeitet werden). Decode hat extrem niedrige arithmetische Intensität (lädt dieselben Gewichte, berechnet aber nur für 1 Token). Die GPU ist ein Supercomputer, der von einem strohhalmdünnen Speicherrohr als Geisel gehalten wird.',

    // Section 4: Throughput-Latency Tradeoff
    tradeoffTitle: 'Der Durchsatz-Latenz-Kompromiss',
    tradeoffDesc: 'Batching ist kein Gratismittagessen. Mehr Batching bedeutet mehr Token pro Sekunde insgesamt, aber jeder einzelne Nutzer wartet länger auf seine Antwort. Betreiber müssen Servereffizienz gegen Nutzererlebnis abwägen.',
    lowBatch: 'Kleine Batch-Größe',
    lowBatchDesc: 'Niedriger Gesamtdurchsatz, aber jeder Nutzer bekommt schnelle Antworten (45 Tok/s pro Nutzer)',
    highBatch: 'Große Batch-Größe',
    highBatchDesc: 'Hoher Gesamtdurchsatz, aber jeder Nutzer wartet viel länger (9 Tok/s pro Nutzer)',
    totalWord: 'gesamt',
    perUserWord: 'pro Nutzer',
    tradeoffInsight: 'Der Sweet Spot hängt vom Anwendungsfall ab: Echtzeit-Chat braucht niedrige Latenz (kleine Batches), während Batch-Verarbeitungsjobs den Durchsatz maximieren können (große Batches). Die meisten Produktionssysteme zielen auf Batch-Größen von 32-128.',

    // Section 5: Continuous Batching
    continuousTitle: 'Continuous Batching',
    continuousSubtitle: 'Wie moderne Serving-Engines GPU-Leerlauf eliminieren',
    continuousExplain: 'Statisches Batching verschwendet GPU-Zeit, weil alle Anfragen auf die längste warten müssen. Continuous Batching (auch „Orca-Stil") lässt Anfragen unabhängig beitreten und verlassen. Wenn eine Anfrage fertig ist, wird ihr Slot sofort mit einer neuen gefüllt. Probiere die interaktive Timeline unten — wechsle zwischen statisch und kontinuierlich, um den Unterschied in der GPU-Auslastung zu sehen.',
    staticBatchTitle: 'Statisches Batching',
    continuousBatchTitle: 'Continuous Batching',
    slotLabel: 'Slot',
    activeLabel: 'Aktiv',
    idleLabel: 'Leerlauf',
    newReqLabel: 'Neue Anfrage',
    finishedLabel: 'Fertig',
    addRequest: 'Anfrage hinzufügen',
    timeStep: 'Zeitschritt',
    gpuUtilization: 'GPU-Auslastung',
    staticInsight: 'Beachte die roten Leerlaufblöcke — Slots, die früh fertig werden, sitzen leer, bis die längste Anfrage abgeschlossen ist. Das verschwendet in der Praxis 30-50% der GPU-Kapazität.',
    continuousInsight: 'Mit Continuous Batching werden fertige Slots sofort nachgefüllt. Die GPU-Auslastung bleibt nahe 100%, solange es wartende Anfragen gibt. So funktionieren vLLM, TGI und andere moderne Engines.',

    // Section 6: Per-User vs System Throughput
    perUserTitle: 'Pro-Nutzer vs. Systemdurchsatz',
    perUserSubtitle: 'Was mit jedem Nutzer passiert, wenn das System skaliert',
    perUserExplain: 'Je mehr gleichzeitige Nutzer hinzukommen, desto mehr Token pro Sekunde liefert das System insgesamt — aber jeder einzelne Nutzer bekommt einen kleineren Anteil der Bandbreite. Ab einem kritischen Punkt kollabiert das System komplett: Speichererschöpfung, Swapping und kaskadierende Fehler zerstören den Durchsatz für alle.',
    concurrentUsers: 'Gleichzeitige Nutzer',
    bandwidthPerUser: 'Anteil jedes Nutzers an der GPU-Bandbreite:',
    eachUserGets: 'Jeder Nutzer erhält',
    moreUsers: 'weitere',
    systemOverload: 'System überlastet!',
    overloadExplain: 'Ab ~80 gleichzeitigen Nutzern geht dem System der VRAM für KV-Caches aus. Speicherdruck verursacht Thrashing, und der Gesamtdurchsatz kollabiert — niemand bekommt mehr gute Performance.',

    // Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Batching amortisiert die Kosten des Ladens von Modellgewichten über mehrere Anfragen und erhöht den Gesamtdurchsatz dramatisch',
    takeaway2: 'Aber es gibt ein Limit: Zu viele gebatchte Anfragen verursachen Speichererschöpfung und Durchsatzkollaps (OOM)',
    takeaway3: 'Prefill ist rechengebunden und 1000× schneller pro Token als Decode, das speicherbandbreitenbegrenzt ist',
    takeaway4: 'Continuous Batching eliminiert GPU-Leerlauf, indem Slots dynamisch gefüllt werden, sobald Anfragen abgeschlossen sind',
    takeaway5: 'Die Pro-Nutzer-Geschwindigkeit sinkt immer mit mehr gleichzeitigen Nutzern — das System tauscht individülle Geschwindigkeit gegen Gesamtkapazität',
  },

  // Local Inference page
  localInference: {
    title: 'Lokale Modellinferenz',
    description: 'Führe grosse Sprachmodelle auf deiner eigenen Hardware aus -- keine Cloud, keine API-Keys, keine Limits.',

    whyTitle: 'Warum lokal ausführen?',
    whyDesc: 'Modelle auf dem eigenen Rechner zu betreiben bietet Möglichkeiten, die Cloud-APIs nicht bieten können.',
    whyPrivacy: 'Volle Privatsphäre',
    whyPrivacyDesc: 'Deine Daten verlassen nie deinen Rechner. Kein Logging, kein Zugriff Dritter, keine Compliance-Sorgen.',
    whyCost: 'Keine API-Kosten',
    whyCostDesc: 'Nach der einmaligen Hardware-Investition ist jeder Token kostenlos. Führe so viele Anfragen aus wie du willst.',
    whyOffline: 'Offline-Zugang',
    whyOfflineDesc: 'Funktioniert ohne Internet. Nutze KI im Flugzeug, in sicheren Umgebungen oder überall ohne Verbindung.',
    whyCustom: 'Volle Anpassung',
    whyCustomDesc: 'Wähle jedes Modell, jede Quantisierung, alle Parameter. Optimiere für deinen spezifischen Anwendungsfall.',
    whyLearning: 'Tiefes Lernen',
    whyLearningDesc: 'Nichts lehrt dich besser, wie LLMs funktionieren, als direkt mit ihnen zu experimentieren.',
    whyControl: 'Totale Kontrolle',
    whyControlDesc: 'Keine Rate-Limits, keine Filter die du nicht gewählt hast, keine überraschenden API-Änderungen.',

    hardwareTitle: 'Hardware-Anforderungen',
    hardwareDesc: 'Wähle eine Modellgröße und Quantisierungsstufe um zu sehen, wie viel VRAM du brauchst und welche GPUs das schaffen.',
    hwModelSize: 'Modellgröße',
    hwQuantLevel: 'Quantisierung',
    hwVramNeeded: 'Benötigter VRAM',
    hwSpeed: 'Geschätzte Geschwindigkeit',
    hwSpeedNote: 'Ungefähr, variiert je nach GPU und Konfiguration',
    hwGpuCompat: 'GPU-Kompatibilität',

    moeTitle: 'Der MoE-Vorteil für lokale Inferenz',
    moeDesc: 'Mixture-of-Experts-Modelle (MoE) leiten jeden Token nur durch eine Teilmenge von "Experten"-Schichten. Der Hauptvorteil ist Geschwindigkeit: weniger aktive Parameter bedeuten schnellere Generierung. Aber alle Parameter bleiben im VRAM — MoE spart keinen Speicher.',
    moeBenefit1: 'Schnellere Generierung',
    moeBenefit1Desc: 'Pro Token rechnet nur eine Teilmenge der Experten. Mixtral 8x7B aktiviert 12,9B seiner 46,7B Parameter — und generiert Tokens ~3x schneller als ein vergleichbar intelligentes dichtes 70B-Modell.',
    moeBenefit2: 'Intelligenz großer Modelle',
    moeBenefit2Desc: 'Alle 46,7B Parameter speichern Wissen über alle Experten. Die Reasoning-Qualität liegt weit über dem, was ein dichtes 13B-Modell erreichen könnte.',
    moeBenefit3: 'VRAM basiert auf Gesamtparametern',
    moeBenefit3Desc: 'Alle Experten-Gewichte müssen in den Speicher geladen werden. Mixtral 8x7B bei Q4 braucht ~26 GB VRAM — ähnlich einem dichten 30B-Modell, nicht 13B. MoE spart Rechenzeit, nicht Speicher.',
    moeIntelligence: 'Intelligenz',
    moeSpeed: 'Geschwindigkeit (Tokens/s, RTX 4090)',
    moeVram: 'VRAM-Verbrauch (Q4)',
    moeTotalParams: 'Gesamt',
    moeActiveParams: 'Aktiv',
    moeSweetSpot: '✨ Geschwindigkeitsvorteil',
    moeInsightLabel: 'Die Erkenntnis:',
    moeInsight: 'Mixtral 8x7B aktiviert nur 12,9B seiner 46,7B Parameter pro Token — das liefert 70B-Klasse-Intelligenz bei 3,5-facher Geschwindigkeit. Aber es braucht trotzdem ~26 GB VRAM, weil alle Experten-Gewichte geladen werden müssen. MoE tauscht VRAM gegen Geschwindigkeit, nicht umgekehrt.',
    moeLinkDesc: 'MoE ist ein fundamentaler Architekturwandel, kein bloßer Optimierungstrick. Zu verstehen, wie Expert-Routing funktioniert, hilft dir das richtige Modell für deine Hardware zu wählen.',
    moeLinkText: 'Tiefer Einblick in Mixture of Experts',

    toolsTitle: 'Beliebte Tools',
    toolsDesc: 'Das lokale Inferenz-Ökosystem ist schnell gereift. Hier sind die wichtigsten Tools, von anfängerfreundlich bis produktionsreif.',

    quantTitle: 'Der Quantisierungs-Kompromiss',
    quantDesc: 'Quantisierung ist die Schlüsseltechnologie, die lokale Inferenz praktikabel macht. Durch Reduzierung der Präzision der Modellgewichte passen viel größere Modelle in begrenzten VRAM.',
    quantSummary: 'Ein 70B-Parameter-Modell bei FP16 braucht 140 GB Speicher -- weit jenseits jeder Consumer-GPU. Bei Q4-Quantisierung passt es in 40 GB, was es auf High-End-Consumer-Hardware mit nur geringem Qualitätsverlust ausführbar macht.',
    quantLink: 'Tiefer Einblick in Quantisierung',

    startTitle: 'Erste Schritte',
    startDesc: 'Folge diesen fünf Schritten um von null zum ersten lokalen Modell zu kommen.',
    step1Title: 'Tool wählen',
    step1Desc: 'Starte mit Ollama oder LM Studio -- sie erledigen alles für dich. Wechsle zu llama.cpp oder vLLM wenn du mehr Kontrolle brauchst.',
    step2Title: 'VRAM prüfen',
    step2Desc: 'Führe nvidia-smi (NVIDIA) oder den Aktivitätsmonitor (Mac) aus. Das bestimmt welche Modelle du ausführen kannst.',
    step3Title: 'Modellgröße wählen',
    step3Desc: 'Starte mit 7B-Modellen. Sie sind schnell, leistungsfähig und passen auf die meisten GPUs. Wechsle zu 13B oder 70B wenn du mehr brauchst.',
    step4Title: 'Quantisierungsstufe wählen',
    step4Desc: 'Q4 ist der Sweet Spot für die meisten: gute Qualität bei vernünftigem VRAM-Verbrauch. Nimm Q8 bei genug Speicher, Q2 wenn es knapp ist.',
    step5Title: 'Ausführen',
    step5Desc: 'Lade das Modell herunter und starte den Chat. Mit Ollama: ollama pull llama3.2, dann ollama run llama3.2. Das wars.',

    quickstartTitle: 'Schnellstart-Demo',
    quickstartDesc: 'So sieht es aus, Ollama zu installieren und dein erstes Modell auszuführen -- drei Befehle und du chattest.',
    qsReplay: 'Wiederholen',

    tipsTitle: 'Tipps und Tricks',
    tip1: 'Kontextlänge beeinflusst VRAM-Verbrauch direkt. Ein 7B-Modell mit 128K Kontext braucht deutlich mehr Speicher als mit 4K. Starte klein und erhöhe nach Bedarf.',
    tip2: 'GPU-Offloading teilt ein Modell zwischen GPU und CPU auf. GPU-Geschwindigkeit für passende Layer, CPU für den Rest. Langsamer als volle GPU, aber größere Modelle möglich.',
    tip3: 'Reine CPU-Inferenz funktioniert, ist aber 5-10x langsamer als GPU. Gut zum Testen, weniger für interaktive Nutzung. Apple Silicon ist die Ausnahme -- Unified Memory macht CPU-Inferenz schnell.',
    tip4: 'Fuer 8 GB VRAM: 7B Q4. Fuer 12 GB: 7B Q8 oder 13B Q4. Fuer 24 GB: 13B Q8 oder 70B Q4. Fuer 32 GB+: 70B Q4-Q8 komfortabel.',
    tip5: 'Llama 3.2, Mistral, Phi-3 und Qwen 2.5 sind hervorragend für lokale Inferenz. Jedes glänzt bei anderen Aufgaben -- experimentiere um das Beste für dich zu finden.',
    tip6: 'Betreibe Modelle als API-Server (Ollama und LM Studio unterstützen das) um lokale Modelle in eigene Anwendungen, Skripte und Workflows zu integrieren.',
  },

  // LoRA page
  lora: {
    title: 'Fine-Tuning & LoRA',
    description: 'Wie LoRA es ermöglicht, riesige Sprachmodelle für spezifische Aufgaben anzupassen, indem nur winzige Low-Rank-Matrizen trainiert werden — spart Speicher, Zeit und Geld.',

    whatIs: 'Was ist Fine-Tuning?',
    whatIsDesc: 'Du hast ein vortrainiertes Sprachmodell mit Milliarden von Parametern, das viel über die Welt weiß. Aber du möchtest, dass es bei einer bestimmten Aufgabe herausragend ist — juristische Schriftsätze schreiben, in Rust programmieren oder im Stil deiner Marke sprechen. Fine-Tuning passt das Modell an, indem das Training mit deinen spezialisierten Daten fortgesetzt wird.',
    problem: '"Das Problem: Volles Fine-Tuning bedeutet, ALLE Parameter zu aktualisieren."',
    problemDesc: 'Bei einem 70B-Parameter-Modell bedeutet das, 70 Milliarden Gewichte zu speichern und zu aktualisieren. Du brauchst eine vollständige Kopie des Modells im Speicher, plus Optimizer-Zustände (2-3x die Modellgröße). Das sind Hunderte Gigabyte VRAM — teuer, langsam und für die meisten Teams unpraktisch.',

    insightTitle: 'Die LoRA-Erkenntnis',
    insightDesc: 'LoRA (Low-Rank Adaptation) basiert auf einer Schlüsselbeobachtung: Wenn man ein Modell feinabstimmt, sind die Gewichtsaktualisierungen tendenziell niedrigrangig. Statt eine riesige d×d-Gewichtsmatrix W direkt zu aktualisieren, zerlegt man das Update als ΔW = A × B, wobei A d×r und B r×d ist, mit r viel kleiner als d.',

    matrixTitle: 'LoRA-Matrixzerlegung',
    matrixDesc: 'Passe den Rang r an, um zu sehen, wie LoRA ein großes Gewichtsupdate in zwei kleine Matrizen zerlegt.',
    rankLabel: 'Rang',
    matrix: 'Matrix',
    fullParams: 'Volle Parameter',
    loraParams: 'LoRA-Parameter',
    savings: 'Parameter-Einsparung',
    onlyPct: 'Nur {pct}% des Originals',
    params: 'Param.',

    whyEasyTitle: 'Warum LoRA einfach zu trainieren ist',
    whyEasyDesc: 'Indem nur die kleinen A- und B-Matrizen trainiert werden, während das Basismodell eingefroren bleibt, reduziert LoRA den Bedarf an Speicher, Rechenleistung und Speicherplatz dramatisch.',
    memoryTitle: 'VRAM- & Speichervergleich',
    memoryDesc: 'Wähle eine Modellgröße, um den GPU-VRAM-Bedarf für volles Fine-Tuning vs. LoRA zu vergleichen.',
    fullFineTune: 'Volles Fine-Tuning',
    loraFineTune: 'LoRA Fine-Tuning',
    vramNeeded: 'Benötigter GPU-VRAM',
    adapterStorage: 'Speicher: Volles Modell vs. LoRA-Adapter',
    fullModel: 'Vollständige Modellkopie',
    loraAdapter: 'LoRA-Adapter',
    storageSavings: '~{x}× kleiner — du kannst Hunderte Adapter für verschiedene Aufgaben speichern!',
    rankAffectsMemory: 'Höherer Rang → mehr trainierbare Parameter → mehr VRAM und größere Adapter-Dateien',
    memoryBenefit: 'Weniger Speicher',
    memoryBenefitDesc: 'Nur die kleinen A- und B-Matrizen benötigen Gradienten und Optimizer-Zustände.',
    speedBenefit: 'Schnelleres Training',
    speedBenefitDesc: 'Viel weniger Parameter zum Aktualisieren bedeutet schnellere Iterationen.',
    swapBenefit: 'Hot-Swappable',
    swapBenefitDesc: 'Ein Basismodell behalten, winzige Adapter zur Inferenzzeit für verschiedene Aufgaben tauschen.',
    frozenTitle: 'Kein katastrophales Vergessen',
    frozenDesc: 'Da die Basismodell-Gewichte vollständig eingefroren bleiben, kann LoRA das bestehende Wissen des Modells nicht zerstören. Der Adapter fügt nur zum vorhandenen Wissen hinzu — er nimmt nie etwas weg. Das ist ein enormer Vorteil gegenüber vollem Fine-Tuning, bei dem aggressives Training dazu führen kann, dass das Modell seine allgemeinen Fähigkeiten vergisst.',

    useCasesTitle: 'Anwendungsfälle',
    useCasesDesc: 'LoRA-Adapter werden überall eingesetzt, um Foundation-Modelle zu spezialisieren:',
    useCase1Title: 'Aufgabenspezifische Anpassung',
    useCase1Desc: 'Adapter für Programmierung, medizinische Diagnose, juristische Analyse oder Kundensupport trainieren. Jede Domäne erhält ihren eigenen kleinen Adapter.',
    useCase2Title: 'Stil- & Tonanpassung',
    useCase2Desc: 'Eine bestimmte Markenstimme treffen, zwischen formell und lässig wechseln oder den Schreibstil anpassen, ohne das gesamte Modell neu zu trainieren.',
    useCase3Title: 'Sprachanpassung',
    useCase3Desc: 'Leistung in unterrepräsentierten Sprachen verbessern, indem ein LoRA mit sprachspezifischen Daten trainiert wird.',
    useCase4Title: 'Instruktionsbefolgung',
    useCase4Desc: 'Ein Basismodell besser Anweisungen befolgen lassen, indem ein Adapter mit Instruktions-Antwort-Paaren trainiert wird.',

    antiUseCasesTitle: 'Wann man LoRA NICHT verwenden sollte',
    antiUseCasesDesc: 'LoRA ist mächtig, aber nicht für jeden Einsatz das richtige Werkzeug:',
    antiUseCase1Title: 'Prompt Engineering reicht aus',
    antiUseCase1Desc: 'Wenn du das gewünschte Verhalten mit einem guten System-Prompt oder Few-Shot-Beispielen erreichst, brauchst du keinen Adapter. Es ist günstiger, schneller und einfacher zu iterieren.',
    antiUseCase2Title: 'Du brauchst breites neues Wissen',
    antiUseCase2Desc: 'LoRA ist super für Stil und Verhalten, aber schlecht darin, große Mengen an Faktenwissen zu injizieren. Nutze stattdessen RAG (Retrieval) für wissensintensive Aufgaben.',
    antiUseCase3Title: 'Dein Datensatz ist winzig oder verrauscht',
    antiUseCase3Desc: 'Mit weniger als ~100 qualitativ hochwertigen Beispielen wird LoRA überfitten oder kaum etwas lernen. Saubere, kuratierte Daten sind essenziell.',
    antiUseCase4Title: 'Du brauchst Echtzeit-Anpassung',
    antiUseCase4Desc: 'LoRA erfordert einen Trainingsschritt. Wenn sich dein Modell on-the-fly an neue Informationen anpassen soll, nutze In-Context Learning oder RAG.',

    whyNotTitle: 'Warum LoRA nicht für Pre-Training verwendet wird',
    whyNotDesc: 'LoRA ist fantastisch für Anpassung, aber grundsätzlich limitiert für das Erlernen von komplett neuem Wissen. Hier ist warum:',
    whyNot1Title: 'Low-Rank-Beschränkung',
    whyNot1Desc: 'LoRA beschränkt Updates auf einen niedrigrangigen Unterraum. Fine-Tuning-Änderungen sind empirisch niedrigrangig (kleine Anpassungen), aber Pre-Training muss fundamentale Repräsentationen lernen, die vollrangig sind.',
    whyNot2Title: 'Begrenzte Ausdrucksfähigkeit',
    whyNot2Desc: 'Ein Rang-8-Update einer 4096×4096-Matrix kann nur einen winzigen Bruchteil möglicher Änderungen erfassen. Pre-Training braucht die volle Ausdrucksfähigkeit unbeschränkter Gewichtsupdates.',
    whyNot3Title: 'Abnehmender Grenznutzen',
    whyNot3Desc: 'Mit zunehmendem Rang zur Erfassung komplexerer Änderungen nähert man sich den Kosten des vollen Fine-Tunings — ab diesem Punkt bietet LoRA keinen Vorteil mehr.',
    rankQualityTitle: 'Rang vs. Approximationsqualität',
    rankQualityDesc: 'Sieh, wie steigender Rang aufgabenspezifische Anpassung verbessert, aber bei allgemeinem Wissen scheitert.',
    taskSpecific: 'Aufgabenspezifische Qualität',
    taskSpecificHint: 'Anpassung an eine spezifische Domäne — sättigt schnell bei moderatem Rang',
    generalKnowledge: 'Allgemeines Wissenslernen',
    generalKnowledgeHint: 'Grundlegend neues Wissen lernen — braucht vollrangige Updates',
    reconstructionQuality: 'Matrix-Rekonstruktionsqualität',
    reconstructionHint: 'Wie gut die niedrigrangige Approximation beliebige Gewichtsupdates erfasst',
    overkillTitle: 'Abnehmender Grenznutzen!',
    overkillDesc: 'Bei diesem Rang verwendest du so viele Parameter, dass volles Fine-Tuning effizienter wäre. Die Low-Rank-Beschränkung erzeugt Overhead ohne sinnvolle Einsparungen.',
    sweetSpotTitle: 'Sweet Spot',
    sweetSpotDesc: 'Ränge 8-64 bieten typischerweise das beste Verhältnis: exzellente Aufgabenanpassung mit minimalen Parametern. Die meisten Praktiker verwenden r=8 oder r=16.',
    lowRankDesc: 'Sehr niedrige Ränge sind extrem parametereffizient, könnten aber wichtige Anpassungsmuster verpassen. Gut für sehr einfache Aufgaben.',

    variantsTitle: 'LoRA-Varianten & Evolution',
    variantsDesc: 'Das ursprüngliche LoRA-Paper hat eine Familie von Verbesserungen hervorgebracht. Klicke auf jede Karte für mehr Details.',
    qloraTitle: 'QLoRA',
    qloraDesc: 'Quantisiertes Basismodell + LoRA-Adapter = Fine-Tuning auf Consumer-GPUs.',
    qloraDetail: 'QLoRA quantisiert das eingefrorene Basismodell auf 4-Bit-Präzision (NF4-Format), was seinen Speicherbedarf um 4× reduziert. Die LoRA-Adapter werden weiterhin in 16-Bit trainiert. So kannst du ein 65B-Modell auf einer einzelnen 48GB-GPU feinabstimmen — etwas, das normalerweise mehrere A100s erfordern würde. Es führte Paged Optimizers und Double Quantization für weitere Einsparungen ein.',
    doraTitle: 'DoRA (Weight-Decomposed LoRA)',
    doraDesc: 'Trennt Gewichtsmagnitude von Richtung für bessere Trainingsdynamik.',
    doraDetail: 'DoRA zerlegt das Gewichtsupdate in Magnituden- und Richtungskomponenten und wendet LoRA nur auf die Richtung an. Das ahmt nach, wie volles Fine-Tuning tatsächlich funktioniert — es ändert die Richtung mehr als die Magnitude. DoRA übertrifft Standard-LoRA konsistent über Aufgaben hinweg bei gleicher Anzahl trainierbarer Parameter.',
    loraPlusTitle: 'LoRA+',
    loraPlusDesc: 'Unterschiedliche Lernraten für A- und B-Matrizen = schnellere Konvergenz.',
    loraPlusDetail: 'LoRA+ beobachtet, dass die A- und B-Matrizen unterschiedliche optimale Lernraten haben. Indem die Lernrate von B etwa 2-4× höher als A gesetzt wird, verbessert sich die Konvergenzgeschwindigkeit signifikant. Das ist eine einfache Änderung, die nichts extra kostet und konsistent die Ergebnisse verbessert.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'LoRA zerlegt Gewichtsupdates in zwei kleine Matrizen (A×B) und reduziert trainierbare Parameter um 99%+, bei gleichbleibender Qualität',
    takeaway2: 'Das Basismodell bleibt eingefroren — kein katastrophales Vergessen, und winzige Adapter können zur Inferenzzeit für verschiedene Aufgaben getauscht werden',
    takeaway3: 'LoRA funktioniert, weil Fine-Tuning-Änderungen empirisch niedrigrangig sind: Man braucht keine vollrangigen Updates für Aufgabenanpassung',
    takeaway4: 'QLoRA erweitert dies durch Quantisierung des Basismodells und ermöglicht Fine-Tuning von 70B+-Modellen auf Consumer-Hardware',
    takeaway5: 'LoRA ist nicht geeignet für Pre-Training — das Erlernen fundamentalen Wissens erfordert vollrangige, unbeschränkte Gewichtsupdates',
  },

  // Getting Started page
  gettingStarted: {
    title: 'Erste Schritte',
    description: 'Dein erster LLM-API-Aufruf in 10 Minuten — komplett kostenlos. Keine Kreditkarte nötig.',
    whyTitle: 'Warum hier starten?',
    whyDesc: 'Large Language Models sind mächtig, aber der Einstieg kann überwältigend wirken. Dieser Guide kommt auf den Punkt: Du wählst einen kostenlosen API-Anbieter, gibst deinen Key ein und sprichst mit einem KI-Modell — alles in deinem Browser.',
    whyPromise: '⏱️ Du hast in unter 10 Minuten einen funktionierenden LLM-Aufruf. Keine Kreditkarte nötig.',
    getKeyTitle: 'Kostenlosen API-Key holen',
    getKeyDesc: 'Alle drei Anbieter bieten kostenlose Tarife — keine Kreditkarte nötig. Wähle den, der dich anspricht (du kannst die anderen später ausprobieren):',
    openrouterDesc: 'Größte kostenlose Modellauswahl. Zugriff auf Llama 3.3 70B, DeepSeek R1, Qwen3, Gemma 3 und Mistral Small 3.1 — alles kostenlos. Ideal zum Erkunden.',
    groqDesc: 'Blitzschnelle Inferenz auf spezieller LPU-Hardware. Kostenloser Developer-Tarif mit Llama 3.3 70B, Qwen3 32B und GPT OSS 120B mit großzügigen Rate-Limits.',
    cerebrasDesc: 'Die schnellste verfügbare Inferenz. Wafer-Scale-Chips liefern unglaubliche Geschwindigkeit. Kostenloser Tarif mit allen Modellen: Llama 3.3 70B, Qwen3 32B und GPT OSS 120B.',
    getKey: 'API-Key holen →',
    apiKeyLabel: 'API-Key',
    keyDisclaimer: 'Dein Key wird nur im localStorage deines Browsers gespeichert. Er verlässt nie unsere Server.',
    keyPrivacyNote: 'Dein API-Key verlässt niemals deinen Browser — alle Anfragen gehen direkt von deinem Browser an die API des Anbieters. Der Key wird nur im Arbeitsspeicher gehalten und automatisch gelöscht, wenn du die Seite verlässt oder neu lädst.',
    keyPrivacyVerify: 'Im Quellcode überprüfen',
    modelLabel: 'Modell',
    temperatureLabel: 'Temperatur',
    maxTokensLabel: 'Max Tokens',
    messagePlaceholder: 'Nachricht eingeben... z.B. "Erkläre Quantencomputing in einfachen Worten"',
    sendButton: 'Senden',
    stopButton: 'Stopp',
    responseLabel: 'Antwort',
    breakdownModel: 'Modell',
    breakdownModelHint: 'Das genaue Modell, das deine Anfrage verarbeitet hat.',
    breakdownFinishReason: 'Abschlussgrund',
    breakdownFinishStop: 'Natürlich beendet — das Modell hat alles gesagt, was es wollte.',
    breakdownFinishLength: 'Durch das max_tokens-Limit abgeschnitten. Erhöhe es für längere Antworten.',
    breakdownFinishOther: 'Das Modell hat aus einem anderen Grund gestoppt.',
    breakdownResponseTime: 'Antwortzeit',
    breakdownResponseTimeHint: 'Gesamte Laufzeit vom Browser zur API und zurück.',
    breakdownTokens: 'Token-Verbrauch',
    breakdownPromptTokens: 'Prompt-Tokens',
    breakdownCompletionTokens: 'Completion-Tokens',
    breakdownTotalTokens: 'Gesamt',
    breakdownTokensHint: 'Prompt-Tokens = Kosten deiner Eingabe. Completion-Tokens = Ausgabe des Modells. So funktioniert die API-Abrechnung (kostenlose Tarife berechnen nichts).',
    rawJsonLabel: 'Rohe JSON-Antwort — für Neugierige',
    playgroundTitle: 'Dein erster LLM-Aufruf',
    playgroundDesc: 'Gib oben deinen API-Key ein, schreibe eine Nachricht und klicke auf Senden',
    understandTitle: 'Die Antwort verstehen',
    understandContent: 'Der eigentliche Text, den das Modell generiert hat. Das würdest du einem Nutzer zeigen.',
    understandTokens: 'Wie viele Tokens verwendet wurden. Prompt-Tokens = deine Eingabe, Completion-Tokens = die Ausgabe des Modells. So funktioniert die Abrechnung (kostenlose Tarife berechnen nichts).',
    understandFinish: '"stop" bedeutet, das Modell hat natürlich geendet. "length" bedeutet, es hat das max_tokens-Limit erreicht und wurde abgeschnitten.',
    understandModel: 'Das genaue Modell, das deine Anfrage verarbeitet hat. Einige Anbieter können an verschiedene Versionen weiterleiten.',
    nextStepsTitle: 'Nächste Schritte',
    nextStepsDesc: 'Jetzt, wo du deinen ersten LLM-Aufruf gemacht hast, erkunde diese Themen:',
    nextTemperature: 'Lerne, wie Temperatur Kreativität vs. Vorhersagbarkeit steuert',
    nextSystemPrompts: 'Setze die Persönlichkeit und Regeln für deine KI',
    nextTokenization: 'Verstehe, wie Text zu Zahlen wird, die das Modell verarbeiten kann',
    modelUsed: 'Modell',
    finishReason: 'Abschlussgrund',
    finishStop: 'Natürlich beendet.',
    finishLength: 'Durch max_tokens-Limit abgeschnitten.',
    responseTime: 'Antwortzeit',
    tokenUsage: 'Token-Verbrauch',
    promptTokens: 'Prompt-Tokens',
    completionTokens: 'Completion-Tokens',
    totalTokens: 'Gesamt',
    tokenExplain: 'Prompt-Tokens = deine Eingabe. Completion-Tokens = Ausgabe des Modells. So funktioniert die API-Abrechnung.',
  },
  learningPath: {
    title: 'Dein Lernpfad',
    subtitle: 'Ein kuratierter Fahrplan von Null bis Experte. Folge diesen Themen der Reihe nach für das beste Lernerlebnis.',
    beginner: '🌱 Grundlagen',
    intermediate: '🔧 Vertiefung',
    advanced: '🚀 Expertenwissen',
    beginnerHint: 'Starte hier — Kernkonzepte, die jeder kennen sollte',
    intermediateHint: 'Tiefer eintauchen — Architektur, Agents und Praxismuster',
    advancedHint: 'Meisterniveau — Orchestrierung, Optimierung und Sicherheit',
    progress: 'Gesamtfortschritt',
    lpPromptBasics: 'Lerne die Grundlagen, um effektiv mit KI-Modellen zu kommunizieren',
    lpTemperature: 'Verstehe, wie Zufälligkeit kreative vs. präzise Ausgaben steuert',
    lpTokenization: 'Sieh, wie Text in Tokens zerlegt wird, die das Modell versteht',
    lpNeuralNetworks: 'Die Bausteine hinter jedem KI-Modell',
    lpVision: 'Wie Modelle Bilder sehen und verstehen',
    lpLocalInference: 'KI-Modelle auf dem eigenen Rechner ausführen',
    lpSystemPrompts: 'Persönlichkeit, Regeln und Verhalten für deine KI definieren',
    lpEmbeddings: 'Text in Zahlen verwandeln, die Bedeutung erfassen',
    lpAttention: 'Der Mechanismus, der Modelle auf das Wesentliche fokussieren lässt',
    lpAgentLoop: 'Wie KI-Agents denken, handeln und iterieren',
    lpToolDesign: 'Gib deinen Agents die richtigen Werkzeuge',
    lpRag: 'Suche mit Generierung kombinieren für fundierte Antworten',
    lpMemory: 'Agents ein dauerhaftes Gedächtnis über Gespräche hinweg geben',
    lpAgenticPatterns: 'Bewährte Architekturen für komplexe Agent-Systeme',
    lpOrchestration: 'Mehrere Agents koordiniert zusammenarbeiten lassen',
    lpMoe: 'Wie Modelle spezialisierte Experten-Subnetzwerke nutzen',
    lpQuantization: 'Modelle verkleinern für schnellere Ausführung mit weniger Speicher',
    lpAgentSecurity: 'Agents vor Prompt Injection und Missbrauch schützen',
  },
}
