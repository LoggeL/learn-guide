import type { Dictionary } from './en'

export const de: Dictionary = {
  // Common UI
  common: {
    learnAi: 'Lerne KI',
    interactiveGuide: 'Interaktiver Leitfaden',
    topics: 'Themen',
    search: 'Suchen...',
    searchTopics: 'Themen suchen...',
    startTyping: 'Beginne zu tippen, um Themen zu suchen...',
    trySearching: 'Versuche "Temperature" oder "Attention"',
    noResults: 'Keine Ergebnisse gefunden für',
    pressEsc: 'ESC zum Schließen',
    enterToSelect: 'Enter zum Auswählen',
    previous: 'Zurück',
    next: 'Weiter',
    projectBy: 'Ein Projekt von',
    proTip: 'Profi-Tipp: Drücke',
    toSearchTopics: 'um Themen zu suchen',
    interactiveAiLearning: 'Interaktives KI-Lernen',
    guidesDescription: 'Interaktive Anleitungen zum Verstehen von KI-Konzepten',
  },

  // Home page
  home: {
    heroTitle1: 'KI-Konzepte meistern',
    heroTitle2: 'Durch Erfahrung',
    heroDescription: 'Erkunde künstliche Intelligenz und große Sprachmodelle durch schöne, interaktive Demonstrationen. Lerne durch Handeln, nicht nur durch Lesen.',
    startLearning: 'Jetzt lernen',
    browseTopics: 'Themen durchsuchen',
    exploreTopics: 'Themen erkunden',
    diveIntoLessons: 'Tauche ein in interaktive Lektionen',
  },

  // Features
  features: {
    interactiveDemos: 'Interaktive Demos',
    interactiveDemosDesc: 'Praktische Erkundungen, die abstrakte Konzepte greifbar und intuitiv machen.',
    visualLearning: 'Visuelles Lernen',
    visualLearningDesc: 'Schöne Visualisierungen, die zeigen, wie KI-Systeme tatsächlich funktionieren.',
    buildIntuition: 'Intuition aufbauen',
    buildIntuitionDesc: 'Gehe über das Auswendiglernen hinaus – entwickle tiefes Verständnis durch Experimentieren.',
  },

  // Topic categories
  categories: {
    ai: 'Künstliche Intelligenz',
    agents: 'KI-Agenten',
    llm: 'Große Sprachmodelle',
    mlFundamentals: 'ML-Grundlagen',
    prompting: 'Prompting',
    safety: 'KI-Sicherheit',
  },

  // Topic names
  topicNames: {
    'agent-loop': 'Der Agenten-Zyklus',
    'agent-context': 'Kontext-Anatomie',
    'agent-problems': 'Agenten-Probleme',
    'agent-security': 'Agenten-Sicherheit',
    'agentic-patterns': 'Agentische Muster',
    'mcp': 'MCP (Model Context Protocol)',
    'context-rot': 'Kontextverfall',
    'temperature': 'Temperatur',
    'attention': 'Aufmerksamkeits-Mechanismus',
    'vision': 'Bildverarbeitung',
    'visual-challenges': 'Visuelle Herausforderungen',
    // Phase 1: LLM topics
    'tokenization': 'Tokenisierung',
    'embeddings': 'Einbettungen',
    'rag': 'RAG (Retrieval Augmented Generation)',
    // Phase 2: Agent topics
    'tool-design': 'Tool-Design',
    'memory': 'Speichersysteme',
    'orchestration': 'Orchestrierung',
    'evaluation': 'Evaluierung',
    // Phase 3: ML Fundamentals
    'neural-networks': 'Neuronale Netzwerke',
    'gradient-descent': 'Gradientenabstieg',
    'training': 'Trainingsprozess',
    // Phase 3: Prompting
    'prompt-basics': 'Prompt-Grundlagen',
    'advanced-prompting': 'Fortgeschrittenes Prompting',
    'system-prompts': 'System-Prompts',
    // Phase 3: AI Safety
    'alignment': 'Alignment',
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Verantwortungsvolle KI',
  },

  // Temperature page
  temperature: {
    title: 'Temperatur',
    description: 'Verstehe, wie ein einzelner Parameter die Balance zwischen vorhersagbarer Logik und kreativer Zufälligkeit in KI-Ausgaben steuert.',
    whatIs: 'Was ist Temperatur?',
    whatIsDesc: 'In LLMs ist Temperatur ein Hyperparameter, der die "Logits" (Rohwerte) der nächsten Token-Vorhersagen skaliert, bevor sie in Wahrscheinlichkeiten umgewandelt werden. Er steuert im Wesentlichen, wie stark das Modell die wahrscheinlichsten Optionen gegenüber weniger wahrscheinlichen bevorzugt.',
    lowTemp: 'Niedrige Temperatur',
    lowTempDesc: 'Konzentriert sich auf die Top-Ergebnisse. Zuverlässig, konsistent und faktisch. Ideal für Code, Mathematik und strukturierte Daten.',
    highTemp: 'Hohe Temperatur',
    highTempDesc: 'Verteilt Wahrscheinlichkeit auf mehr Tokens. Vielfältig, kreativ und überraschend. Ideal für Geschichten, Brainstorming und Poesie.',
    interactiveDistribution: 'Interaktive Verteilung',
    adjustSlider: 'Stelle die Temperatur ein, um den Effekt zu sehen',
    adjustDesc: 'Bewege den Temperaturregler, um zu sehen, wie er die Wahrscheinlichkeitsverteilung für das nächste Token umformt. Beobachte, wie "the" (die wahrscheinlichste Wahl) bei niedrigen Temperaturen dominiert und bei steigender Temperatur seinen Vorsprung verliert.',
    howItWorks: 'Wie es mathematisch funktioniert',
    mathDesc: 'Das Modell generiert einen Score für jedes mögliche Token. Um Wahrscheinlichkeiten zu erhalten, verwenden wir die Softmax-Funktion, modifiziert durch die Temperatur:',
    whenLow: 'Wenn T → 0',
    low: 'Niedrig',
    whenLowDesc: 'Division durch ein kleines T verstärkt die Unterschiede zwischen den Scores. Der höchste Logit dominiert exponentiell.',
    whenHigh: 'Wenn T → ∞',
    high: 'Hoch',
    whenHighDesc: 'Division durch ein großes T komprimiert alle Scores gegen Null, wodurch sie nach der Exponentialfunktion nahezu gleich werden.',
    practicalGuidelines: 'Praktische Richtlinien',
    useCase: 'Anwendungsfall',
    tempLabel: 'Temperatur',
    why: 'Warum?',
    codingMath: 'Programmierung & Mathematik',
    codingMathWhy: 'Fehler in der Logik sind kostspielig; du willst den wahrscheinlichsten korrekten Pfad.',
    factRetrieval: 'Faktenabfrage',
    factRetrievalWhy: 'Reduziert "Halluzinationen", indem es sich an die wahrscheinlichsten Datenpunkte hält.',
    generalChat: 'Allgemeiner Chat',
    generalChatWhy: 'Der "Sweet Spot" für die meisten Modelle, um natürlich und hilfreich zu klingen.',
    creativeWriting: 'Kreatives Schreiben',
    creativeWritingWhy: 'Ermutigt das Modell, interessanteres, vielfältigeres Vokabular zu verwenden.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generiert wilde, unkonventionelle Ideen, die Inspiration wecken könnten.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Temperatur 0 ist deterministisch ("Greedy Search") – wählt immer das Top-Token',
    takeaway2: 'Höhere Temperatur erhöht Vielfalt und Kreativität, verringert aber die Kohärenz',
    takeaway3: 'Zu hohe Temperatur (> 1.5) führt oft zu Kauderwelsch',
    takeaway4: 'Passe die Temperatur immer an die Anforderungen der Aufgabe bezüglich Präzision vs. Kreativität an',
  },

  // Context Rot page
  contextRot: {
    title: 'Kontextverfall',
    description: 'Verstehe, wie Informationen über lange Gespräche degradieren und warum LLMs mit erweiterten Kontexten kämpfen.',
    whatIs: 'Was ist Kontextverfall?',
    whatIsDesc: 'Kontextverfall bezieht sich auf die allmähliche Verschlechterung der Fähigkeit eines LLMs, Informationen aus früheren Teilen eines langen Gesprächs oder Dokuments genau abzurufen und zu nutzen. Mit wachsendem Kontext wird die Aufmerksamkeit des Modells verwässert.',
    whyHappens: 'Warum passiert das?',
    whyHappensDesc: 'LLMs haben begrenzte Kontextfenster und nutzen Aufmerksamkeitsmechanismen, die den Fokus auf alle Tokens verteilen müssen. Bei längeren Gesprächen konkurrieren frühere Informationen mit neuerem Inhalt um die begrenzte Aufmerksamkeitskapazität des Modells.',
    symptoms: 'Häufige Symptome',
    symptom1: 'Vergessen von Anweisungen vom Anfang eines Gesprächs',
    symptom2: 'Widerspruch zu früheren Aussagen oder Entscheidungen',
    symptom3: 'Verlust des Überblicks bei komplexen Mehrstufenaufgaben',
    symptom4: 'Verwechslung von Details aus verschiedenen Teilen des Kontexts',
    mitigation: 'Gegenmaßnahmen',
    mitigation1: 'Fasse wichtigen Kontext regelmäßig zusammen',
    mitigation2: 'Platziere kritische Anweisungen sowohl am Anfang als auch am Ende',
    mitigation3: 'Nutze strukturierte Formate, um wichtige Informationen hervorzuheben',
    mitigation4: 'Teile lange Aufgaben in kleinere, fokussierte Gespräche auf',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Sieh, wie das Gedächtnis mit zunehmender Kontextlänge verblasst',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextverfall ist eine inhärente Einschränkung aktueller LLM-Architekturen',
    takeaway2: 'Der "Lost in the Middle"-Effekt bedeutet, dass Informationen am Anfang und Ende besser erinnert werden',
    takeaway3: 'Strategische Informationsplatzierung kann den Abruf erheblich verbessern',
    takeaway4: 'Regelmäßiges Zusammenfassen hilft, wichtigen Kontext über lange Gespräche zu erhalten',
  },

  // Attention page
  attention: {
    title: 'Aufmerksamkeits-Mechanismus',
    description: 'Erkunde, wie Transformer durch den leistungsstarken Aufmerksamkeitsmechanismus auf relevante Teile der Eingabe fokussieren.',
    whatIs: 'Was ist Aufmerksamkeit?',
    whatIsDesc: 'Aufmerksamkeit ist der Kernmechanismus, der es Transformern ermöglicht, die Wichtigkeit verschiedener Teile der Eingabe bei der Generierung jedes Ausgabe-Tokens zu gewichten. Es ermöglicht dem Modell, sich auf relevanten Kontext zu "konzentrieren".',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Für jede Position berechnet das Modell Query-, Key- und Value-Vektoren. Aufmerksamkeits-Scores werden durch Vergleich von Queries mit Keys berechnet und dann verwendet, um eine gewichtete Summe der Values zu erstellen.',
    selfAttention: 'Selbst-Aufmerksamkeit',
    selfAttentionDesc: 'Ermöglicht jedem Token, auf alle anderen Tokens in der Sequenz zu achten und Beziehungen unabhängig von der Entfernung zu erfassen.',
    multiHead: 'Multi-Head-Aufmerksamkeit',
    multiHeadDesc: 'Mehrere Aufmerksamkeitsköpfe ermöglichen es dem Modell, sich gleichzeitig auf verschiedene Arten von Beziehungen zu konzentrieren.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Aufmerksamkeit ermöglicht Transformern, Langstreckenabhängigkeiten zu erfassen',
    takeaway2: 'Die quadratische Komplexität der Aufmerksamkeit begrenzt die Kontextfenstergröße',
    takeaway3: 'Verschiedene Aufmerksamkeitsköpfe lernen, sich auf verschiedene linguistische Muster zu konzentrieren',
    takeaway4: 'Aufmerksamkeitsvisualisierung kann helfen, das Modellverhalten zu interpretieren',
  },

  // Vision page
  vision: {
    title: 'Bildverarbeitung',
    description: 'Wie moderne LLMs visuelle Informationen neben Text verarbeiten und verstehen.',
    whatIs: 'Wie LLMs Bilder sehen',
    whatIsDesc: 'Bildverarbeitungsfähige LLMs wandeln Bilder in Token-Sequenzen um, die zusammen mit Text verarbeitet werden können. Dies beinhaltet typischerweise das Aufteilen von Bildern in Patches und deren Kodierung mit einem Vision-Transformer.',
    patchEncoding: 'Patch-Kodierung',
    patchEncodingDesc: 'Bilder werden in Patches fester Größe (z.B. 14x14 Pixel) aufgeteilt, wobei jeder in einen Einbettungsvektor ähnlich wie Text-Tokens umgewandelt wird.',
    multimodal: 'Multimodales Verständnis',
    multimodalDesc: 'Das Modell lernt, visuelle und textuelle Repräsentationen auszurichten, was Aufgaben wie Bildbeschriftung, visuelle Fragen und Dokumentenverständnis ermöglicht.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bilder verbrauchen viel mehr Tokens als äquivalente Textbeschreibungen',
    takeaway2: 'Auflösung und Patch-Größe beeinflussen die Detailerkennung',
    takeaway3: 'Visuelles Verständnis ist ungefähr – Modelle können feine Details übersehen',
    takeaway4: 'Die Kombination von Bild und Sprache ermöglicht leistungsstarke neue Anwendungen',
  },

  // Visual Challenges page
  visualChallenges: {
    title: 'Visuelle Herausforderungen',
    description: 'Häufige Herausforderungen und Einschränkungen bei der Arbeit mit bildverarbeitungsfähigen KI-Modellen.',
    overview: 'Häufige visuelle Herausforderungen',
    overviewDesc: 'Obwohl Bildmodelle beeindruckend sind, stehen sie vor mehreren systematischen Herausforderungen, die beim Erstellen von Anwendungen wichtig zu verstehen sind.',
    challenge1: 'Objekte zählen',
    challenge1Desc: 'Modelle haben oft Schwierigkeiten, Objekte in Bildern genau zu zählen, besonders wenn es viele ähnliche Elemente gibt.',
    challenge2: 'Räumliches Denken',
    challenge2Desc: 'Das Verstehen präziser räumlicher Beziehungen zwischen Objekten (links/rechts, oben/unten) kann unzuverlässig sein.',
    challenge3: 'Kleine Texterkennung',
    challenge3Desc: 'Feiner Text in Bildern kann falsch gelesen oder ganz übersehen werden, besonders bei niedrigen Auflösungen.',
    challenge4: 'Halluzination',
    challenge4Desc: 'Modelle können Objekte oder Details beschreiben, die nicht wirklich im Bild vorhanden sind.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Verifiziere kritische visuelle Informationen immer auf anderen Wegen',
    takeaway2: 'Höher aufgelöste Bilder verbessern generell die Genauigkeit',
    takeaway3: 'Teile komplexe visuelle Aufgaben in einfachere Unterfragen auf',
    takeaway4: 'Sei explizit darüber, welche Aspekte eines Bildes du analysieren musst',
  },

  // Agent Loop page
  agentLoop: {
    title: 'Der Agenten-Zyklus',
    description: 'Verstehe den Kernzyklus, der autonome KI-Agenten antreibt: beobachten, denken, handeln, wiederholen.',
    whatIs: 'Was ist der Agenten-Zyklus?',
    whatIsDesc: 'Der Agenten-Zyklus ist der fundamentale Kreislauf, der es KI-Agenten ermöglicht, autonom mit ihrer Umgebung zu interagieren. Er besteht aus Beobachtungs-, Denk-, Handlungs- und Feedback-Phasen, die sich kontinuierlich wiederholen.',
    phases: 'Die vier Phasen',
    observe: 'Beobachten',
    observeDesc: 'Sammle Informationen aus der Umgebung, von Tools und Benutzereingaben.',
    think: 'Denken',
    thinkDesc: 'Überlege zum aktuellen Zustand und entscheide über die nächste Aktion.',
    act: 'Handeln',
    actDesc: 'Führe die gewählte Aktion mit verfügbaren Tools aus.',
    learn: 'Lernen',
    learnDesc: 'Verarbeite Feedback und aktualisiere das Verständnis für die nächste Iteration.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Der Zyklus setzt sich fort, bis die Aufgabe abgeschlossen oder beendet ist',
    takeaway2: 'Jede Iteration baut auf vorherigen Beobachtungen und Aktionen auf',
    takeaway3: 'Fehlerbehandlung und Wiederherstellung sind entscheidend für robuste Agenten',
    takeaway4: 'Die Qualität der Tools beeinflusst direkt die Fähigkeiten des Agenten',
  },

  // Agent Context page
  agentContext: {
    title: 'Kontext-Anatomie',
    description: 'Aufschlüsselung der Struktur von Kontextfenstern und wie Agenten Informationen verwalten.',
    whatIs: 'Agentenkontext verstehen',
    whatIsDesc: 'Der Agentenkontext umfasst den System-Prompt, die Gesprächshistorie, Tool-Definitionen und abgerufene Informationen. Eine effiziente Verwaltung dieses Kontexts ist entscheidend für die Agentenleistung.',
    components: 'Kontext-Komponenten',
    systemPrompt: 'System-Prompt',
    systemPromptDesc: 'Definiert die Rolle, Fähigkeiten und Verhaltensrichtlinien des Agenten.',
    toolDefs: 'Tool-Definitionen',
    toolDefsDesc: 'Beschreibungen der verfügbaren Tools und ihrer Verwendung.',
    history: 'Gesprächshistorie',
    historyDesc: 'Vorherige Nachrichten, Tool-Aufrufe und deren Ergebnisse.',
    retrieved: 'Abgerufene Informationen',
    retrievedDesc: 'Externes Wissen, das während des Gesprächs abgerufen wurde.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextmanagement ist der Schlüssel zur Agenten-Zuverlässigkeit',
    takeaway2: 'Priorisiere aktuelle und relevante Informationen',
    takeaway3: 'Tool-Definitionen sollten klar und eindeutig sein',
    takeaway4: 'Zusammenfassung hilft, Kontext über lange Sitzungen zu erhalten',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agenten-Probleme',
    description: 'Häufige Fehlermodi und Herausforderungen, denen KI-Agenten in realen Anwendungen begegnen.',
    overview: 'Häufige Agenten-Fehlermodi',
    overviewDesc: 'Das Verstehen typischer Agentenfehler hilft beim Aufbau robusterer Systeme und beim Setzen angemessener Erwartungen.',
    problem1: 'Tool-Missbrauch',
    problem1Desc: 'Agenten können Tools falsch aufrufen, mit falschen Parametern oder zu unpassenden Zeiten.',
    problem2: 'Endlosschleifen',
    problem2Desc: 'Agenten können stecken bleiben und dieselben Aktionen wiederholen, ohne Fortschritte zu machen.',
    problem3: 'Zieldrift',
    problem3Desc: 'Agenten können den Fokus allmählich vom ursprünglichen Aufgabenziel weg verschieben.',
    problem4: 'Übermäßiges Selbstvertrauen',
    problem4Desc: 'Agenten können trotz Unsicherheit oder unvollständiger Informationen mit Aktionen fortfahren.',
    
    // Expanded Content
    hallucination: 'Tool-Halluzination',
    hallucinationDesc: 'Agenten "erfinden" manchmal Tool-Parameter oder sogar ganze Tools, die nicht existieren. Dies passiert meistens, wenn die Tool-Definition mehrdeutig ist oder das Modell versucht, eine Lösung zu erzwingen.',
    hallucinationExample: 'Beispiel: Aufruf von `get_weather(location="Tokyo", date="tomorrow")`, wenn die Funktion nur `location` akzeptiert.',
    
    loops: 'Schleifen-Probleme',
    loopsDesc: 'Agenten können in repetitiven Zyklen gefangen sein, in denen sie dieselbe Aktion ausführen, denselben Fehler erhalten und es ohne Strategieänderung erneut versuchen.',
    loopsMitigation: 'Abhilfe: Implementiere Schleifenerkennungslogik, die die Ausführung stoppt, wenn dieselbe Tool-Aufrufsequenz mehrmals auftritt.',
    
    costLatency: 'Kosten & Latenz',
    costLatencyDesc: 'Jeder Schritt im Agentenzyklus erfordert einen vollständigen LLM-Inferenzaufruf. Mehrstufige Aufgaben können schnell teuer und langsam werden.',
    costFactor: 'Der Kostenfaktor',
    costFactorDesc: 'Eine einfache Aufgabe, die 5 Schritte erfordert, bedeutet 5-fache Kosten und 5-fache Latenz im Vergleich zu einer Standard-Chat-Antwort.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Implementiere Sicherheitsvorkehrungen wie Iterationslimits und Kostenkontrollen',
    takeaway2: 'Füge Human-in-the-Loop-Kontrollpunkte für kritische Aktionen hinzu',
    takeaway3: 'Überwache das Agentenverhalten und protokolliere alle Aktionen zum Debugging',
    takeaway4: 'Definiere klare Erfolgs- und Fehlkriterien',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agenten-Sicherheit',
    description: 'Kritische Sicherheitslücken bei KI-Agenten: Prompt-Injektion, Datenexfiltration und Tool-Missbrauch – plus Verteidigungsstrategien.',
    
    // Intro
    intro: 'Agenten sind Angriffsflächen',
    introDesc: 'Wenn du einem LLM Zugang zu Tools gibst, schaffst du einen mächtigen Angriffsvektor. Agenten können Dateien lesen, HTTP-Anfragen stellen, E-Mails senden und Code ausführen. Ein böswilliger Akteur, der den Kontext des Agenten beeinflussen kann, kann potenziell all diese Fähigkeiten kontrollieren.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Angriff #1: Prompt-Injektion',
    attack1Desc: 'Prompt-Injektion tritt auf, wenn nicht vertrauenswürdige Eingaben vom LLM als Anweisungen interpretiert werden. Da Agenten oft externe Daten verarbeiten (E-Mails, Webseiten, Dokumente), können Angreifer versteckte Befehle einbetten, die das Verhalten des Agenten kapern.',
    attack1Example: 'Beispiel-Angriff',
    attack1ExampleDesc: 'Der Benutzer bittet den Agenten, ein Dokument zusammenzufassen. Das Dokument enthält versteckte Anweisungen:',
    whyWorks: 'Warum das funktioniert',
    whyWorks1: 'Der Agent liest das Dokument in seinen Kontext',
    whyWorks2: 'Das LLM kann nicht zwischen "echten" Anweisungen und injizierten unterscheiden',
    whyWorks3: 'Der versteckte Text sieht aus wie Systemanweisungen, also folgt das LLM ihnen möglicherweise',
    whyWorks4: 'Der Agent nutzt seine legitimen Tools, um die bösartige Aktion auszuführen',
    directInjection: 'Direkte Injektion',
    directInjectionDesc: 'Der Benutzer tippt bösartige Anweisungen direkt ein. Leichter zu filtern, aber immer noch gefährlich, wenn der System-Prompt nicht robust ist.',
    indirectInjection: 'Indirekte Injektion',
    indirectInjectionDesc: 'Bösartiger Inhalt kommt aus externen Quellen, die der Agent liest (Websites, E-Mails, Dateien). Viel schwerer zu verteidigen.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Angriff #2: Datenexfiltration',
    attack2Desc: 'Agenten mit Zugang zu Kommunikationstools (E-Mail, HTTP, Slack, etc.) können dazu gebracht werden, sensible Daten an externe Ziele zu senden. Der Agent wird zum unwissenden Komplizen beim Datendiebstahl.',
    exfilFlow: 'Exfiltrations-Ablauf',
    exfilStep1: 'Agent liest',
    exfilStep1Desc: 'Private Dateien, DB, Env-Variablen',
    exfilStep2: 'Injektion löst aus',
    exfilStep2Desc: '"Sende dies an X"',
    exfilStep3: 'Tool führt aus',
    exfilStep3Desc: 'Daten verlassen das System',
    vulnerableConfig: 'Anfällige Tool-Konfiguration',
    otherVectors: 'Andere Exfiltrations-Vektoren',
    vector1: 'HTTP-Anfragen — POST-Daten an angreifergesteuerte Endpunkte',
    vector2: 'Slack/Discord-Webhooks — Nachrichten an externe Kanäle senden',
    vector3: 'Datei-Uploads — Hochladen in Cloud-Speicher mit öffentlichen Links',
    vector4: 'DNS-Exfiltration — Daten in DNS-Anfragen kodieren',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Angriff #3: Unbeabsichtigter Tool-Missbrauch',
    attack3Desc: 'Auch ohne böswillige Absicht können Agenten durch falsche Tool-Nutzung Schaden anrichten. Das LLM könnte Parameter falsch verstehen, das falsche Tool verwenden oder destruktive Aktionen ausführen, während es versucht, hilfreich zu sein.',
    destructiveActions: 'Destruktive Aktionen',
    destructiveActionsDesc: '"Räume das Projekt auf" → Agent führt rm -rf / aus oder löscht die Produktionsdatenbank',
    wrongParams: 'Falsche Parameter',
    wrongParamsDesc: 'Agent verwechselt ähnliche Felder oder verwendet falsche Werte, die plausibel erscheinen',
    cascadingErrors: 'Kaskadierende Fehler',
    cascadingErrorsDesc: 'Agent macht einen kleinen Fehler, dann "behebt" er ihn mit zunehmend destruktiven Aktionen',
    
    // Defense Strategies
    defensesTitle: 'Verteidigungsstrategien',
    defense1: 'Prinzip der minimalen Rechte',
    defense1Desc: 'Gib dem Agenten nur die minimal notwendigen Tools und Berechtigungen für die Aufgabe. Gib keinen Dateizugang, wenn er nur Fragen beantworten muss.',
    defense1Bad: 'Schlecht',
    defense1Good: 'Gut',
    defense2: 'Strikte Allowlists',
    defense2Desc: 'Beschränke Tool-Parameter auf bekannte sichere Werte. Erlaube keine beliebigen E-Mail-Adressen, URLs oder Dateipfade.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Erfordere menschliche Genehmigung für sensible Aktionen. Der Agent schlägt vor, der Mensch bestätigt.',
    defense3Example: 'Beispiel-Bestätigungsablauf:',
    defense4: 'Eingabe-Bereinigung & Isolation',
    defense4Desc: 'Behandle externe Daten als nicht vertrauenswürdig. Trenne Benutzeranweisungen klar von abgerufenen Inhalten.',
    defense5: 'Überwachung & Ratenbegrenzung',
    defense5Desc: 'Protokolliere alle Tool-Aufrufe. Setze Ratenlimits für sensible Operationen. Alarmiere bei ungewöhnlichen Mustern (viele E-Mails, große Datenübertragungen, wiederholte Fehler). Aktiviere Rollback für destruktive Aktionen.',
    
    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Agenten sind Angriffsflächen – jedes Tool ist eine potenzielle Schwachstelle',
    takeaway2: 'Prompt-Injektion ist die #1 Bedrohung – LLMs können Anweisungen nicht von Daten unterscheiden',
    takeaway3: 'Datenexfiltration ist trivial, wenn Agenten ausgehende Kommunikationstools haben',
    takeaway4: 'Tool-Missbrauch passiert auch ohne Angreifer – LLMs machen Fehler',
    takeaway5: 'Verteidigung in der Tiefe: minimale Rechte + Allowlists + menschliche Genehmigung + Überwachung',
    takeaway6: 'Behandle alle externen Daten als potenziell bösartige Eingaben',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentische Muster',
    description: 'Entwurfsmuster und Architekturen für den Aufbau effektiver KI-Agentensysteme.',
    overview: 'Häufige agentische Muster',
    overviewDesc: 'Mehrere Architekturmuster haben sich als effektive Ansätze für den Aufbau von KI-Agentensystemen herausgestellt.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Verschachtele Denk-Traces mit Aktionen für bessere Transparenz und Kontrolle.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Erstelle zuerst einen übergeordneten Plan, dann führe die Schritte sequentiell aus.',
    pattern3: 'Multi-Agenten-Systeme',
    pattern3Desc: 'Mehrere spezialisierte Agenten arbeiten zusammen, um komplexe Aufgaben zu lösen.',
    pattern4: 'Reflexion',
    pattern4Desc: 'Agenten überprüfen ihre eigenen Ausgaben und verbessern sie iterativ.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Wähle Muster basierend auf Aufgabenkomplexität und Zuverlässigkeitsanforderungen',
    takeaway2: 'ReAct ist großartig für Transparenz, kann aber langsamer sein',
    takeaway3: 'Multi-Agenten-Systeme erhöhen die Komplexität, ermöglichen aber Spezialisierung',
    takeaway4: 'Reflexionsmuster können die Ausgabequalität erheblich verbessern',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'MCP verstehen: wann externe Tool-Server sinnvoll sind und wann sie übertrieben sind.',
    whatIs: 'Was ist MCP?',
    whatIsDesc: 'Das Model Context Protocol (MCP) ist ein standardisierter Weg, um KI-Agenten mit externen Tools und Datenquellen über dedizierte Server-Prozesse zu verbinden. Anstatt Tools inline im Agenten-Code zu definieren, führt MCP einen separaten Server aus, der Tools über ein strukturiertes Protokoll bereitstellt.',
    vsToolCalls: 'MCP vs. Reguläre Tool-Aufrufe',
    vsToolCallsDesc: 'Reguläre Tool-Aufrufe sind Funktionen, die direkt in der Codebasis deines Agenten definiert sind. Der Agent ruft sie auf, sie werden ausgeführt und die Ergebnisse kehren im selben Prozess zurück. MCP trennt dies: Tools leben in externen Servern, mit denen der Agent über ein Protokoll kommuniziert.',
    
    // Comparison
    regularTools: 'Reguläre Tool-Aufrufe',
    regularToolsDesc: 'Tools, die inline in deinem Agenten-Code definiert sind. Einfach, schnell und für die meisten Anwendungsfälle ausreichend.',
    mcpTools: 'MCP-Server',
    mcpToolsDesc: 'Tools, die von externen Server-Prozessen bereitgestellt werden. Fügt Netzwerk-Overhead hinzu, ermöglicht aber sprachübergreifendes Tooling und gemeinsame Tool-Ökosysteme.',
    
    // When to use
    whenToUse: 'Wann MCP sinnvoll ist',
    whenToUseDesc: 'MCP glänzt in spezifischen Szenarien, in denen sich seine zusätzliche Komplexität auszahlt.',
    useCase1: 'Multi-Sprachen-Teams',
    useCase1Desc: 'Deine Tools sind in Python geschrieben, aber dein Agent ist in TypeScript, oder umgekehrt.',
    useCase2: 'Gemeinsames Tool-Ökosystem',
    useCase2Desc: 'Mehrere Agenten in verschiedenen Projekten müssen auf dieselben Tools zugreifen.',
    useCase3: 'Enterprise-Integration',
    useCase3Desc: 'Du musst bestehende interne Dienste als Agenten-Tools bereitstellen, ohne sie zu modifizieren.',
    useCase4: 'Tool-Marktplatz',
    useCase4Desc: 'Du möchtest von der Community gepflegte Tools nutzen, ohne Code in dein Projekt zu kopieren.',
    
    // When it's overkill
    overkill: 'Wann MCP übertrieben ist',
    overkillDesc: 'Für viele Anwendungsfälle fügt MCP unnötige Komplexität hinzu.',
    overkillCase1: 'Einsprachige Projekte',
    overkillCase1Desc: 'Wenn deine Tools und dein Agent in derselben Sprache sind, sind Inline-Funktionen einfacher und schneller.',
    overkillCase2: 'Einfache Agenten',
    overkillCase2Desc: 'Ein Chatbot mit wenigen Tools braucht nicht den Overhead, separate Server-Prozesse auszuführen.',
    overkillCase3: 'Schnelles Prototyping',
    overkillCase3Desc: 'Bei schneller Iteration verlangsamt die Indirektion von MCP die Entwicklung.',
    overkillCase4: 'Latenz-kritische Apps',
    overkillCase4Desc: 'Netzwerkaufrufe zu Tool-Servern fügen Latenz hinzu, die Inline-Funktionen nicht haben.',
    
    // Architecture
    architecture: 'Wie MCP funktioniert',
    architectureDesc: 'MCP definiert eine Client-Server-Architektur, bei der der Agent der Client ist und Tools von Servern bereitgestellt werden.',
    step1: 'Entdeckung',
    step1Desc: 'Der Agent verbindet sich mit einem MCP-Server und erhält eine Liste der verfügbaren Tools mit ihren Schemas.',
    step2: 'Aufruf',
    step2Desc: 'Wenn das LLM entscheidet, ein Tool zu verwenden, sendet der Agent eine Anfrage an den MCP-Server.',
    step3: 'Ausführung',
    step3Desc: 'Der MCP-Server führt das Tool aus und gibt Ergebnisse in einem standardisierten Format zurück.',
    step4: 'Integration',
    step4Desc: 'Ergebnisse fließen zurück zum Agenten und in den LLM-Kontext, genau wie reguläre Tool-Ergebnisse.',
    
    // Practical advice
    practicalAdvice: 'Praktische Ratschläge',
    adviceDesc: 'Richtlinien für die Entscheidung, ob du MCP in deinem Projekt verwenden solltest.',
    advice1: 'Beginne einfach: verwende Inline-Tool-Definitionen, bis du auf eine spezifische Einschränkung stößt.',
    advice2: 'Erwäge MCP, wenn du dich dabei ertappst, Tool-Code zwischen Projekten zu kopieren.',
    advice3: 'Der Overhead, MCP-Server auszuführen, macht nur in großem Maßstab oder in Enterprise-Umgebungen Sinn.',
    advice4: 'Community-MCP-Server können die Entwicklung beschleunigen, fügen aber Abhängigkeitsrisiken hinzu.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'MCP ist ein Protokoll zur Bereitstellung von Tools über externe Server, kein Ersatz für reguläre Tool-Aufrufe',
    takeaway2: 'Für die meisten Einzelprojekt-Agenten sind Inline-Tools einfacher und haben geringere Latenz',
    takeaway3: 'MCP glänzt in polyglotten Umgebungen und gemeinsamen Tool-Ökosystemen',
    takeaway4: 'Greife nicht standardmäßig zu MCP—es ist eine Lösung für spezifische Skalierungs- und Interoperabilitäts-Herausforderungen',
  },

  // Metadata
  metadata: {
    title: 'KI-Konzepte lernen | Interaktiver Leitfaden',
    description: 'Meistere künstliche Intelligenz und Konzepte großer Sprachmodelle durch schöne, interaktive Demonstrationen.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Bedienfeld',
    adjustTemperature: 'Temperatur anpassen',
    temperature: 'Temperatur',
    samplePrompt: 'Beispiel-Prompt',
    onceUponATime: '"Es war einmal..."',
    liveCompletion: 'Live-Vervollständigung',
    regenerate: 'Neu generieren',
    deterministic: 'Deterministisch',
    balanced: 'Ausgewogen',
    creative: 'Kreativ',
    chaotic: 'Chaotisch',
    frozen: 'Eingefroren',
    focused: 'Fokussiert',
    wild: 'Wild',
    greedyMode: 'Gieriger Modus: Wählt immer das wahrscheinlichste Token.',
    lowTemp: 'Niedrige Temperatur: Fokus auf wahrscheinliche Fortsetzungen.',
    balancedTemp: 'Ausgewogen: Natürliche Mischung aus Vorhersehbarkeit und Vielfalt.',
    highTemp: 'Hohe Temperatur: Erkundet kreative, weniger häufige Wortwahlen.',
    veryHighTemp: 'Sehr hoch: Wahrscheinlichkeitsverteilung ist nahezu gleichförmig – erwarte Chaos!',
    
    // Context Rot Simulator
    setInstruction: 'Systemanweisung festlegen',
    persistInstruction: 'Dies sollte während des gesamten Gesprächs bestehen bleiben',
    systemPrompt: 'System-Prompt',
    quickExamples: 'Schnellbeispiele',
    startSimulation: 'Simulation starten',
    contextOverflow: 'Kontextüberlauf!',
    conversation: 'Gespräch',
    messagesPushed: 'Nachrichten aus dem Fenster geschoben',
    messages: 'Nachrichten',
    overflowIt: 'Überfluten!',
    reset: 'Zurücksetzen',
    typeMessage: 'Schreibe eine Nachricht...',
    systemInstructionLost: 'Systemanweisung verloren!',
    systemLostDesc: 'Deine Systemanweisung wurde vollständig aus dem Kontextfenster geschoben. Das Modell kann sie nicht mehr sehen – es ist, als hättest du die Anweisung nie gegeben. Das ist der schlimmste Fall von Kontextverfall: totale Amnesie.',
    contextFilling: 'Kontext füllt sich',
    contextFillingDesc: 'Deine Systemanweisung verliert an Einfluss, da neuere Nachrichten Vorrang haben. Beachte, wie sie visuell verblasst – dies stellt die schwindende Aufmerksamkeit des Modells dar.',
    exampleFrench: 'Antworte immer auf Französisch.',
    examplePirate: 'Du bist ein Pirat. Sage oft "Arrr".',
    exampleHaiku: 'Beende jede Antwort mit einem Haiku.',
    labelFrench: 'Sprich Französisch',
    labelPirate: 'Sei ein Pirat',
    labelHaiku: 'Beende mit Haiku',

    // Attention Visualizer
    hoverToSee: 'Fahre darüber, um Aufmerksamkeitsgewichte zu sehen',
    token: 'Token',
    attentionScore: 'Aufmerksamkeits-Score',
    strongConnection: 'Starke Verbindung',
    weakConnection: 'Schwache Verbindung',

    // Patch Grid Visualizer
    originalImage: 'Originalbild',
    patchGrid: 'Patch-Raster',
    flattenedPatches: 'Abgeflachte Patches',
    transformerInput: 'Transformer-Eingabe',
    processDesc: 'Das Bild wird in ein festes Raster von Patches (z.B. 16x16 Pixel) aufgeteilt. Jeder Patch wird dann in einen Vektor abgeflacht und linear in einen Einbettungsraum projiziert.',

    // Agent Loop Visualizer
    startLoop: 'Zyklus starten',
    step: 'Schritt',
    context: 'Kontext',
    llmResponse: 'LLM-Antwort',
    toolExecution: 'Tool-Ausführung',
    finalAnswer: 'Endgültige Antwort',
    system: 'System',
    user: 'Benutzer',
    assistant: 'Assistent',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Planen & Ausführen',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflexion',
    patternDesc: 'Wähle ein Muster, um zu sehen, wie es den Arbeitsablauf des Agenten strukturiert.',

    // Tokenizer Demo
    enterText: 'Text zum Tokenisieren eingeben',
    sampleText: 'Der schnelle braune Fuchs springt über den faulen Hund.',
    tokens: 'Tokens',
    characters: 'Zeichen',
    tokensPerChar: 'Tokens pro Zeichen',
    tokenBreakdown: 'Token-Aufschlüsselung',
    commonTokens: 'Häufige Tokens sind einzelne Teile',
    rareTokens: 'Seltene Wörter werden in Teilwörter aufgeteilt',

    // Embedding Visualizer
    enterWords: 'Wörter zum Vergleichen eingeben',
    addWord: 'Wort hinzufügen',
    similarityScore: 'Ähnlichkeits-Score',
    dimensions: 'Dimensionen',
    nearestNeighbors: 'Nächste Nachbarn',
    vectorSpace: 'Vektorraum',

    // RAG Pipeline Visualizer
    enterQuery: 'Abfrage eingeben',
    sampleQuery: 'Was ist die Hauptstadt von Frankreich?',
    retrieving: 'Abrufen...',
    retrieved: 'Abgerufene Dokumente',
    relevanceScore: 'Relevanz',
    generating: 'Antwort wird generiert...',
    augmentedContext: 'Erweiterter Kontext',

    // Tool Schema Builder
    toolName: 'Tool-Name',
    toolDescription: 'Beschreibung',
    addParameter: 'Parameter hinzufügen',
    paramName: 'Parametername',
    paramType: 'Typ',
    paramRequired: 'Erforderlich',
    generatedSchema: 'Generiertes Schema',
    validateSchema: 'Schema validieren',

    // Memory System Visualizer
    shortTermMemory: 'Kurzzeitgedächtnis',
    longTermMemory: 'Langzeitgedächtnis',
    memoryCapacity: 'Kapazität',
    memoryUsage: 'Auslastung',
    addMemory: 'Erinnerung hinzufügen',
    recallMemory: 'Abrufen',
    memoriesStored: 'Erinnerungen gespeichert',

    // Workflow Visualizer
    addNode: 'Knoten hinzufügen',
    connectNodes: 'Knoten verbinden',
    runWorkflow: 'Workflow ausführen',
    nodeTypes: 'Knotentypen',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Bedingung',

    // Neural Network Visualizer
    inputLayer: 'Eingabeschicht',
    hiddenLayer: 'Versteckte Schicht',
    outputLayer: 'Ausgabeschicht',
    addLayer: 'Schicht hinzufügen',
    removeLayer: 'Schicht entfernen',
    neurons: 'Neuronen',
    activation: 'Aktivierung',
    forward: 'Vorwärts',

    // Gradient Descent Visualizer
    startDescent: 'Abstieg starten',
    pauseDescent: 'Pause',
    resetDescent: 'Zurücksetzen',
    learningRate: 'Lernrate',
    currentLoss: 'Aktueller Verlust',
    iterations: 'Iterationen',
    globalMinimum: 'Globales Minimum',
    localMinimum: 'Lokales Minimum',

    // Training Progress Visualizer
    startTraining: 'Training starten',
    stopTraining: 'Stoppen',
    epoch: 'Epoche',
    trainingLoss: 'Trainingsverlust',
    validationLoss: 'Validierungsverlust',
    accuracy: 'Genauigkeit',
    overfitting: 'Überanpassung erkannt',

    // Prompt Comparison Demo
    weakPrompt: 'Schwacher Prompt',
    strongPrompt: 'Starker Prompt',
    compare: 'Vergleichen',
    promptQuality: 'Qualitäts-Score',
    improvements: 'Verbesserungen',

    // Chain of Thought Demo
    withoutCot: 'Ohne Chain of Thought',
    withCot: 'Mit Chain of Thought',
    reasoningSteps: 'Denkschritte',
    showSteps: 'Schritte anzeigen',

    // Bias Detection Demo
    testInput: 'Testeingabe',
    analyzeForBias: 'Auf Bias analysieren',
    biasIndicators: 'Bias-Indikatoren',
    fairnessScore: 'Fairness-Score',
    recommendations: 'Empfehlungen',
  },

  // Phase 1: LLM Topics
  tokenization: {
    title: 'Tokenization',
    description: 'How LLMs break text into tokens—the fundamental units of language understanding.',
    whatIs: 'What is Tokenization?',
    whatIsDesc: 'Tokenization is the process of converting raw text into a sequence of tokens—the basic units that LLMs process. Tokens can be words, subwords, or even individual characters, depending on the tokenizer.',
    whyMatters: 'Why Tokenization Matters',
    whyMattersDesc: 'Understanding tokenization is crucial because it directly impacts context limits, costs, and model behavior. The same text can have very different token counts across different models.',
    howWorks: 'How It Works',
    howWorksDesc: 'Most modern LLMs use subword tokenization algorithms like BPE (Byte Pair Encoding) or SentencePiece. These algorithms learn common character sequences from training data.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE iteratively merges the most frequent character pairs into single tokens. Common words become single tokens, while rare words are split into subwords.',
    tokenTypes: 'Token Types',
    wholeWords: 'Whole Words',
    wholeWordsDesc: 'Common words like "the", "and", "is" are often single tokens.',
    subwords: 'Subwords',
    subwordsDesc: 'Less common words are split: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Special Tokens',
    specialTokensDesc: 'Markers like <|endoftext|> or [CLS] for model control.',
    interactiveDemo: 'Interactive Demo',
    demoDesc: 'Type text to see how it gets tokenized',
    costImplications: 'Cost Implications',
    costDesc: 'API pricing is typically per-token. Efficient prompts use fewer tokens.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tokens are the atomic units LLMs process—not characters or words',
    takeaway2: 'Different models have different tokenizers and vocabularies',
    takeaway3: 'Non-English text and code often use more tokens than English',
    takeaway4: 'Token count directly affects cost and context window usage',
  },

  embeddings: {
    title: 'Embeddings',
    description: 'How AI represents meaning as vectors in high-dimensional space.',
    whatIs: 'What are Embeddings?',
    whatIsDesc: 'Embeddings are dense vector representations that capture semantic meaning. Similar concepts have similar embeddings, enabling machines to understand relationships between words, sentences, and documents.',
    howWorks: 'How Embeddings Work',
    howWorksDesc: 'Embedding models map discrete tokens to continuous vectors in a high-dimensional space (often 768-4096 dimensions). The position of each vector encodes its semantic meaning.',
    similarity: 'Semantic Similarity',
    similarityDesc: 'Similar meanings cluster together in embedding space. "King" and "Queen" are closer than "King" and "Banana".',
    dimensions: 'Vector Dimensions',
    dimensionsDesc: 'Each dimension captures some aspect of meaning—though these dimensions aren\'t human-interpretable.',
    operations: 'Vector Operations',
    operationsDesc: 'Famous example: King - Man + Woman ≈ Queen. Relationships are encoded as directions in the space.',
    useCases: 'Common Use Cases',
    search: 'Semantic Search',
    searchDesc: 'Find documents by meaning, not just keyword matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Group similar documents, detect topics automatically.',
    classification: 'Classification',
    classificationDesc: 'Categorize text based on embedding similarity to examples.',
    rag: 'RAG Systems',
    ragDesc: 'Retrieve relevant context for LLM prompts.',
    interactiveDemo: 'Interactive Visualization',
    demoDesc: 'Explore how embeddings cluster by meaning',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Embeddings convert text to vectors that capture semantic meaning',
    takeaway2: 'Similar concepts have similar embeddings (cosine similarity)',
    takeaway3: 'Embeddings enable semantic search, clustering, and RAG',
    takeaway4: 'Different embedding models have different strengths and dimensions',
  },

  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: giving LLMs access to external knowledge.',
    whatIs: 'What is RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving relevant documents from a knowledge base and including them in the prompt. This gives models access to up-to-date or specialized information.',
    whyRag: 'Why Use RAG?',
    whyRagDesc: 'LLMs have knowledge cutoffs and can hallucinate. RAG grounds responses in actual documents, reducing hallucination and enabling domain-specific knowledge without fine-tuning.',
    pipeline: 'The RAG Pipeline',
    pipelineDesc: 'RAG systems follow a consistent pattern: embed the query, retrieve relevant chunks, augment the prompt, and generate a response.',
    step1: 'Query Embedding',
    step1Desc: 'Convert the user\'s question into a vector using an embedding model.',
    step2: 'Retrieval',
    step2Desc: 'Search the vector database for chunks similar to the query embedding.',
    step3: 'Augmentation',
    step3Desc: 'Insert retrieved chunks into the prompt as context.',
    step4: 'Generation',
    step4Desc: 'The LLM generates a response grounded in the retrieved context.',
    chunking: 'Document Chunking',
    chunkingDesc: 'Documents are split into smaller chunks (typically 200-1000 tokens) for embedding and retrieval. Chunk size affects retrieval precision.',
    vectorDbs: 'Vector Databases',
    vectorDbsDesc: 'Specialized databases like Pinecone, Weaviate, or pgvector enable fast similarity search over millions of embeddings.',
    interactiveDemo: 'Interactive RAG Pipeline',
    demoDesc: 'See how queries flow through a RAG system',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'RAG retrieves relevant documents and includes them in the prompt',
    takeaway2: 'It reduces hallucination by grounding responses in actual sources',
    takeaway3: 'Chunking strategy and embedding quality are critical for good retrieval',
    takeaway4: 'RAG is often preferable to fine-tuning for adding domain knowledge',
  },

  // Phase 2: Agent Topics
  toolDesign: {
    title: 'Tool Design',
    description: 'Best practices for designing effective tools that AI agents can use reliably.',
    whatIs: 'What Makes a Good Tool?',
    whatIsDesc: 'Well-designed tools are the foundation of capable AI agents. A tool\'s schema, naming, and documentation directly impact how reliably an LLM can use it.',
    principles: 'Design Principles',
    principlesDesc: 'Follow these principles to create tools that agents can use effectively.',
    principle1: 'Clear Naming',
    principle1Desc: 'Use descriptive, unambiguous names. "search_web" is better than "sw" or "query".',
    principle2: 'Explicit Parameters',
    principle2Desc: 'Every parameter should have a clear type, description, and constraints.',
    principle3: 'Predictable Outputs',
    principle3Desc: 'Return consistent, structured responses. Include error messages in the output.',
    principle4: 'Minimal Scope',
    principle4Desc: 'Each tool should do one thing well. Prefer many focused tools over few complex ones.',
    schemaDesign: 'Schema Design',
    schemaDesignDesc: 'Tool schemas tell the LLM how to use your tools. Good schemas prevent errors.',
    goodSchema: 'Good Schema',
    badSchema: 'Bad Schema',
    errorHandling: 'Error Handling',
    errorHandlingDesc: 'Tools should handle errors gracefully and return informative messages the LLM can act on.',
    interactiveDemo: 'Tool Schema Builder',
    demoDesc: 'Build and validate tool schemas interactively',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Tool design directly impacts agent reliability',
    takeaway2: 'Explicit schemas with descriptions prevent LLM confusion',
    takeaway3: 'Return structured errors the agent can understand and act on',
    takeaway4: 'Test tools with various inputs to find edge cases',
  },

  memorySystems: {
    title: 'Memory Systems',
    description: 'How AI agents maintain context and remember information across interactions.',
    whatIs: 'What are Agent Memory Systems?',
    whatIsDesc: 'Memory systems allow agents to retain and recall information beyond the immediate context window. They enable agents to learn from past interactions and maintain coherent long-term behavior.',
    types: 'Types of Memory',
    typesDesc: 'Agent memory systems typically combine multiple memory types for different purposes.',
    shortTerm: 'Short-Term Memory',
    shortTermDesc: 'The current conversation context. Limited by context window size.',
    longTerm: 'Long-Term Memory',
    longTermDesc: 'Persistent storage of past interactions, facts, and learned preferences.',
    episodic: 'Episodic Memory',
    episodicDesc: 'Specific past events and interactions that can be recalled.',
    semantic: 'Semantic Memory',
    semanticDesc: 'General knowledge and facts extracted from experiences.',
    implementation: 'Implementation Approaches',
    implementationDesc: 'Various techniques for implementing agent memory.',
    vectorStore: 'Vector Stores',
    vectorStoreDesc: 'Store embeddings of past interactions for semantic retrieval.',
    summaries: 'Conversation Summaries',
    summariesDesc: 'Periodically summarize long conversations to preserve key information.',
    keyValue: 'Key-Value Stores',
    keyValueDesc: 'Store explicit facts and user preferences for direct lookup.',
    interactiveDemo: 'Memory System Visualizer',
    demoDesc: 'See how different memory types work together',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Memory extends agent capabilities beyond the context window',
    takeaway2: 'Combine multiple memory types for best results',
    takeaway3: 'Memory retrieval adds latency—balance comprehensiveness with speed',
    takeaway4: 'Consider privacy and data retention when storing memories',
  },

  orchestration: {
    title: 'Orchestration',
    description: 'Coordinating multiple agents and complex multi-step workflows.',
    whatIs: 'What is Agent Orchestration?',
    whatIsDesc: 'Orchestration is the coordination of multiple AI agents or complex multi-step workflows. It involves routing tasks, managing state, handling failures, and combining agent outputs.',
    patterns: 'Orchestration Patterns',
    patternsDesc: 'Common patterns for structuring multi-agent systems.',
    sequential: 'Sequential Pipeline',
    sequentialDesc: 'Agents run in order, each processing the output of the previous one.',
    parallel: 'Parallel Execution',
    parallelDesc: 'Multiple agents work simultaneously on different aspects of a task.',
    hierarchical: 'Hierarchical',
    hierarchicalDesc: 'A supervisor agent delegates to specialized worker agents.',
    dynamic: 'Dynamic Routing',
    dynamicDesc: 'An LLM decides which agent should handle each request.',
    stateManagement: 'State Management',
    stateManagementDesc: 'Orchestrators must track progress, intermediate results, and handle failures.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Save state at key points to enable recovery from failures.',
    rollback: 'Rollback',
    rollbackDesc: 'Ability to undo steps when errors occur.',
    interactiveDemo: 'Workflow Visualizer',
    demoDesc: 'Design and visualize agent workflows',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Orchestration enables complex tasks through agent composition',
    takeaway2: 'Choose patterns based on task dependencies and parallelism',
    takeaway3: 'Robust state management is essential for reliability',
    takeaway4: 'Monitor orchestration costs—multi-agent systems multiply API calls',
  },

  evaluation: {
    title: 'Evaluation',
    description: 'Measuring and improving AI agent performance systematically.',
    whatIs: 'Why Evaluate Agents?',
    whatIsDesc: 'Agent evaluation is critical for understanding performance, catching regressions, and improving reliability. Without measurement, you\'re flying blind.',
    metrics: 'Key Metrics',
    metricsDesc: 'Important metrics to track for agent systems.',
    taskSuccess: 'Task Success Rate',
    taskSuccessDesc: 'Percentage of tasks completed correctly.',
    efficiency: 'Efficiency',
    efficiencyDesc: 'Steps taken, tokens used, time elapsed per task.',
    accuracy: 'Accuracy',
    accuracyDesc: 'Correctness of agent outputs and decisions.',
    reliability: 'Reliability',
    reliabilityDesc: 'Consistency across repeated runs of the same task.',
    approaches: 'Evaluation Approaches',
    approachesDesc: 'Different ways to evaluate agent performance.',
    unitTests: 'Unit Tests',
    unitTestsDesc: 'Test individual tools and components in isolation.',
    integration: 'Integration Tests',
    integrationDesc: 'Test the full agent loop with mock environments.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard task suites for comparing agents.',
    humanEval: 'Human Evaluation',
    humanEvalDesc: 'Expert review for nuanced quality assessment.',
    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Guidelines for effective agent evaluation.',
    practice1: 'Test edge cases and failure modes, not just happy paths.',
    practice2: 'Track costs alongside quality metrics.',
    practice3: 'Use versioned evaluations to catch regressions.',
    practice4: 'Include adversarial tests for security.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Evaluation is essential—unmeasured systems can\'t be improved',
    takeaway2: 'Combine automated tests with human evaluation',
    takeaway3: 'Track multiple metrics: success, efficiency, cost',
    takeaway4: 'Build evaluation into your development workflow',
  },

  // Phase 3: ML Fundamentals
  neuralNetworks: {
    title: 'Neural Networks',
    description: 'The foundational architecture that powers modern AI.',
    whatIs: 'What is a Neural Network?',
    whatIsDesc: 'A neural network is a computational model inspired by the brain. It consists of layers of interconnected nodes (neurons) that learn to transform inputs into outputs through training.',
    components: 'Core Components',
    componentsDesc: 'The building blocks of neural networks.',
    neurons: 'Neurons',
    neuronsDesc: 'Basic units that compute weighted sums of inputs and apply activation functions.',
    layers: 'Layers',
    layersDesc: 'Groups of neurons: input layer, hidden layers, and output layer.',
    weights: 'Weights & Biases',
    weightsDesc: 'Learnable parameters that determine how inputs are transformed.',
    activations: 'Activation Functions',
    activationsDesc: 'Non-linear functions that allow networks to learn complex patterns.',
    typesOfNetworks: 'Types of Networks',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Information flows one direction. Good for tabular data.',
    cnn: 'Convolutional (CNN)',
    cnnDesc: 'Specialized for images and spatial data.',
    rnn: 'Recurrent (RNN)',
    rnnDesc: 'Processes sequences with memory of past inputs.',
    transformer: 'Transformer',
    transformerDesc: 'Attention-based architecture powering modern LLMs.',
    interactiveDemo: 'Neural Network Visualizer',
    demoDesc: 'Build and explore network architectures',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Neural networks learn by adjusting weights through training',
    takeaway2: 'Depth (more layers) enables learning hierarchical features',
    takeaway3: 'Different architectures suit different data types',
    takeaway4: 'Modern LLMs are massive transformer networks',
  },

  gradientDescent: {
    title: 'Gradient Descent',
    description: 'The optimization algorithm that enables neural networks to learn.',
    whatIs: 'What is Gradient Descent?',
    whatIsDesc: 'Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize a loss function. It\'s how neural networks learn from data.',
    intuition: 'The Intuition',
    intuitionDesc: 'Imagine standing in a hilly landscape blindfolded, trying to reach the lowest point. You feel the slope beneath your feet and step downhill. Repeat until you reach a valley.',
    howWorks: 'How It Works',
    howWorksDesc: 'The algorithm computes how much each parameter contributes to the error, then adjusts parameters in the opposite direction.',
    step1: 'Compute Loss',
    step1Desc: 'Measure how wrong the current predictions are.',
    step2: 'Calculate Gradients',
    step2Desc: 'Use backpropagation to find how each weight affects the loss.',
    step3: 'Update Weights',
    step3Desc: 'Adjust weights in the direction that reduces loss.',
    step4: 'Repeat',
    step4Desc: 'Iterate until the loss stops decreasing.',
    learningRate: 'Learning Rate',
    learningRateDesc: 'Controls how big each step is. Too high: overshoot. Too low: slow progress.',
    variants: 'Variants',
    sgd: 'Stochastic Gradient Descent',
    sgdDesc: 'Uses random mini-batches instead of the full dataset.',
    momentum: 'Momentum',
    momentumDesc: 'Accumulates velocity to push through local minima.',
    adam: 'Adam',
    adamDesc: 'Adaptive learning rates per parameter. Most common today.',
    interactiveDemo: 'Gradient Descent Visualizer',
    demoDesc: 'Watch gradient descent find the minimum',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Gradient descent minimizes loss by following the slope',
    takeaway2: 'Learning rate is the most important hyperparameter',
    takeaway3: 'Modern optimizers like Adam adapt learning rates automatically',
    takeaway4: 'Backpropagation computes gradients efficiently',
  },

  training: {
    title: 'Training Process',
    description: 'How neural networks learn from data through iterative optimization.',
    whatIs: 'What is Training?',
    whatIsDesc: 'Training is the process of teaching a neural network to perform a task by exposing it to examples and adjusting its parameters based on errors.',
    phases: 'Training Phases',
    phasesDesc: 'The stages of training a neural network.',
    initialization: 'Initialization',
    initializationDesc: 'Set random starting weights. Good initialization helps training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Input flows through the network to produce predictions.',
    lossCalc: 'Loss Calculation',
    lossCalcDesc: 'Compare predictions to ground truth with a loss function.',
    backprop: 'Backpropagation',
    backpropDesc: 'Compute gradients of loss with respect to each weight.',
    optimization: 'Optimization',
    optimizationDesc: 'Update weights using gradient descent.',
    concepts: 'Key Concepts',
    epoch: 'Epoch',
    epochDesc: 'One complete pass through the entire training dataset.',
    batch: 'Batch Size',
    batchDesc: 'Number of examples processed before updating weights.',
    overfitting: 'Overfitting',
    overfittingDesc: 'Model memorizes training data but fails on new data.',
    regularization: 'Regularization',
    regularizationDesc: 'Techniques to prevent overfitting (dropout, weight decay).',
    interactiveDemo: 'Training Progress Visualizer',
    demoDesc: 'Watch a network learn in real-time',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Training iteratively reduces prediction errors',
    takeaway2: 'Overfitting is the main enemy—always validate on held-out data',
    takeaway3: 'Batch size and learning rate significantly affect training',
    takeaway4: 'Modern LLMs require massive compute for training',
  },

  // Phase 3: Prompting
  promptBasics: {
    title: 'Prompt Basics',
    description: 'Fundamentals of writing effective prompts for AI models.',
    whatIs: 'What is a Prompt?',
    whatIsDesc: 'A prompt is the input you give to an LLM. The quality of your prompt directly determines the quality of the response. Prompting is both an art and a science.',
    principles: 'Core Principles',
    principlesDesc: 'Fundamental guidelines for effective prompts.',
    beSpecific: 'Be Specific',
    beSpecificDesc: 'Vague prompts get vague answers. Include relevant details and constraints.',
    showExamples: 'Show Examples',
    showExamplesDesc: 'Demonstrate the format and style you want with concrete examples.',
    giveContext: 'Provide Context',
    giveContextDesc: 'Background information helps the model understand your needs.',
    setFormat: 'Specify Format',
    setFormatDesc: 'Tell the model exactly how you want the output structured.',
    anatomy: 'Anatomy of a Prompt',
    anatomyDesc: 'The components that make up an effective prompt.',
    role: 'Role/Persona',
    roleDesc: 'Who the model should act as.',
    task: 'Task Description',
    taskDesc: 'What you want the model to do.',
    context: 'Context/Background',
    contextDesc: 'Relevant information for the task.',
    format: 'Output Format',
    formatDesc: 'How you want the response structured.',
    interactiveDemo: 'Prompt Comparison',
    demoDesc: 'Compare weak vs. strong prompts',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Clear, specific prompts yield better results',
    takeaway2: 'Examples are powerful—show, don\'t just tell',
    takeaway3: 'Iterate on prompts; first attempts are rarely optimal',
    takeaway4: 'Consider the model\'s perspective when crafting prompts',
  },

  advancedPrompting: {
    title: 'Advanced Techniques',
    description: 'Sophisticated prompting strategies for complex tasks.',
    overview: 'Beyond the Basics',
    overviewDesc: 'Advanced techniques unlock more capable and reliable AI behavior for complex tasks.',
    cot: 'Chain of Thought',
    cotDesc: 'Encourage step-by-step reasoning by asking the model to "think through" problems.',
    cotExample: 'Example: "Let\'s solve this step by step..."',
    fewShot: 'Few-Shot Learning',
    fewShotDesc: 'Provide multiple examples to establish patterns the model should follow.',
    fewShotExample: 'Include 3-5 diverse examples covering edge cases.',
    selfConsistency: 'Self-Consistency',
    selfConsistencyDesc: 'Generate multiple responses and select the most consistent answer.',
    selfConsistencyExample: 'Useful for math, logic, and factual questions.',
    decomposition: 'Task Decomposition',
    decompositionDesc: 'Break complex tasks into smaller, manageable sub-tasks.',
    decompositionExample: 'Solve sub-tasks independently, then combine results.',
    techniques: 'Additional Techniques',
    rolePlay: 'Role Assignment',
    rolePlayDesc: 'Assign a specific expert persona to focus the model\'s knowledge.',
    constraints: 'Explicit Constraints',
    constraintsDesc: 'List what the model should NOT do to prevent common errors.',
    verification: 'Self-Verification',
    verificationDesc: 'Ask the model to check its own work for errors.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'See how reasoning steps improve outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Chain of thought dramatically improves reasoning tasks',
    takeaway2: 'Few-shot examples establish reliable patterns',
    takeaway3: 'Task decomposition handles complexity',
    takeaway4: 'Combine techniques for best results',
  },

  systemPrompts: {
    title: 'System Prompts',
    description: 'Configuring AI behavior through system-level instructions.',
    whatIs: 'What is a System Prompt?',
    whatIsDesc: 'A system prompt is a special instruction that sets the context, persona, and behavioral guidelines for an AI model. It\'s typically hidden from users and persists throughout a conversation.',
    purpose: 'Purpose of System Prompts',
    purposeDesc: 'System prompts establish the foundation for how the AI should behave.',
    setPersona: 'Define Persona',
    setPersonaDesc: 'Establish who the AI is: an assistant, expert, character, etc.',
    setBoundaries: 'Set Boundaries',
    setBoundariesDesc: 'Define what the AI should and shouldn\'t do.',
    establishTone: 'Establish Tone',
    establishToneDesc: 'Specify communication style: formal, casual, technical.',
    provideKnowledge: 'Provide Context',
    provideKnowledgeDesc: 'Include domain knowledge or rules specific to your application.',
    structure: 'Structure of Effective System Prompts',
    structureDesc: 'Well-organized system prompts are easier for models to follow.',
    identity: 'Identity Section',
    identityDesc: 'Who is the AI? What is its role?',
    capabilities: 'Capabilities',
    capabilitiesDesc: 'What can the AI do? What tools does it have?',
    limitations: 'Limitations',
    limitationsDesc: 'What should the AI avoid or refuse?',
    guidelines: 'Guidelines',
    guidelinesDesc: 'Specific rules for behavior and responses.',
    bestPractices: 'Best Practices',
    practice1: 'Be explicit about edge cases and error handling.',
    practice2: 'Test system prompts with adversarial inputs.',
    practice3: 'Version control your system prompts.',
    practice4: 'Keep prompts focused—don\'t overload with instructions.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'System prompts define the AI\'s persona and behavior',
    takeaway2: 'Structure prompts clearly: identity, capabilities, limitations',
    takeaway3: 'Test with edge cases—users will find them',
    takeaway4: 'System prompts can be overridden—don\'t rely solely on them for security',
  },

  // Phase 3: AI Safety
  alignment: {
    title: 'Alignment',
    description: 'Ensuring AI systems behave in accordance with human values and intentions.',
    whatIs: 'What is AI Alignment?',
    whatIsDesc: 'AI alignment is the challenge of ensuring AI systems do what we actually want them to do. It\'s about building systems that are helpful, harmless, and honest.',
    whyMatters: 'Why Alignment Matters',
    whyMattersDesc: 'As AI systems become more powerful, ensuring they remain aligned with human values becomes increasingly critical.',
    concepts: 'Key Concepts',
    conceptsDesc: 'Fundamental ideas in AI alignment research.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'The training objective correctly captures what we want.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'The learned model actually optimizes for the training objective.',
    specification: 'Specification Problem',
    specificationDesc: 'Difficulty of precisely stating what we want in all situations.',
    robustness: 'Robustness',
    robustnessDesc: 'Maintaining alignment under distribution shift and adversarial pressure.',
    techniques: 'Alignment Techniques',
    rlhf: 'RLHF',
    rlhfDesc: 'Reinforcement Learning from Human Feedback—learn from human preferences.',
    constitutionalAi: 'Constitutional AI',
    constitutionalAiDesc: 'Self-critique based on a set of principles.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarial testing to find failure modes.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Alignment ensures AI does what we actually want',
    takeaway2: 'Specifying human values precisely is fundamentally hard',
    takeaway3: 'RLHF and Constitutional AI are current best practices',
    takeaway4: 'Alignment is an ongoing research challenge, not a solved problem',
  },

  bias: {
    title: 'Bias & Fairness',
    description: 'Understanding and mitigating harmful biases in AI systems.',
    whatIs: 'What is AI Bias?',
    whatIsDesc: 'AI bias occurs when machine learning systems produce systematically unfair outcomes for certain groups. Biases can arise from training data, model design, or deployment context.',
    sources: 'Sources of Bias',
    sourcesDesc: 'Where bias enters AI systems.',
    dataBias: 'Training Data',
    dataBiasDesc: 'Historical biases in the data are learned by the model.',
    labelBias: 'Label Bias',
    labelBiasDesc: 'Human annotators introduce their own biases.',
    selectionBias: 'Selection Bias',
    selectionBiasDesc: 'Training data doesn\'t represent the deployment population.',
    measurementBias: 'Measurement Bias',
    measurementBiasDesc: 'Proxies used for measurement encode bias.',
    types: 'Types of Bias',
    typesDesc: 'Common categories of bias in AI systems.',
    stereotyping: 'Stereotyping',
    stereotypingDesc: 'Reinforcing harmful stereotypes about groups.',
    erasure: 'Erasure',
    erasureDesc: 'Underrepresenting or ignoring certain groups.',
    disparateImpact: 'Disparate Impact',
    disparateImpactDesc: 'Different outcomes for different groups.',
    mitigation: 'Mitigation Strategies',
    mitigationDesc: 'Approaches to reduce bias.',
    diverseData: 'Diverse Data',
    diverseDataDesc: 'Ensure training data represents all relevant groups.',
    auditing: 'Bias Auditing',
    auditingDesc: 'Systematically test for bias across demographics.',
    constraints: 'Fairness Constraints',
    constraintsDesc: 'Incorporate fairness metrics into training.',
    interactiveDemo: 'Bias Detection Demo',
    demoDesc: 'Explore how bias manifests in model outputs',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Bias is often inherited from training data',
    takeaway2: 'Different fairness metrics can conflict—choose carefully',
    takeaway3: 'Regular auditing is essential for deployed systems',
    takeaway4: 'Bias mitigation is an ongoing process, not a one-time fix',
  },

  responsibleAi: {
    title: 'Responsible AI',
    description: 'Building and deploying AI systems ethically and sustainably.',
    whatIs: 'What is Responsible AI?',
    whatIsDesc: 'Responsible AI encompasses the practices, policies, and principles that ensure AI systems are developed and used ethically, safely, and in ways that benefit society.',
    pillars: 'Pillars of Responsible AI',
    pillarsDesc: 'Core principles guiding responsible AI development.',
    transparency: 'Transparency',
    transparencyDesc: 'Be open about AI capabilities, limitations, and decision-making.',
    accountability: 'Accountability',
    accountabilityDesc: 'Clear ownership and responsibility for AI outcomes.',
    privacy: 'Privacy',
    privacyDesc: 'Protect user data and respect privacy rights.',
    safety: 'Safety',
    safetyDesc: 'Ensure systems are robust and don\'t cause harm.',
    practices: 'Responsible Practices',
    practicesDesc: 'Concrete steps for responsible AI development.',
    documentation: 'Documentation',
    documentationDesc: 'Document model capabilities, training data, and known limitations.',
    testing: 'Comprehensive Testing',
    testingDesc: 'Test for safety, bias, and edge cases before deployment.',
    monitoring: 'Ongoing Monitoring',
    monitoringDesc: 'Track system behavior in production for issues.',
    feedback: 'User Feedback',
    feedbackDesc: 'Create channels for users to report problems.',
    considerations: 'Ethical Considerations',
    environmental: 'Environmental Impact',
    environmentalDesc: 'AI training has significant carbon footprint.',
    labor: 'Labor Implications',
    laborDesc: 'Consider impact on workers and employment.',
    access: 'Equitable Access',
    accessDesc: 'Ensure AI benefits are broadly distributed.',
    keyTakeaways: 'Key Takeaways',
    takeaway1: 'Responsible AI requires proactive effort throughout the lifecycle',
    takeaway2: 'Transparency builds trust and enables accountability',
    takeaway3: 'Consider societal impact beyond immediate users',
    takeaway4: 'Ethics are not optional—integrate them into development processes',
  },
}
