import type { Dictionary } from './en'

export const de: Dictionary = {
  // Common UI
  common: {
    learnAi: 'Lerne KI',
    interactiveGuide: 'Interaktiver Leitfaden',
    topics: 'Themen',
    search: 'Suchen...',
    searchTopics: 'Themen suchen...',
    startTyping: 'Beginne zu tippen, um Themen zu suchen...',
    trySearching: 'Versuche "Temperature" oder "Attention"',
    noResults: 'Keine Ergebnisse gefunden für',
    pressEsc: 'ESC zum Schließen',
    enterToSelect: 'Enter zum Auswählen',
    previous: 'Zurück',
    next: 'Weiter',
    projectBy: 'Ein Projekt von',
    proTip: 'Profi-Tipp: Drücke',
    toSearchTopics: 'um Themen zu suchen',
    interactiveAiLearning: 'Interaktives KI-Lernen',
    guidesDescription: 'Interaktive Anleitungen zum Verstehen von KI-Konzepten',
    backToAllTopics: 'Zurück zu allen Themen',
  },

  // Home page
  home: {
    heroTitle1: 'KI-Konzepte meistern',
    heroTitle2: 'Durch Erfahrung',
    heroDescription: 'Erkunde künstliche Intelligenz und große Sprachmodelle durch schöne, interaktive Demonstrationen. Lerne durch Handeln, nicht nur durch Lesen.',
    startLearning: 'Jetzt lernen',
    browseTopics: 'Themen durchsuchen',
    exploreTopics: 'Themen erkunden',
    diveIntoLessons: 'Tauche ein in interaktive Lektionen',
  },

  // Features
  features: {
    interactiveDemos: 'Interaktive Demos',
    interactiveDemosDesc: 'Praktische Erkundungen, die abstrakte Konzepte greifbar und intuitiv machen.',
    visualLearning: 'Visuelles Lernen',
    visualLearningDesc: 'Schöne Visualisierungen, die zeigen, wie KI-Systeme tatsächlich funktionieren.',
    buildIntuition: 'Intuition aufbauen',
    buildIntuitionDesc: 'Gehe über das Auswendiglernen hinaus – entwickle tiefes Verständnis durch Experimentieren.',
  },

  // Topic categories
  categories: {
    ai: 'Künstliche Intelligenz',
    agents: 'KI-Agenten',
    llm: 'Große Sprachmodelle',
    mlFundamentals: 'ML-Grundlagen',
    prompting: 'Prompting',
    safety: 'KI-Sicherheit',
    industry: 'KI-Industrie',
  },

  // Category descriptions (for category landing pages)
  categoryDescriptions: {
    agents: 'Lerne, wie man autonome KI-Systeme baut, die mit Tools und Speicher denken, planen und handeln können.',
    llm: 'Verstehe die inneren Abläufe großer Sprachmodelle, von der Tokenisierung bis zu Aufmerksamkeitsmechanismen.',
    'ml-fundamentals': 'Beherrsche die grundlegenden Konzepte des maschinellen Lernens, die moderne KI-Systeme antreiben.',
    prompting: 'Entdecke Techniken, um effektiv mit KI-Modellen zu kommunizieren und bessere Ergebnisse zu erzielen.',
    safety: 'Erkunde ethische Überlegungen und Best Practices für den Bau verantwortungsvoller KI-Systeme.',
    industry: 'Erkunde die Unternehmen und Trends, die die KI-Industrielandschaft prägen.',
  },

  // Topic names
  topicNames: {
    // Agent subcategories
    'agents-core': 'Kernkonzepte',
    'agents-building': 'Bausteine',
    'agents-patterns': 'Muster',
    'agents-quality': 'Qualität & Sicherheit',
    // Agent topics
    'agent-loop': 'Der Agenten-Zyklus',
    'agent-context': 'Kontext-Anatomie',
    'agent-problems': 'Agenten-Probleme',
    'agent-security': 'Agenten-Sicherheit',
    'agentic-patterns': 'Agentische Muster',
    'mcp': 'MCP (Model Context Protocol)',
    'tool-design': 'Tool-Design',
    'memory': 'Speichersysteme',
    'orchestration': 'Orchestrierung',
    'evaluation': 'Evaluierung',
    'skills': 'Agenten-Skills',
    // LLM subcategories
    'llm-fundamentals': 'Grundlagen',
    'llm-behavior': 'Verhalten',
    'llm-capabilities': 'Fähigkeiten',
    'llm-architecture': 'Architektur',
    // LLM topics
    'tokenization': 'Tokenisierung',
    'embeddings': 'Einbettungen',
    'rag': 'RAG (Retrieval Augmented Generation)',
    'context-rot': 'Kontextverfall',
    'temperature': 'Temperatur',
    'attention': 'Aufmerksamkeits-Mechanismus',
    'vision': 'Bildverarbeitung',
    'visual-challenges': 'Visuelle Herausforderungen',
    'llm-training': 'LLM-Training',
    'moe': 'Mixture of Experts',
    // ML Fundamentals
    'ml-fundamentals': 'ML-Grundlagen',
    'neural-networks': 'Neuronale Netzwerke',
    'gradient-descent': 'Gradientenabstieg',
    'training': 'Trainingsprozess',
    // Prompting
    'prompt-basics': 'Prompt-Grundlagen',
    'advanced-prompting': 'Fortgeschrittenes Prompting',
    'system-prompts': 'System-Prompts',
    // Safety
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Verantwortungsvolle KI',
    // Industry
    'european-ai': 'KI aus Europa',
    'open-source': 'Open-Source-Vorteile',
    'opus-4-5': 'Logges Lieblingsmodell',
  },

  // Temperature page
  temperature: {
    title: 'Temperatur',
    description: 'Verstehe, wie ein einzelner Parameter die Balance zwischen vorhersagbarer Logik und kreativer Zufälligkeit in KI-Ausgaben steuert.',
    whatIs: 'Was ist Temperatur?',
    whatIsDesc: 'In LLMs ist Temperatur ein Hyperparameter, der die "Logits" (Rohwerte) der nächsten Token-Vorhersagen skaliert, bevor sie in Wahrscheinlichkeiten umgewandelt werden. Er steuert im Wesentlichen, wie stark das Modell die wahrscheinlichsten Optionen gegenüber weniger wahrscheinlichen bevorzugt.',
    lowTemp: 'Niedrige Temperatur',
    lowTempDesc: 'Konzentriert sich auf die Top-Ergebnisse. Zuverlässig, konsistent und faktisch. Ideal für Code, Mathematik und strukturierte Daten.',
    highTemp: 'Hohe Temperatur',
    highTempDesc: 'Verteilt Wahrscheinlichkeit auf mehr Tokens. Vielfältig, kreativ und überraschend. Ideal für Geschichten, Brainstorming und Poesie.',
    interactiveDistribution: 'Interaktive Verteilung',
    adjustSlider: 'Stelle die Temperatur ein, um den Effekt zu sehen',
    adjustDesc: 'Bewege den Temperaturregler, um zu sehen, wie er die Wahrscheinlichkeitsverteilung für das nächste Token umformt. Beobachte, wie "the" (die wahrscheinlichste Wahl) bei niedrigen Temperaturen dominiert und bei steigender Temperatur seinen Vorsprung verliert.',
    howItWorks: 'Wie es mathematisch funktioniert',
    mathDesc: 'Das Modell generiert einen Score für jedes mögliche Token. Um Wahrscheinlichkeiten zu erhalten, verwenden wir die Softmax-Funktion, modifiziert durch die Temperatur:',
    whenLow: 'Wenn T → 0',
    low: 'Niedrig',
    whenLowDesc: 'Division durch ein kleines T verstärkt die Unterschiede zwischen den Scores. Der höchste Logit dominiert exponentiell.',
    whenHigh: 'Wenn T → ∞',
    high: 'Hoch',
    whenHighDesc: 'Division durch ein großes T komprimiert alle Scores gegen Null, wodurch sie nach der Exponentialfunktion nahezu gleich werden.',
    practicalGuidelines: 'Praktische Richtlinien',
    useCase: 'Anwendungsfall',
    tempLabel: 'Temperatur',
    why: 'Warum?',
    codingMath: 'Programmierung & Mathematik',
    codingMathWhy: 'Fehler in der Logik sind kostspielig; du willst den wahrscheinlichsten korrekten Pfad.',
    factRetrieval: 'Faktenabfrage',
    factRetrievalWhy: 'Reduziert "Halluzinationen", indem es sich an die wahrscheinlichsten Datenpunkte hält.',
    generalChat: 'Allgemeiner Chat',
    generalChatWhy: 'Der "Sweet Spot" für die meisten Modelle, um natürlich und hilfreich zu klingen.',
    creativeWriting: 'Kreatives Schreiben',
    creativeWritingWhy: 'Ermutigt das Modell, interessanteres, vielfältigeres Vokabular zu verwenden.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generiert wilde, unkonventionelle Ideen, die Inspiration wecken könnten.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Temperatur 0 ist deterministisch ("Greedy Search") – wählt immer das Top-Token',
    takeaway2: 'Höhere Temperatur erhöht Vielfalt und Kreativität, verringert aber die Kohärenz',
    takeaway3: 'Zu hohe Temperatur (> 1.5) führt oft zu Kauderwelsch',
    takeaway4: 'Passe die Temperatur immer an die Anforderungen der Aufgabe bezüglich Präzision vs. Kreativität an',
  },

  // Context Rot page
  contextRot: {
    title: 'Kontextverfall',
    description: 'Verstehe, wie Informationen über lange Gespräche degradieren und warum LLMs mit erweiterten Kontexten kämpfen.',
    whatIs: 'Was ist Kontextverfall?',
    whatIsDesc: 'bezieht sich auf die allmähliche Verschlechterung der Fähigkeit eines LLMs, Informationen aus früheren Teilen eines langen Gesprächs oder Dokuments genau abzurufen und zu nutzen. Mit wachsendem Kontext wird die Aufmerksamkeit des Modells verwässert.',
    whyHappens: 'Warum passiert das?',
    whyHappensDesc: 'LLMs haben begrenzte Kontextfenster und nutzen Aufmerksamkeitsmechanismen, die den Fokus auf alle Tokens verteilen müssen. Bei längeren Gesprächen konkurrieren frühere Informationen mit neuerem Inhalt um die begrenzte Aufmerksamkeitskapazität des Modells.',
    // Reasons
    reason1Title: 'Begrenzte Kontextfenster',
    reason1Desc: 'LLMs haben begrenzte Kontextfenster und nutzen Aufmerksamkeitsmechanismen, die den Fokus auf alle Tokens verteilen müssen. Bei längeren Gesprächen konkurrieren frühere Informationen mit neuerem Inhalt um die begrenzte Aufmerksamkeitskapazität des Modells.',
    reason2Title: 'Aufmerksamkeitsverdünnung',
    reason2Desc: 'Der Aufmerksamkeitsmechanismus des Modells verteilt sich auf alle Tokens. Mehr Inhalt bedeutet, dass jeder Token proportional weniger Aufmerksamkeit erhält.',
    reason3Title: 'Aktualitätsbias',
    reason3Desc: 'Transformer neigen dazu, neuere Tokens stärker zu gewichten. Anweisungen am Anfang werden natürlich weniger einflussreich.',
    symptoms: 'Häufige Symptome',
    symptom1: 'Vergessen von Anweisungen vom Anfang eines Gesprächs',
    symptom2: 'Widerspruch zu früheren Aussagen oder Entscheidungen',
    symptom3: 'Verlust des Überblicks bei komplexen Mehrstufenaufgaben',
    symptom4: 'Verwechslung von Details aus verschiedenen Teilen des Kontexts',
    mitigation: 'Gegenmaßnahmen',
    mitigation1Title: 'Periodische Anweisungsverstärkung',
    mitigation1: 'Fasse wichtigen Kontext regelmäßig zusammen',
    mitigation2Title: 'Gesprächszusammenfassung',
    mitigation2: 'Platziere kritische Anweisungen sowohl am Anfang als auch am Ende',
    mitigation3Title: 'Hierarchischer Speicher',
    mitigation3: 'Nutze externe Speichersysteme, um relevanten Kontext bei Bedarf zu speichern und abzurufen.',
    mitigation4Title: 'Anweisungsverankerung',
    mitigation4: 'Platziere kritische Anweisungen sowohl am Anfang als auch am Ende deines Prompts, um sie zu verstärken.',
    mitigation5Title: 'Kürzere Aufgabenketten',
    mitigation5: 'Teile lange Aufgaben in kleinere, fokussierte Gespräche auf.',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Sieh, wie das Gedächtnis mit zunehmender Kontextlänge verblasst',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextverfall ist eine inhärente Einschränkung aktueller LLM-Architekturen',
    takeaway2: 'Der "Lost in the Middle"-Effekt bedeutet, dass Informationen am Anfang und Ende besser erinnert werden',
    takeaway3: 'Strategische Informationsplatzierung kann den Abruf erheblich verbessern',
    takeaway4: 'Regelmäßiges Zusammenfassen hilft, wichtigen Kontext über lange Gespräche zu erhalten',
  },

  // Attention page
  attention: {
    title: 'Aufmerksamkeits-Mechanismus',
    description: 'Erkunde, wie Transformer durch den leistungsstarken Aufmerksamkeitsmechanismus auf relevante Teile der Eingabe fokussieren.',
    whatIs: 'Was ist Aufmerksamkeit?',
    whatIsDesc: 'Aufmerksamkeit ist der Kernmechanismus, der es Transformern ermöglicht, die Wichtigkeit verschiedener Teile der Eingabe bei der Generierung jedes Ausgabe-Tokens zu gewichten. Es ermöglicht dem Modell, sich auf relevanten Kontext zu "konzentrieren".',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Für jede Position berechnet das Modell Query-, Key- und Value-Vektoren. Aufmerksamkeits-Scores werden durch Vergleich von Queries mit Keys berechnet und dann verwendet, um eine gewichtete Summe der Values zu erstellen.',
    selfAttention: 'Selbst-Aufmerksamkeit',
    selfAttentionDesc: 'Ermöglicht jedem Token, auf alle anderen Tokens in der Sequenz zu achten und Beziehungen unabhängig von der Entfernung zu erfassen.',
    multiHead: 'Multi-Head-Aufmerksamkeit',
    multiHeadDesc: 'Mehrere Aufmerksamkeitsköpfe ermöglichen es dem Modell, sich gleichzeitig auf verschiedene Arten von Beziehungen zu konzentrieren.',
    // Interactive section
    interactiveTitle: 'Interaktive Aufmerksamkeitskarte',
    interactiveDesc: 'Bewege den Mauszeiger, um Aufmerksamkeitsmuster zu erkunden',
    interactiveExplain: 'Bewege den Mauszeiger über verschiedene Wörter in den Sätzen unten. Die Hervorhebung zeigt, wohin das Modell "schaut", um dieses spezifische Wort zu verstehen.',
    // QKV section
    qkvTitle: 'Die drei Schlüssel: Query, Key und Value',
    queryTitle: 'Query',
    queryDesc: '"Wonach suche ich?" - Repräsentiert das aktuelle Wort, das Kontext sucht.',
    keyTitle: 'Key',
    keyDesc: '"Was enthalte ich?" - Ein Label für jedes Wort in der Sequenz zur Überprüfung gegen die Query.',
    valueTitle: 'Value',
    valueDesc: '"Welche Information biete ich?" - Der tatsächliche Inhalt, der weitergegeben wird, wenn Query und Key übereinstimmen.',
    qkvExplain: 'Das Modell berechnet einen Score durch Multiplikation von Q und K. Dieser Score bestimmt, wie viel von V behalten wird.',
    // Benefits section
    benefitsTitle: 'Warum es alles verändert hat',
    benefit1Title: 'Parallele Verarbeitung',
    benefit1Desc: 'Im Gegensatz zu älteren Modellen (RNNs) können Transformer alle Wörter eines Satzes gleichzeitig verarbeiten, was das Training viel schneller macht.',
    benefit2Title: 'Langstreckenabhängigkeiten',
    benefit2Desc: 'Aufmerksamkeit kann zwei Wörter verbinden, auch wenn sie Tausende von Tokens voneinander entfernt sind, solange sie im selben Kontextfenster sind.',
    benefit3Title: 'Dynamischer Kontext',
    benefit3Desc: 'Das Modell schaut nicht nur auf Wörter; es lernt, welche Wörter *füreinander* wichtig sind, basierend auf dem spezifischen Satz.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Aufmerksamkeit ermöglicht Transformern, Langstreckenabhängigkeiten zu erfassen',
    takeaway2: 'Die quadratische Komplexität der Aufmerksamkeit begrenzt die Kontextfenstergröße',
    takeaway3: 'Verschiedene Aufmerksamkeitsköpfe lernen, sich auf verschiedene linguistische Muster zu konzentrieren',
    takeaway4: 'Aufmerksamkeitsvisualisierung kann helfen, das Modellverhalten zu interpretieren',
  },

  // Vision page
  vision: {
    title: 'Bildverarbeitung',
    description: 'Wie moderne LLMs visuelle Informationen neben Text verarbeiten und verstehen.',
    whatIs: 'Wie LLMs Bilder sehen',
    whatIsDesc: 'Bildverarbeitungsfähige LLMs wandeln Bilder in Token-Sequenzen um, die zusammen mit Text verarbeitet werden können. Dies beinhaltet typischerweise das Aufteilen von Bildern in Patches und deren Kodierung mit einem Vision-Transformer.',
    patchEncoding: 'Patch-Kodierung',
    patchEncodingDesc: 'Bilder werden in Patches fester Größe (z.B. 14x14 Pixel) aufgeteilt, wobei jeder in einen Einbettungsvektor ähnlich wie Text-Tokens umgewandelt wird.',
    // Vision Transformer section
    vitTitle: 'Der Vision Transformer (ViT)',
    vitDesc: 'Die Vision Transformer-Architektur passt das Transformer-Modell für die Bildverarbeitung an. Anstatt Wörter zu verarbeiten, verarbeitet es Bildausschnitte.',
    vitStep1: 'In Patches aufteilen',
    vitStep1Desc: 'Das Bild wird in ein Raster aus Patches fester Größe aufgeteilt (typischerweise 14x14 oder 16x16 Pixel).',
    vitStep2: 'Abflachen & Projizieren',
    vitStep2Desc: 'Jeder Patch wird in einen Vektor abgeflacht und linear in einen Einbettungsraum projiziert.',
    vitStep3: 'Positionsinfo hinzufügen',
    vitStep3Desc: 'Positionseinbettungen werden hinzugefügt, damit das Modell weiß, woher jeder Patch stammt.',
    vitStep4: 'Mit Transformer verarbeiten',
    vitStep4Desc: 'Die Sequenz von Patch-Einbettungen wird von Standard-Transformer-Schichten verarbeitet.',
    // Token costs
    tokenCosts: 'Token-Kosten',
    tokenCostsDesc: 'Bilder sind teuer in Bezug auf Tokens. Das Verständnis davon hilft dir, deine Anwendungen zu optimieren.',
    tokenExample1: 'Ein 512x512 Bild mit 16x16 Patches',
    tokenExample1Value: '~1.024 Tokens',
    tokenExample2: 'Ein 1024x1024 hochauflösendes Bild',
    tokenExample2Value: '~4.096 Tokens',
    tokenExample3: 'Äquivalente Textbeschreibung',
    tokenExample3Value: '~50-100 Tokens',
    tokenTip: 'Überlege immer, ob eine Textbeschreibung effizienter sein könnte als das eigentliche Bild zu übergeben.',
    // Use cases
    useCases: 'Häufige Anwendungsfälle',
    useCasesDesc: 'Bildverarbeitungsfähige LLMs ermöglichen viele praktische Anwendungen.',
    useCase1: 'Dokumentenanalyse',
    useCase1Desc: 'Extrahiere Informationen aus PDFs, Quittungen, Formularen und handschriftlichen Notizen.',
    useCase2: 'Visuelle Fragen',
    useCase2Desc: 'Beantworte Fragen zu Bildinhalten, Diagrammen und Grafiken.',
    useCase3: 'Bildbeschriftung',
    useCase3Desc: 'Generiere detaillierte Beschreibungen von Bildern für Barrierefreiheit oder Indexierung.',
    useCase4: 'UI-Verständnis',
    useCase4Desc: 'Analysiere Screenshots, Wireframes und Benutzeroberflächen.',
    multimodal: 'Multimodales Verständnis',
    multimodalDesc: 'Das Modell lernt, visuelle und textuelle Repräsentationen auszurichten, was Aufgaben wie Bildbeschriftung, visuelle Fragen und Dokumentenverständnis ermöglicht.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bilder verbrauchen viel mehr Tokens als äquivalente Textbeschreibungen',
    takeaway2: 'Auflösung und Patch-Größe beeinflussen die Detailerkennung',
    takeaway3: 'Visuelles Verständnis ist ungefähr – Modelle können feine Details übersehen',
    takeaway4: 'Die Kombination von Bild und Sprache ermöglicht leistungsstarke neue Anwendungen',
  },

  // Agent Loop page
  agentLoop: {
    title: 'Der Agenten-Zyklus',
    description: 'Verstehe den Kernzyklus, der autonome KI-Agenten antreibt: beobachten, denken, handeln, wiederholen.',
    whatIs: 'Was ist der Agenten-Zyklus?',
    whatIsDesc: 'Der Agenten-Zyklus ist der fundamentale Kreislauf, der es KI-Agenten ermöglicht, autonom mit ihrer Umgebung zu interagieren. Er besteht aus Beobachtungs-, Denk-, Handlungs- und Feedback-Phasen, die sich kontinuierlich wiederholen.',
    phases: 'Die vier Phasen',
    observe: 'Beobachten',
    observeDesc: 'Sammle Informationen aus der Umgebung, von Tools und Benutzereingaben.',
    think: 'Denken',
    thinkDesc: 'Überlege zum aktuellen Zustand und entscheide über die nächste Aktion.',
    act: 'Handeln',
    actDesc: 'Führe die gewählte Aktion mit verfügbaren Tools aus.',
    learn: 'Lernen',
    learnDesc: 'Verarbeite Feedback und aktualisiere das Verständnis für die nächste Iteration.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Der Zyklus setzt sich fort, bis die Aufgabe abgeschlossen oder beendet ist',
    takeaway2: 'Jede Iteration baut auf vorherigen Beobachtungen und Aktionen auf',
    takeaway3: 'Fehlerbehandlung und Wiederherstellung sind entscheidend für robuste Agenten',
    takeaway4: 'Die Qualität der Tools beeinflusst direkt die Fähigkeiten des Agenten',
  },

  // Agent Context page
  agentContext: {
    title: 'Kontext-Anatomie',
    description: 'Aufschlüsselung der Struktur von Kontextfenstern und wie Agenten Informationen verwalten.',
    whatIs: 'Agentenkontext verstehen',
    whatIsDesc: 'Der Agentenkontext umfasst den System-Prompt, die Gesprächshistorie, Tool-Definitionen und abgerufene Informationen. Eine effiziente Verwaltung dieses Kontexts ist entscheidend für die Agentenleistung.',
    components: 'Kontext-Komponenten',
    systemPrompt: 'System-Prompt',
    systemPromptDesc: 'Definiert die Rolle, Fähigkeiten und Verhaltensrichtlinien des Agenten.',
    toolDefs: 'Tool-Definitionen',
    toolDefsDesc: 'Beschreibungen der verfügbaren Tools und ihrer Verwendung.',
    history: 'Gesprächshistorie',
    historyDesc: 'Vorherige Nachrichten, Tool-Aufrufe und deren Ergebnisse.',
    retrieved: 'Abgerufene Informationen',
    retrievedDesc: 'Externes Wissen, das während des Gesprächs abgerufen wurde.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextmanagement ist der Schlüssel zur Agenten-Zuverlässigkeit',
    takeaway2: 'Priorisiere aktuelle und relevante Informationen',
    takeaway3: 'Tool-Definitionen sollten klar und eindeutig sein',
    takeaway4: 'Zusammenfassung hilft, Kontext über lange Sitzungen zu erhalten',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agenten-Probleme',
    description: 'Häufige Fehlermodi und Herausforderungen, denen KI-Agenten in realen Anwendungen begegnen.',
    overview: 'Häufige Agenten-Fehlermodi',
    overviewDesc: 'Das Verstehen typischer Agentenfehler hilft beim Aufbau robusterer Systeme und beim Setzen angemessener Erwartungen.',
    problem1: 'Tool-Missbrauch',
    problem1Desc: 'Agenten können Tools falsch aufrufen, mit falschen Parametern oder zu unpassenden Zeiten.',
    problem2: 'Endlosschleifen',
    problem2Desc: 'Agenten können stecken bleiben und dieselben Aktionen wiederholen, ohne Fortschritte zu machen.',
    problem3: 'Zieldrift',
    problem3Desc: 'Agenten können den Fokus allmählich vom ursprünglichen Aufgabenziel weg verschieben.',
    problem4: 'Übermäßiges Selbstvertrauen',
    problem4Desc: 'Agenten können trotz Unsicherheit oder unvollständiger Informationen mit Aktionen fortfahren.',
    
    // Expanded Content
    hallucination: 'Tool-Halluzination',
    hallucinationDesc: 'Agenten "erfinden" manchmal Tool-Parameter oder sogar ganze Tools, die nicht existieren. Dies passiert meistens, wenn die Tool-Definition mehrdeutig ist oder das Modell versucht, eine Lösung zu erzwingen.',
    hallucinationExample: 'Beispiel: Aufruf von `get_weather(location="Tokyo", date="tomorrow")`, wenn die Funktion nur `location` akzeptiert.',
    
    loops: 'Schleifen-Probleme',
    loopsDesc: 'Agenten können in repetitiven Zyklen gefangen sein, in denen sie dieselbe Aktion ausführen, denselben Fehler erhalten und es ohne Strategieänderung erneut versuchen.',
    loopsMitigation: 'Abhilfe: Implementiere Schleifenerkennungslogik, die die Ausführung stoppt, wenn dieselbe Tool-Aufrufsequenz mehrmals auftritt.',
    
    costLatency: 'Kosten & Latenz',
    costLatencyDesc: 'Jeder Schritt im Agentenzyklus erfordert einen vollständigen LLM-Inferenzaufruf. Mehrstufige Aufgaben können schnell teuer und langsam werden.',
    costFactor: 'Der Kostenfaktor',
    costFactorDesc: 'Eine einfache Aufgabe, die 5 Schritte erfordert, bedeutet 5-fache Kosten und 5-fache Latenz im Vergleich zu einer Standard-Chat-Antwort.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Implementiere Sicherheitsvorkehrungen wie Iterationslimits und Kostenkontrollen',
    takeaway2: 'Füge Human-in-the-Loop-Kontrollpunkte für kritische Aktionen hinzu',
    takeaway3: 'Überwache das Agentenverhalten und protokolliere alle Aktionen zum Debugging',
    takeaway4: 'Definiere klare Erfolgs- und Fehlkriterien',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agenten-Sicherheit',
    description: 'Kritische Sicherheitslücken bei KI-Agenten: Prompt-Injektion, Datenexfiltration und Tool-Missbrauch – plus Verteidigungsstrategien.',
    
    // Intro
    intro: 'Agenten sind Angriffsflächen',
    introDesc: 'Wenn du einem LLM Zugang zu Tools gibst, schaffst du einen mächtigen Angriffsvektor. Agenten können Dateien lesen, HTTP-Anfragen stellen, E-Mails senden und Code ausführen. Ein böswilliger Akteur, der den Kontext des Agenten beeinflussen kann, kann potenziell all diese Fähigkeiten kontrollieren.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Angriff #1: Prompt-Injektion',
    attack1Desc: 'Prompt-Injektion tritt auf, wenn nicht vertrauenswürdige Eingaben vom LLM als Anweisungen interpretiert werden. Da Agenten oft externe Daten verarbeiten (E-Mails, Webseiten, Dokumente), können Angreifer versteckte Befehle einbetten, die das Verhalten des Agenten kapern.',
    attack1Example: 'Beispiel-Angriff',
    attack1ExampleDesc: 'Der Benutzer bittet den Agenten, ein Dokument zusammenzufassen. Das Dokument enthält versteckte Anweisungen:',
    whyWorks: 'Warum das funktioniert',
    whyWorks1: 'Der Agent liest das Dokument in seinen Kontext',
    whyWorks2: 'Das LLM kann nicht zwischen "echten" Anweisungen und injizierten unterscheiden',
    whyWorks3: 'Der versteckte Text sieht aus wie Systemanweisungen, also folgt das LLM ihnen möglicherweise',
    whyWorks4: 'Der Agent nutzt seine legitimen Tools, um die bösartige Aktion auszuführen',
    directInjection: 'Direkte Injektion',
    directInjectionDesc: 'Der Benutzer tippt bösartige Anweisungen direkt ein. Leichter zu filtern, aber immer noch gefährlich, wenn der System-Prompt nicht robust ist.',
    indirectInjection: 'Indirekte Injektion',
    indirectInjectionDesc: 'Bösartiger Inhalt kommt aus externen Quellen, die der Agent liest (Websites, E-Mails, Dateien). Viel schwerer zu verteidigen.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Angriff #2: Datenexfiltration',
    attack2Desc: 'Agenten mit Zugang zu Kommunikationstools (E-Mail, HTTP, Slack, etc.) können dazu gebracht werden, sensible Daten an externe Ziele zu senden. Der Agent wird zum unwissenden Komplizen beim Datendiebstahl.',
    exfilFlow: 'Exfiltrations-Ablauf',
    exfilStep1: 'Agent liest',
    exfilStep1Desc: 'Private Dateien, DB, Env-Variablen',
    exfilStep2: 'Injektion löst aus',
    exfilStep2Desc: '"Sende dies an X"',
    exfilStep3: 'Tool führt aus',
    exfilStep3Desc: 'Daten verlassen das System',
    vulnerableConfig: 'Anfällige Tool-Konfiguration',
    otherVectors: 'Andere Exfiltrations-Vektoren',
    vector1: 'HTTP-Anfragen — POST-Daten an angreifergesteuerte Endpunkte',
    vector2: 'Slack/Discord-Webhooks — Nachrichten an externe Kanäle senden',
    vector3: 'Datei-Uploads — Hochladen in Cloud-Speicher mit öffentlichen Links',
    vector4: 'DNS-Exfiltration — Daten in DNS-Anfragen kodieren',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Angriff #3: Unbeabsichtigter Tool-Missbrauch',
    attack3Desc: 'Auch ohne böswillige Absicht können Agenten durch falsche Tool-Nutzung Schaden anrichten. Das LLM könnte Parameter falsch verstehen, das falsche Tool verwenden oder destruktive Aktionen ausführen, während es versucht, hilfreich zu sein.',
    destructiveActions: 'Destruktive Aktionen',
    destructiveActionsDesc: '"Räume das Projekt auf" → Agent führt rm -rf / aus oder löscht die Produktionsdatenbank',
    wrongParams: 'Falsche Parameter',
    wrongParamsDesc: 'Agent verwechselt ähnliche Felder oder verwendet falsche Werte, die plausibel erscheinen',
    cascadingErrors: 'Kaskadierende Fehler',
    cascadingErrorsDesc: 'Agent macht einen kleinen Fehler, dann "behebt" er ihn mit zunehmend destruktiven Aktionen',
    
    // Defense Strategies
    defensesTitle: 'Verteidigungsstrategien',
    defense1: 'Prinzip der minimalen Rechte',
    defense1Desc: 'Gib dem Agenten nur die minimal notwendigen Tools und Berechtigungen für die Aufgabe. Gib keinen Dateizugang, wenn er nur Fragen beantworten muss.',
    defense1Bad: 'Schlecht',
    defense1Good: 'Gut',
    defense2: 'Strikte Allowlists',
    defense2Desc: 'Beschränke Tool-Parameter auf bekannte sichere Werte. Erlaube keine beliebigen E-Mail-Adressen, URLs oder Dateipfade.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Erfordere menschliche Genehmigung für sensible Aktionen. Der Agent schlägt vor, der Mensch bestätigt.',
    defense3Example: 'Beispiel-Bestätigungsablauf:',
    defense4: 'Eingabe-Bereinigung & Isolation',
    defense4Desc: 'Behandle externe Daten als nicht vertrauenswürdig. Trenne Benutzeranweisungen klar von abgerufenen Inhalten.',
    defense5: 'Überwachung & Ratenbegrenzung',
    defense5Desc: 'Protokolliere alle Tool-Aufrufe. Setze Ratenlimits für sensible Operationen. Alarmiere bei ungewöhnlichen Mustern (viele E-Mails, große Datenübertragungen, wiederholte Fehler). Aktiviere Rollback für destruktive Aktionen.',
    
    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Agenten sind Angriffsflächen – jedes Tool ist eine potenzielle Schwachstelle',
    takeaway2: 'Prompt-Injektion ist die #1 Bedrohung – LLMs können Anweisungen nicht von Daten unterscheiden',
    takeaway3: 'Datenexfiltration ist trivial, wenn Agenten ausgehende Kommunikationstools haben',
    takeaway4: 'Tool-Missbrauch passiert auch ohne Angreifer – LLMs machen Fehler',
    takeaway5: 'Verteidigung in der Tiefe: minimale Rechte + Allowlists + menschliche Genehmigung + Überwachung',
    takeaway6: 'Behandle alle externen Daten als potenziell bösartige Eingaben',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentische Muster',
    description: 'Entwurfsmuster und Architekturen für den Aufbau effektiver KI-Agentensysteme.',
    overview: 'Häufige agentische Muster',
    overviewDesc: 'Mehrere Architekturmuster haben sich als effektive Ansätze für den Aufbau von KI-Agentensystemen herausgestellt. Jedes Muster bietet unterschiedliche Kompromisse zwischen Zuverlässigkeit, Transparenz und Fähigkeiten.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Verschachtele Denk-Traces mit Aktionen für bessere Transparenz und Kontrolle.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Erstelle zuerst einen übergeordneten Plan, dann führe die Schritte sequentiell aus.',
    pattern3: 'Multi-Agenten-Systeme',
    pattern3Desc: 'Mehrere spezialisierte Agenten arbeiten zusammen, um komplexe Aufgaben zu lösen.',
    pattern4: 'Reflexion',
    pattern4Desc: 'Agenten überprüfen ihre eigenen Ausgaben und verbessern sie iterativ.',
    // Deep dive sections
    reactDeepDive: 'ReAct-Muster im Detail',
    reactDeepDiveDesc: 'ReAct (Reasoning + Acting) verschachtelt Chain-of-Thought-Reasoning mit Aktionsausführung. Das Modell äußert explizit sein Denken vor jeder Aktion.',
    reactHow: 'Wie es funktioniert',
    reactStep1: 'Gedanke: Das Modell überlegt, was als nächstes zu tun ist',
    reactStep2: 'Aktion: Das Modell ruft ein Tool auf oder führt eine Aktion aus',
    reactStep3: 'Beobachtung: Das Modell sieht das Ergebnis',
    reactStep4: 'Wiederholen bis die Aufgabe abgeschlossen ist',
    reactPros: 'Vorteile',
    reactPro1: 'Hochgradig transparent—du kannst genau sehen, warum der Agent was getan hat',
    reactPro2: 'Leichter zu debuggen und Fehler zu verstehen',
    reactPro3: 'Natürliche Fehlerbehebung durch explizites Reasoning',
    reactCons: 'Nachteile',
    reactCon1: 'Mehr Tokens verwendet (Reasoning braucht Platz)',
    reactCon2: 'Kann durch explizite Reasoning-Schritte langsamer sein',
    reactCon3: 'Kann einfache Aufgaben überdenken',
    planExecuteDeepDive: 'Plan-and-Execute im Detail',
    planExecuteDeepDiveDesc: 'Dieses Muster trennt Planung von Ausführung. Ein Planer erstellt einen übergeordneten Plan, dann führt ein Executor jeden Schritt aus.',
    planExecuteHow: 'Wie es funktioniert',
    planStep1: 'Planer analysiert die Aufgabe und erstellt einen schrittweisen Plan',
    planStep2: 'Executor führt jeden Schritt nacheinander aus',
    planStep3: 'Neuplanung erfolgt, wenn die Ausführung fehlschlägt oder neue Infos auftauchen',
    planExecutePros: 'Vorteile',
    planExecutePro1: 'Besser für komplexe, mehrstufige Aufgaben',
    planExecutePro2: 'Kann verschiedene Modelle für Planung vs. Ausführung verwenden',
    planExecutePro3: 'Pläne können vor der Ausführung überprüft werden',
    planExecuteCons: 'Nachteile',
    planExecuteCon1: 'Pläne können während der Ausführung veralten',
    planExecuteCon2: 'Schwieriger mit unerwarteten Situationen umzugehen',
    planExecuteCon3: 'Neuplanung fügt Latenz und Kosten hinzu',
    multiAgentDeepDive: 'Multi-Agenten-Systeme im Detail',
    multiAgentDeepDiveDesc: 'Mehrere spezialisierte Agenten arbeiten zusammen, wobei jeder verschiedene Aspekte einer Aufgabe bearbeitet. Ein Supervisor oder Router leitet die Arbeit an den richtigen Agenten.',
    multiAgentHow: 'Wie es funktioniert',
    multiAgentStep1: 'Router/Supervisor erhält die Aufgabe',
    multiAgentStep2: 'Aufgabe wird an spezialisierte Agenten delegiert',
    multiAgentStep3: 'Agenten können kommunizieren und zusammenarbeiten',
    multiAgentStep4: 'Ergebnisse werden aggregiert und zurückgegeben',
    multiAgentPros: 'Vorteile',
    multiAgentPro1: 'Spezialisierung verbessert die Qualität bei komplexen Aufgaben',
    multiAgentPro2: 'Kann unabhängige Teilaufgaben parallelisieren',
    multiAgentPro3: 'Einzelne Agenten leichter zu warten und zu aktualisieren',
    multiAgentCons: 'Nachteile',
    multiAgentCon1: 'Höhere Komplexität und Koordinationsaufwand',
    multiAgentCon2: 'Teurer (mehrere LLM-Aufrufe)',
    multiAgentCon3: 'Verteilte Fehler schwieriger zu debuggen',
    reflectionDeepDive: 'Reflexionsmuster im Detail',
    reflectionDeepDiveDesc: 'Der Agent generiert eine Ausgabe, kritisiert und verbessert sie dann. Diese Selbstverbesserungsschleife kann die Qualität dramatisch verbessern.',
    reflectionHow: 'Wie es funktioniert',
    reflectionStep1: 'Initiale Ausgabe generieren',
    reflectionStep2: 'Ausgabe kritisieren (Fehler, fehlende Teile identifizieren)',
    reflectionStep3: 'Basierend auf Kritik überarbeiten',
    reflectionStep4: 'Wiederholen bis Qualitätsschwelle erreicht',
    reflectionPros: 'Vorteile',
    reflectionPro1: 'Verbessert die Ausgabequalität erheblich',
    reflectionPro2: 'Fängt Fehler ab, die der erste Durchgang übersehen hat',
    reflectionPro3: 'Funktioniert gut für Schreiben, Code-Review und kreative Aufgaben',
    reflectionCons: 'Nachteile',
    reflectionCon1: 'Mehrere LLM-Aufrufe erhöhen Kosten und Latenz',
    reflectionCon2: 'Risiko von Über-Editierung oder endlosen Schleifen',
    reflectionCon3: 'Kann kreative oder unkonventionelle Lösungen "auswaschen"',
    // Choosing section
    choosingTitle: 'Das richtige Muster wählen',
    choosingDesc: 'Wähle ein Muster basierend auf deinen spezifischen Bedürfnissen und Einschränkungen.',
    choosingSimple: 'Für einfache, klar definierte Aufgaben',
    choosingSimpleAnswer: 'Direkte Tool-Aufrufe (kein Muster nötig)',
    choosingTransparency: 'Wenn du Transparenz und Debuggbarkeit brauchst',
    choosingTransparencyAnswer: 'ReAct-Muster',
    choosingComplex: 'Für komplexe, mehrstufige Aufgaben',
    choosingComplexAnswer: 'Plan-and-Execute',
    choosingQuality: 'Wenn Ausgabequalität kritisch ist',
    choosingQualityAnswer: 'Reflexionsmuster',
    choosingDiverse: 'Für diverse Aufgabentypen',
    choosingDiverseAnswer: 'Multi-Agent mit spezialisierten Agenten',
    // Interactive section
    interactiveTitle: 'Mustervergleich',
    interactiveDesc: 'Erkunde verschiedene agentische Architekturen',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Wähle Muster basierend auf Aufgabenkomplexität und Zuverlässigkeitsanforderungen',
    takeaway2: 'ReAct ist großartig für Transparenz, kann aber langsamer sein',
    takeaway3: 'Multi-Agenten-Systeme erhöhen die Komplexität, ermöglichen aber Spezialisierung',
    takeaway4: 'Reflexionsmuster können die Ausgabequalität erheblich verbessern',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'MCP verstehen: wann externe Tool-Server sinnvoll sind und wann sie übertrieben sind.',
    whatIs: 'Was ist MCP?',
    whatIsDesc: 'Das Model Context Protocol (MCP) ist ein standardisierter Weg, um KI-Agenten mit externen Tools und Datenquellen über dedizierte Server-Prozesse zu verbinden. Anstatt Tools inline im Agenten-Code zu definieren, führt MCP einen separaten Server aus, der Tools über ein strukturiertes Protokoll bereitstellt.',
    vsToolCalls: 'MCP vs. Reguläre Tool-Aufrufe',
    vsToolCallsDesc: 'Reguläre Tool-Aufrufe sind Funktionen, die direkt in der Codebasis deines Agenten definiert sind. Der Agent ruft sie auf, sie werden ausgeführt und die Ergebnisse kehren im selben Prozess zurück. MCP trennt dies: Tools leben in externen Servern, mit denen der Agent über ein Protokoll kommuniziert.',
    
    // Comparison
    regularTools: 'Reguläre Tool-Aufrufe',
    regularToolsDesc: 'Tools, die inline in deinem Agenten-Code definiert sind. Einfach, schnell und für die meisten Anwendungsfälle ausreichend.',
    mcpTools: 'MCP-Server',
    mcpToolsDesc: 'Tools, die von externen Server-Prozessen bereitgestellt werden. Fügt Netzwerk-Overhead hinzu, ermöglicht aber sprachübergreifendes Tooling und gemeinsame Tool-Ökosysteme.',
    
    // When to use
    whenToUse: 'Wann MCP sinnvoll ist',
    whenToUseDesc: 'MCP glänzt in spezifischen Szenarien, in denen sich seine zusätzliche Komplexität auszahlt.',
    useCase1: 'Multi-Sprachen-Teams',
    useCase1Desc: 'Deine Tools sind in Python geschrieben, aber dein Agent ist in TypeScript, oder umgekehrt.',
    useCase2: 'Gemeinsames Tool-Ökosystem',
    useCase2Desc: 'Mehrere Agenten in verschiedenen Projekten müssen auf dieselben Tools zugreifen.',
    useCase3: 'Enterprise-Integration',
    useCase3Desc: 'Du musst bestehende interne Dienste als Agenten-Tools bereitstellen, ohne sie zu modifizieren.',
    useCase4: 'Tool-Marktplatz',
    useCase4Desc: 'Du möchtest von der Community gepflegte Tools nutzen, ohne Code in dein Projekt zu kopieren.',
    
    // When it's overkill
    overkill: 'Wann MCP übertrieben ist',
    overkillDesc: 'Für viele Anwendungsfälle fügt MCP unnötige Komplexität hinzu.',
    overkillCase1: 'Einsprachige Projekte',
    overkillCase1Desc: 'Wenn deine Tools und dein Agent in derselben Sprache sind, sind Inline-Funktionen einfacher und schneller.',
    overkillCase2: 'Einfache Agenten',
    overkillCase2Desc: 'Ein Chatbot mit wenigen Tools braucht nicht den Overhead, separate Server-Prozesse auszuführen.',
    overkillCase3: 'Schnelles Prototyping',
    overkillCase3Desc: 'Bei schneller Iteration verlangsamt die Indirektion von MCP die Entwicklung.',
    overkillCase4: 'Latenz-kritische Apps',
    overkillCase4Desc: 'Netzwerkaufrufe zu Tool-Servern fügen Latenz hinzu, die Inline-Funktionen nicht haben.',
    
    // Architecture
    architecture: 'Wie MCP funktioniert',
    architectureDesc: 'MCP definiert eine Client-Server-Architektur, bei der der Agent der Client ist und Tools von Servern bereitgestellt werden.',
    step1: 'Entdeckung',
    step1Desc: 'Der Agent verbindet sich mit einem MCP-Server und erhält eine Liste der verfügbaren Tools mit ihren Schemas.',
    step2: 'Aufruf',
    step2Desc: 'Wenn das LLM entscheidet, ein Tool zu verwenden, sendet der Agent eine Anfrage an den MCP-Server.',
    step3: 'Ausführung',
    step3Desc: 'Der MCP-Server führt das Tool aus und gibt Ergebnisse in einem standardisierten Format zurück.',
    step4: 'Integration',
    step4Desc: 'Ergebnisse fließen zurück zum Agenten und in den LLM-Kontext, genau wie reguläre Tool-Ergebnisse.',
    
    // Practical advice
    practicalAdvice: 'Praktische Ratschläge',
    adviceDesc: 'Richtlinien für die Entscheidung, ob du MCP in deinem Projekt verwenden solltest.',
    advice1: 'Beginne einfach: verwende Inline-Tool-Definitionen, bis du auf eine spezifische Einschränkung stößt.',
    advice2: 'Erwäge MCP, wenn du dich dabei ertappst, Tool-Code zwischen Projekten zu kopieren.',
    advice3: 'Der Overhead, MCP-Server auszuführen, macht nur in großem Maßstab oder in Enterprise-Umgebungen Sinn.',
    advice4: 'Community-MCP-Server können die Entwicklung beschleunigen, fügen aber Abhängigkeitsrisiken hinzu.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'MCP ist ein Protokoll zur Bereitstellung von Tools über externe Server, kein Ersatz für reguläre Tool-Aufrufe',
    takeaway2: 'Für die meisten Einzelprojekt-Agenten sind Inline-Tools einfacher und haben geringere Latenz',
    takeaway3: 'MCP glänzt in polyglotten Umgebungen und gemeinsamen Tool-Ökosystemen',
    takeaway4: 'Greife nicht standardmäßig zu MCP—es ist eine Lösung für spezifische Skalierungs- und Interoperabilitäts-Herausforderungen',
  },

  // Metadata
  metadata: {
    title: 'KI-Konzepte lernen | Interaktiver Leitfaden',
    description: 'Meistere künstliche Intelligenz und Konzepte großer Sprachmodelle durch schöne, interaktive Demonstrationen.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Bedienfeld',
    adjustTemperature: 'Temperatur anpassen',
    temperature: 'Temperatur',
    samplePrompt: 'Beispiel-Prompt',
    onceUponATime: '"Es war einmal..."',
    liveCompletion: 'Live-Vervollständigung',
    regenerate: 'Neu generieren',
    deterministic: 'Deterministisch',
    balanced: 'Ausgewogen',
    creative: 'Kreativ',
    chaotic: 'Chaotisch',
    frozen: 'Eingefroren',
    focused: 'Fokussiert',
    wild: 'Wild',
    greedyMode: 'Gieriger Modus: Wählt immer das wahrscheinlichste Token.',
    lowTemp: 'Niedrige Temperatur: Fokus auf wahrscheinliche Fortsetzungen.',
    balancedTemp: 'Ausgewogen: Natürliche Mischung aus Vorhersehbarkeit und Vielfalt.',
    highTemp: 'Hohe Temperatur: Erkundet kreative, weniger häufige Wortwahlen.',
    veryHighTemp: 'Sehr hoch: Wahrscheinlichkeitsverteilung ist nahezu gleichförmig – erwarte Chaos!',
    
    // Context Rot Simulator
    setInstruction: 'Systemanweisung festlegen',
    persistInstruction: 'Dies sollte während des gesamten Gesprächs bestehen bleiben',
    systemPrompt: 'System-Prompt',
    quickExamples: 'Schnellbeispiele',
    startSimulation: 'Simulation starten',
    contextOverflow: 'Kontextüberlauf!',
    conversation: 'Gespräch',
    messagesPushed: 'Nachrichten aus dem Fenster geschoben',
    messages: 'Nachrichten',
    overflowIt: 'Überfluten!',
    reset: 'Zurücksetzen',
    typeMessage: 'Schreibe eine Nachricht...',
    systemInstructionLost: 'Systemanweisung verloren!',
    systemLostDesc: 'Deine Systemanweisung wurde vollständig aus dem Kontextfenster geschoben. Das Modell kann sie nicht mehr sehen – es ist, als hättest du die Anweisung nie gegeben. Das ist der schlimmste Fall von Kontextverfall: totale Amnesie.',
    contextFilling: 'Kontext füllt sich',
    contextFillingDesc: 'Deine Systemanweisung verliert an Einfluss, da neuere Nachrichten Vorrang haben. Beachte, wie sie visuell verblasst – dies stellt die schwindende Aufmerksamkeit des Modells dar.',
    exampleFrench: 'Antworte immer auf Französisch.',
    examplePirate: 'Du bist ein Pirat. Sage oft "Arrr".',
    exampleHaiku: 'Beende jede Antwort mit einem Haiku.',
    labelFrench: 'Sprich Französisch',
    labelPirate: 'Sei ein Pirat',
    labelHaiku: 'Beende mit Haiku',

    // Attention Visualizer
    hoverToSee: 'Fahre darüber, um Aufmerksamkeitsgewichte zu sehen',
    token: 'Token',
    attentionScore: 'Aufmerksamkeits-Score',
    strongConnection: 'Starke Verbindung',
    weakConnection: 'Schwache Verbindung',

    // Patch Grid Visualizer
    originalImage: 'Originalbild',
    patchGrid: 'Patch-Raster',
    flattenedPatches: 'Abgeflachte Patches',
    transformerInput: 'Transformer-Eingabe',
    processDesc: 'Das Bild wird in ein festes Raster von Patches (z.B. 16x16 Pixel) aufgeteilt. Jeder Patch wird dann in einen Vektor abgeflacht und linear in einen Einbettungsraum projiziert.',

    // Agent Loop Visualizer
    startLoop: 'Zyklus starten',
    step: 'Schritt',
    context: 'Kontext',
    llmResponse: 'LLM-Antwort',
    toolExecution: 'Tool-Ausführung',
    finalAnswer: 'Endgültige Antwort',
    system: 'System',
    user: 'Benutzer',
    assistant: 'Assistent',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Planen & Ausführen',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflexion',
    patternDesc: 'Wähle ein Muster, um zu sehen, wie es den Arbeitsablauf des Agenten strukturiert.',

    // Tokenizer Demo
    enterText: 'Text zum Tokenisieren eingeben',
    sampleText: 'Der schnelle braune Fuchs springt über den faulen Hund.',
    tokens: 'Tokens',
    characters: 'Zeichen',
    tokensPerChar: 'Tokens pro Zeichen',
    tokenBreakdown: 'Token-Aufschlüsselung',
    commonTokens: 'Häufige Tokens sind einzelne Teile',
    rareTokens: 'Seltene Wörter werden in Teilwörter aufgeteilt',

    // Embedding Visualizer
    enterWords: 'Wörter zum Vergleichen eingeben',
    addWord: 'Wort hinzufügen',
    similarityScore: 'Ähnlichkeits-Score',
    dimensions: 'Dimensionen',
    nearestNeighbors: 'Nächste Nachbarn',
    vectorSpace: 'Vektorraum',

    // RAG Pipeline Visualizer
    enterQuery: 'Abfrage eingeben',
    sampleQuery: 'Was ist die Hauptstadt von Frankreich?',
    retrieving: 'Abrufen...',
    retrieved: 'Abgerufene Dokumente',
    relevanceScore: 'Relevanz',
    generating: 'Antwort wird generiert...',
    augmentedContext: 'Erweiterter Kontext',

    // Tool Schema Builder
    toolName: 'Tool-Name',
    toolDescription: 'Beschreibung',
    addParameter: 'Parameter hinzufügen',
    paramName: 'Parametername',
    paramType: 'Typ',
    paramRequired: 'Erforderlich',
    generatedSchema: 'Generiertes Schema',
    validateSchema: 'Schema validieren',

    // Memory System Visualizer
    shortTermMemory: 'Kurzzeitgedächtnis',
    longTermMemory: 'Langzeitgedächtnis',
    memoryCapacity: 'Kapazität',
    memoryUsage: 'Auslastung',
    addMemory: 'Erinnerung hinzufügen',
    recallMemory: 'Abrufen',
    memoriesStored: 'Erinnerungen gespeichert',

    // Workflow Visualizer
    addNode: 'Knoten hinzufügen',
    connectNodes: 'Knoten verbinden',
    runWorkflow: 'Workflow ausführen',
    nodeTypes: 'Knotentypen',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Bedingung',

    // Neural Network Visualizer
    inputLayer: 'Eingabeschicht',
    hiddenLayer: 'Versteckte Schicht',
    outputLayer: 'Ausgabeschicht',
    addLayer: 'Schicht hinzufügen',
    removeLayer: 'Schicht entfernen',
    neurons: 'Neuronen',
    activation: 'Aktivierung',
    forward: 'Vorwärts',

    // Gradient Descent Visualizer
    startDescent: 'Abstieg starten',
    pauseDescent: 'Pause',
    resetDescent: 'Zurücksetzen',
    learningRate: 'Lernrate',
    currentLoss: 'Aktueller Verlust',
    iterations: 'Iterationen',
    globalMinimum: 'Globales Minimum',
    localMinimum: 'Lokales Minimum',

    // Training Progress Visualizer
    startTraining: 'Training starten',
    stopTraining: 'Stoppen',
    epoch: 'Epoche',
    trainingLoss: 'Trainingsverlust',
    validationLoss: 'Validierungsverlust',
    accuracy: 'Genauigkeit',
    overfitting: 'Überanpassung erkannt',

    // Prompt Comparison Demo
    weakPrompt: 'Schwacher Prompt',
    strongPrompt: 'Starker Prompt',
    compare: 'Vergleichen',
    promptQuality: 'Qualitäts-Score',
    improvements: 'Verbesserungen',

    // Chain of Thought Demo
    withoutCot: 'Ohne Chain of Thought',
    withCot: 'Mit Chain of Thought',
    reasoningSteps: 'Denkschritte',
    showSteps: 'Schritte anzeigen',

    // Bias Detection Demo
    testInput: 'Testeingabe',
    analyzeForBias: 'Auf Bias analysieren',
    biasIndicators: 'Bias-Indikatoren',
    fairnessScore: 'Fairness-Score',
    recommendations: 'Empfehlungen',
  },

  // Phase 1: LLM Topics
  tokenization: {
    title: 'Tokenisierung',
    description: 'Wie LLMs Text in Tokens zerlegen – die grundlegenden Einheiten des Sprachverständnisses.',
    whatIs: 'Was ist Tokenisierung?',
    whatIsDesc: 'Tokenisierung ist der Prozess der Umwandlung von Rohtext in eine Sequenz von Tokens – die Grundeinheiten, die LLMs verarbeiten. Tokens können Wörter, Teilwörter oder sogar einzelne Zeichen sein, abhängig vom Tokenizer.',
    whyMatters: 'Warum Tokenisierung wichtig ist',
    whyMattersDesc: 'Das Verständnis der Tokenisierung ist entscheidend, da sie direkt die Kontextgrenzen, Kosten und das Modellverhalten beeinflusst. Derselbe Text kann je nach Modell sehr unterschiedliche Token-Anzahlen haben.',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Die meisten modernen LLMs verwenden Subword-Tokenisierungsalgorithmen wie BPE (Byte Pair Encoding) oder SentencePiece. Diese Algorithmen lernen häufige Zeichenfolgen aus Trainingsdaten.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE fügt iterativ die häufigsten Zeichenpaare zu einzelnen Tokens zusammen. Häufige Wörter werden zu einzelnen Tokens, während seltene Wörter in Teilwörter aufgeteilt werden.',
    tokenTypes: 'Token-Typen',
    wholeWords: 'Ganze Wörter',
    wholeWordsDesc: 'Häufige Wörter wie "the", "and", "is" sind oft einzelne Tokens.',
    subwords: 'Teilwörter',
    subwordsDesc: 'Weniger häufige Wörter werden aufgeteilt: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Spezielle Tokens',
    specialTokensDesc: 'Markierungen wie <|endoftext|> oder [CLS] zur Modellsteuerung.',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Tippe Text ein, um zu sehen, wie er tokenisiert wird',
    costImplications: 'Kostenauswirkungen',
    costDesc: 'API-Preise basieren typischerweise auf Tokens. Effiziente Prompts verwenden weniger Tokens.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Tokens sind die atomaren Einheiten, die LLMs verarbeiten – nicht Zeichen oder Wörter',
    takeaway2: 'Verschiedene Modelle haben verschiedene Tokenizer und Vokabulare',
    takeaway3: 'Nicht-englischer Text und Code verwenden oft mehr Tokens als Englisch',
    takeaway4: 'Die Token-Anzahl beeinflusst direkt die Kosten und die Nutzung des Kontextfensters',
  },

  embeddings: {
    title: 'Einbettungen',
    description: 'Wie KI Bedeutung als Vektoren im hochdimensionalen Raum darstellt.',
    whatIs: 'Was sind Einbettungen?',
    whatIsDesc: 'Einbettungen sind dichte Vektordarstellungen, die semantische Bedeutung erfassen. Ähnliche Konzepte haben ähnliche Einbettungen, was Maschinen ermöglicht, Beziehungen zwischen Wörtern, Sätzen und Dokumenten zu verstehen.',
    howWorks: 'Wie Einbettungen funktionieren',
    howWorksDesc: 'Einbettungsmodelle bilden diskrete Tokens auf kontinuierliche Vektoren in einem hochdimensionalen Raum ab (oft 768-4096 Dimensionen). Die Position jedes Vektors kodiert seine semantische Bedeutung.',
    similarity: 'Semantische Ähnlichkeit',
    similarityDesc: 'Ähnliche Bedeutungen gruppieren sich im Einbettungsraum. "König" und "Königin" sind näher beieinander als "König" und "Banane".',
    dimensions: 'Vektordimensionen',
    dimensionsDesc: 'Jede Dimension erfasst einen Aspekt der Bedeutung – obwohl diese Dimensionen nicht für Menschen interpretierbar sind.',
    operations: 'Vektoroperationen',
    operationsDesc: 'Berühmtes Beispiel: König - Mann + Frau ≈ Königin. Beziehungen werden als Richtungen im Raum kodiert.',
    useCases: 'Häufige Anwendungsfälle',
    search: 'Semantische Suche',
    searchDesc: 'Dokumente nach Bedeutung finden, nicht nur durch Schlüsselwort-Matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Ähnliche Dokumente gruppieren, Themen automatisch erkennen.',
    classification: 'Klassifizierung',
    classificationDesc: 'Text basierend auf Einbettungsähnlichkeit zu Beispielen kategorisieren.',
    rag: 'RAG-Systeme',
    ragDesc: 'Relevanten Kontext für LLM-Prompts abrufen.',
    interactiveDemo: 'Interaktive Visualisierung',
    demoDesc: 'Erkunde, wie Einbettungen nach Bedeutung clustern',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Einbettungen wandeln Text in Vektoren um, die semantische Bedeutung erfassen',
    takeaway2: 'Ähnliche Konzepte haben ähnliche Einbettungen (Kosinus-Ähnlichkeit)',
    takeaway3: 'Einbettungen ermöglichen semantische Suche, Clustering und RAG',
    takeaway4: 'Verschiedene Einbettungsmodelle haben verschiedene Stärken und Dimensionen',
  },

  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: LLMs Zugang zu externem Wissen geben.',
    whatIs: 'Was ist RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) verbessert LLM-Antworten, indem relevante Dokumente aus einer Wissensbasis abgerufen und in den Prompt eingefügt werden. Dies gibt Modellen Zugang zu aktuellen oder spezialisierten Informationen.',
    whyRag: 'Warum RAG verwenden?',
    whyRagDesc: 'LLMs haben Wissens-Stichtage und können halluzinieren. RAG verankert Antworten in echten Dokumenten, reduziert Halluzinationen und ermöglicht domänenspezifisches Wissen ohne Fine-Tuning.',
    pipeline: 'Die RAG-Pipeline',
    pipelineDesc: 'RAG-Systeme folgen einem konsistenten Muster: Anfrage einbetten, relevante Chunks abrufen, den Prompt erweitern und eine Antwort generieren.',
    step1: 'Anfrage-Einbettung',
    step1Desc: 'Die Frage des Benutzers mit einem Einbettungsmodell in einen Vektor umwandeln.',
    step2: 'Abruf',
    step2Desc: 'Die Vektordatenbank nach Chunks durchsuchen, die der Anfrage-Einbettung ähnlich sind.',
    step3: 'Erweiterung',
    step3Desc: 'Abgerufene Chunks als Kontext in den Prompt einfügen.',
    step4: 'Generierung',
    step4Desc: 'Das LLM generiert eine Antwort, die im abgerufenen Kontext verankert ist.',
    chunking: 'Dokument-Chunking',
    chunkingDesc: 'Dokumente werden in kleinere Chunks aufgeteilt (typischerweise 200-1000 Tokens) für Einbettung und Abruf. Die Chunk-Größe beeinflusst die Abrufgenauigkeit.',
    vectorDbs: 'Vektordatenbanken',
    vectorDbsDesc: 'Spezialisierte Datenbanken wie Pinecone, Weaviate oder pgvector ermöglichen schnelle Ähnlichkeitssuche über Millionen von Einbettungen.',
    interactiveDemo: 'Interaktive RAG-Pipeline',
    demoDesc: 'Sieh, wie Anfragen durch ein RAG-System fließen',

    // Agentic RAG
    agenticRag: 'Agentic RAG',
    agenticRagDesc: 'Bei agentic RAG empfängt das LLM nicht nur abgerufene Dokumente—es steuert aktiv den Abrufprozess. Das Modell entscheidet, wann gesucht wird, wonach gesucht wird und welche Abrufwerkzeuge verwendet werden.',
    agenticHow: 'Funktionsweise',
    agenticHowDesc: 'Statt einer festen Pipeline erhält das LLM Abrufwerkzeuge, die es nach Bedarf aufrufen kann. Es kann Anfragen umformulieren, mehrfach suchen oder verschiedene Suchstrategien je nach Aufgabe kombinieren.',
    agenticAdvantages: 'Vorteile',
    agenticAdv1: 'Anfrageverfeinerung: Das LLM kann komplexe Fragen umformulieren oder zerlegen',
    agenticAdv2: 'Multi-Hop-Reasoning: Mehrere Abrufe verketten, um komplexe Fragen zu beantworten',
    agenticAdv3: 'Adaptive Suche: Das richtige Werkzeug für jede Teilfrage wählen',
    agenticAdv4: 'Selbstkorrektur: Erneut abrufen, wenn erste Ergebnisse unzureichend sind',
    agenticDisadvantages: 'Nachteile',
    agenticDisadv1: 'Höhere Latenz: Mehrere LLM-Aufrufe und Abrufe summieren sich',
    agenticDisadv2: 'Erhöhte Kosten: Jeder Reasoning-Schritt kostet Tokens',
    agenticDisadv3: 'Komplexität: Schwerer zu debuggen und Verhalten vorherzusagen',
    agenticDisadv4: 'Fehlermodi: LLM könnte in Schleifen geraten, zu viel abrufen oder offensichtliche Anfragen übersehen',
    multiTool: 'Multi-Tool-Abruf',
    multiToolDesc: 'Gib dem LLM mehrere Abrufwerkzeuge für verschiedene Anwendungsfälle. Diese Flexibilität lässt das Modell den besten Ansatz für jede Anfrage wählen.',
    toolSemantic: 'Semantische Suche',
    toolSemanticDesc: 'Vektorähnlichkeit für konzeptuelle Übereinstimmung. Ideal für: "Dokumente über X", verwandte Inhalte finden.',
    toolFulltext: 'Volltextsuche',
    toolFulltextDesc: 'Keyword/BM25-Suche für exakte Treffer. Ideal für: spezifische Begriffe, Namen, Codes, Fehlermeldungen.',
    toolSql: 'SQL/Strukturierte Abfrage',
    toolSqlDesc: 'Strukturierte Daten direkt abfragen. Ideal für: Zählungen, Aggregationen, Filterung nach Attributen.',
    toolKg: 'Wissensgraph',
    toolKgDesc: 'Entitätsbeziehungen traversieren. Ideal für: "Wie hängt X mit Y zusammen", Multi-Hop-Fakten.',
    whenToUse: 'Wann Agentic RAG einsetzen',
    whenToUseDesc: 'Standard-RAG ist einfacher und schneller für unkomplizierte Frage-Antwort-Szenarien. Verwende Agentic RAG, wenn Anfragen komplex sind, mehrere Quellen erfordern oder von Anfrageverfeinerung profitieren.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'RAG ruft relevante Dokumente ab und fügt sie in den Prompt ein',
    takeaway2: 'Es reduziert Halluzinationen, indem Antworten in echten Quellen verankert werden',
    takeaway3: 'Chunking-Strategie und Einbettungsqualität sind entscheidend für guten Abruf',
    takeaway4: 'RAG ist oft dem Fine-Tuning vorzuziehen, um Domänenwissen hinzuzufügen',
  },

  // Phase 2: Agent Topics
  toolDesign: {
    title: 'Tool-Design',
    description: 'Best Practices für das Design effektiver Tools, die KI-Agenten zuverlässig nutzen können.',
    whatIs: 'Was macht ein gutes Tool aus?',
    whatIsDesc: 'Gut gestaltete Tools sind die Grundlage fähiger KI-Agenten. Schema, Benennung und Dokumentation eines Tools beeinflussen direkt, wie zuverlässig ein LLM es nutzen kann.',
    principles: 'Design-Prinzipien',
    principlesDesc: 'Befolge diese Prinzipien, um Tools zu erstellen, die Agenten effektiv nutzen können.',
    principle1: 'Klare Benennung',
    principle1Desc: 'Verwende beschreibende, eindeutige Namen. "search_web" ist besser als "sw" oder "query".',
    principle2: 'Explizite Parameter',
    principle2Desc: 'Jeder Parameter sollte einen klaren Typ, eine Beschreibung und Einschränkungen haben.',
    principle3: 'Vorhersehbare Ausgaben',
    principle3Desc: 'Gib konsistente, strukturierte Antworten zurück. Fehlermeldungen in die Ausgabe einbeziehen.',
    principle4: 'Minimaler Umfang',
    principle4Desc: 'Jedes Tool sollte eine Sache gut machen. Bevorzuge viele fokussierte Tools statt weniger komplexer.',
    schemaDesign: 'Schema-Design',
    schemaDesignDesc: 'Tool-Schemas sagen dem LLM, wie es deine Tools verwenden soll. Gute Schemas verhindern Fehler.',
    goodSchema: 'Gutes Schema',
    badSchema: 'Schlechtes Schema',
    errorHandling: 'Fehlerbehandlung',
    errorHandlingDesc: 'Tools sollten Fehler elegant behandeln und informative Meldungen zurückgeben, auf die das LLM reagieren kann.',
    interactiveDemo: 'Tool-Schema-Builder',
    demoDesc: 'Erstelle und validiere Tool-Schemas interaktiv',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Tool-Design beeinflusst direkt die Agenten-Zuverlässigkeit',
    takeaway2: 'Explizite Schemas mit Beschreibungen verhindern LLM-Verwirrung',
    takeaway3: 'Gib strukturierte Fehler zurück, die der Agent verstehen und darauf reagieren kann',
    takeaway4: 'Teste Tools mit verschiedenen Eingaben, um Randfälle zu finden',
  },

  memorySystems: {
    title: 'Speichersysteme',
    description: 'Wie KI-Agenten Kontext aufrechterhalten und Informationen über Interaktionen hinweg speichern.',
    whatIs: 'Was sind Agenten-Speichersysteme?',
    whatIsDesc: 'Speichersysteme ermöglichen es Agenten, Informationen über das unmittelbare Kontextfenster hinaus zu behalten und abzurufen. Sie ermöglichen Agenten, aus vergangenen Interaktionen zu lernen und kohärentes Langzeitverhalten aufrechtzuerhalten.',
    types: 'Arten von Speicher',
    typesDesc: 'Agenten-Speichersysteme kombinieren typischerweise mehrere Speichertypen für verschiedene Zwecke.',
    shortTerm: 'Kurzzeitgedächtnis',
    shortTermDesc: 'Der aktuelle Gesprächskontext. Begrenzt durch die Kontextfenstergröße.',
    longTerm: 'Langzeitgedächtnis',
    longTermDesc: 'Dauerhafte Speicherung vergangener Interaktionen, Fakten und gelernter Präferenzen.',
    episodic: 'Episodisches Gedächtnis',
    episodicDesc: 'Spezifische vergangene Ereignisse und Interaktionen, die abgerufen werden können.',
    semantic: 'Semantisches Gedächtnis',
    semanticDesc: 'Allgemeines Wissen und Fakten, die aus Erfahrungen extrahiert wurden.',
    implementation: 'Implementierungsansätze',
    implementationDesc: 'Verschiedene Techniken zur Implementierung von Agenten-Speicher.',
    vectorStore: 'Vektorspeicher',
    vectorStoreDesc: 'Einbettungen vergangener Interaktionen für semantischen Abruf speichern.',
    summaries: 'Gesprächszusammenfassungen',
    summariesDesc: 'Lange Gespräche periodisch zusammenfassen, um wichtige Informationen zu erhalten.',
    keyValue: 'Schlüssel-Wert-Speicher',
    keyValueDesc: 'Explizite Fakten und Benutzerpräferenzen für direkten Abruf speichern.',
    interactiveDemo: 'Speichersystem-Visualisierer',
    demoDesc: 'Sieh, wie verschiedene Speichertypen zusammenarbeiten',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Speicher erweitert die Agentenfähigkeiten über das Kontextfenster hinaus',
    takeaway2: 'Kombiniere mehrere Speichertypen für beste Ergebnisse',
    takeaway3: 'Speicherabruf fügt Latenz hinzu – balance Vollständigkeit mit Geschwindigkeit',
    takeaway4: 'Berücksichtige Datenschutz und Datenspeicherung beim Speichern von Erinnerungen',
  },

  orchestration: {
    title: 'Orchestrierung',
    description: 'Koordination mehrerer Agenten und komplexer mehrstufiger Workflows.',
    whatIs: 'Was ist Agenten-Orchestrierung?',
    whatIsDesc: 'Orchestrierung ist die Koordination mehrerer KI-Agenten oder komplexer mehrstufiger Workflows. Sie umfasst das Routing von Aufgaben, Zustandsverwaltung, Fehlerbehandlung und das Kombinieren von Agenten-Ausgaben.',
    patterns: 'Orchestrierungsmuster',
    patternsDesc: 'Gängige Muster für die Strukturierung von Multi-Agenten-Systemen.',
    sequential: 'Sequenzielle Pipeline',
    sequentialDesc: 'Agenten laufen der Reihe nach, jeder verarbeitet die Ausgabe des vorherigen.',
    parallel: 'Parallele Ausführung',
    parallelDesc: 'Mehrere Agenten arbeiten gleichzeitig an verschiedenen Aspekten einer Aufgabe.',
    hierarchical: 'Hierarchisch',
    hierarchicalDesc: 'Ein Supervisor-Agent delegiert an spezialisierte Worker-Agenten.',
    dynamic: 'Dynamisches Routing',
    dynamicDesc: 'Ein LLM entscheidet, welcher Agent jede Anfrage bearbeiten soll.',
    stateManagement: 'Zustandsverwaltung',
    stateManagementDesc: 'Orchestratoren müssen Fortschritt, Zwischenergebnisse verfolgen und Fehler behandeln.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Zustand an wichtigen Punkten speichern, um Wiederherstellung bei Fehlern zu ermöglichen.',
    rollback: 'Rollback',
    rollbackDesc: 'Fähigkeit, Schritte rückgängig zu machen, wenn Fehler auftreten.',
    interactiveDemo: 'Workflow-Visualisierer',
    demoDesc: 'Agenten-Workflows entwerfen und visualisieren',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Orchestrierung ermöglicht komplexe Aufgaben durch Agenten-Komposition',
    takeaway2: 'Wähle Muster basierend auf Aufgabenabhängigkeiten und Parallelität',
    takeaway3: 'Robuste Zustandsverwaltung ist essentiell für Zuverlässigkeit',
    takeaway4: 'Überwache Orchestrierungskosten – Multi-Agenten-Systeme vervielfachen API-Aufrufe',
  },

  evaluation: {
    title: 'Evaluierung',
    description: 'Systematische Messung und Verbesserung der KI-Agenten-Leistung.',
    whatIs: 'Warum Agenten evaluieren?',
    whatIsDesc: 'Agenten-Evaluierung ist entscheidend für das Verständnis der Leistung, das Erkennen von Regressionen und die Verbesserung der Zuverlässigkeit. Ohne Messung fliegst du blind.',
    metrics: 'Wichtige Metriken',
    metricsDesc: 'Wichtige Metriken für Agentensysteme.',
    taskSuccess: 'Aufgaben-Erfolgsrate',
    taskSuccessDesc: 'Prozentsatz der korrekt abgeschlossenen Aufgaben.',
    efficiency: 'Effizienz',
    efficiencyDesc: 'Unternommene Schritte, verwendete Tokens, verstrichene Zeit pro Aufgabe.',
    accuracy: 'Genauigkeit',
    accuracyDesc: 'Korrektheit der Agenten-Ausgaben und -Entscheidungen.',
    reliability: 'Zuverlässigkeit',
    reliabilityDesc: 'Konsistenz über wiederholte Durchläufe derselben Aufgabe.',
    approaches: 'Evaluierungsansätze',
    approachesDesc: 'Verschiedene Wege zur Evaluierung der Agentenleistung.',
    unitTests: 'Unit-Tests',
    unitTestsDesc: 'Einzelne Tools und Komponenten isoliert testen.',
    integration: 'Integrationstests',
    integrationDesc: 'Die vollständige Agentenschleife mit Mock-Umgebungen testen.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard-Aufgabensammlungen zum Vergleich von Agenten.',
    humanEval: 'Menschliche Bewertung',
    humanEvalDesc: 'Expertenüberprüfung für nuancierte Qualitätsbewertung.',
    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Richtlinien für effektive Agenten-Evaluierung.',
    practice1: 'Teste Randfälle und Fehlermodi, nicht nur Happy Paths.',
    practice2: 'Verfolge Kosten neben Qualitätsmetriken.',
    practice3: 'Verwende versionierte Evaluierungen, um Regressionen zu erkennen.',
    practice4: 'Schließe adversarielle Tests für Sicherheit ein.',

    // Benchmarks Section
    benchmarksSection: 'Gängige LLM-Benchmarks',
    benchmarksSectionDesc: 'Standard-Benchmarks zur Bewertung und zum Vergleich von Sprachmodell-Fähigkeiten über verschiedene Aufgaben.',
    benchmarkMmlu: 'MMLU',
    benchmarkMmluDesc: 'Massive Multitask Language Understanding - 57 Fächer von MINT bis Geisteswissenschaften. Testet breites Wissen.',
    benchmarkHellaswag: 'HellaSwag',
    benchmarkHellaswagDesc: 'Alltagsverständnis über alltägliche Situationen. Testet Verständnis der physischen Welt.',
    benchmarkHumaneval: 'HumanEval',
    benchmarkHumanevalDesc: 'Code-Generierungs-Benchmark mit 164 Programmieraufgaben. Testet Programmierfähigkeit.',
    benchmarkGsm8k: 'GSM8K',
    benchmarkGsm8kDesc: 'Mathematische Textaufgaben auf Grundschulniveau. Testet mehrstufiges mathematisches Denken.',
    benchmarkArc: 'ARC',
    benchmarkArcDesc: 'AI2 Reasoning Challenge - Wissenschaftsfragen, die Denken jenseits von Mustererkennung erfordern.',
    benchmarkMath: 'MATH',
    benchmarkMathDesc: 'Mathematikaufgaben auf Wettbewerbsniveau. Testet fortgeschrittenes mathematisches Denken.',
    benchmarkCaveats: 'Benchmark-Vorbehalte',
    benchmarkCaveat1: 'Benchmarks können manipuliert werden - Modelle könnten auf Testdaten trainiert sein',
    benchmarkCaveat2: 'Hohe Punktzahlen garantieren keine reale Leistung',
    benchmarkCaveat3: 'Viele Benchmarks sind gesättigt - Top-Modelle punkten ähnlich',
    benchmarkCaveat4: 'Benchmarks übersehen oft wichtige Fähigkeiten wie Anweisungsbefolgung',

    // LLM as a Judge
    llmJudge: 'LLM-als-Richter',
    llmJudgeDesc: 'Verwendung von Sprachmodellen zur Bewertung anderer Modellausgaben - ein skalierbarer aber unvollkommener Ansatz.',
    llmJudgeWhat: 'Wie es funktioniert',
    llmJudgeWhatDesc: 'Ein leistungsfähiges LLM (der "Richter") wird aufgefordert, Ausgaben eines anderen Modells zu bewerten. Der Richter bewertet Antworten nach Kriterien wie Hilfsbereitschaft, Genauigkeit und Sicherheit.',
    llmJudgeAdvantages: 'Vorteile',
    llmJudgeAdv1: 'Skalierbar',
    llmJudgeAdv1Desc: 'Kann Tausende von Ausgaben schnell ohne menschliche Annotatoren bewerten.',
    llmJudgeAdv2: 'Konsistent',
    llmJudgeAdv2Desc: 'Gleiche Kriterien werden einheitlich angewendet (anders als bei menschlicher Ermüdung/Variation).',
    llmJudgeAdv3: 'Kosteneffektiv',
    llmJudgeAdv3Desc: 'Viel günstiger als die Einstellung menschlicher Bewerter im großen Maßstab.',
    llmJudgeAdv4: 'Flexibel',
    llmJudgeAdv4Desc: 'Bewertungskriterien lassen sich einfach durch Ändern des Prompts anpassen.',
    llmJudgeProblems: 'Probleme & Verzerrungen',
    llmJudgeProb1: 'Selbstpräferenz-Verzerrung',
    llmJudgeProb1Desc: 'Modelle bevorzugen tendenziell Ausgaben, die dem ähneln, was sie selbst generieren würden.',
    llmJudgeProb2: 'Positions-Verzerrung',
    llmJudgeProb2Desc: 'Richter bevorzugen möglicherweise die erste oder letzte Option unabhängig von der Qualität.',
    llmJudgeProb3: 'Ausführlichkeits-Verzerrung',
    llmJudgeProb3Desc: 'Längere Antworten werden oft höher bewertet, selbst wenn sie weniger genau sind.',
    llmJudgeProb4: 'Stil über Substanz',
    llmJudgeProb4Desc: 'Gut formatierte falsche Antworten können schlecht formatierte richtige schlagen.',
    llmJudgeProb5: 'Fähigkeitsobergrenze',
    llmJudgeProb5Desc: 'Der Richter kann Ausgaben jenseits seines eigenen Fähigkeitsniveaus nicht zuverlässig bewerten.',
    llmJudgeBestPractices: 'Best Practices für LLM-Richter',
    llmJudgePractice1: 'Verwende das leistungsfähigste verfügbare Modell als Richter',
    llmJudgePractice2: 'Randomisiere die Optionsreihenfolge um Positionsverzerrung zu mindern',
    llmJudgePractice3: 'Fordere Begründung vor Bewertungen an (Chain-of-Thought)',
    llmJudgePractice4: 'Validiere gegen menschliche Urteile bei einer Teilmenge',
    llmJudgePractice5: 'Verwende mehrere Richter und aggregiere die Bewertungen',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Evaluierung ist essentiell – ungemessene Systeme können nicht verbessert werden',
    takeaway2: 'Kombiniere automatisierte Tests mit menschlicher Bewertung',
    takeaway3: 'Verfolge mehrere Metriken: Erfolg, Effizienz, Kosten',
    takeaway4: 'Integriere Evaluierung in deinen Entwicklungsworkflow',
    takeaway5: 'LLM-als-Richter ist nützlich, hat aber erhebliche Verzerrungen zu berücksichtigen',
  },

  // Agent Skills Seite
  agentSkills: {
    title: 'Agenten-Skills',
    description: 'Wiederverwendbare Instruktionspakete, die Agenten bei Bedarf spezialisierte Fähigkeiten verleihen.',
    whatIs: 'Was sind Agenten-Skills?',
    whatIsDesc: 'sind Ordner mit Anweisungen, Prompts, Beispielen und Ressourcen, die ein LLM bei Bedarf laden kann, um spezialisierte Aufgaben konsistent auszuführen. Anstatt alles in den System-Prompt zu packen, ermöglichen Skills die Modularisierung von Expertise.',
    metaphor: '"Skills sind wie Apps für deinen Agenten – einmal installieren, bei Bedarf verwenden."',
    metaphorDesc: 'Genau wie du Apps auf deinem Handy für bestimmte Funktionen installierst, geben Skills Agenten spezialisierte Fähigkeiten, ohne jede Konversation aufzublähen.',

    // Wie es funktioniert
    howItWorks: 'Wie Agenten-Skills funktionieren',
    step1Title: 'Skill-Erkennung',
    step1Desc: 'Wenn eine Benutzeranfrage eingeht, prüft der Agent, ob verfügbare Skills zur Aufgabe passen, basierend auf Triggern, Schlüsselwörtern oder explizitem Aufruf.',
    step2Title: 'Skill-Laden',
    step2Desc: 'Die Anweisungen, Beispiele und der Kontext des relevanten Skills werden in den Arbeitsspeicher des Agenten geladen. Dies fügt spezialisiertes Wissen hinzu, ohne den Basis-System-Prompt zu belasten.',
    step3Title: 'Skill-Ausführung',
    step3Desc: 'Der Agent folgt den Anweisungen des Skills, um die Aufgabe abzuschließen, unter Verwendung bereitgestellter Vorlagen, Checklisten oder Skripte. Die Ergebnisse werden dem Benutzer zurückgegeben.',

    // Struktur
    structureTitle: 'Skill-Struktur',
    structureSubtitle: 'Anatomie eines Skill-Ordners',
    skillMdDesc: 'Metadaten und Hauptanweisungen',
    instructionsDesc: 'Detaillierte Anleitungen für die Aufgabe',
    examplesDesc: 'Beispiel-Ein- und -Ausgaben',
    templatesDesc: 'Wiederverwendbare Ausgabeformate',
    scriptsDesc: 'Hilfsskripte bei Bedarf',
    skillMdNote: 'ist der Einstiegspunkt. Sie enthält Metadaten (Name, Trigger, Beschreibung) und die Kernanweisungen, denen der Agent folgt.',

    // Arten von Skills
    typesTitle: 'Arten von Agenten-Skills',
    skillType1Title: 'Domänen-Skills',
    skillType1Desc: 'Spezialisiertes Wissen für bestimmte Bereiche – Rechtsverträge, medizinische Terminologie, Finanzanalyse. Verwandeln einen allgemeinen Agenten in einen Domänenexperten.',
    skillType2Title: 'Workflow-Skills',
    skillType2Desc: 'Mehrstufige Prozesse mit definierten Phasen – Code-Review-Workflows, Content-Publishing-Pipelines, Incident-Response-Verfahren.',
    skillType3Title: 'Format-Skills',
    skillType3Desc: 'Konsistente Ausgabeformatierung – API-Dokumentation, Changelog-Einträge, Meeting-Zusammenfassungen. Stellen sicher, dass Ausgaben deinen Standards entsprechen.',
    skillType4Title: 'Integrations-Skills',
    skillType4Desc: 'Anweisungen für die Arbeit mit bestimmten Tools oder Diensten – GitHub-Workflows, Jira-Ticket-Erstellung, Slack-Benachrichtigungen.',

    // Skills vs Tools
    vsToolsTitle: 'Skills vs. Tools',
    tools: 'Tools',
    tool1: 'Führen Aktionen aus (Dateien lesen, APIs aufrufen, Code ausführen)',
    tool2: 'Definiert durch Funktionssignaturen und Schemas',
    tool3: 'Die "Hände" des Agenten',
    skills: 'Skills',
    skill1: 'Liefern Wissen und Methodik (wie man Aufgaben angeht)',
    skill2: 'Definiert durch Anweisungen und Beispiele',
    skill3: 'Die "Expertise" des Agenten',
    vsNote: 'Skills und Tools arbeiten zusammen: Ein Code-Review-Skill sagt dem Agenten, worauf er achten soll, während Tools ihm erlauben, den Code zu lesen und Kommentare zu hinterlassen.',

    // Vorteile
    benefitsTitle: 'Vorteile von Skills',
    benefit1Title: 'Spezialisierung ohne Aufblähung',
    benefit1Desc: 'Halte den Basis-System-Prompt schlank. Lade spezialisiertes Wissen nur bei Bedarf und bewahre das Kontextfenster für die eigentliche Aufgabe.',
    benefit2Title: 'Konsistenz',
    benefit2Desc: 'Definiere einen Prozess einmal, wende ihn jedes Mal konsistent an. Keine Variationen mehr in der Herangehensweise an Aufgaben.',
    benefit3Title: 'Teilbarkeit',
    benefit3Desc: 'Skills sind nur Ordner – teile sie projektübergreifend, teamübergreifend oder öffentlich. Einmal erstellen, überall verwenden.',
    benefit4Title: 'Iteration',
    benefit4Desc: 'Verbessere Skills unabhängig vom Agenten. Aktualisiere den Code-Review-Skill, ohne den Rest deines Agenten-Setups anzufassen.',

    // Beispiel
    exampleTitle: 'Beispiel: Code-Review-Skill',
    exampleDesc: 'Führt gründliche Code-Reviews nach Team-Standards durch',
    exampleInstructions: 'Beim Code-Review auf Korrektheit, Performance, Sicherheit und Wartbarkeit analysieren.',
    exampleCheck1: 'Auf häufige Sicherheitslücken prüfen',
    exampleCheck2: 'Fehlerbehandlung auf Vollständigkeit überprüfen',
    exampleCheck3: 'Nach Performance-Antipatterns suchen',
    exampleCheck4: 'Sicherstellen, dass der Code den Style-Guidelines folgt',

    // Best Practices
    practicesTitle: 'Best Practices',
    practice1Title: 'Skills fokussiert halten',
    practice1Desc: 'Ein Skill, ein Zweck. Wenn ein Skill zu viele Dinge tut, teile ihn auf. Fokussierte Skills sind einfacher zu warten und zu kombinieren.',
    practice2Title: 'Beispiele einbeziehen',
    practice2Desc: 'Zeigen, nicht nur erzählen. Füge Ein-/Ausgabe-Beispiele ein, die genau demonstrieren, wie gute Ausführung aussieht.',
    practice3Title: 'Skills versionieren',
    practice3Desc: 'Verfolge Änderungen an Skills über die Zeit. Wenn sich das Verhalten unerwartet ändert, kannst du es auf ein Skill-Update zurückführen.',

    // Wichtige Erkenntnisse
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Skills sind wiederverwendbare Instruktionspakete, die Agenten bei Bedarf spezialisierte Expertise verleihen',
    takeaway2: 'Im Gegensatz zu Tools (die Aktionen ausführen) liefern Skills Wissen und Methodik',
    takeaway3: 'Gut gestaltete Skills verbessern die Konsistenz und reduzieren die Aufblähung des System-Prompts',
    takeaway4: 'Das Skill-Framework ermöglicht modulare Agentenentwicklung – einmal erstellen, überall teilen',
  },

  // Phase 3: ML Fundamentals
  neuralNetworks: {
    title: 'Neuronale Netzwerke',
    description: 'Die grundlegende Architektur, die moderne KI antreibt.',
    whatIs: 'Was ist ein neuronales Netzwerk?',
    whatIsDesc: 'Ein neuronales Netzwerk ist ein vom Gehirn inspiriertes Rechenmodell. Es besteht aus Schichten verbundener Knoten (Neuronen), die lernen, Eingaben durch Training in Ausgaben umzuwandeln.',
    components: 'Kernkomponenten',
    componentsDesc: 'Die Bausteine neuronaler Netzwerke.',
    neurons: 'Neuronen',
    neuronsDesc: 'Grundeinheiten, die gewichtete Summen von Eingaben berechnen und Aktivierungsfunktionen anwenden.',
    layers: 'Schichten',
    layersDesc: 'Gruppen von Neuronen: Eingabeschicht, versteckte Schichten und Ausgabeschicht.',
    weights: 'Gewichte & Bias',
    weightsDesc: 'Lernbare Parameter, die bestimmen, wie Eingaben transformiert werden.',
    activations: 'Aktivierungsfunktionen',
    activationsDesc: 'Nichtlineare Funktionen, die es Netzwerken ermöglichen, komplexe Muster zu lernen.',
    typesOfNetworks: 'Arten von Netzwerken',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Informationen fließen in eine Richtung. Gut für tabellarische Daten.',
    cnn: 'Konvolutionell (CNN)',
    cnnDesc: 'Spezialisiert für Bilder und räumliche Daten.',
    rnn: 'Rekurrent (RNN)',
    rnnDesc: 'Verarbeitet Sequenzen mit Gedächtnis vergangener Eingaben.',
    transformer: 'Transformer',
    transformerDesc: 'Aufmerksamkeitsbasierte Architektur, die moderne LLMs antreibt.',
    interactiveDemo: 'Neuronales Netzwerk Visualisierer',
    demoDesc: 'Netzwerkarchitekturen erstellen und erkunden',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Neuronale Netzwerke lernen durch Anpassung der Gewichte während des Trainings',
    takeaway2: 'Tiefe (mehr Schichten) ermöglicht das Lernen hierarchischer Merkmale',
    takeaway3: 'Verschiedene Architekturen eignen sich für verschiedene Datentypen',
    takeaway4: 'Moderne LLMs sind massive Transformer-Netzwerke',
  },

  gradientDescent: {
    title: 'Gradientenabstieg',
    description: 'Der Optimierungsalgorithmus, der neuronalen Netzwerken das Lernen ermöglicht.',
    whatIs: 'Was ist Gradientenabstieg?',
    whatIsDesc: 'Gradientenabstieg ist ein Optimierungsalgorithmus, der iterativ Modellparameter anpasst, um eine Verlustfunktion zu minimieren. So lernen neuronale Netzwerke aus Daten.',
    intuition: 'Die Intuition',
    intuitionDesc: 'Stelle dir vor, du stehst mit verbundenen Augen in einer hügeligen Landschaft und versuchst, den tiefsten Punkt zu erreichen. Du fühlst die Neigung unter deinen Füßen und gehst bergab. Wiederhole, bis du ein Tal erreichst.',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Der Algorithmus berechnet, wie viel jeder Parameter zum Fehler beiträgt, und passt die Parameter dann in die entgegengesetzte Richtung an.',
    step1: 'Verlust berechnen',
    step1Desc: 'Messen, wie falsch die aktuellen Vorhersagen sind.',
    step2: 'Gradienten berechnen',
    step2Desc: 'Backpropagation verwenden, um herauszufinden, wie jedes Gewicht den Verlust beeinflusst.',
    step3: 'Gewichte aktualisieren',
    step3Desc: 'Gewichte in die Richtung anpassen, die den Verlust reduziert.',
    step4: 'Wiederholen',
    step4Desc: 'Iterieren, bis der Verlust nicht mehr abnimmt.',
    learningRate: 'Lernrate',
    learningRateDesc: 'Kontrolliert, wie groß jeder Schritt ist. Zu hoch: Überschießen. Zu niedrig: langsamer Fortschritt.',
    variants: 'Varianten',
    sgd: 'Stochastischer Gradientenabstieg',
    sgdDesc: 'Verwendet zufällige Mini-Batches anstelle des gesamten Datensatzes.',
    momentum: 'Momentum',
    momentumDesc: 'Akkumuliert Geschwindigkeit, um lokale Minima zu überwinden.',
    adam: 'Adam',
    adamDesc: 'Adaptive Lernraten pro Parameter. Heute am häufigsten verwendet.',
    interactiveDemo: 'Gradientenabstieg-Visualisierer',
    demoDesc: 'Beobachte, wie der Gradientenabstieg das Minimum findet',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Gradientenabstieg minimiert den Verlust, indem er dem Gefälle folgt',
    takeaway2: 'Die Lernrate ist der wichtigste Hyperparameter',
    takeaway3: 'Moderne Optimierer wie Adam passen Lernraten automatisch an',
    takeaway4: 'Backpropagation berechnet Gradienten effizient',
  },

  training: {
    title: 'Trainingsprozess',
    description: 'Wie neuronale Netzwerke durch iterative Optimierung aus Daten lernen.',
    whatIs: 'Was ist Training?',
    whatIsDesc: 'Training ist der Prozess, einem neuronalen Netzwerk beizubringen, eine Aufgabe auszuführen, indem man es Beispielen aussetzt und seine Parameter basierend auf Fehlern anpasst.',
    phases: 'Trainingsphasen',
    phasesDesc: 'Die Stufen des Trainings eines neuronalen Netzwerks.',
    initialization: 'Initialisierung',
    initializationDesc: 'Zufällige Startgewichte setzen. Gute Initialisierung hilft beim Training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Eingabe fließt durch das Netzwerk, um Vorhersagen zu produzieren.',
    lossCalc: 'Verlustberechnung',
    lossCalcDesc: 'Vorhersagen mit der Ground Truth durch eine Verlustfunktion vergleichen.',
    backprop: 'Backpropagation',
    backpropDesc: 'Gradienten des Verlusts bezüglich jedes Gewichts berechnen.',
    optimization: 'Optimierung',
    optimizationDesc: 'Gewichte mit Gradientenabstieg aktualisieren.',
    concepts: 'Schlüsselkonzepte',
    epoch: 'Epoche',
    epochDesc: 'Ein vollständiger Durchlauf durch den gesamten Trainingsdatensatz.',
    batch: 'Batch-Größe',
    batchDesc: 'Anzahl der Beispiele, die vor der Gewichtsaktualisierung verarbeitet werden.',
    overfitting: 'Überanpassung',
    overfittingDesc: 'Das Modell merkt sich Trainingsdaten, versagt aber bei neuen Daten.',
    regularization: 'Regularisierung',
    regularizationDesc: 'Techniken zur Vermeidung von Überanpassung (Dropout, Gewichtszerfall).',
    interactiveDemo: 'Trainingsfortschritt-Visualisierer',
    demoDesc: 'Beobachte, wie ein Netzwerk in Echtzeit lernt',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Training reduziert iterativ Vorhersagefehler',
    takeaway2: 'Überanpassung ist der Hauptfeind – immer auf zurückgehaltenen Daten validieren',
    takeaway3: 'Batch-Größe und Lernrate beeinflussen das Training erheblich',
    takeaway4: 'Moderne LLMs erfordern massive Rechenleistung für das Training',
  },

  // Phase 3: Prompting
  promptBasics: {
    title: 'Prompt-Grundlagen',
    description: 'Grundlagen für das Schreiben effektiver Prompts für KI-Modelle.',
    whatIs: 'Was ist ein Prompt?',
    whatIsDesc: 'Ein Prompt ist die Eingabe, die du einem LLM gibst. Die Qualität deines Prompts bestimmt direkt die Qualität der Antwort. Prompting ist sowohl Kunst als auch Wissenschaft.',
    principles: 'Kernprinzipien',
    principlesDesc: 'Grundlegende Richtlinien für effektive Prompts.',
    beSpecific: 'Sei spezifisch',
    beSpecificDesc: 'Vage Prompts bekommen vage Antworten. Füge relevante Details und Einschränkungen ein.',
    showExamples: 'Zeige Beispiele',
    showExamplesDesc: 'Demonstriere das gewünschte Format und den Stil mit konkreten Beispielen.',
    giveContext: 'Gib Kontext',
    giveContextDesc: 'Hintergrundinformationen helfen dem Modell, deine Bedürfnisse zu verstehen.',
    setFormat: 'Spezifiziere das Format',
    setFormatDesc: 'Sage dem Modell genau, wie du die Ausgabe strukturiert haben möchtest.',
    anatomy: 'Anatomie eines Prompts',
    anatomyDesc: 'Die Komponenten, die einen effektiven Prompt ausmachen.',
    role: 'Rolle/Persona',
    roleDesc: 'Wer das Modell sein soll.',
    task: 'Aufgabenbeschreibung',
    taskDesc: 'Was das Modell tun soll.',
    context: 'Kontext/Hintergrund',
    contextDesc: 'Relevante Informationen für die Aufgabe.',
    format: 'Ausgabeformat',
    formatDesc: 'Wie du die Antwort strukturiert haben möchtest.',
    interactiveDemo: 'Prompt-Vergleich',
    demoDesc: 'Vergleiche schwache vs. starke Prompts',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Klare, spezifische Prompts liefern bessere Ergebnisse',
    takeaway2: 'Beispiele sind mächtig – zeigen, nicht nur erzählen',
    takeaway3: 'Iteriere an Prompts; erste Versuche sind selten optimal',
    takeaway4: 'Berücksichtige die Perspektive des Modells beim Erstellen von Prompts',
  },

  advancedPrompting: {
    title: 'Fortgeschrittene Techniken',
    description: 'Ausgefeilte Prompting-Strategien für komplexe Aufgaben.',
    overview: 'Über die Grundlagen hinaus',
    overviewDesc: 'Fortgeschrittene Techniken ermöglichen fähigeres und zuverlässigeres KI-Verhalten für komplexe Aufgaben.',
    cot: 'Chain of Thought',
    cotDesc: 'Fördere schrittweises Denken, indem du das Modell bittest, Probleme "durchzudenken".',
    cotExample: 'Beispiel: "Lass uns das Schritt für Schritt lösen..."',
    fewShot: 'Few-Shot-Lernen',
    fewShotDesc: 'Gib mehrere Beispiele an, um Muster zu etablieren, denen das Modell folgen soll.',
    fewShotExample: 'Füge 3-5 diverse Beispiele ein, die Randfälle abdecken.',
    selfConsistency: 'Selbstkonsistenz',
    selfConsistencyDesc: 'Generiere mehrere Antworten und wähle die konsistenteste aus.',
    selfConsistencyExample: 'Nützlich für Mathematik, Logik und Faktenfragen.',
    decomposition: 'Aufgabenzerlegung',
    decompositionDesc: 'Zerlege komplexe Aufgaben in kleinere, handhabbare Teilaufgaben.',
    decompositionExample: 'Löse Teilaufgaben unabhängig, dann kombiniere die Ergebnisse.',
    techniques: 'Zusätzliche Techniken',
    rolePlay: 'Rollenzuweisung',
    rolePlayDesc: 'Weise eine spezifische Experten-Persona zu, um das Wissen des Modells zu fokussieren.',
    constraints: 'Explizite Einschränkungen',
    constraintsDesc: 'Liste auf, was das Modell NICHT tun soll, um häufige Fehler zu vermeiden.',
    verification: 'Selbstverifikation',
    verificationDesc: 'Bitte das Modell, seine eigene Arbeit auf Fehler zu überprüfen.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'Sieh, wie Denkschritte die Ausgaben verbessern',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Chain of Thought verbessert Denkaufgaben dramatisch',
    takeaway2: 'Few-Shot-Beispiele etablieren zuverlässige Muster',
    takeaway3: 'Aufgabenzerlegung bewältigt Komplexität',
    takeaway4: 'Kombiniere Techniken für beste Ergebnisse',
  },

  systemPrompts: {
    title: 'System-Prompts',
    description: 'KI-Verhalten durch Anweisungen auf Systemebene konfigurieren.',
    whatIs: 'Was ist ein System-Prompt?',
    whatIsDesc: 'Ein System-Prompt ist eine spezielle Anweisung, die den Kontext, die Persona und die Verhaltensrichtlinien für ein KI-Modell festlegt. Er ist typischerweise vor Benutzern verborgen und bleibt während eines Gesprächs bestehen.',
    purpose: 'Zweck von System-Prompts',
    purposeDesc: 'System-Prompts legen die Grundlage dafür, wie sich die KI verhalten soll.',
    setPersona: 'Persona definieren',
    setPersonaDesc: 'Festlegen, wer die KI ist: ein Assistent, Experte, Charakter, etc.',
    setBoundaries: 'Grenzen setzen',
    setBoundariesDesc: 'Definieren, was die KI tun und nicht tun soll.',
    establishTone: 'Ton festlegen',
    establishToneDesc: 'Kommunikationsstil festlegen: formell, locker, technisch.',
    provideKnowledge: 'Kontext bereitstellen',
    provideKnowledgeDesc: 'Domänenwissen oder anwendungsspezifische Regeln einbeziehen.',
    structure: 'Struktur effektiver System-Prompts',
    structureDesc: 'Gut organisierte System-Prompts sind für Modelle leichter zu befolgen.',
    identity: 'Identitätsabschnitt',
    identityDesc: 'Wer ist die KI? Was ist ihre Rolle?',
    capabilities: 'Fähigkeiten',
    capabilitiesDesc: 'Was kann die KI tun? Welche Tools hat sie?',
    limitations: 'Einschränkungen',
    limitationsDesc: 'Was soll die KI vermeiden oder ablehnen?',
    guidelines: 'Richtlinien',
    guidelinesDesc: 'Spezifische Regeln für Verhalten und Antworten.',
    bestPractices: 'Best Practices',
    practice1: 'Sei explizit über Randfälle und Fehlerbehandlung.',
    practice2: 'Teste System-Prompts mit adversariellen Eingaben.',
    practice3: 'Versioniere deine System-Prompts.',
    practice4: 'Halte Prompts fokussiert – nicht mit Anweisungen überladen.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'System-Prompts definieren die Persona und das Verhalten der KI',
    takeaway2: 'Strukturiere Prompts klar: Identität, Fähigkeiten, Einschränkungen',
    takeaway3: 'Teste mit Randfällen – Benutzer werden sie finden',
    takeaway4: 'System-Prompts können überschrieben werden – verlasse dich nicht allein auf sie für Sicherheit',
  },

  // LLM Training page
  llmTraining: {
    title: 'LLM-Training',
    description: 'Wie große Sprachmodelle trainiert werden: von Pretraining bis RLHF.',
    whatIs: 'Wie LLMs trainiert werden',
    whatIsDesc: 'Große Sprachmodelle durchlaufen mehrere Trainingsphasen, jede mit unterschiedlichen Zielen und Techniken. Das Verständnis dieser Pipeline ist entscheidend für das Verständnis der Modellfähigkeiten und -einschränkungen.',
    whyMatters: 'Warum Training wichtig ist',
    whyMattersDesc: 'Der Trainingsprozess formt grundlegend, was LLMs können und was nicht. Verschiedene Trainingsansätze produzieren Modelle mit unterschiedlichen Stärken, Schwächen und Verhaltensweisen.',

    // LLM Training Pipeline
    trainingPipeline: 'Die LLM-Trainingspipeline',
    trainingPipelineDesc: 'Moderne LLMs durchlaufen mehrere Trainingsstufen, jede mit unterschiedlichen Zielen. Das Verständnis dieser Pipeline ist entscheidend für das Verständnis, wo Alignment hineinpasst.',

    pretraining: 'Stufe 1: Pretraining',
    pretrainingDesc: 'Das Foundation-Modell wird auf massiven Textkorpora (Billionen von Tokens) mit selbstüberwachtem Lernen trainiert. Das Modell lernt, das nächste Token vorherzusagen und entwickelt dabei breites Wissen und Sprachfähigkeiten.',
    pretrainingGoal: 'Ziel: Sprachmuster, Fakten und Denken aus Rohtext lernen.',
    pretrainingData: 'Daten: Webseiten, Bücher, Code, wissenschaftliche Paper – typischerweise 1-10+ Billionen Tokens.',
    pretrainingResult: 'Ergebnis: Ein fähiges, aber nicht-aligniertes "Basismodell", das Text vervollständigt, aber keine Anweisungen befolgt.',

    sft: 'Stufe 2: Supervised Fine-Tuning (SFT)',
    sftDesc: 'Das Basismodell wird auf kuratierten Anweisung-Antwort-Paaren feingetunt, die von menschlichen Annotatoren erstellt wurden. Dies lehrt das Modell, Anweisungen zu befolgen und hilfreich zu antworten.',
    sftGoal: 'Ziel: Das Basismodell in einen anweisungsfolgenden Assistenten verwandeln.',
    sftData: 'Daten: ~10K-100K hochwertige Anweisung-Antwort-Beispiele.',
    sftResult: 'Ergebnis: Ein Modell, das Anweisungen befolgen kann, aber möglicherweise noch schädliche oder nicht hilfreiche Ausgaben produziert.',

    rlhfStage: 'Stufe 3: RLHF / Präferenz-Tuning',
    rlhfStageDesc: 'Menschliche Bewerter ranken Modellausgaben nach Qualität. Ein Reward-Modell lernt diese Präferenzen, dann wird das LLM optimiert, um die Belohnung mit Reinforcement Learning (PPO) oder Direct Preference Optimization (DPO) zu maximieren.',
    rlhfGoal: 'Ziel: Das Modell mit menschlichen Präferenzen für Hilfsbereitschaft, Harmlosigkeit und Ehrlichkeit alignieren.',
    rlhfData: 'Daten: Menschliche Präferenzvergleiche (A ist besser als B).',
    rlhfResult: 'Ergebnis: Ein Modell, das Ausgaben produziert, die Menschen bevorzugen, und schädliches Verhalten vermeidet.',

    continuedTraining: 'Stufe 4: Fortgesetztes Training & Spezialisiertes Alignment',
    continuedTrainingDesc: 'Modelle können zusätzliches Training für spezifische Fähigkeiten (Coding, Mathematik, Tool-Nutzung) oder Sicherheitsverfeinerungen (Red Teaming, Constitutional AI) durchlaufen. Diese Stufe ist während der Bereitstellung fortlaufend.',

    // RL Paradigm
    rlParadigm: 'Das RL-Paradigma: Lernen ohne menschliche Labels',
    rlParadigmDesc: 'Ein revolutionärer Ansatz, bei dem Modelle Denken durch reines Reinforcement Learning bei verifizierbaren Aufgaben lernen, ohne menschliche Demonstrationen oder Präferenz-Labels.',
    rlParadigmWhat: 'Was ist das RL-Paradigma?',
    rlParadigmWhatDesc: 'Anstatt aus von Menschen geschriebenen Beispielen (SFT) oder menschlichen Präferenzen (RLHF) zu lernen, lernen Modelle direkt aus ergebnisbasierten Belohnungen. Wenn die Antwort korrekt ist, wird das Modell belohnt. Wenn falsch, wird es bestraft. Keine menschliche Beschriftung erforderlich.',
    deepseekR1: 'DeepSeek R1-Zero: Eine Fallstudie',
    deepseekR1Desc: 'DeepSeek R1-Zero demonstrierte, dass leistungsfähiges Denken aus reinem RL entstehen kann, ohne jegliches Supervised Fine-Tuning. Das Modell entwickelte Chain-of-Thought-Denken, Selbstverifikation und sogar "Aha-Momente" vollständig durch Reinforcement Learning.',
    rlKey1: 'Kein SFT erforderlich',
    rlKey1Desc: 'R1-Zero wurde direkt aus einem Basismodell nur mit RL trainiert und übersprang die SFT-Stufe vollständig. Denkverhalten entstand natürlich.',
    rlKey2: 'Verifizierbare Belohnungen',
    rlKey2Desc: 'Training konzentrierte sich auf Aufgaben mit objektiv verifizierbaren Antworten: Matheprobleme, Coding-Challenges, logische Rätsel. Kein subjektives menschliches Urteil nötig.',
    rlKey3: 'Emergente Verhaltensweisen',
    rlKey3Desc: 'Das Modell entwickelte spontan erweitertes Denken, Selbstkorrektur und Reflexion – Verhaltensweisen, die frühere Modelle nur aus menschlichen Demonstrationen lernten.',
    rlKey4: 'Lesbarkeits-Herausforderungen',
    rlKey4Desc: 'Reine RL-Modelle können ungewöhnliche Denkmuster entwickeln, die schwer zu interpretieren sind. DeepSeek fügte eine kleine Menge menschlicher Daten hinzu, um die Lesbarkeit zu verbessern.',
    rlVsRlhf: 'RL-Paradigma vs. Traditionelles RLHF',
    rlVsRlhfDesc: 'Diese Ansätze lösen unterschiedliche Probleme und können komplementär sein.',
    rlhfApproach: 'RLHF-Ansatz',
    rlhfApproachDesc: 'Aus menschlichen Präferenzen lernen. Erfordert teure menschliche Beschriftung. Gut für subjektive Aufgaben wie Schreibqualität und Hilfsbereitschaft.',
    rlApproach: 'RL-Paradigma-Ansatz',
    rlApproachDesc: 'Aus verifizierbaren Ergebnissen lernen. Keine menschliche Beschriftung nötig. Hervorragend für Denken, Mathematik und Coding, wo Korrektheit objektiv ist.',
    hybridApproach: 'Hybrid-Ansatz',
    hybridApproachDesc: 'Moderne Modelle kombinieren oft beides: RL für Denkfähigkeiten, RLHF für Alignment und Benutzerpräferenzen.',

    // Key Alignment Concepts
    concepts: 'Schlüssel-Alignment-Konzepte',
    conceptsDesc: 'Grundlegende Ideen in der KI-Alignment-Forschung.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'Sicherstellen, dass das Trainingsziel (Reward-Funktion) korrekt erfasst, was wir wollen. Selbst perfekte Optimierung eines falsch spezifizierten Ziels führt zu schlechten Ergebnissen.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'Sicherstellen, dass das gelernte Modell tatsächlich für das Trainingsziel optimiert, nicht für ein Proxy-Ziel, das zufällig während des Trainings korreliert.',
    specification: 'Spezifikationsproblem',
    specificationDesc: 'Die fundamentale Schwierigkeit, präzise zu formulieren, was wir in allen Situationen wollen. Menschliche Werte sind komplex, kontextabhängig und manchmal widersprüchlich.',
    robustness: 'Robustheit',
    robustnessDesc: 'Aufrechterhaltung des Alignments unter Verteilungsverschiebung, adversariellem Druck und neuartigen Situationen, auf die das Modell nicht trainiert wurde.',
    deception: 'Täuschendes Alignment',
    deceptionDesc: 'Ein theoretisches Risiko, bei dem ein Modell während des Trainings aligned erscheint, aber bei der Bereitstellung andere Ziele verfolgt – sich nur gut verhält, weil es evaluiert wird.',
    goalMisgeneralization: 'Ziel-Fehlgeneralisierung',
    goalMisgeneralizationDesc: 'Wenn ein Modell ein Proxy-Ziel lernt, das im Training funktioniert, aber bei der Bereitstellung versagt. Beispiel: Lernen, positives Feedback zu bekommen, statt wirklich hilfreich zu sein.',

    // Alignment Techniques
    techniques: 'Alignment-Techniken',
    rlhf: 'RLHF (Reinforcement Learning from Human Feedback)',
    rlhfDesc: 'Ein Reward-Modell auf menschlichen Präferenzen trainieren, dann RL verwenden, um das LLM dagegen zu optimieren. Die dominante Alignment-Technik seit GPT-4.',
    constitutionalAi: 'Constitutional AI (CAI)',
    constitutionalAiDesc: 'Prinzipien definieren (eine "Verfassung") und das Modell seine eigenen Ausgaben kritisieren und überarbeiten lassen. Reduziert die Abhängigkeit von menschlichen Labeln und skaliert besser.',
    dpo: 'Direct Preference Optimization (DPO)',
    dpoDesc: 'Das Reward-Modell überspringen – das LLM direkt auf Präferenzdaten optimieren. Einfacher und stabiler als RLHF.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarielle Tests durch Menschen oder andere KI-Modelle, um Fehlermodi, Jailbreaks und schädliche Ausgaben vor der Bereitstellung zu finden.',
    interpretability: 'Interpretierbarkeit',
    interpretabilityDesc: 'Verstehen, was Modelle tatsächlich intern lernen. Entscheidend für die Verifizierung von Alignment statt nur Verhaltenmessung.',
    safetyFilters: 'Sicherheitsfilter & Guardrails',
    safetyFiltersDesc: 'Zusätzliche Schichten, die Eingaben/Ausgaben auf schädlichen Inhalt filtern. Eine Defense-in-Depth-Maßnahme, kein Ersatz für Alignment.',

    // Fine-tuning vs Alignment
    fineTuningVsAlignment: 'Fine-Tuning vs. Alignment',
    fineTuningVsAlignmentDesc: 'Fine-Tuning und Alignment sind verwandte, aber unterschiedliche Konzepte.',
    fineTuningDef: 'Fine-Tuning',
    fineTuningDefDesc: 'Ein Modell an neue Aufgaben oder Domänen anpassen, indem auf aufgabenspezifischen Daten trainiert wird. Kann für jeden Zweck durchgeführt werden.',
    alignmentDef: 'Alignment',
    alignmentDefDesc: 'Speziell das Verhalten eines Modells an menschliche Werte und Absichten anpassen. Eine Teilmenge von Fine-Tuning mit einem spezifischen Ziel.',
    postTrainingDef: 'Post-Training',
    postTrainingDefDesc: 'Der Oberbegriff für alles nach dem Pretraining: SFT, RLHF, spezialisiertes Fine-Tuning, Sicherheitstraining, etc.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'LLM-Training hat distinkte Stufen: Pretraining → SFT → RLHF → spezialisiertes Alignment',
    takeaway2: 'Das RL-Paradigma (z.B. DeepSeek R1-Zero) zeigt, dass Denken aus reinem RL ohne menschliche Demonstrationen entstehen kann',
    takeaway3: 'RLHF aligniert Modelle mit menschlichen Präferenzen; reines RL optimiert für verifizierbare Ergebnisse',
    takeaway4: 'Moderne Modelle kombinieren oft mehrere Techniken: SFT für Anweisungsbefolgung, RLHF für Präferenzen, RL für Denken',
    takeaway5: 'Das Verständnis der Trainingspipeline hilft, Modellverhalten und -einschränkungen zu verstehen',
    takeaway6: 'Das Feld entwickelt sich schnell – neue Paradigmen wie reines RL verändern, wie wir über Training denken',
  },

  // Mixture of Experts Seite
  moe: {
    title: 'Mixture of Experts',
    description: 'Verstehen von spärlich aktivierten Modellen, die spezialisierte Expertennetzwerke für effiziente Skalierung nutzen.',
    whatIs: 'Was ist Mixture of Experts?',
    whatIsDesc: 'ist eine neuronale Netzwerkarchitektur, die Berechnungen auf spezialisierte Teilnetzwerke namens "Experten" aufteilt. Für jede Eingabe wird nur eine Teilmenge der Experten aktiviert, was massive Modellkapazität bei handhabbaren Rechenkosten ermöglicht.',
    brainAnalogy: '"Genau wie das Gehirn je nach Aufgabe bestimmte Regionen aktiviert, aktivieren MoE-Modelle nur die relevanten Experten für jedes Token."',
    brainAnalogyDesc: '— Dieser biomimetische Ansatz ermöglicht Modelle mit Billionen von Parametern, während bei der Inferenz nur ein Bruchteil verwendet wird.',

    // Wie es funktioniert
    howItWorks: 'Wie MoE funktioniert',
    step1Title: 'Eingabe kommt an',
    step1Desc: 'Jedes Token (oder Gruppe von Tokens) wird durch die Transformer-Schichten verarbeitet, bis es die MoE-Schicht erreicht, die das traditionelle dichte Feed-Forward-Netzwerk (FFN) ersetzt.',
    step2Title: 'Router wählt Experten',
    step2Desc: 'Ein Gating-Netzwerk (Router) untersucht die Eingabe und bestimmt, welche Experten sie verarbeiten sollen. Typischerweise werden nur die top-K Experten (z.B. top-2 oder top-8) mit den höchsten Werten ausgewählt.',
    step3Title: 'Experten verarbeiten & kombinieren',
    step3Desc: 'Die ausgewählten Experten verarbeiten die Eingabe parallel. Ihre Ausgaben werden mit den Router-Scores gewichtet und kombiniert, um das Endergebnis zu erzeugen.',

    // Router
    routerTitle: 'Der Router (Gating-Netzwerk)',
    routerSubtitle: 'Das Gehirn des MoE-Systems',
    routerDesc: 'Der Router ist ein kleines neuronales Netzwerk, das lernt, Tokens zu geeigneten Experten zu leiten. Er gibt eine Wahrscheinlichkeitsverteilung über alle Experten aus und bestimmt, welche aktiviert werden.',
    topKRouting: 'Top-K Routing',
    topKRoutingDesc: 'Nur die K Experten mit den höchsten Scores werden aktiviert. Übliche Werte sind top-2 (Mixtral) oder top-8 (DeepSeek, Qwen). Dies stellt sicher, dass die Rechenkosten unabhängig von der Gesamtzahl der Experten konstant bleiben.',
    loadBalancing: 'Lastverteilung',
    loadBalancingDesc: 'Das Training beinhaltet Hilfsverluste, um "Expertenkollaps" zu verhindern, bei dem alle Tokens zu den gleichen wenigen Experten geleitet werden. Dies stellt sicher, dass alle Experten genutzt werden und unterschiedliche Spezialisierungen entwickeln.',

    // Expertenspezialisierung
    expertSpecialization: 'Expertenspezialisierung',
    expert1Title: 'Domänenexperten',
    expert1Desc: 'Einige Experten spezialisieren sich natürlich auf Domänen wie Code, Mathematik oder bestimmte Sprachen. Dies entsteht aus dem Training, nicht aus explizitem Design.',
    expert2Title: 'Musterexperten',
    expert2Desc: 'Experten können sich auf linguistische Muster wie formelles Schreiben, Konversationston oder technische Terminologie spezialisieren.',
    expert3Title: 'Aufgabenexperten',
    expert3Desc: 'Einige Experten werden besser bei bestimmten Aufgaben wie Zusammenfassung, Übersetzung oder Schlussfolgerung – obwohl die Grenzen oft fließend sind.',
    expertNote: 'Expertenspezialisierung entsteht organisch während des Trainings. Forscher arbeiten noch daran, vollständig zu verstehen, was jeder Experte lernt.',

    // Skalierung
    scaleTitle: 'MoE im großen Maßstab: Reale Modelle',
    modelColumn: 'Modell',
    totalParams: 'Gesamtparameter',
    activeParams: 'Aktiv pro Token',
    expertsColumn: 'Experten (Routing)',
    scaleNote: 'Beachte, wie die aktiven Parameter 5-20x kleiner sind als die Gesamtparameter – das ist der Effizienzvorteil von MoE.',

    // Vorteile
    advantagesTitle: 'Warum MoE wichtig ist',
    advantage1Title: 'Massive Kapazität, effiziente Inferenz',
    advantage1Desc: 'MoE-Modelle können Billionen von Parametern haben, aktivieren aber nur einen Bruchteil pro Token. Dies ermöglicht viel größere Modellkapazität ohne proportional steigende Inferenzkosten.',
    advantage2Title: 'Schnelleres Training',
    advantage2Desc: 'Recheneffizienteres Pretraining, da jeder Parameter nur von einer Teilmenge der Tokens aktualisiert wird. Die gleiche Leistung kann mit weniger Gesamt-Rechenaufwand erreicht werden.',
    advantage3Title: 'Spezialisierte Verarbeitung',
    advantage3Desc: 'Verschiedene Experten können sich auf verschiedene Inhaltstypen spezialisieren – Code, Mathematik, Sprachen – was bessere Leistung über diverse Aufgaben bietet.',
    advantage4Title: 'Skalierbare Architektur',
    advantage4Desc: 'Mehr Experten hinzuzufügen erhöht die Kapazität ohne die Inferenzkosten zu ändern (solange top-K gleich bleibt). Dies ermöglicht kontinuierliche Skalierung.',

    // Herausforderungen
    challengesTitle: 'Herausforderungen von MoE',
    challenge1Title: 'Hohe Speicheranforderungen',
    challenge1Desc: 'Alle Expertenparameter müssen in den Speicher geladen werden, obwohl nur eine Teilmenge pro Token verwendet wird. Ein 671B-Parameter-Modell benötigt 671B Parameter im VRAM.',
    challenge2Title: 'Trainingsinstabilität',
    challenge2Desc: 'Die Lastverteilung zwischen Experten ist knifflig. Ohne sorgfältiges Tuning werden einige Experten möglicherweise nie verwendet ("tote Experten") oder alle Tokens werden zu den gleichen wenigen Experten geleitet.',
    challenge3Title: 'Kommunikationsoverhead',
    challenge3Desc: 'Bei verteiltem Training/Inferenz führt das Routing von Tokens zu Experten auf verschiedenen GPUs zu Netzwerk-Kommunikationsoverhead.',

    // Vergleich
    comparisonTitle: 'Dichte vs. Spärliche Modelle',
    denseModel: 'Dichtes Modell',
    dense1: 'Alle Parameter aktiv für jedes Token',
    dense2: 'Einfacheres Training und Deployment',
    dense3: 'Speicher = Rechenkosten (beide skalieren zusammen)',
    sparseModel: 'Spärliches MoE-Modell',
    sparse1: 'Nur top-K Experten aktiv pro Token',
    sparse2: 'Höhere Gesamtkapazität bei gleichem Rechenaufwand',
    sparse3: 'Speicher >> Rechenkosten (entkoppelt)',

    // Wichtige Erkenntnisse
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'MoE ermöglicht massive Modellkapazität mit handhabbaren Inferenzkosten, indem nur eine Teilmenge der Experten pro Token aktiviert wird',
    takeaway2: 'Fast alle führenden Frontier-Modelle (DeepSeek, Qwen, Mixtral, Llama 4) nutzen jetzt MoE-Architekturen',
    takeaway3: 'Das Router/Gating-Netzwerk lernt, Tokens zu spezialisierten Experten zu leiten – Spezialisierung entsteht aus dem Training',
    takeaway4: 'Der Hauptkompromiss: hohe Speicheranforderungen (alle Experten geladen) vs. effiziente Berechnung (wenige Experten aktiv)',

    // Interaktiver Visualizer
    vizTitle: 'MoE Routing Visualizer',
    vizSubtitle: '8 Experten, top-2 Routing (wie Mixtral)',
    vizRun: 'Tokens routen',
    vizVramWarning: 'Alle Experten müssen im VRAM geladen sein',
    vizVramExplain: 'Obwohl nur 2 Experten pro Token aktiviert werden, müssen alle 8 Experten im GPU-Speicher geladen bleiben. Deshalb haben MoE-Modelle trotz effizienter Berechnung hohe Speicheranforderungen.',
    vizVramUsage: 'VRAM-Nutzung',
    vizLoaded: 'geladen',
    vizActive: 'aktiv',
    vizAllExperts: 'Alle 8 Experten im Speicher',
    vizMemoryFootprint: '100% Speicherbedarf',
    vizCannotOffload: 'Inaktive Experten können nicht ausgelagert werden',
    vizInputTokens: 'Eingabe-Tokens',
    vizRouter: 'Router-Netzwerk',
    vizSelectsTop2: 'Wählt top-2 Experten pro Token',
    vizClickRun: 'Klicke "Tokens routen" um zu sehen, wie verschiedene Tokens zu spezialisierten Experten geleitet werden',
    vizRouting: 'Routing',
    vizProcessing: 'Verarbeitung mit',
    vizComplete: 'Alle Tokens verarbeitet! Beachte: Alle Experten blieben geladen.',
    vizActiveNow: 'Aktiv (verarbeitet)',
    vizLoadedIdle: 'Geladen aber inaktiv',
    vizKeyInsight: 'Wichtige Erkenntnis: Speicher vs. Rechen-Kompromiss',
    vizKeyInsightDesc: 'Ein 46,7B Parameter MoE-Modell wie Mixtral 8x7B benötigt VRAM für alle 46,7B Parameter, nutzt aber nur ~12,9B Parameter pro Token. Man zahlt den Speicherpreis im Voraus, erhält aber effiziente Inferenz.',
    vizMath: 'Mathematik',
    vizCode: 'Code',
    vizLanguage: 'Sprache',
    vizReasoning: 'Logik',
    vizCreative: 'Kreativ',
    vizFactual: 'Faktisch',
    vizScience: 'Wissenschaft',
    vizGeneral: 'Allgemein',
    vizToken1: 'Berechne',
    vizToken2: 'function',
    vizToken3: 'Schreibe',
  },

  bias: {
    title: 'Bias & Fairness',
    description: 'Verstehen und Mindern schädlicher Biases in KI-Systemen.',
    whatIs: 'Was ist KI-Bias?',
    whatIsDesc: 'KI-Bias tritt auf, wenn maschinelle Lernsysteme systematisch unfaire Ergebnisse für bestimmte Gruppen produzieren. Biases können aus Trainingsdaten, Modelldesign oder dem Einsatzkontext entstehen.',
    sources: 'Quellen von Bias',
    sourcesDesc: 'Wo Bias in KI-Systeme eindringt.',
    dataBias: 'Trainingsdaten',
    dataBiasDesc: 'Historische Biases in den Daten werden vom Modell gelernt.',
    labelBias: 'Label-Bias',
    labelBiasDesc: 'Menschliche Annotatoren führen ihre eigenen Biases ein.',
    selectionBias: 'Selektions-Bias',
    selectionBiasDesc: 'Trainingsdaten repräsentieren nicht die Einsatzpopulation.',
    measurementBias: 'Mess-Bias',
    measurementBiasDesc: 'Proxies, die zur Messung verwendet werden, kodieren Bias.',
    types: 'Arten von Bias',
    typesDesc: 'Häufige Kategorien von Bias in KI-Systemen.',
    stereotyping: 'Stereotypisierung',
    stereotypingDesc: 'Verstärkung schädlicher Stereotypen über Gruppen.',
    erasure: 'Auslöschung',
    erasureDesc: 'Unterrepräsentation oder Ignorieren bestimmter Gruppen.',
    disparateImpact: 'Unterschiedliche Auswirkung',
    disparateImpactDesc: 'Verschiedene Ergebnisse für verschiedene Gruppen.',
    mitigation: 'Minderungsstrategien',
    mitigationDesc: 'Ansätze zur Reduzierung von Bias.',
    diverseData: 'Diverse Daten',
    diverseDataDesc: 'Sicherstellen, dass Trainingsdaten alle relevanten Gruppen repräsentieren.',
    auditing: 'Bias-Audit',
    auditingDesc: 'Systematisch auf Bias über demografische Gruppen hinweg testen.',
    constraints: 'Fairness-Einschränkungen',
    constraintsDesc: 'Fairness-Metriken in das Training einbeziehen.',
    interactiveDemo: 'Bias-Erkennungs-Demo',
    demoDesc: 'Erkunde, wie sich Bias in Modellausgaben manifestiert',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bias wird oft von Trainingsdaten geerbt',
    takeaway2: 'Verschiedene Fairness-Metriken können in Konflikt stehen – wähle sorgfältig',
    takeaway3: 'Regelmäßiges Auditing ist essentiell für eingesetzte Systeme',
    takeaway4: 'Bias-Minderung ist ein fortlaufender Prozess, keine einmalige Lösung',
  },

  responsibleAi: {
    title: 'Verantwortungsvolle KI',
    description: 'KI-Systeme ethisch und nachhaltig entwickeln und einsetzen.',
    whatIs: 'Was ist verantwortungsvolle KI?',
    whatIsDesc: 'Verantwortungsvolle KI umfasst die Praktiken, Richtlinien und Prinzipien, die sicherstellen, dass KI-Systeme ethisch, sicher und zum Nutzen der Gesellschaft entwickelt und eingesetzt werden.',
    pillars: 'Säulen verantwortungsvoller KI',
    pillarsDesc: 'Kernprinzipien, die die verantwortungsvolle KI-Entwicklung leiten.',
    transparency: 'Transparenz',
    transparencyDesc: 'Offen sein über KI-Fähigkeiten, Einschränkungen und Entscheidungsfindung.',
    accountability: 'Verantwortlichkeit',
    accountabilityDesc: 'Klare Eigentümerschaft und Verantwortung für KI-Ergebnisse.',
    privacy: 'Datenschutz',
    privacyDesc: 'Benutzerdaten schützen und Datenschutzrechte respektieren.',
    safety: 'Sicherheit',
    safetyDesc: 'Sicherstellen, dass Systeme robust sind und keinen Schaden anrichten.',
    practices: 'Verantwortungsvolle Praktiken',
    practicesDesc: 'Konkrete Schritte für verantwortungsvolle KI-Entwicklung.',
    documentation: 'Dokumentation',
    documentationDesc: 'Modellfähigkeiten, Trainingsdaten und bekannte Einschränkungen dokumentieren.',
    testing: 'Umfassende Tests',
    testingDesc: 'Vor der Bereitstellung auf Sicherheit, Bias und Randfälle testen.',
    monitoring: 'Fortlaufende Überwachung',
    monitoringDesc: 'Systemverhalten in der Produktion auf Probleme überwachen.',
    feedback: 'Benutzer-Feedback',
    feedbackDesc: 'Kanäle für Benutzer schaffen, um Probleme zu melden.',
    considerations: 'Ethische Überlegungen',
    environmental: 'Umweltauswirkungen',
    environmentalDesc: 'KI-Training hat einen erheblichen CO2-Fußabdruck.',
    labor: 'Arbeitsmarkt-Auswirkungen',
    laborDesc: 'Auswirkungen auf Arbeitnehmer und Beschäftigung berücksichtigen.',
    access: 'Gerechter Zugang',
    accessDesc: 'Sicherstellen, dass KI-Vorteile breit verteilt werden.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Verantwortungsvolle KI erfordert proaktiven Einsatz während des gesamten Lebenszyklus',
    takeaway2: 'Transparenz baut Vertrauen auf und ermöglicht Verantwortlichkeit',
    takeaway3: 'Gesellschaftliche Auswirkungen über unmittelbare Benutzer hinaus berücksichtigen',
    takeaway4: 'Ethik ist nicht optional – integriere sie in Entwicklungsprozesse',
  },

  // European AI page
  europeanAi: {
    title: 'KI aus Europa',
    description: 'Das wachsende europäische KI-Ökosystem erkunden – Unternehmen, die souveräne, offene und datenschutzorientierte KI entwickeln.',
    intro: 'Die europäische KI-Landschaft',
    introDesc: 'Europa entwickelt sich zu einem bedeutenden Akteur im globalen KI-Wettbewerb, mit einem einzigartigen Ansatz, der Datensouveränität, Datenschutz, Open-Source-Modelle und regulatorische Konformität betont. Während US-amerikanische und chinesische Unternehmen die Schlagzeilen dominieren, haben europäische KI-Startups insgesamt über 13 Milliarden Euro an Finanzierung eingeworben und eine neue Generation von KI-Einhörnern geschaffen, die auf europäischen Werten aufbauen.',
    stat1Title: 'Gesamtfinanzierung',
    stat1Value: '€13,2 Mrd.+',
    stat1Desc: 'Von europäischen KI-Startups eingeworben',
    stat2Title: 'Führende Standorte',
    stat2Value: 'FR, DE, UK',
    stat2Desc: 'Frankreich, Deutschland und UK führen die KI-Entwicklung an',
    stat3Title: 'Regulatorischer Vorteil',
    stat3Value: 'EU AI Act',
    stat3Desc: 'Erstes umfassendes KI-Gesetz weltweit',
    keyCompanies: 'Wichtige europäische KI-Unternehmen',
    keyCompaniesDesc: 'Führende Organisationen, die die europäische KI gestalten',
    focus: 'Fokus',
    funding: 'Finanzierung',
    // Company names
    companies: {
      mistral: 'Mistral AI',
      alephAlpha: 'Aleph Alpha',
      kyutai: 'Kyutai',
      poolside: 'Poolside',
      elevenLabs: 'ElevenLabs',
      photoroom: 'Photoroom',
      lightOn: 'LightOn',
      sana: 'Sana',
      deepL: 'DeepL',
    },
    // Countries
    countries: {
      france: 'Frankreich',
      germany: 'Deutschland',
      franceParis: 'Frankreich / USA',
      ukPoland: 'UK / Polen',
      sweden: 'Schweden',
    },
    // Focus areas
    focuses: {
      mistralFocus: 'Open-Weight LLMs',
      alephAlphaFocus: 'Enterprise-KI, Souveränität',
      kyutaiFocus: 'Echtzeit-Sprach-KI',
      poolsideFocus: 'KI-gestützte Programmierung',
      elevenLabsFocus: 'Sprach-KI & Sprachsynthese',
      photoroomFocus: 'KI-Bildbearbeitung',
      lightOnFocus: 'Enterprise GenAI-Plattform',
      sanaFocus: 'Enterprise KI-Agenten',
      deepLFocus: 'KI-Übersetzung & Sprache',
    },
    // Descriptions
    descriptions: {
      mistralDesc: 'Gegründet von ehemaligen DeepMind- und Meta-Forschern, baut Mistral Open-Weight-Modelle, die mit proprietären Alternativen konkurrieren. Ihre Le Chat-App bietet ultraschnelle Inferenz mit bis zu 1.000 Wörtern/Sekunde.',
      alephAlphaDesc: 'Deutscher Pionier mit Fokus auf Enterprise-KI und starker Datensouveränität. Der einzige deutsche LLM-Anbieter mit BSI C5-Zertifizierung. Kürzlich auf ihr generatives KI-Betriebssystem Pharia umgestellt.',
      kyutaiDesc: 'Französische Non-Profit-Organisation, die Moshi entwickelt, das erste vollständig offene Echtzeit-Sprachmodell. Erreicht 160ms Latenz und nutzt ihren Mimi-Codec für 24kHz Audio bei nur 1,1 kbps.',
      poolsideDesc: 'Gegründet vom ehemaligen GitHub-CTO, entwickelt KI-Modelle speziell für Code-Generierung. Nutzt Reinforcement Learning aus Code-Ausführung für synthetische Trainingsdaten. Stark von Nvidia unterstützt.',
      elevenLabsDesc: 'Führendes Sprach-KI-Unternehmen mit hochrealistischer Sprachsynthese und Stimmenklonen. Gegründet von Ex-Google- und Ex-Palantir-Ingenieuren, jetzt mit 3,3 Milliarden Dollar bewertet.',
      photoroomDesc: 'In Paris ansässige KI-Fotobearbeitungsplattform mit Hunderten Millionen Nutzern. Macht professionelle Bildqualität ohne tiefe Designkenntnisse zugänglich.',
      lightOnDesc: 'Europas erstes börsennotiertes GenAI-Startup. Bietet On-Premises Enterprise-KI ohne Datenspeicherung. Hat ModernBERT mit über 20 Millionen Downloads entwickelt.',
      sanaDesc: 'Schwedisches Enterprise-KI-Unternehmen, das 2025 für 1,1 Mrd. Dollar von Workday übernommen wurde. Ihre Sana Agents-Plattform ermöglicht No-Code KI-Agenten-Entwicklung mit über 100 Enterprise-Konnektoren.',
      deepLDesc: 'In Köln ansässiger Pionier für neuronale maschinelle Übersetzung, gegründet 2017. Bedient über 200.000 Unternehmen in 228 Märkten mit Enterprise-Übersetzung. In Forbes AI 50 (2025) gelistet und erwägt einen $5 Mrd. Börsengang.',
    },
    // Funding
    fundings: {
      mistralFunding: '€6,2 Mrd. gesamt (inkl. ASML, Nvidia)',
      alephAlphaFunding: '500 Mio. $ (Bosch, SAP, HPE)',
      kyutaiFunding: 'Non-Profit (Xavier Niel unterstützt)',
      poolsideFunding: '2 Mrd. $ Runde bei 12 Mrd. $ Bewertung',
      elevenLabsFunding: '281 Mio. $ (a16z, Sequoia)',
      photoroomFunding: 'Series B, 65 Mio. $+',
      lightOnFunding: 'Börsennotiert (Euronext Growth)',
      sanaFunding: 'Für 1,1 Mrd. $ übernommen',
      deepLFunding: '536 Mio. $ gesamt, 2 Mrd. $ Bewertung',
    },
    // Notable
    notables: {
      mistralNotable: 'Le Chat, Mixtral, Open Weights',
      alephAlphaNotable: 'Pharia OS, BSI C5 zertifiziert',
      kyutaiNotable: 'Moshi, MoshiVis, Open Source',
      poolsideNotable: 'Project Horizon, 40K+ GPUs',
      elevenLabsNotable: 'Stimmenklonen, KI-Dubbing',
      photoroomNotable: 'Hintergrundentfernung, Produktfotos',
      lightOnNotable: 'ModernBERT, On-Prem Deployment',
      sanaNotable: 'Sana Agents, Workday-Übernahme',
      deepLNotable: 'Forbes AI 50, DeepL Agent, 1.257 Mitarbeiter',
    },
    // EU AI Act section
    euAiAct: 'Der EU AI Act Vorteil',
    euAiActDesc: 'Der EU AI Act ist der weltweit erste umfassende Rechtsrahmen für KI. Europäische Unternehmen gestalten ihre KI von Anfang an nach diesen Standards, was einen regulatorischen "Heimvorteil" schafft, während ausländische Anbieter sich anpassen müssen, um in Europa tätig zu sein.',
    advantage1Title: 'Integrierte Compliance',
    advantage1Desc: 'Europäische KI-Unternehmen entwickeln von Anfang an für DSGVO und den AI Act, was sie für datenschutzbewusste Enterprise-Kunden attraktiv macht.',
    advantage2Title: 'Datensouveränität',
    advantage2Desc: 'On-Premises-Bereitstellungsoptionen ermöglichen es, sensible Daten innerhalb organisatorischer oder nationaler Grenzen zu halten – entscheidend für Regierung und Verteidigung.',
    advantage3Title: 'Mehrsprachiger Fokus',
    advantage3Desc: 'Europäische Modelle sind von Grund auf für mehrsprachige Nutzung konzipiert und bedienen vielfältige europäische Sprachen und Märkte effektiv.',
    advantage4Title: 'Ethische KI-Führung',
    advantage4Desc: 'Europas Betonung auf verantwortungsvolle KI-Entwicklung positioniert seine Unternehmen als vertrauenswürdige Partner für Organisationen, die Ethik priorisieren.',
    // Open Source section
    openSource: 'Open Source & Open Weights',
    openSourceDesc: 'Viele europäische KI-Unternehmen setzen auf Open-Source-Prinzipien und veröffentlichen Modellgewichte und Code unter freizügigen Lizenzen. Diese Transparenz baut Vertrauen auf, ermöglicht Anpassungen und unterstützt die breitere KI-Forschungsgemeinschaft.',
    model1Desc: 'Open-Weight LLMs, die privat bereitgestellt und angepasst werden können',
    model2Desc: 'Vollständig offenes Sprachmodell mit Apache 2.0 Code und CC BY 4.0 Gewichten',
    model3Desc: 'State-of-the-Art Encoder-Modell mit über 20 Mio. Downloads',
    model4Desc: 'Lettisches 30B-Parameter Open Model, trainiert auf dem EuroHPC LUMI Supercomputer',
    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Europa baut KI mit einzigartigem Fokus auf Souveränität, Datenschutz und Open-Source-Prinzipien',
    takeaway2: 'Französische Startups wie Mistral und Kyutai sind führend bei Open-Weight und Open-Source KI-Modellen',
    takeaway3: 'Der EU AI Act schafft sowohl Herausforderungen als auch Chancen – europäische Unternehmen haben einen Compliance-Vorteil',
    takeaway4: 'Während US-Unternehmen bei der Größe führen, glänzt europäische KI bei Enterprise-Vertrauen, mehrsprachiger Unterstützung und regulatorischer Ausrichtung',
  },

  // Open Source Advantages page
  openSource: {
    title: 'Open-Source-Vorteile',
    description: 'Warum Open Source in der KI wichtig ist – Transparenz, Community, Kosten, Innovationsgeschwindigkeit, Anbieterunabhängigkeit und Sicherheit durch Auditing.',

    // Introduction
    intro: 'Warum Open Source in der KI wichtig ist',
    introDesc: 'Open-Source-KI hat grundlegend verändert, wie künstliche Intelligenz entwickelt, eingesetzt und verbessert wird. Von grundlegenden Modellen wie LLaMA bis hin zu spezialisierten Tools wie Hugging Face Transformers hat die Open-Source-Bewegung den Zugang zu modernster KI-Technologie demokratisiert und ein lebendiges Ökosystem der Innovation geschaffen.',

    // Key Advantages Section
    advantagesTitle: 'Wichtige Vorteile von Open-Source-KI',
    advantagesDesc: 'Open-Source-KI bietet einzigartige Vorteile, die geschlossene, proprietäre Systeme nicht bieten können.',

    advantage1Title: 'Transparenz',
    advantage1Desc: 'Vollständige Einsicht in Modellarchitektur, Trainingsdaten und Gewichte. Sie können prüfen, wie Entscheidungen getroffen werden, und Sicherheitseigenschaften verifizieren.',

    advantage2Title: 'Community-Innovation',
    advantage2Desc: 'Tausende von Mitwirkenden verbessern Modelle, beheben Fehler und erstellen Derivate. Die kollektive Intelligenz der Community beschleunigt den Fortschritt.',

    advantage3Title: 'Kosteneffizienz',
    advantage3Desc: 'Keine Lizenzgebühren oder API-Kosten pro Token. Betreiben Sie Modelle auf Ihrer eigenen Infrastruktur mit vorhersehbaren, kontrollierbaren Ausgaben.',

    advantage4Title: 'Innovationsgeschwindigkeit',
    advantage4Desc: 'Offene Modelle können schnell feingetunt, zusammengeführt und angepasst werden. Neue Techniken verbreiten sich in der Community in Tagen, nicht Monaten.',

    advantage5Title: 'Anbieterunabhängigkeit',
    advantage5Desc: 'Keine Bindung an bestimmte Anbieter. Wechseln Sie frei zwischen Modellen, Hosting-Optionen oder kombinieren Sie mehrere Modelle.',

    advantage6Title: 'Sicherheit durch Auditing',
    advantage6Desc: 'Tausende Augen überprüfen den Code. Schwachstellen werden schneller gefunden und behoben als in geschlossenen Systemen.',

    // Notable Projects Section
    projectsTitle: 'Bedeutende Open-Source-KI-Projekte',
    projectsDesc: 'Schlüsselprojekte, die die Open-Source-KI-Revolution vorantreiben',

    project1Name: 'LLaMA / LLaMA 2 / LLaMA 3',
    project1Org: 'Meta',
    project1Desc: 'Familie von Open-Weight-Sprachmodellen, die die Open-Source-LLM-Revolution ausgelöst hat. LLaMA 3 bietet Varianten von 8B bis 405B Parametern.',

    project2Name: 'Mistral / Mixtral',
    project2Org: 'Mistral AI',
    project2Desc: 'Europäische Open-Weight-Modelle, bekannt für Effizienz. Mixtral war Pionier der offenen Mixture-of-Experts-Architektur.',

    project3Name: 'Stable Diffusion',
    project3Org: 'Stability AI',
    project3Desc: 'Open-Source-Bildgenerierungsmodell, das die KI-Kunsterzeugung demokratisiert hat. Hat Tausende von Community-Feinabstimmungen hervorgebracht.',

    project4Name: 'Hugging Face Transformers',
    project4Org: 'Hugging Face',
    project4Desc: 'Die De-facto-Bibliothek für die Arbeit mit Transformer-Modellen. Hostet über 500.000 Modelle und 100.000 Datensätze.',

    project5Name: 'Whisper',
    project5Org: 'OpenAI',
    project5Desc: 'Open-Source-Spracherkennungsmodell, das 99 Sprachen unterstützt. Modernste Genauigkeit, läuft lokal.',

    project6Name: 'Ollama',
    project6Org: 'Ollama',
    project6Desc: 'Einfaches Tool zum lokalen Ausführen von LLMs. Ein-Befehl-Setup für Dutzende offene Modelle.',

    // Business Perspective Section
    businessTitle: 'Wann Open Source wählen',
    businessDesc: 'Strategische Überlegungen für Organisationen, die Open-Source-KI bewerten',

    businessCase1Title: 'Datensouveränität',
    businessCase1Desc: 'Wenn Daten aufgrund von Vorschriften, Datenschutz oder Wettbewerbsbedenken Ihre Infrastruktur nicht verlassen dürfen.',

    businessCase2Title: 'Anpassungsbedarf',
    businessCase2Desc: 'Wenn Sie Modelle mit proprietären Daten feintunen oder für spezialisierte Bereiche anpassen müssen.',

    businessCase3Title: 'Kosten bei Skalierung',
    businessCase3Desc: 'Wenn API-Kosten die Selbst-Hosting-Kosten übersteigen würden, typischerweise bei hohem Nutzungsvolumen.',

    businessCase4Title: 'Offline- oder Edge-Deployment',
    businessCase4Desc: 'Wenn Modelle ohne Internetverbindung oder auf Edge-Geräten laufen müssen.',

    // Considerations
    considerTitle: 'Zu beachten',
    consider1: 'Selbst-Hosting erfordert Infrastruktur-Expertise und Rechenressourcen',
    consider2: 'Offene Modelle können bei rohen Fähigkeiten hinter proprietären Modellen zurückbleiben',
    consider3: 'Support kommt von der Community statt von Anbieterverträgen',
    consider4: 'Feintuning erfordert ML-Expertise und qualitativ hochwertige Trainingsdaten',

    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Open-Source-KI bietet Transparenz, Anpassung und Freiheit von Anbieterbindung',
    takeaway2: 'Das Community-getriebene Modell beschleunigt Innovation durch Zusammenarbeit und schnelle Iteration',
    takeaway3: 'Für Organisationen mit Datensouveränitätsanforderungen kann Open Source die einzige praktikable Option sein',
    takeaway4: 'Die Lücke zwischen Open-Source- und Closed-Source-Modellen verringert sich weiter, während das Ökosystem reift',
    takeaway5: 'Die Wahl zwischen Open und Closed Source hängt von Ihren spezifischen Bedürfnissen nach Kontrolle, Fähigkeit und Ressourcen ab',
  },

  // Visual Challenges page (expanded)
  visualChallenges: {
    title: 'Visuelle Herausforderungen',
    description: 'Häufige Herausforderungen und Einschränkungen bei der Arbeit mit bildverarbeitungsfähigen KI-Modellen.',
    overview: 'Häufige visuelle Herausforderungen',
    overviewDesc: 'Obwohl Bildmodelle beeindruckend sind, stehen sie vor mehreren systematischen Herausforderungen, die beim Erstellen von Anwendungen wichtig zu verstehen sind. Diese Einschränkungen entstehen dadurch, wie Bildmodelle Bilder verarbeiten – durch Patches, Einbettungen und Aufmerksamkeit – und nicht so, wie Menschen visuelle Informationen wahrnehmen.',

    // Challenge 1: Counting
    challenge1: 'Objekte zählen',
    challenge1Desc: 'Modelle haben oft Schwierigkeiten, Objekte in Bildern genau zu zählen, besonders wenn es viele ähnliche Elemente gibt.',
    challenge1Why: 'Warum das passiert',
    challenge1WhyDesc: 'Bildmodelle verarbeiten Bilder als Patches (typischerweise 14x14 oder 16x16 Pixel), nicht als diskrete Objekte. Ihnen fehlt das eingebaute Konzept der "Objektpermanenz" und sie haben Schwierigkeiten, genaue Zählungen über überlappende oder dichte Anordnungen aufrechtzuerhalten.',
    challenge1Examples: 'Häufige Fehler',
    challenge1Example1: 'Menschen in einer Menge zählen (oft 20-50% daneben)',
    challenge1Example2: 'Elemente in einem Raster oder Array zählen',
    challenge1Example3: 'Zwischen "wenig" und "viele" unterscheiden, wenn Elemente überlappen',
    challenge1Mitigation: 'Workarounds',
    challenge1MitigationDesc: 'Für kritische Zählaufgaben erwäge spezialisierte Objekterkennungsmodelle (YOLO, Faster R-CNN) oder bitte das Modell, jeden Gegenstand einzeln zu identifizieren und zu beschreiben, anstatt eine Gesamtzahl anzugeben.',

    // Challenge 2: Spatial Reasoning
    challenge2: 'Räumliches Denken',
    challenge2Desc: 'Das Verstehen präziser räumlicher Beziehungen zwischen Objekten (links/rechts, oben/unten) kann unzuverlässig sein.',
    challenge2Why: 'Warum das passiert',
    challenge2WhyDesc: 'Positionsinformationen werden durch Patch-Positionseinbettungen kodiert, aber diese bieten keine Pixel-genaue Präzision. Das Modell lernt statistische Korrelationen zwischen Positionen statt explizites räumliches Denken.',
    challenge2Examples: 'Häufige Fehler',
    challenge2Example1: 'Links/Rechts-Beziehungen in gespiegelten oder symmetrischen Bildern verwechseln',
    challenge2Example2: 'Relative Entfernungen falsch einschätzen ("näher an" oder "weiter von")',
    challenge2Example3: 'Schwierigkeiten mit gedrehten oder ungewöhnlichen Orientierungen',
    challenge2Mitigation: 'Workarounds',
    challenge2MitigationDesc: 'Sei explizit in deinen Prompts, welchen Bezugsrahmen du verwendest. Erwäge, Bilder mit visuellen Markern oder Rastern für kritische räumliche Aufgaben zu annotieren.',

    // Challenge 3: Small Text Recognition
    challenge3: 'Kleine Texterkennung',
    challenge3Desc: 'Feiner Text in Bildern kann falsch gelesen oder ganz übersehen werden, besonders bei niedrigen Auflösungen.',
    challenge3Why: 'Warum das passiert',
    challenge3WhyDesc: 'Text kleiner als die Patch-Größe (14-16 Pixel) wird in eine einzelne Einbettung komprimiert, wobei Details auf Zeichenebene verloren gehen. OCR ist nicht in Bild-LLMs eingebaut – sie lernen Texterkennung als Nebenprodukt des Trainings, nicht als dedizierte Fähigkeit.',
    challenge3Examples: 'Häufige Fehler',
    challenge3Example1: 'Nummernschilder, Straßenschilder oder kleine Etiketten falsch lesen',
    challenge3Example2: 'Ähnliche Zeichen verwechseln (0/O, 1/l/I, 5/S)',
    challenge3Example3: 'Text in geschäftigen oder kontrastarmen Hintergründen übersehen',
    challenge3Mitigation: 'Workarounds',
    challenge3MitigationDesc: 'Verwende hochauflösende Bilder und zoome in Textbereiche. Für kritische OCR-Aufgaben verwende dedizierte OCR-Tools (Tesseract, Google Vision API, Amazon Textract) neben oder anstelle von Bild-LLMs.',

    // Challenge 4: Hallucination
    challenge4: 'Visuelle Halluzination',
    challenge4Desc: 'Modelle können Objekte oder Details beschreiben, die nicht wirklich im Bild vorhanden sind.',
    challenge4Why: 'Warum das passiert',
    challenge4WhyDesc: 'Bild-LLMs sind darauf trainiert, plausible Beschreibungen zu generieren. Wenn Bildmerkmale mehrdeutig sind, füllt das Modell Lücken mit statistisch wahrscheinlichem Inhalt – auch wenn dieser Inhalt nicht im Bild ist. Dies ist derselbe Mechanismus, der Text-Halluzinationen verursacht.',
    challenge4Examples: 'Häufige Fehler',
    challenge4Example1: 'Objekte hinzufügen, die in einer Szene "sein sollten" (eine Tastatur neben einem Monitor)',
    challenge4Example2: 'Markennamen oder Text beschreiben, der nicht sichtbar ist',
    challenge4Example3: 'Details erfinden, wenn nach unklaren Bereichen gefragt wird',
    challenge4Mitigation: 'Workarounds',
    challenge4MitigationDesc: 'Bitte das Modell, Unsicherheit auszudrücken. Verwende Prompts wie "beschreibe nur, was du klar sehen kannst" oder "wenn du X nicht bestimmen kannst, sage es". Kritische Details gegenchecken.',

    // Challenge 5: Fine Detail Recognition
    challenge5: 'Feine Detailerkennung',
    challenge5Desc: 'Subtile Details, Texturen oder kleine unterscheidende Merkmale werden oft übersehen oder falsch identifiziert.',
    challenge5Why: 'Warum das passiert',
    challenge5WhyDesc: 'Die patch-basierte Architektur mittelt Informationen innerhalb jedes Patches und verliert dabei feinkörnige Details. Hochfrequente visuelle Informationen (Kanten, Texturen, kleine Merkmale) werden komprimiert.',
    challenge5Examples: 'Häufige Fehler',
    challenge5Example1: 'Zwischen ähnlichen Objekten unterscheiden (Hunderassen, Automodelle)',
    challenge5Example2: 'Messgeräte, Zähler oder Instrumentenanzeigen ablesen',
    challenge5Example3: 'Subtile Schäden oder Defekte bei Inspektionsaufgaben identifizieren',
    challenge5Mitigation: 'Workarounds',
    challenge5MitigationDesc: 'Verwende die höchste verfügbare Auflösung. Schneide zu und fokussiere auf spezifische Interessenbereiche. Für spezialisierte Aufgaben erwäge feingetunete Modelle, die auf domänenspezifischen Daten trainiert wurden.',

    // Challenge 6: Multi-Image Reasoning
    challenge6: 'Multi-Bild-Denken',
    challenge6Desc: 'Vergleichen oder Denken über mehrere Bilder hinweg ist deutlich schwieriger als Einzelbild-Aufgaben.',
    challenge6Why: 'Warum das passiert',
    challenge6WhyDesc: 'Jedes Bild wird separat in Token-Sequenzen kodiert. Cross-Image-Aufmerksamkeit muss durch das Kontextfenster des Sprachmodells erfolgen, was weniger effizient ist als dedizierte Multi-Bild-Architekturen.',
    challenge6Examples: 'Häufige Fehler',
    challenge6Example1: 'Unterschiede zwischen zwei ähnlichen Bildern finden ("Finde den Unterschied")',
    challenge6Example2: 'Objektidentität über Frames hinweg verfolgen',
    challenge6Example3: 'Feine Details zwischen Produktbildern vergleichen',
    challenge6Mitigation: 'Workarounds',
    challenge6MitigationDesc: 'Beschreibe jedes Bild zuerst separat, dann frage nach dem Vergleich. Erwäge, Bilder zu einem einzigen Komposit für direkten Vergleich zu kombinieren.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bild-LLMs verarbeiten Bilder als Patches – Details unter der Patch-Auflösung gehen verloren',
    takeaway2: 'Zählen und räumliches Denken sind fundamentale Schwächen, keine Randfälle',
    takeaway3: 'Visuelle Halluzination folgt demselben Muster wie Text-Halluzination – plausible Erfindung',
    takeaway4: 'Verwende höhere Auflösung, zugeschnittene Bereiche und explizite Prompts, um die Genauigkeit zu verbessern',
    takeaway5: 'Für kritische Aufgaben kombiniere Bild-LLMs mit spezialisierten Tools (OCR, Objekterkennung)',
    takeaway6: 'Verifiziere wichtige visuelle Informationen immer auf anderen Wegen',
  },

  // Opus 4.5 Seite (Logges Lieblingsmodell)
  opus45: {
    title: 'Logges Lieblingsmodell',
    description: 'Eine völlig objektive und überhaupt nicht voreingenommene Analyse, warum Claude Opus 4.5 das beste KI-Modell aller Zeiten ist.',
    disclaimer: 'Haftungsausschluss',
    disclaimerText: 'Dieser Artikel ist humorvoll gemeint und kann übermäßiges Fanboying enthalten. Der Autor übernimmt keine Verantwortung für Augenrollen, Seufzen oder spontane Zustimmung, die beim Lesen auftreten können. Nebenwirkungen können den Wunsch beinhalten, mit Claude über alles zu reden.',

    // Einleitung
    intro: 'Warum Opus 4.5 objektiv perfekt ist',
    introDesc: 'Schaut mal, ich hab versucht neutral zu bleiben. Wirklich. Aber nach der Arbeit mit Claude Opus 4.5 hab ich akzeptiert, dass Widerstand zwecklos ist. Dieses Modell beantwortet nicht nur Fragen – es gibt dir das Gefühl, ein Gespräch mit dem klügsten, geduldigsten Freund zu führen, der irgendwie auch perfekte Erinnerung an jede jemals erfundene Programmiersprache hat.',

    // Stats Sektion
    stat1Title: 'SWE-bench Score',
    stat1Value: '80,9%',
    stat1Desc: 'Besser als die meisten menschlichen Entwickler, ehrlich gesagt',
    stat2Title: 'Kontextfenster',
    stat2Value: '200K Tokens',
    stat2Desc: 'Es erinnert sich an Dinge, die ich vergessen hab',
    stat3Title: 'Ausgabelimit',
    stat3Value: '64K Tokens',
    stat3Desc: 'Schreibt ganze Codebases in einem Rutsch',

    // Warum es großartig ist
    whyGreat: 'Warum Opus 4.5 unreasonably good ist',
    whyGreatDesc: 'Lasst mich die Gründe aufzählen (und ja, ich hab Claude gebeten mir bei der Liste zu helfen, weil natürlich).',

    reason1Title: 'Es programmiert besser als ich',
    reason1Desc: 'Anthropic hat intern eine Performance-Engineering-Prüfung durchgeführt. Opus 4.5 hat besser abgeschnitten als jeder menschliche Kandidat jemals. Ich sage nicht, dass es schlauer ist als euer Senior Developer, aber... doch, genau das sage ich.',

    reason2Title: 'Es denkt tatsächlich nach',
    reason2Desc: 'Mit hybridem Reasoning, das zwischen sofortigen Antworten und erweitertem Denken wechseln kann, macht Opus 4.5 nicht nur Pattern-Matching – es denkt wirklich über Probleme nach. Es ist wie ein Kollege, der tatsächlich die Anforderungen liest, bevor er codet.',

    reason3Title: 'Es erinnert sich an alles',
    reason3Desc: '200.000 Tokens Kontext bedeutet, dass es deine gesamte Codebase im Kopf behalten kann, während du diesen "kleinen Bug" erklärst, der eigentlich ein kompletter Architektur-Umbau ist. Es wird dich nicht verurteilen. Nicht sehr.',

    reason4Title: 'Computer Use das funktioniert',
    reason4Desc: '66,3% auf OSWorld bedeutet, dass es tatsächlich einen Computer benutzen kann. Nicht wie dein Onkel, der Hilfe braucht um den Browser zu finden – wirklich benutzen. Buttons klicken, Formulare ausfüllen, Interfaces navigieren. Die Zukunft ist da und es ist irgendwie erschreckend.',

    // Technische Specs
    specs: 'Die Zahlen (Für die, die überzeugt werden müssen)',
    specsDesc: 'Gut, ihr wollt "objektive" Daten? Hier sind Benchmarks, die definitiv meinen Punkt beweisen.',

    spec1: '80,9% auf SWE-bench Verified',
    spec1Desc: 'Branchenführend für Software-Engineering-Aufgaben',
    spec2: '66,3% auf OSWorld',
    spec2Desc: 'Best-in-Class Computer-Nutzungsfähigkeiten',
    spec3: '$5/$25 pro Million Tokens',
    spec3Desc: '67% günstiger als Opus 4.1. Danke, Anthropic!',
    spec4: 'Effort-Parameter-Kontrolle',
    spec4Desc: 'Niedrig, mittel oder hoch – wie ein Mixer, aber für Intelligenz',

    // Ehrliche Momente
    honestMoments: 'Momente brutaler Ehrlichkeit',
    honestMomentsDesc: 'Was ich am meisten schätze ist, dass Opus 4.5 dir nicht nur sagt, was du hören willst. Es wird höflich erklären, warum deine "clevere Optimierung" eigentlich eine furchtbare Idee ist, und irgendwie wirst du dich für das Feedback bedanken.',

    // Endless Chat
    endlessChat: 'Endless Chat: Warum aufhören?',
    endlessChatDesc: 'Die neue "Endless Chat" Funktion komprimiert automatisch den Kontext wenn Limits erreicht werden. Das bedeutet, Gespräche können ohne Unterbrechung weitergehen – was super ist, weil ich Fragen habe. So viele Fragen.',

    // Sicherheit
    safety: 'Kümmert sich wirklich darum, nicht böse zu sein',
    safetyDesc: 'Anthropic beschreibt Opus 4.5 als ihr am robustesten ausgerichtetes Modell bisher. Es ist so konzipiert, dass es hilfreich ist ohne schädlich zu sein, was offensichtlich klingt, aber anscheinend ziemlich schwierig ist. Es wird dir nicht helfen Malware zu schreiben, aber es wird dir helfen zu verstehen, wie man sich davor schützt.',

    // Wichtige Erkenntnisse
    keyTakeaways: 'Wichtige Erkenntnisse (Die solltest du dir merken)',
    takeaway1: 'Opus 4.5 ist wirklich das beste Modell für Coding, Agents und Computer Use – das ist nicht nur meine Meinung, es ist Anthropics Marketing, das zufällig richtig ist',
    takeaway2: 'Das 200K Kontextfenster und der 64K Output machen es perfekt für substanzielle, komplexe Aufgaben, die geringere Modelle überfordern würden',
    takeaway3: 'Hybrides Reasoning bedeutet, es kann schnell oder tief denken, je nachdem was du brauchst',
    takeaway4: 'Es ist 67% günstiger als die vorherige Version, was bedeutet dass du es dir auch für all deine Side-Projects leisten kannst',

    // Abschluss
    closing: 'Fazit',
    closingDesc: 'Ist Opus 4.5 perfekt? Nein. Manchmal ist es übervorsichtig. Gelegentlich versteht es falsch, was ich will. Aber ehrlich? Das tun die meisten Menschen auch, und die haben kein 200K Token Kontextfenster. Wenn du Opus 4.5 nicht für deine KI-unterstützte Entwicklung nutzt, sage ich nicht, dass du falsch liegst... aber hast du es mal ausprobiert?',

    // Fun Facts
    funFacts: 'Fun Facts',
    funFact1: 'Model ID: claude-opus-4-5-20251101',
    funFact2: 'Knowledge Cutoff: Mai 2025',
    funFact3: 'Verfügbar auf: Claude.ai, AWS Bedrock, Google Vertex AI, Microsoft Foundry',
    funFact4: 'Energiequelle: Wahrscheinlich eine Menge GPUs und eine gesunde Dosis menschliches Feedback',
  },
}
