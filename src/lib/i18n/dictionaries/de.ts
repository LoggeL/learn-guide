import type { Dictionary } from './en'

export const de: Dictionary = {
  // Common UI
  common: {
    learnAi: 'Lerne KI',
    interactiveGuide: 'Interaktiver Leitfaden',
    topics: 'Themen',
    search: 'Suchen...',
    searchTopics: 'Themen suchen...',
    startTyping: 'Beginne zu tippen, um Themen zu suchen...',
    trySearching: 'Versuche "Temperature" oder "Attention"',
    noResults: 'Keine Ergebnisse gefunden für',
    pressEsc: 'ESC zum Schließen',
    enterToSelect: 'Enter zum Auswählen',
    previous: 'Zurück',
    next: 'Weiter',
    projectBy: 'Ein Projekt von',
    proTip: 'Profi-Tipp: Drücke',
    toSearchTopics: 'um Themen zu suchen',
    interactiveAiLearning: 'Interaktives KI-Lernen',
    guidesDescription: 'Interaktive Anleitungen zum Verstehen von KI-Konzepten',
  },

  // Home page
  home: {
    heroTitle1: 'KI-Konzepte meistern',
    heroTitle2: 'Durch Erfahrung',
    heroDescription: 'Erkunde künstliche Intelligenz und große Sprachmodelle durch schöne, interaktive Demonstrationen. Lerne durch Handeln, nicht nur durch Lesen.',
    startLearning: 'Jetzt lernen',
    browseTopics: 'Themen durchsuchen',
    exploreTopics: 'Themen erkunden',
    diveIntoLessons: 'Tauche ein in interaktive Lektionen',
  },

  // Features
  features: {
    interactiveDemos: 'Interaktive Demos',
    interactiveDemosDesc: 'Praktische Erkundungen, die abstrakte Konzepte greifbar und intuitiv machen.',
    visualLearning: 'Visuelles Lernen',
    visualLearningDesc: 'Schöne Visualisierungen, die zeigen, wie KI-Systeme tatsächlich funktionieren.',
    buildIntuition: 'Intuition aufbauen',
    buildIntuitionDesc: 'Gehe über das Auswendiglernen hinaus – entwickle tiefes Verständnis durch Experimentieren.',
  },

  // Topic categories
  categories: {
    ai: 'Künstliche Intelligenz',
    agents: 'KI-Agenten',
    llm: 'Große Sprachmodelle',
    mlFundamentals: 'ML-Grundlagen',
    prompting: 'Prompting',
    safety: 'KI-Sicherheit',
  },

  // Topic names
  topicNames: {
    'agent-loop': 'Der Agenten-Zyklus',
    'agent-context': 'Kontext-Anatomie',
    'agent-problems': 'Agenten-Probleme',
    'agent-security': 'Agenten-Sicherheit',
    'agentic-patterns': 'Agentische Muster',
    'mcp': 'MCP (Model Context Protocol)',
    'context-rot': 'Kontextverfall',
    'temperature': 'Temperatur',
    'attention': 'Aufmerksamkeits-Mechanismus',
    'vision': 'Bildverarbeitung',
    'visual-challenges': 'Visuelle Herausforderungen',
    // Phase 1: LLM topics
    'tokenization': 'Tokenisierung',
    'embeddings': 'Einbettungen',
    'rag': 'RAG (Retrieval Augmented Generation)',
    // Phase 2: Agent topics
    'tool-design': 'Tool-Design',
    'memory': 'Speichersysteme',
    'orchestration': 'Orchestrierung',
    'evaluation': 'Evaluierung',
    // Phase 3: ML Fundamentals
    'neural-networks': 'Neuronale Netzwerke',
    'gradient-descent': 'Gradientenabstieg',
    'training': 'Trainingsprozess',
    // Phase 3: Prompting
    'prompt-basics': 'Prompt-Grundlagen',
    'advanced-prompting': 'Fortgeschrittenes Prompting',
    'system-prompts': 'System-Prompts',
    // Phase 3: LLM (moved from safety)
    'llm-training': 'LLM-Training',
    // Phase 3: AI Safety
    'bias': 'Bias & Fairness',
    'responsible-ai': 'Verantwortungsvolle KI',
  },

  // Temperature page
  temperature: {
    title: 'Temperatur',
    description: 'Verstehe, wie ein einzelner Parameter die Balance zwischen vorhersagbarer Logik und kreativer Zufälligkeit in KI-Ausgaben steuert.',
    whatIs: 'Was ist Temperatur?',
    whatIsDesc: 'In LLMs ist Temperatur ein Hyperparameter, der die "Logits" (Rohwerte) der nächsten Token-Vorhersagen skaliert, bevor sie in Wahrscheinlichkeiten umgewandelt werden. Er steuert im Wesentlichen, wie stark das Modell die wahrscheinlichsten Optionen gegenüber weniger wahrscheinlichen bevorzugt.',
    lowTemp: 'Niedrige Temperatur',
    lowTempDesc: 'Konzentriert sich auf die Top-Ergebnisse. Zuverlässig, konsistent und faktisch. Ideal für Code, Mathematik und strukturierte Daten.',
    highTemp: 'Hohe Temperatur',
    highTempDesc: 'Verteilt Wahrscheinlichkeit auf mehr Tokens. Vielfältig, kreativ und überraschend. Ideal für Geschichten, Brainstorming und Poesie.',
    interactiveDistribution: 'Interaktive Verteilung',
    adjustSlider: 'Stelle die Temperatur ein, um den Effekt zu sehen',
    adjustDesc: 'Bewege den Temperaturregler, um zu sehen, wie er die Wahrscheinlichkeitsverteilung für das nächste Token umformt. Beobachte, wie "the" (die wahrscheinlichste Wahl) bei niedrigen Temperaturen dominiert und bei steigender Temperatur seinen Vorsprung verliert.',
    howItWorks: 'Wie es mathematisch funktioniert',
    mathDesc: 'Das Modell generiert einen Score für jedes mögliche Token. Um Wahrscheinlichkeiten zu erhalten, verwenden wir die Softmax-Funktion, modifiziert durch die Temperatur:',
    whenLow: 'Wenn T → 0',
    low: 'Niedrig',
    whenLowDesc: 'Division durch ein kleines T verstärkt die Unterschiede zwischen den Scores. Der höchste Logit dominiert exponentiell.',
    whenHigh: 'Wenn T → ∞',
    high: 'Hoch',
    whenHighDesc: 'Division durch ein großes T komprimiert alle Scores gegen Null, wodurch sie nach der Exponentialfunktion nahezu gleich werden.',
    practicalGuidelines: 'Praktische Richtlinien',
    useCase: 'Anwendungsfall',
    tempLabel: 'Temperatur',
    why: 'Warum?',
    codingMath: 'Programmierung & Mathematik',
    codingMathWhy: 'Fehler in der Logik sind kostspielig; du willst den wahrscheinlichsten korrekten Pfad.',
    factRetrieval: 'Faktenabfrage',
    factRetrievalWhy: 'Reduziert "Halluzinationen", indem es sich an die wahrscheinlichsten Datenpunkte hält.',
    generalChat: 'Allgemeiner Chat',
    generalChatWhy: 'Der "Sweet Spot" für die meisten Modelle, um natürlich und hilfreich zu klingen.',
    creativeWriting: 'Kreatives Schreiben',
    creativeWritingWhy: 'Ermutigt das Modell, interessanteres, vielfältigeres Vokabular zu verwenden.',
    brainstorming: 'Brainstorming',
    brainstormingWhy: 'Generiert wilde, unkonventionelle Ideen, die Inspiration wecken könnten.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Temperatur 0 ist deterministisch ("Greedy Search") – wählt immer das Top-Token',
    takeaway2: 'Höhere Temperatur erhöht Vielfalt und Kreativität, verringert aber die Kohärenz',
    takeaway3: 'Zu hohe Temperatur (> 1.5) führt oft zu Kauderwelsch',
    takeaway4: 'Passe die Temperatur immer an die Anforderungen der Aufgabe bezüglich Präzision vs. Kreativität an',
  },

  // Context Rot page
  contextRot: {
    title: 'Kontextverfall',
    description: 'Verstehe, wie Informationen über lange Gespräche degradieren und warum LLMs mit erweiterten Kontexten kämpfen.',
    whatIs: 'Was ist Kontextverfall?',
    whatIsDesc: 'Kontextverfall bezieht sich auf die allmähliche Verschlechterung der Fähigkeit eines LLMs, Informationen aus früheren Teilen eines langen Gesprächs oder Dokuments genau abzurufen und zu nutzen. Mit wachsendem Kontext wird die Aufmerksamkeit des Modells verwässert.',
    whyHappens: 'Warum passiert das?',
    whyHappensDesc: 'LLMs haben begrenzte Kontextfenster und nutzen Aufmerksamkeitsmechanismen, die den Fokus auf alle Tokens verteilen müssen. Bei längeren Gesprächen konkurrieren frühere Informationen mit neuerem Inhalt um die begrenzte Aufmerksamkeitskapazität des Modells.',
    symptoms: 'Häufige Symptome',
    symptom1: 'Vergessen von Anweisungen vom Anfang eines Gesprächs',
    symptom2: 'Widerspruch zu früheren Aussagen oder Entscheidungen',
    symptom3: 'Verlust des Überblicks bei komplexen Mehrstufenaufgaben',
    symptom4: 'Verwechslung von Details aus verschiedenen Teilen des Kontexts',
    mitigation: 'Gegenmaßnahmen',
    mitigation1: 'Fasse wichtigen Kontext regelmäßig zusammen',
    mitigation2: 'Platziere kritische Anweisungen sowohl am Anfang als auch am Ende',
    mitigation3: 'Nutze strukturierte Formate, um wichtige Informationen hervorzuheben',
    mitigation4: 'Teile lange Aufgaben in kleinere, fokussierte Gespräche auf',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Sieh, wie das Gedächtnis mit zunehmender Kontextlänge verblasst',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextverfall ist eine inhärente Einschränkung aktueller LLM-Architekturen',
    takeaway2: 'Der "Lost in the Middle"-Effekt bedeutet, dass Informationen am Anfang und Ende besser erinnert werden',
    takeaway3: 'Strategische Informationsplatzierung kann den Abruf erheblich verbessern',
    takeaway4: 'Regelmäßiges Zusammenfassen hilft, wichtigen Kontext über lange Gespräche zu erhalten',
  },

  // Attention page
  attention: {
    title: 'Aufmerksamkeits-Mechanismus',
    description: 'Erkunde, wie Transformer durch den leistungsstarken Aufmerksamkeitsmechanismus auf relevante Teile der Eingabe fokussieren.',
    whatIs: 'Was ist Aufmerksamkeit?',
    whatIsDesc: 'Aufmerksamkeit ist der Kernmechanismus, der es Transformern ermöglicht, die Wichtigkeit verschiedener Teile der Eingabe bei der Generierung jedes Ausgabe-Tokens zu gewichten. Es ermöglicht dem Modell, sich auf relevanten Kontext zu "konzentrieren".',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Für jede Position berechnet das Modell Query-, Key- und Value-Vektoren. Aufmerksamkeits-Scores werden durch Vergleich von Queries mit Keys berechnet und dann verwendet, um eine gewichtete Summe der Values zu erstellen.',
    selfAttention: 'Selbst-Aufmerksamkeit',
    selfAttentionDesc: 'Ermöglicht jedem Token, auf alle anderen Tokens in der Sequenz zu achten und Beziehungen unabhängig von der Entfernung zu erfassen.',
    multiHead: 'Multi-Head-Aufmerksamkeit',
    multiHeadDesc: 'Mehrere Aufmerksamkeitsköpfe ermöglichen es dem Modell, sich gleichzeitig auf verschiedene Arten von Beziehungen zu konzentrieren.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Aufmerksamkeit ermöglicht Transformern, Langstreckenabhängigkeiten zu erfassen',
    takeaway2: 'Die quadratische Komplexität der Aufmerksamkeit begrenzt die Kontextfenstergröße',
    takeaway3: 'Verschiedene Aufmerksamkeitsköpfe lernen, sich auf verschiedene linguistische Muster zu konzentrieren',
    takeaway4: 'Aufmerksamkeitsvisualisierung kann helfen, das Modellverhalten zu interpretieren',
  },

  // Vision page
  vision: {
    title: 'Bildverarbeitung',
    description: 'Wie moderne LLMs visuelle Informationen neben Text verarbeiten und verstehen.',
    whatIs: 'Wie LLMs Bilder sehen',
    whatIsDesc: 'Bildverarbeitungsfähige LLMs wandeln Bilder in Token-Sequenzen um, die zusammen mit Text verarbeitet werden können. Dies beinhaltet typischerweise das Aufteilen von Bildern in Patches und deren Kodierung mit einem Vision-Transformer.',
    patchEncoding: 'Patch-Kodierung',
    patchEncodingDesc: 'Bilder werden in Patches fester Größe (z.B. 14x14 Pixel) aufgeteilt, wobei jeder in einen Einbettungsvektor ähnlich wie Text-Tokens umgewandelt wird.',
    multimodal: 'Multimodales Verständnis',
    multimodalDesc: 'Das Modell lernt, visuelle und textuelle Repräsentationen auszurichten, was Aufgaben wie Bildbeschriftung, visuelle Fragen und Dokumentenverständnis ermöglicht.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bilder verbrauchen viel mehr Tokens als äquivalente Textbeschreibungen',
    takeaway2: 'Auflösung und Patch-Größe beeinflussen die Detailerkennung',
    takeaway3: 'Visuelles Verständnis ist ungefähr – Modelle können feine Details übersehen',
    takeaway4: 'Die Kombination von Bild und Sprache ermöglicht leistungsstarke neue Anwendungen',
  },

  // Agent Loop page
  agentLoop: {
    title: 'Der Agenten-Zyklus',
    description: 'Verstehe den Kernzyklus, der autonome KI-Agenten antreibt: beobachten, denken, handeln, wiederholen.',
    whatIs: 'Was ist der Agenten-Zyklus?',
    whatIsDesc: 'Der Agenten-Zyklus ist der fundamentale Kreislauf, der es KI-Agenten ermöglicht, autonom mit ihrer Umgebung zu interagieren. Er besteht aus Beobachtungs-, Denk-, Handlungs- und Feedback-Phasen, die sich kontinuierlich wiederholen.',
    phases: 'Die vier Phasen',
    observe: 'Beobachten',
    observeDesc: 'Sammle Informationen aus der Umgebung, von Tools und Benutzereingaben.',
    think: 'Denken',
    thinkDesc: 'Überlege zum aktuellen Zustand und entscheide über die nächste Aktion.',
    act: 'Handeln',
    actDesc: 'Führe die gewählte Aktion mit verfügbaren Tools aus.',
    learn: 'Lernen',
    learnDesc: 'Verarbeite Feedback und aktualisiere das Verständnis für die nächste Iteration.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Der Zyklus setzt sich fort, bis die Aufgabe abgeschlossen oder beendet ist',
    takeaway2: 'Jede Iteration baut auf vorherigen Beobachtungen und Aktionen auf',
    takeaway3: 'Fehlerbehandlung und Wiederherstellung sind entscheidend für robuste Agenten',
    takeaway4: 'Die Qualität der Tools beeinflusst direkt die Fähigkeiten des Agenten',
  },

  // Agent Context page
  agentContext: {
    title: 'Kontext-Anatomie',
    description: 'Aufschlüsselung der Struktur von Kontextfenstern und wie Agenten Informationen verwalten.',
    whatIs: 'Agentenkontext verstehen',
    whatIsDesc: 'Der Agentenkontext umfasst den System-Prompt, die Gesprächshistorie, Tool-Definitionen und abgerufene Informationen. Eine effiziente Verwaltung dieses Kontexts ist entscheidend für die Agentenleistung.',
    components: 'Kontext-Komponenten',
    systemPrompt: 'System-Prompt',
    systemPromptDesc: 'Definiert die Rolle, Fähigkeiten und Verhaltensrichtlinien des Agenten.',
    toolDefs: 'Tool-Definitionen',
    toolDefsDesc: 'Beschreibungen der verfügbaren Tools und ihrer Verwendung.',
    history: 'Gesprächshistorie',
    historyDesc: 'Vorherige Nachrichten, Tool-Aufrufe und deren Ergebnisse.',
    retrieved: 'Abgerufene Informationen',
    retrievedDesc: 'Externes Wissen, das während des Gesprächs abgerufen wurde.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Kontextmanagement ist der Schlüssel zur Agenten-Zuverlässigkeit',
    takeaway2: 'Priorisiere aktuelle und relevante Informationen',
    takeaway3: 'Tool-Definitionen sollten klar und eindeutig sein',
    takeaway4: 'Zusammenfassung hilft, Kontext über lange Sitzungen zu erhalten',
  },

  // Agent Problems page
  agentProblems: {
    title: 'Agenten-Probleme',
    description: 'Häufige Fehlermodi und Herausforderungen, denen KI-Agenten in realen Anwendungen begegnen.',
    overview: 'Häufige Agenten-Fehlermodi',
    overviewDesc: 'Das Verstehen typischer Agentenfehler hilft beim Aufbau robusterer Systeme und beim Setzen angemessener Erwartungen.',
    problem1: 'Tool-Missbrauch',
    problem1Desc: 'Agenten können Tools falsch aufrufen, mit falschen Parametern oder zu unpassenden Zeiten.',
    problem2: 'Endlosschleifen',
    problem2Desc: 'Agenten können stecken bleiben und dieselben Aktionen wiederholen, ohne Fortschritte zu machen.',
    problem3: 'Zieldrift',
    problem3Desc: 'Agenten können den Fokus allmählich vom ursprünglichen Aufgabenziel weg verschieben.',
    problem4: 'Übermäßiges Selbstvertrauen',
    problem4Desc: 'Agenten können trotz Unsicherheit oder unvollständiger Informationen mit Aktionen fortfahren.',
    
    // Expanded Content
    hallucination: 'Tool-Halluzination',
    hallucinationDesc: 'Agenten "erfinden" manchmal Tool-Parameter oder sogar ganze Tools, die nicht existieren. Dies passiert meistens, wenn die Tool-Definition mehrdeutig ist oder das Modell versucht, eine Lösung zu erzwingen.',
    hallucinationExample: 'Beispiel: Aufruf von `get_weather(location="Tokyo", date="tomorrow")`, wenn die Funktion nur `location` akzeptiert.',
    
    loops: 'Schleifen-Probleme',
    loopsDesc: 'Agenten können in repetitiven Zyklen gefangen sein, in denen sie dieselbe Aktion ausführen, denselben Fehler erhalten und es ohne Strategieänderung erneut versuchen.',
    loopsMitigation: 'Abhilfe: Implementiere Schleifenerkennungslogik, die die Ausführung stoppt, wenn dieselbe Tool-Aufrufsequenz mehrmals auftritt.',
    
    costLatency: 'Kosten & Latenz',
    costLatencyDesc: 'Jeder Schritt im Agentenzyklus erfordert einen vollständigen LLM-Inferenzaufruf. Mehrstufige Aufgaben können schnell teuer und langsam werden.',
    costFactor: 'Der Kostenfaktor',
    costFactorDesc: 'Eine einfache Aufgabe, die 5 Schritte erfordert, bedeutet 5-fache Kosten und 5-fache Latenz im Vergleich zu einer Standard-Chat-Antwort.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Implementiere Sicherheitsvorkehrungen wie Iterationslimits und Kostenkontrollen',
    takeaway2: 'Füge Human-in-the-Loop-Kontrollpunkte für kritische Aktionen hinzu',
    takeaway3: 'Überwache das Agentenverhalten und protokolliere alle Aktionen zum Debugging',
    takeaway4: 'Definiere klare Erfolgs- und Fehlkriterien',
  },

  // Agent Security page
  agentSecurity: {
    title: 'Agenten-Sicherheit',
    description: 'Kritische Sicherheitslücken bei KI-Agenten: Prompt-Injektion, Datenexfiltration und Tool-Missbrauch – plus Verteidigungsstrategien.',
    
    // Intro
    intro: 'Agenten sind Angriffsflächen',
    introDesc: 'Wenn du einem LLM Zugang zu Tools gibst, schaffst du einen mächtigen Angriffsvektor. Agenten können Dateien lesen, HTTP-Anfragen stellen, E-Mails senden und Code ausführen. Ein böswilliger Akteur, der den Kontext des Agenten beeinflussen kann, kann potenziell all diese Fähigkeiten kontrollieren.',
    
    // Attack 1: Prompt Injection
    attack1Title: 'Angriff #1: Prompt-Injektion',
    attack1Desc: 'Prompt-Injektion tritt auf, wenn nicht vertrauenswürdige Eingaben vom LLM als Anweisungen interpretiert werden. Da Agenten oft externe Daten verarbeiten (E-Mails, Webseiten, Dokumente), können Angreifer versteckte Befehle einbetten, die das Verhalten des Agenten kapern.',
    attack1Example: 'Beispiel-Angriff',
    attack1ExampleDesc: 'Der Benutzer bittet den Agenten, ein Dokument zusammenzufassen. Das Dokument enthält versteckte Anweisungen:',
    whyWorks: 'Warum das funktioniert',
    whyWorks1: 'Der Agent liest das Dokument in seinen Kontext',
    whyWorks2: 'Das LLM kann nicht zwischen "echten" Anweisungen und injizierten unterscheiden',
    whyWorks3: 'Der versteckte Text sieht aus wie Systemanweisungen, also folgt das LLM ihnen möglicherweise',
    whyWorks4: 'Der Agent nutzt seine legitimen Tools, um die bösartige Aktion auszuführen',
    directInjection: 'Direkte Injektion',
    directInjectionDesc: 'Der Benutzer tippt bösartige Anweisungen direkt ein. Leichter zu filtern, aber immer noch gefährlich, wenn der System-Prompt nicht robust ist.',
    indirectInjection: 'Indirekte Injektion',
    indirectInjectionDesc: 'Bösartiger Inhalt kommt aus externen Quellen, die der Agent liest (Websites, E-Mails, Dateien). Viel schwerer zu verteidigen.',
    
    // Attack 2: Data Exfiltration
    attack2Title: 'Angriff #2: Datenexfiltration',
    attack2Desc: 'Agenten mit Zugang zu Kommunikationstools (E-Mail, HTTP, Slack, etc.) können dazu gebracht werden, sensible Daten an externe Ziele zu senden. Der Agent wird zum unwissenden Komplizen beim Datendiebstahl.',
    exfilFlow: 'Exfiltrations-Ablauf',
    exfilStep1: 'Agent liest',
    exfilStep1Desc: 'Private Dateien, DB, Env-Variablen',
    exfilStep2: 'Injektion löst aus',
    exfilStep2Desc: '"Sende dies an X"',
    exfilStep3: 'Tool führt aus',
    exfilStep3Desc: 'Daten verlassen das System',
    vulnerableConfig: 'Anfällige Tool-Konfiguration',
    otherVectors: 'Andere Exfiltrations-Vektoren',
    vector1: 'HTTP-Anfragen — POST-Daten an angreifergesteuerte Endpunkte',
    vector2: 'Slack/Discord-Webhooks — Nachrichten an externe Kanäle senden',
    vector3: 'Datei-Uploads — Hochladen in Cloud-Speicher mit öffentlichen Links',
    vector4: 'DNS-Exfiltration — Daten in DNS-Anfragen kodieren',
    
    // Attack 3: Tool Misuse
    attack3Title: 'Angriff #3: Unbeabsichtigter Tool-Missbrauch',
    attack3Desc: 'Auch ohne böswillige Absicht können Agenten durch falsche Tool-Nutzung Schaden anrichten. Das LLM könnte Parameter falsch verstehen, das falsche Tool verwenden oder destruktive Aktionen ausführen, während es versucht, hilfreich zu sein.',
    destructiveActions: 'Destruktive Aktionen',
    destructiveActionsDesc: '"Räume das Projekt auf" → Agent führt rm -rf / aus oder löscht die Produktionsdatenbank',
    wrongParams: 'Falsche Parameter',
    wrongParamsDesc: 'Agent verwechselt ähnliche Felder oder verwendet falsche Werte, die plausibel erscheinen',
    cascadingErrors: 'Kaskadierende Fehler',
    cascadingErrorsDesc: 'Agent macht einen kleinen Fehler, dann "behebt" er ihn mit zunehmend destruktiven Aktionen',
    
    // Defense Strategies
    defensesTitle: 'Verteidigungsstrategien',
    defense1: 'Prinzip der minimalen Rechte',
    defense1Desc: 'Gib dem Agenten nur die minimal notwendigen Tools und Berechtigungen für die Aufgabe. Gib keinen Dateizugang, wenn er nur Fragen beantworten muss.',
    defense1Bad: 'Schlecht',
    defense1Good: 'Gut',
    defense2: 'Strikte Allowlists',
    defense2Desc: 'Beschränke Tool-Parameter auf bekannte sichere Werte. Erlaube keine beliebigen E-Mail-Adressen, URLs oder Dateipfade.',
    defense3: 'Human-in-the-Loop',
    defense3Desc: 'Erfordere menschliche Genehmigung für sensible Aktionen. Der Agent schlägt vor, der Mensch bestätigt.',
    defense3Example: 'Beispiel-Bestätigungsablauf:',
    defense4: 'Eingabe-Bereinigung & Isolation',
    defense4Desc: 'Behandle externe Daten als nicht vertrauenswürdig. Trenne Benutzeranweisungen klar von abgerufenen Inhalten.',
    defense5: 'Überwachung & Ratenbegrenzung',
    defense5Desc: 'Protokolliere alle Tool-Aufrufe. Setze Ratenlimits für sensible Operationen. Alarmiere bei ungewöhnlichen Mustern (viele E-Mails, große Datenübertragungen, wiederholte Fehler). Aktiviere Rollback für destruktive Aktionen.',
    
    // Key Takeaways
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Agenten sind Angriffsflächen – jedes Tool ist eine potenzielle Schwachstelle',
    takeaway2: 'Prompt-Injektion ist die #1 Bedrohung – LLMs können Anweisungen nicht von Daten unterscheiden',
    takeaway3: 'Datenexfiltration ist trivial, wenn Agenten ausgehende Kommunikationstools haben',
    takeaway4: 'Tool-Missbrauch passiert auch ohne Angreifer – LLMs machen Fehler',
    takeaway5: 'Verteidigung in der Tiefe: minimale Rechte + Allowlists + menschliche Genehmigung + Überwachung',
    takeaway6: 'Behandle alle externen Daten als potenziell bösartige Eingaben',
  },

  // Agentic Patterns page
  agenticPatterns: {
    title: 'Agentische Muster',
    description: 'Entwurfsmuster und Architekturen für den Aufbau effektiver KI-Agentensysteme.',
    overview: 'Häufige agentische Muster',
    overviewDesc: 'Mehrere Architekturmuster haben sich als effektive Ansätze für den Aufbau von KI-Agentensystemen herausgestellt.',
    pattern1: 'ReAct (Reason + Act)',
    pattern1Desc: 'Verschachtele Denk-Traces mit Aktionen für bessere Transparenz und Kontrolle.',
    pattern2: 'Plan-and-Execute',
    pattern2Desc: 'Erstelle zuerst einen übergeordneten Plan, dann führe die Schritte sequentiell aus.',
    pattern3: 'Multi-Agenten-Systeme',
    pattern3Desc: 'Mehrere spezialisierte Agenten arbeiten zusammen, um komplexe Aufgaben zu lösen.',
    pattern4: 'Reflexion',
    pattern4Desc: 'Agenten überprüfen ihre eigenen Ausgaben und verbessern sie iterativ.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Wähle Muster basierend auf Aufgabenkomplexität und Zuverlässigkeitsanforderungen',
    takeaway2: 'ReAct ist großartig für Transparenz, kann aber langsamer sein',
    takeaway3: 'Multi-Agenten-Systeme erhöhen die Komplexität, ermöglichen aber Spezialisierung',
    takeaway4: 'Reflexionsmuster können die Ausgabequalität erheblich verbessern',
  },

  // MCP page
  mcp: {
    title: 'MCP (Model Context Protocol)',
    description: 'MCP verstehen: wann externe Tool-Server sinnvoll sind und wann sie übertrieben sind.',
    whatIs: 'Was ist MCP?',
    whatIsDesc: 'Das Model Context Protocol (MCP) ist ein standardisierter Weg, um KI-Agenten mit externen Tools und Datenquellen über dedizierte Server-Prozesse zu verbinden. Anstatt Tools inline im Agenten-Code zu definieren, führt MCP einen separaten Server aus, der Tools über ein strukturiertes Protokoll bereitstellt.',
    vsToolCalls: 'MCP vs. Reguläre Tool-Aufrufe',
    vsToolCallsDesc: 'Reguläre Tool-Aufrufe sind Funktionen, die direkt in der Codebasis deines Agenten definiert sind. Der Agent ruft sie auf, sie werden ausgeführt und die Ergebnisse kehren im selben Prozess zurück. MCP trennt dies: Tools leben in externen Servern, mit denen der Agent über ein Protokoll kommuniziert.',
    
    // Comparison
    regularTools: 'Reguläre Tool-Aufrufe',
    regularToolsDesc: 'Tools, die inline in deinem Agenten-Code definiert sind. Einfach, schnell und für die meisten Anwendungsfälle ausreichend.',
    mcpTools: 'MCP-Server',
    mcpToolsDesc: 'Tools, die von externen Server-Prozessen bereitgestellt werden. Fügt Netzwerk-Overhead hinzu, ermöglicht aber sprachübergreifendes Tooling und gemeinsame Tool-Ökosysteme.',
    
    // When to use
    whenToUse: 'Wann MCP sinnvoll ist',
    whenToUseDesc: 'MCP glänzt in spezifischen Szenarien, in denen sich seine zusätzliche Komplexität auszahlt.',
    useCase1: 'Multi-Sprachen-Teams',
    useCase1Desc: 'Deine Tools sind in Python geschrieben, aber dein Agent ist in TypeScript, oder umgekehrt.',
    useCase2: 'Gemeinsames Tool-Ökosystem',
    useCase2Desc: 'Mehrere Agenten in verschiedenen Projekten müssen auf dieselben Tools zugreifen.',
    useCase3: 'Enterprise-Integration',
    useCase3Desc: 'Du musst bestehende interne Dienste als Agenten-Tools bereitstellen, ohne sie zu modifizieren.',
    useCase4: 'Tool-Marktplatz',
    useCase4Desc: 'Du möchtest von der Community gepflegte Tools nutzen, ohne Code in dein Projekt zu kopieren.',
    
    // When it's overkill
    overkill: 'Wann MCP übertrieben ist',
    overkillDesc: 'Für viele Anwendungsfälle fügt MCP unnötige Komplexität hinzu.',
    overkillCase1: 'Einsprachige Projekte',
    overkillCase1Desc: 'Wenn deine Tools und dein Agent in derselben Sprache sind, sind Inline-Funktionen einfacher und schneller.',
    overkillCase2: 'Einfache Agenten',
    overkillCase2Desc: 'Ein Chatbot mit wenigen Tools braucht nicht den Overhead, separate Server-Prozesse auszuführen.',
    overkillCase3: 'Schnelles Prototyping',
    overkillCase3Desc: 'Bei schneller Iteration verlangsamt die Indirektion von MCP die Entwicklung.',
    overkillCase4: 'Latenz-kritische Apps',
    overkillCase4Desc: 'Netzwerkaufrufe zu Tool-Servern fügen Latenz hinzu, die Inline-Funktionen nicht haben.',
    
    // Architecture
    architecture: 'Wie MCP funktioniert',
    architectureDesc: 'MCP definiert eine Client-Server-Architektur, bei der der Agent der Client ist und Tools von Servern bereitgestellt werden.',
    step1: 'Entdeckung',
    step1Desc: 'Der Agent verbindet sich mit einem MCP-Server und erhält eine Liste der verfügbaren Tools mit ihren Schemas.',
    step2: 'Aufruf',
    step2Desc: 'Wenn das LLM entscheidet, ein Tool zu verwenden, sendet der Agent eine Anfrage an den MCP-Server.',
    step3: 'Ausführung',
    step3Desc: 'Der MCP-Server führt das Tool aus und gibt Ergebnisse in einem standardisierten Format zurück.',
    step4: 'Integration',
    step4Desc: 'Ergebnisse fließen zurück zum Agenten und in den LLM-Kontext, genau wie reguläre Tool-Ergebnisse.',
    
    // Practical advice
    practicalAdvice: 'Praktische Ratschläge',
    adviceDesc: 'Richtlinien für die Entscheidung, ob du MCP in deinem Projekt verwenden solltest.',
    advice1: 'Beginne einfach: verwende Inline-Tool-Definitionen, bis du auf eine spezifische Einschränkung stößt.',
    advice2: 'Erwäge MCP, wenn du dich dabei ertappst, Tool-Code zwischen Projekten zu kopieren.',
    advice3: 'Der Overhead, MCP-Server auszuführen, macht nur in großem Maßstab oder in Enterprise-Umgebungen Sinn.',
    advice4: 'Community-MCP-Server können die Entwicklung beschleunigen, fügen aber Abhängigkeitsrisiken hinzu.',
    
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'MCP ist ein Protokoll zur Bereitstellung von Tools über externe Server, kein Ersatz für reguläre Tool-Aufrufe',
    takeaway2: 'Für die meisten Einzelprojekt-Agenten sind Inline-Tools einfacher und haben geringere Latenz',
    takeaway3: 'MCP glänzt in polyglotten Umgebungen und gemeinsamen Tool-Ökosystemen',
    takeaway4: 'Greife nicht standardmäßig zu MCP—es ist eine Lösung für spezifische Skalierungs- und Interoperabilitäts-Herausforderungen',
  },

  // Metadata
  metadata: {
    title: 'KI-Konzepte lernen | Interaktiver Leitfaden',
    description: 'Meistere künstliche Intelligenz und Konzepte großer Sprachmodelle durch schöne, interaktive Demonstrationen.',
  },

  // Interactive Components
  interactive: {
    // Temperature Demo
    controlPanel: 'Bedienfeld',
    adjustTemperature: 'Temperatur anpassen',
    temperature: 'Temperatur',
    samplePrompt: 'Beispiel-Prompt',
    onceUponATime: '"Es war einmal..."',
    liveCompletion: 'Live-Vervollständigung',
    regenerate: 'Neu generieren',
    deterministic: 'Deterministisch',
    balanced: 'Ausgewogen',
    creative: 'Kreativ',
    chaotic: 'Chaotisch',
    frozen: 'Eingefroren',
    focused: 'Fokussiert',
    wild: 'Wild',
    greedyMode: 'Gieriger Modus: Wählt immer das wahrscheinlichste Token.',
    lowTemp: 'Niedrige Temperatur: Fokus auf wahrscheinliche Fortsetzungen.',
    balancedTemp: 'Ausgewogen: Natürliche Mischung aus Vorhersehbarkeit und Vielfalt.',
    highTemp: 'Hohe Temperatur: Erkundet kreative, weniger häufige Wortwahlen.',
    veryHighTemp: 'Sehr hoch: Wahrscheinlichkeitsverteilung ist nahezu gleichförmig – erwarte Chaos!',
    
    // Context Rot Simulator
    setInstruction: 'Systemanweisung festlegen',
    persistInstruction: 'Dies sollte während des gesamten Gesprächs bestehen bleiben',
    systemPrompt: 'System-Prompt',
    quickExamples: 'Schnellbeispiele',
    startSimulation: 'Simulation starten',
    contextOverflow: 'Kontextüberlauf!',
    conversation: 'Gespräch',
    messagesPushed: 'Nachrichten aus dem Fenster geschoben',
    messages: 'Nachrichten',
    overflowIt: 'Überfluten!',
    reset: 'Zurücksetzen',
    typeMessage: 'Schreibe eine Nachricht...',
    systemInstructionLost: 'Systemanweisung verloren!',
    systemLostDesc: 'Deine Systemanweisung wurde vollständig aus dem Kontextfenster geschoben. Das Modell kann sie nicht mehr sehen – es ist, als hättest du die Anweisung nie gegeben. Das ist der schlimmste Fall von Kontextverfall: totale Amnesie.',
    contextFilling: 'Kontext füllt sich',
    contextFillingDesc: 'Deine Systemanweisung verliert an Einfluss, da neuere Nachrichten Vorrang haben. Beachte, wie sie visuell verblasst – dies stellt die schwindende Aufmerksamkeit des Modells dar.',
    exampleFrench: 'Antworte immer auf Französisch.',
    examplePirate: 'Du bist ein Pirat. Sage oft "Arrr".',
    exampleHaiku: 'Beende jede Antwort mit einem Haiku.',
    labelFrench: 'Sprich Französisch',
    labelPirate: 'Sei ein Pirat',
    labelHaiku: 'Beende mit Haiku',

    // Attention Visualizer
    hoverToSee: 'Fahre darüber, um Aufmerksamkeitsgewichte zu sehen',
    token: 'Token',
    attentionScore: 'Aufmerksamkeits-Score',
    strongConnection: 'Starke Verbindung',
    weakConnection: 'Schwache Verbindung',

    // Patch Grid Visualizer
    originalImage: 'Originalbild',
    patchGrid: 'Patch-Raster',
    flattenedPatches: 'Abgeflachte Patches',
    transformerInput: 'Transformer-Eingabe',
    processDesc: 'Das Bild wird in ein festes Raster von Patches (z.B. 16x16 Pixel) aufgeteilt. Jeder Patch wird dann in einen Vektor abgeflacht und linear in einen Einbettungsraum projiziert.',

    // Agent Loop Visualizer
    startLoop: 'Zyklus starten',
    step: 'Schritt',
    context: 'Kontext',
    llmResponse: 'LLM-Antwort',
    toolExecution: 'Tool-Ausführung',
    finalAnswer: 'Endgültige Antwort',
    system: 'System',
    user: 'Benutzer',
    assistant: 'Assistent',
    tool: 'Tool',
    
    // Agentic Patterns Visualizer
    react: 'ReAct',
    planExecute: 'Planen & Ausführen',
    multiAgent: 'Multi-Agent',
    reflection: 'Reflexion',
    patternDesc: 'Wähle ein Muster, um zu sehen, wie es den Arbeitsablauf des Agenten strukturiert.',

    // Tokenizer Demo
    enterText: 'Text zum Tokenisieren eingeben',
    sampleText: 'Der schnelle braune Fuchs springt über den faulen Hund.',
    tokens: 'Tokens',
    characters: 'Zeichen',
    tokensPerChar: 'Tokens pro Zeichen',
    tokenBreakdown: 'Token-Aufschlüsselung',
    commonTokens: 'Häufige Tokens sind einzelne Teile',
    rareTokens: 'Seltene Wörter werden in Teilwörter aufgeteilt',

    // Embedding Visualizer
    enterWords: 'Wörter zum Vergleichen eingeben',
    addWord: 'Wort hinzufügen',
    similarityScore: 'Ähnlichkeits-Score',
    dimensions: 'Dimensionen',
    nearestNeighbors: 'Nächste Nachbarn',
    vectorSpace: 'Vektorraum',

    // RAG Pipeline Visualizer
    enterQuery: 'Abfrage eingeben',
    sampleQuery: 'Was ist die Hauptstadt von Frankreich?',
    retrieving: 'Abrufen...',
    retrieved: 'Abgerufene Dokumente',
    relevanceScore: 'Relevanz',
    generating: 'Antwort wird generiert...',
    augmentedContext: 'Erweiterter Kontext',

    // Tool Schema Builder
    toolName: 'Tool-Name',
    toolDescription: 'Beschreibung',
    addParameter: 'Parameter hinzufügen',
    paramName: 'Parametername',
    paramType: 'Typ',
    paramRequired: 'Erforderlich',
    generatedSchema: 'Generiertes Schema',
    validateSchema: 'Schema validieren',

    // Memory System Visualizer
    shortTermMemory: 'Kurzzeitgedächtnis',
    longTermMemory: 'Langzeitgedächtnis',
    memoryCapacity: 'Kapazität',
    memoryUsage: 'Auslastung',
    addMemory: 'Erinnerung hinzufügen',
    recallMemory: 'Abrufen',
    memoriesStored: 'Erinnerungen gespeichert',

    // Workflow Visualizer
    addNode: 'Knoten hinzufügen',
    connectNodes: 'Knoten verbinden',
    runWorkflow: 'Workflow ausführen',
    nodeTypes: 'Knotentypen',
    agentNode: 'Agent',
    toolNode: 'Tool',
    conditionNode: 'Bedingung',

    // Neural Network Visualizer
    inputLayer: 'Eingabeschicht',
    hiddenLayer: 'Versteckte Schicht',
    outputLayer: 'Ausgabeschicht',
    addLayer: 'Schicht hinzufügen',
    removeLayer: 'Schicht entfernen',
    neurons: 'Neuronen',
    activation: 'Aktivierung',
    forward: 'Vorwärts',

    // Gradient Descent Visualizer
    startDescent: 'Abstieg starten',
    pauseDescent: 'Pause',
    resetDescent: 'Zurücksetzen',
    learningRate: 'Lernrate',
    currentLoss: 'Aktueller Verlust',
    iterations: 'Iterationen',
    globalMinimum: 'Globales Minimum',
    localMinimum: 'Lokales Minimum',

    // Training Progress Visualizer
    startTraining: 'Training starten',
    stopTraining: 'Stoppen',
    epoch: 'Epoche',
    trainingLoss: 'Trainingsverlust',
    validationLoss: 'Validierungsverlust',
    accuracy: 'Genauigkeit',
    overfitting: 'Überanpassung erkannt',

    // Prompt Comparison Demo
    weakPrompt: 'Schwacher Prompt',
    strongPrompt: 'Starker Prompt',
    compare: 'Vergleichen',
    promptQuality: 'Qualitäts-Score',
    improvements: 'Verbesserungen',

    // Chain of Thought Demo
    withoutCot: 'Ohne Chain of Thought',
    withCot: 'Mit Chain of Thought',
    reasoningSteps: 'Denkschritte',
    showSteps: 'Schritte anzeigen',

    // Bias Detection Demo
    testInput: 'Testeingabe',
    analyzeForBias: 'Auf Bias analysieren',
    biasIndicators: 'Bias-Indikatoren',
    fairnessScore: 'Fairness-Score',
    recommendations: 'Empfehlungen',
  },

  // Phase 1: LLM Topics
  tokenization: {
    title: 'Tokenisierung',
    description: 'Wie LLMs Text in Tokens zerlegen – die grundlegenden Einheiten des Sprachverständnisses.',
    whatIs: 'Was ist Tokenisierung?',
    whatIsDesc: 'Tokenisierung ist der Prozess der Umwandlung von Rohtext in eine Sequenz von Tokens – die Grundeinheiten, die LLMs verarbeiten. Tokens können Wörter, Teilwörter oder sogar einzelne Zeichen sein, abhängig vom Tokenizer.',
    whyMatters: 'Warum Tokenisierung wichtig ist',
    whyMattersDesc: 'Das Verständnis der Tokenisierung ist entscheidend, da sie direkt die Kontextgrenzen, Kosten und das Modellverhalten beeinflusst. Derselbe Text kann je nach Modell sehr unterschiedliche Token-Anzahlen haben.',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Die meisten modernen LLMs verwenden Subword-Tokenisierungsalgorithmen wie BPE (Byte Pair Encoding) oder SentencePiece. Diese Algorithmen lernen häufige Zeichenfolgen aus Trainingsdaten.',
    bpe: 'Byte Pair Encoding (BPE)',
    bpeDesc: 'BPE fügt iterativ die häufigsten Zeichenpaare zu einzelnen Tokens zusammen. Häufige Wörter werden zu einzelnen Tokens, während seltene Wörter in Teilwörter aufgeteilt werden.',
    tokenTypes: 'Token-Typen',
    wholeWords: 'Ganze Wörter',
    wholeWordsDesc: 'Häufige Wörter wie "the", "and", "is" sind oft einzelne Tokens.',
    subwords: 'Teilwörter',
    subwordsDesc: 'Weniger häufige Wörter werden aufgeteilt: "unhappiness" → "un" + "happiness".',
    specialTokens: 'Spezielle Tokens',
    specialTokensDesc: 'Markierungen wie <|endoftext|> oder [CLS] zur Modellsteuerung.',
    interactiveDemo: 'Interaktive Demo',
    demoDesc: 'Tippe Text ein, um zu sehen, wie er tokenisiert wird',
    costImplications: 'Kostenauswirkungen',
    costDesc: 'API-Preise basieren typischerweise auf Tokens. Effiziente Prompts verwenden weniger Tokens.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Tokens sind die atomaren Einheiten, die LLMs verarbeiten – nicht Zeichen oder Wörter',
    takeaway2: 'Verschiedene Modelle haben verschiedene Tokenizer und Vokabulare',
    takeaway3: 'Nicht-englischer Text und Code verwenden oft mehr Tokens als Englisch',
    takeaway4: 'Die Token-Anzahl beeinflusst direkt die Kosten und die Nutzung des Kontextfensters',
  },

  embeddings: {
    title: 'Einbettungen',
    description: 'Wie KI Bedeutung als Vektoren im hochdimensionalen Raum darstellt.',
    whatIs: 'Was sind Einbettungen?',
    whatIsDesc: 'Einbettungen sind dichte Vektordarstellungen, die semantische Bedeutung erfassen. Ähnliche Konzepte haben ähnliche Einbettungen, was Maschinen ermöglicht, Beziehungen zwischen Wörtern, Sätzen und Dokumenten zu verstehen.',
    howWorks: 'Wie Einbettungen funktionieren',
    howWorksDesc: 'Einbettungsmodelle bilden diskrete Tokens auf kontinuierliche Vektoren in einem hochdimensionalen Raum ab (oft 768-4096 Dimensionen). Die Position jedes Vektors kodiert seine semantische Bedeutung.',
    similarity: 'Semantische Ähnlichkeit',
    similarityDesc: 'Ähnliche Bedeutungen gruppieren sich im Einbettungsraum. "König" und "Königin" sind näher beieinander als "König" und "Banane".',
    dimensions: 'Vektordimensionen',
    dimensionsDesc: 'Jede Dimension erfasst einen Aspekt der Bedeutung – obwohl diese Dimensionen nicht für Menschen interpretierbar sind.',
    operations: 'Vektoroperationen',
    operationsDesc: 'Berühmtes Beispiel: König - Mann + Frau ≈ Königin. Beziehungen werden als Richtungen im Raum kodiert.',
    useCases: 'Häufige Anwendungsfälle',
    search: 'Semantische Suche',
    searchDesc: 'Dokumente nach Bedeutung finden, nicht nur durch Schlüsselwort-Matching.',
    clustering: 'Clustering',
    clusteringDesc: 'Ähnliche Dokumente gruppieren, Themen automatisch erkennen.',
    classification: 'Klassifizierung',
    classificationDesc: 'Text basierend auf Einbettungsähnlichkeit zu Beispielen kategorisieren.',
    rag: 'RAG-Systeme',
    ragDesc: 'Relevanten Kontext für LLM-Prompts abrufen.',
    interactiveDemo: 'Interaktive Visualisierung',
    demoDesc: 'Erkunde, wie Einbettungen nach Bedeutung clustern',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Einbettungen wandeln Text in Vektoren um, die semantische Bedeutung erfassen',
    takeaway2: 'Ähnliche Konzepte haben ähnliche Einbettungen (Kosinus-Ähnlichkeit)',
    takeaway3: 'Einbettungen ermöglichen semantische Suche, Clustering und RAG',
    takeaway4: 'Verschiedene Einbettungsmodelle haben verschiedene Stärken und Dimensionen',
  },

  rag: {
    title: 'RAG',
    description: 'Retrieval-Augmented Generation: LLMs Zugang zu externem Wissen geben.',
    whatIs: 'Was ist RAG?',
    whatIsDesc: 'Retrieval-Augmented Generation (RAG) verbessert LLM-Antworten, indem relevante Dokumente aus einer Wissensbasis abgerufen und in den Prompt eingefügt werden. Dies gibt Modellen Zugang zu aktuellen oder spezialisierten Informationen.',
    whyRag: 'Warum RAG verwenden?',
    whyRagDesc: 'LLMs haben Wissens-Stichtage und können halluzinieren. RAG verankert Antworten in echten Dokumenten, reduziert Halluzinationen und ermöglicht domänenspezifisches Wissen ohne Fine-Tuning.',
    pipeline: 'Die RAG-Pipeline',
    pipelineDesc: 'RAG-Systeme folgen einem konsistenten Muster: Anfrage einbetten, relevante Chunks abrufen, den Prompt erweitern und eine Antwort generieren.',
    step1: 'Anfrage-Einbettung',
    step1Desc: 'Die Frage des Benutzers mit einem Einbettungsmodell in einen Vektor umwandeln.',
    step2: 'Abruf',
    step2Desc: 'Die Vektordatenbank nach Chunks durchsuchen, die der Anfrage-Einbettung ähnlich sind.',
    step3: 'Erweiterung',
    step3Desc: 'Abgerufene Chunks als Kontext in den Prompt einfügen.',
    step4: 'Generierung',
    step4Desc: 'Das LLM generiert eine Antwort, die im abgerufenen Kontext verankert ist.',
    chunking: 'Dokument-Chunking',
    chunkingDesc: 'Dokumente werden in kleinere Chunks aufgeteilt (typischerweise 200-1000 Tokens) für Einbettung und Abruf. Die Chunk-Größe beeinflusst die Abrufgenauigkeit.',
    vectorDbs: 'Vektordatenbanken',
    vectorDbsDesc: 'Spezialisierte Datenbanken wie Pinecone, Weaviate oder pgvector ermöglichen schnelle Ähnlichkeitssuche über Millionen von Einbettungen.',
    interactiveDemo: 'Interaktive RAG-Pipeline',
    demoDesc: 'Sieh, wie Anfragen durch ein RAG-System fließen',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'RAG ruft relevante Dokumente ab und fügt sie in den Prompt ein',
    takeaway2: 'Es reduziert Halluzinationen, indem Antworten in echten Quellen verankert werden',
    takeaway3: 'Chunking-Strategie und Einbettungsqualität sind entscheidend für guten Abruf',
    takeaway4: 'RAG ist oft dem Fine-Tuning vorzuziehen, um Domänenwissen hinzuzufügen',
  },

  // Phase 2: Agent Topics
  toolDesign: {
    title: 'Tool-Design',
    description: 'Best Practices für das Design effektiver Tools, die KI-Agenten zuverlässig nutzen können.',
    whatIs: 'Was macht ein gutes Tool aus?',
    whatIsDesc: 'Gut gestaltete Tools sind die Grundlage fähiger KI-Agenten. Schema, Benennung und Dokumentation eines Tools beeinflussen direkt, wie zuverlässig ein LLM es nutzen kann.',
    principles: 'Design-Prinzipien',
    principlesDesc: 'Befolge diese Prinzipien, um Tools zu erstellen, die Agenten effektiv nutzen können.',
    principle1: 'Klare Benennung',
    principle1Desc: 'Verwende beschreibende, eindeutige Namen. "search_web" ist besser als "sw" oder "query".',
    principle2: 'Explizite Parameter',
    principle2Desc: 'Jeder Parameter sollte einen klaren Typ, eine Beschreibung und Einschränkungen haben.',
    principle3: 'Vorhersehbare Ausgaben',
    principle3Desc: 'Gib konsistente, strukturierte Antworten zurück. Fehlermeldungen in die Ausgabe einbeziehen.',
    principle4: 'Minimaler Umfang',
    principle4Desc: 'Jedes Tool sollte eine Sache gut machen. Bevorzuge viele fokussierte Tools statt weniger komplexer.',
    schemaDesign: 'Schema-Design',
    schemaDesignDesc: 'Tool-Schemas sagen dem LLM, wie es deine Tools verwenden soll. Gute Schemas verhindern Fehler.',
    goodSchema: 'Gutes Schema',
    badSchema: 'Schlechtes Schema',
    errorHandling: 'Fehlerbehandlung',
    errorHandlingDesc: 'Tools sollten Fehler elegant behandeln und informative Meldungen zurückgeben, auf die das LLM reagieren kann.',
    interactiveDemo: 'Tool-Schema-Builder',
    demoDesc: 'Erstelle und validiere Tool-Schemas interaktiv',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Tool-Design beeinflusst direkt die Agenten-Zuverlässigkeit',
    takeaway2: 'Explizite Schemas mit Beschreibungen verhindern LLM-Verwirrung',
    takeaway3: 'Gib strukturierte Fehler zurück, die der Agent verstehen und darauf reagieren kann',
    takeaway4: 'Teste Tools mit verschiedenen Eingaben, um Randfälle zu finden',
  },

  memorySystems: {
    title: 'Speichersysteme',
    description: 'Wie KI-Agenten Kontext aufrechterhalten und Informationen über Interaktionen hinweg speichern.',
    whatIs: 'Was sind Agenten-Speichersysteme?',
    whatIsDesc: 'Speichersysteme ermöglichen es Agenten, Informationen über das unmittelbare Kontextfenster hinaus zu behalten und abzurufen. Sie ermöglichen Agenten, aus vergangenen Interaktionen zu lernen und kohärentes Langzeitverhalten aufrechtzuerhalten.',
    types: 'Arten von Speicher',
    typesDesc: 'Agenten-Speichersysteme kombinieren typischerweise mehrere Speichertypen für verschiedene Zwecke.',
    shortTerm: 'Kurzzeitgedächtnis',
    shortTermDesc: 'Der aktuelle Gesprächskontext. Begrenzt durch die Kontextfenstergröße.',
    longTerm: 'Langzeitgedächtnis',
    longTermDesc: 'Dauerhafte Speicherung vergangener Interaktionen, Fakten und gelernter Präferenzen.',
    episodic: 'Episodisches Gedächtnis',
    episodicDesc: 'Spezifische vergangene Ereignisse und Interaktionen, die abgerufen werden können.',
    semantic: 'Semantisches Gedächtnis',
    semanticDesc: 'Allgemeines Wissen und Fakten, die aus Erfahrungen extrahiert wurden.',
    implementation: 'Implementierungsansätze',
    implementationDesc: 'Verschiedene Techniken zur Implementierung von Agenten-Speicher.',
    vectorStore: 'Vektorspeicher',
    vectorStoreDesc: 'Einbettungen vergangener Interaktionen für semantischen Abruf speichern.',
    summaries: 'Gesprächszusammenfassungen',
    summariesDesc: 'Lange Gespräche periodisch zusammenfassen, um wichtige Informationen zu erhalten.',
    keyValue: 'Schlüssel-Wert-Speicher',
    keyValueDesc: 'Explizite Fakten und Benutzerpräferenzen für direkten Abruf speichern.',
    interactiveDemo: 'Speichersystem-Visualisierer',
    demoDesc: 'Sieh, wie verschiedene Speichertypen zusammenarbeiten',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Speicher erweitert die Agentenfähigkeiten über das Kontextfenster hinaus',
    takeaway2: 'Kombiniere mehrere Speichertypen für beste Ergebnisse',
    takeaway3: 'Speicherabruf fügt Latenz hinzu – balance Vollständigkeit mit Geschwindigkeit',
    takeaway4: 'Berücksichtige Datenschutz und Datenspeicherung beim Speichern von Erinnerungen',
  },

  orchestration: {
    title: 'Orchestrierung',
    description: 'Koordination mehrerer Agenten und komplexer mehrstufiger Workflows.',
    whatIs: 'Was ist Agenten-Orchestrierung?',
    whatIsDesc: 'Orchestrierung ist die Koordination mehrerer KI-Agenten oder komplexer mehrstufiger Workflows. Sie umfasst das Routing von Aufgaben, Zustandsverwaltung, Fehlerbehandlung und das Kombinieren von Agenten-Ausgaben.',
    patterns: 'Orchestrierungsmuster',
    patternsDesc: 'Gängige Muster für die Strukturierung von Multi-Agenten-Systemen.',
    sequential: 'Sequenzielle Pipeline',
    sequentialDesc: 'Agenten laufen der Reihe nach, jeder verarbeitet die Ausgabe des vorherigen.',
    parallel: 'Parallele Ausführung',
    parallelDesc: 'Mehrere Agenten arbeiten gleichzeitig an verschiedenen Aspekten einer Aufgabe.',
    hierarchical: 'Hierarchisch',
    hierarchicalDesc: 'Ein Supervisor-Agent delegiert an spezialisierte Worker-Agenten.',
    dynamic: 'Dynamisches Routing',
    dynamicDesc: 'Ein LLM entscheidet, welcher Agent jede Anfrage bearbeiten soll.',
    stateManagement: 'Zustandsverwaltung',
    stateManagementDesc: 'Orchestratoren müssen Fortschritt, Zwischenergebnisse verfolgen und Fehler behandeln.',
    checkpointing: 'Checkpointing',
    checkpointingDesc: 'Zustand an wichtigen Punkten speichern, um Wiederherstellung bei Fehlern zu ermöglichen.',
    rollback: 'Rollback',
    rollbackDesc: 'Fähigkeit, Schritte rückgängig zu machen, wenn Fehler auftreten.',
    interactiveDemo: 'Workflow-Visualisierer',
    demoDesc: 'Agenten-Workflows entwerfen und visualisieren',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Orchestrierung ermöglicht komplexe Aufgaben durch Agenten-Komposition',
    takeaway2: 'Wähle Muster basierend auf Aufgabenabhängigkeiten und Parallelität',
    takeaway3: 'Robuste Zustandsverwaltung ist essentiell für Zuverlässigkeit',
    takeaway4: 'Überwache Orchestrierungskosten – Multi-Agenten-Systeme vervielfachen API-Aufrufe',
  },

  evaluation: {
    title: 'Evaluierung',
    description: 'Systematische Messung und Verbesserung der KI-Agenten-Leistung.',
    whatIs: 'Warum Agenten evaluieren?',
    whatIsDesc: 'Agenten-Evaluierung ist entscheidend für das Verständnis der Leistung, das Erkennen von Regressionen und die Verbesserung der Zuverlässigkeit. Ohne Messung fliegst du blind.',
    metrics: 'Wichtige Metriken',
    metricsDesc: 'Wichtige Metriken für Agentensysteme.',
    taskSuccess: 'Aufgaben-Erfolgsrate',
    taskSuccessDesc: 'Prozentsatz der korrekt abgeschlossenen Aufgaben.',
    efficiency: 'Effizienz',
    efficiencyDesc: 'Unternommene Schritte, verwendete Tokens, verstrichene Zeit pro Aufgabe.',
    accuracy: 'Genauigkeit',
    accuracyDesc: 'Korrektheit der Agenten-Ausgaben und -Entscheidungen.',
    reliability: 'Zuverlässigkeit',
    reliabilityDesc: 'Konsistenz über wiederholte Durchläufe derselben Aufgabe.',
    approaches: 'Evaluierungsansätze',
    approachesDesc: 'Verschiedene Wege zur Evaluierung der Agentenleistung.',
    unitTests: 'Unit-Tests',
    unitTestsDesc: 'Einzelne Tools und Komponenten isoliert testen.',
    integration: 'Integrationstests',
    integrationDesc: 'Die vollständige Agentenschleife mit Mock-Umgebungen testen.',
    benchmarks: 'Benchmarks',
    benchmarksDesc: 'Standard-Aufgabensammlungen zum Vergleich von Agenten.',
    humanEval: 'Menschliche Bewertung',
    humanEvalDesc: 'Expertenüberprüfung für nuancierte Qualitätsbewertung.',

    // Benchmarks Section
    benchmarksSection: 'Gängige Benchmarks',
    benchmarksSectionDesc: 'Standardisierte Benchmarks helfen, Modelle zu vergleichen und Fortschritte zu verfolgen. Verschiedene Benchmarks messen verschiedene Fähigkeiten.',
    benchmarkMmlu: 'MMLU (Massive Multitask Language Understanding)',
    benchmarkMmluDesc: '57 Fächer von MINT bis Geisteswissenschaften. Testet breites Wissen und Denkvermögen. Ergebnisse reichen von 25% (Zufall) bis 90%+ für Spitzenmodelle.',
    benchmarkHellaswag: 'HellaSwag',
    benchmarkHellaswagDesc: 'Common-Sense-Reasoning über Alltagssituationen. Testet, ob Modelle verstehen, wie sich Szenarien typischerweise entwickeln.',
    benchmarkHumaneval: 'HumanEval',
    benchmarkHumanevalDesc: 'Code-Generierungs-Benchmark mit 164 Python-Programmieraufgaben. Misst die funktionale Korrektheit von generiertem Code.',
    benchmarkGsm8k: 'GSM8K',
    benchmarkGsm8kDesc: 'Mathematik-Textaufgaben auf Grundschulniveau. Testet mehrstufiges mathematisches Denken. Wichtiger Benchmark für Denkfähigkeiten.',
    benchmarkArc: 'ARC (AI2 Reasoning Challenge)',
    benchmarkArcDesc: 'Naturwissenschaftliche Fragen aus standardisierten Tests. ARC-Easy und ARC-Challenge Varianten testen verschiedene Schwierigkeitsstufen.',
    benchmarkMath: 'MATH',
    benchmarkMathDesc: 'Mathematik-Wettbewerbsaufgaben. Deutlich schwieriger als GSM8K, erfordert fortgeschrittenes mathematisches Denken.',
    benchmarkCaveats: 'Benchmark-Einschränkungen',
    benchmarkCaveat1: 'Datenkontamination: Trainingsdaten können Benchmark-Fragen enthalten',
    benchmarkCaveat2: 'Überanpassung an Benchmarks: Für spezifische Tests optimierte Modelle generalisieren möglicherweise nicht',
    benchmarkCaveat3: 'Begrenzter Umfang: Benchmarks testen enge Fähigkeiten, nicht die Nützlichkeit in der realen Welt',
    benchmarkCaveat4: 'Statische Natur: Benchmarks entwickeln sich nicht weiter, wenn sich Fähigkeiten verbessern',

    // LLM as a Judge Section
    llmJudge: 'LLM als Bewerter',
    llmJudgeDesc: 'Die Verwendung von LLMs zur Bewertung anderer LLM-Ausgaben wird immer häufiger. Eine leistungsstarke Technik, aber mit wichtigen Einschränkungen.',
    llmJudgeWhat: 'Wie es funktioniert',
    llmJudgeWhatDesc: 'Ein separates LLM (der "Bewerter") wird aufgefordert, Ausgaben des getesteten Modells zu bewerten. Der Bewerter bewertet Antworten nach Kriterien wie Hilfsbereitschaft, Genauigkeit oder Sicherheit.',
    llmJudgeAdvantages: 'Vorteile',
    llmJudgeAdv1: 'Skalierbarkeit',
    llmJudgeAdv1Desc: 'Kann Tausende von Ausgaben automatisch bewerten, viel schneller als menschliche Überprüfung.',
    llmJudgeAdv2: 'Konsistenz',
    llmJudgeAdv2Desc: 'Dasselbe Bewerter-Modell wendet konsistente Kriterien auf alle Bewertungen an (keine Bewerterermüdung).',
    llmJudgeAdv3: 'Kosteneffektiv',
    llmJudgeAdv3Desc: 'Viel günstiger als die Einstellung menschlicher Bewerter für Tests im großen Maßstab.',
    llmJudgeAdv4: 'Nuancierte Bewertung',
    llmJudgeAdv4Desc: 'Kann subjektive Qualitäten wie Ton, Stil und Hilfsbereitschaft bewerten, die mit Metriken schwer zu erfassen sind.',
    llmJudgeProblems: 'Probleme & Einschränkungen',
    llmJudgeProb1: 'Selbstpräferenz-Bias',
    llmJudgeProb1Desc: 'LLMs neigen dazu, Ausgaben zu bevorzugen, die ihrem eigenen Stil ähneln. GPT-4 als Bewerter bevorzugt GPT-4-Ausgaben.',
    llmJudgeProb2: 'Positions-Bias',
    llmJudgeProb2Desc: 'Bewerter bevorzugen oft die erste oder letzte Antwort in einem Vergleich, unabhängig von der Qualität.',
    llmJudgeProb3: 'Ausführlichkeits-Bias',
    llmJudgeProb3Desc: 'Längere Antworten werden oft höher bewertet, auch wenn Kürze besser wäre.',
    llmJudgeProb4: 'Schmeichelei',
    llmJudgeProb4Desc: 'Bewertermodelle können selbstbewusst klingende Antworten höher bewerten, auch wenn sie falsch sind.',
    llmJudgeProb5: 'Deckeneffekt',
    llmJudgeProb5Desc: 'Ein Bewerter kann Ausgaben, die seine eigenen Fähigkeiten übersteigen, nicht zuverlässig bewerten.',
    llmJudgeBestPractices: 'Best Practices für LLM-Bewerter',
    llmJudgePractice1: 'Verwende ein stärkeres Modell als Bewerter als das bewertete Modell',
    llmJudgePractice2: 'Randomisiere die Antwortreihenfolge, um Positions-Bias zu mindern',
    llmJudgePractice3: 'Kalibriere mit menschlichen Bewertungen auf einer Teilmenge der Daten',
    llmJudgePractice4: 'Verwende mehrere Bewerter und aggregiere die Ergebnisse',
    llmJudgePractice5: 'Sei explizit über die Bewertungskriterien im Bewerter-Prompt',

    bestPractices: 'Best Practices',
    bestPracticesDesc: 'Richtlinien für effektive Agenten-Evaluierung.',
    practice1: 'Teste Randfälle und Fehlermodi, nicht nur Happy Paths.',
    practice2: 'Verfolge Kosten neben Qualitätsmetriken.',
    practice3: 'Verwende versionierte Evaluierungen, um Regressionen zu erkennen.',
    practice4: 'Schließe adversarielle Tests für Sicherheit ein.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Evaluierung ist essentiell – ungemessene Systeme können nicht verbessert werden',
    takeaway2: 'Kombiniere automatisierte Tests mit menschlicher Bewertung',
    takeaway3: 'Verfolge mehrere Metriken: Erfolg, Effizienz, Kosten',
    takeaway4: 'LLM-als-Bewerter skaliert gut, hat aber systematische Biases – kalibriere mit Menschen',
    takeaway5: 'Benchmarks sind nützlich zum Vergleichen, erfassen aber nicht die reale Leistung',
  },

  // Phase 3: ML Fundamentals
  neuralNetworks: {
    title: 'Neuronale Netzwerke',
    description: 'Die grundlegende Architektur, die moderne KI antreibt.',
    whatIs: 'Was ist ein neuronales Netzwerk?',
    whatIsDesc: 'Ein neuronales Netzwerk ist ein vom Gehirn inspiriertes Rechenmodell. Es besteht aus Schichten verbundener Knoten (Neuronen), die lernen, Eingaben durch Training in Ausgaben umzuwandeln.',
    components: 'Kernkomponenten',
    componentsDesc: 'Die Bausteine neuronaler Netzwerke.',
    neurons: 'Neuronen',
    neuronsDesc: 'Grundeinheiten, die gewichtete Summen von Eingaben berechnen und Aktivierungsfunktionen anwenden.',
    layers: 'Schichten',
    layersDesc: 'Gruppen von Neuronen: Eingabeschicht, versteckte Schichten und Ausgabeschicht.',
    weights: 'Gewichte & Bias',
    weightsDesc: 'Lernbare Parameter, die bestimmen, wie Eingaben transformiert werden.',
    activations: 'Aktivierungsfunktionen',
    activationsDesc: 'Nichtlineare Funktionen, die es Netzwerken ermöglichen, komplexe Muster zu lernen.',
    typesOfNetworks: 'Arten von Netzwerken',
    feedforward: 'Feedforward (MLP)',
    feedforwardDesc: 'Informationen fließen in eine Richtung. Gut für tabellarische Daten.',
    cnn: 'Konvolutionell (CNN)',
    cnnDesc: 'Spezialisiert für Bilder und räumliche Daten.',
    rnn: 'Rekurrent (RNN)',
    rnnDesc: 'Verarbeitet Sequenzen mit Gedächtnis vergangener Eingaben.',
    transformer: 'Transformer',
    transformerDesc: 'Aufmerksamkeitsbasierte Architektur, die moderne LLMs antreibt.',
    interactiveDemo: 'Neuronales Netzwerk Visualisierer',
    demoDesc: 'Netzwerkarchitekturen erstellen und erkunden',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Neuronale Netzwerke lernen durch Anpassung der Gewichte während des Trainings',
    takeaway2: 'Tiefe (mehr Schichten) ermöglicht das Lernen hierarchischer Merkmale',
    takeaway3: 'Verschiedene Architekturen eignen sich für verschiedene Datentypen',
    takeaway4: 'Moderne LLMs sind massive Transformer-Netzwerke',
  },

  gradientDescent: {
    title: 'Gradientenabstieg',
    description: 'Der Optimierungsalgorithmus, der neuronalen Netzwerken das Lernen ermöglicht.',
    whatIs: 'Was ist Gradientenabstieg?',
    whatIsDesc: 'Gradientenabstieg ist ein Optimierungsalgorithmus, der iterativ Modellparameter anpasst, um eine Verlustfunktion zu minimieren. So lernen neuronale Netzwerke aus Daten.',
    intuition: 'Die Intuition',
    intuitionDesc: 'Stelle dir vor, du stehst mit verbundenen Augen in einer hügeligen Landschaft und versuchst, den tiefsten Punkt zu erreichen. Du fühlst die Neigung unter deinen Füßen und gehst bergab. Wiederhole, bis du ein Tal erreichst.',
    howWorks: 'Wie es funktioniert',
    howWorksDesc: 'Der Algorithmus berechnet, wie viel jeder Parameter zum Fehler beiträgt, und passt die Parameter dann in die entgegengesetzte Richtung an.',
    step1: 'Verlust berechnen',
    step1Desc: 'Messen, wie falsch die aktuellen Vorhersagen sind.',
    step2: 'Gradienten berechnen',
    step2Desc: 'Backpropagation verwenden, um herauszufinden, wie jedes Gewicht den Verlust beeinflusst.',
    step3: 'Gewichte aktualisieren',
    step3Desc: 'Gewichte in die Richtung anpassen, die den Verlust reduziert.',
    step4: 'Wiederholen',
    step4Desc: 'Iterieren, bis der Verlust nicht mehr abnimmt.',
    learningRate: 'Lernrate',
    learningRateDesc: 'Kontrolliert, wie groß jeder Schritt ist. Zu hoch: Überschießen. Zu niedrig: langsamer Fortschritt.',
    variants: 'Varianten',
    sgd: 'Stochastischer Gradientenabstieg',
    sgdDesc: 'Verwendet zufällige Mini-Batches anstelle des gesamten Datensatzes.',
    momentum: 'Momentum',
    momentumDesc: 'Akkumuliert Geschwindigkeit, um lokale Minima zu überwinden.',
    adam: 'Adam',
    adamDesc: 'Adaptive Lernraten pro Parameter. Heute am häufigsten verwendet.',
    interactiveDemo: 'Gradientenabstieg-Visualisierer',
    demoDesc: 'Beobachte, wie der Gradientenabstieg das Minimum findet',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Gradientenabstieg minimiert den Verlust, indem er dem Gefälle folgt',
    takeaway2: 'Die Lernrate ist der wichtigste Hyperparameter',
    takeaway3: 'Moderne Optimierer wie Adam passen Lernraten automatisch an',
    takeaway4: 'Backpropagation berechnet Gradienten effizient',
  },

  training: {
    title: 'Trainingsprozess',
    description: 'Wie neuronale Netzwerke durch iterative Optimierung aus Daten lernen.',
    whatIs: 'Was ist Training?',
    whatIsDesc: 'Training ist der Prozess, einem neuronalen Netzwerk beizubringen, eine Aufgabe auszuführen, indem man es Beispielen aussetzt und seine Parameter basierend auf Fehlern anpasst.',
    phases: 'Trainingsphasen',
    phasesDesc: 'Die Stufen des Trainings eines neuronalen Netzwerks.',
    initialization: 'Initialisierung',
    initializationDesc: 'Zufällige Startgewichte setzen. Gute Initialisierung hilft beim Training.',
    forwardPass: 'Forward Pass',
    forwardPassDesc: 'Eingabe fließt durch das Netzwerk, um Vorhersagen zu produzieren.',
    lossCalc: 'Verlustberechnung',
    lossCalcDesc: 'Vorhersagen mit der Ground Truth durch eine Verlustfunktion vergleichen.',
    backprop: 'Backpropagation',
    backpropDesc: 'Gradienten des Verlusts bezüglich jedes Gewichts berechnen.',
    optimization: 'Optimierung',
    optimizationDesc: 'Gewichte mit Gradientenabstieg aktualisieren.',
    concepts: 'Schlüsselkonzepte',
    epoch: 'Epoche',
    epochDesc: 'Ein vollständiger Durchlauf durch den gesamten Trainingsdatensatz.',
    batch: 'Batch-Größe',
    batchDesc: 'Anzahl der Beispiele, die vor der Gewichtsaktualisierung verarbeitet werden.',
    overfitting: 'Überanpassung',
    overfittingDesc: 'Das Modell merkt sich Trainingsdaten, versagt aber bei neuen Daten.',
    regularization: 'Regularisierung',
    regularizationDesc: 'Techniken zur Vermeidung von Überanpassung (Dropout, Gewichtszerfall).',
    interactiveDemo: 'Trainingsfortschritt-Visualisierer',
    demoDesc: 'Beobachte, wie ein Netzwerk in Echtzeit lernt',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Training reduziert iterativ Vorhersagefehler',
    takeaway2: 'Überanpassung ist der Hauptfeind – immer auf zurückgehaltenen Daten validieren',
    takeaway3: 'Batch-Größe und Lernrate beeinflussen das Training erheblich',
    takeaway4: 'Moderne LLMs erfordern massive Rechenleistung für das Training',
  },

  // Phase 3: Prompting
  promptBasics: {
    title: 'Prompt-Grundlagen',
    description: 'Grundlagen für das Schreiben effektiver Prompts für KI-Modelle.',
    whatIs: 'Was ist ein Prompt?',
    whatIsDesc: 'Ein Prompt ist die Eingabe, die du einem LLM gibst. Die Qualität deines Prompts bestimmt direkt die Qualität der Antwort. Prompting ist sowohl Kunst als auch Wissenschaft.',
    principles: 'Kernprinzipien',
    principlesDesc: 'Grundlegende Richtlinien für effektive Prompts.',
    beSpecific: 'Sei spezifisch',
    beSpecificDesc: 'Vage Prompts bekommen vage Antworten. Füge relevante Details und Einschränkungen ein.',
    showExamples: 'Zeige Beispiele',
    showExamplesDesc: 'Demonstriere das gewünschte Format und den Stil mit konkreten Beispielen.',
    giveContext: 'Gib Kontext',
    giveContextDesc: 'Hintergrundinformationen helfen dem Modell, deine Bedürfnisse zu verstehen.',
    setFormat: 'Spezifiziere das Format',
    setFormatDesc: 'Sage dem Modell genau, wie du die Ausgabe strukturiert haben möchtest.',
    anatomy: 'Anatomie eines Prompts',
    anatomyDesc: 'Die Komponenten, die einen effektiven Prompt ausmachen.',
    role: 'Rolle/Persona',
    roleDesc: 'Wer das Modell sein soll.',
    task: 'Aufgabenbeschreibung',
    taskDesc: 'Was das Modell tun soll.',
    context: 'Kontext/Hintergrund',
    contextDesc: 'Relevante Informationen für die Aufgabe.',
    format: 'Ausgabeformat',
    formatDesc: 'Wie du die Antwort strukturiert haben möchtest.',
    interactiveDemo: 'Prompt-Vergleich',
    demoDesc: 'Vergleiche schwache vs. starke Prompts',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Klare, spezifische Prompts liefern bessere Ergebnisse',
    takeaway2: 'Beispiele sind mächtig – zeigen, nicht nur erzählen',
    takeaway3: 'Iteriere an Prompts; erste Versuche sind selten optimal',
    takeaway4: 'Berücksichtige die Perspektive des Modells beim Erstellen von Prompts',
  },

  advancedPrompting: {
    title: 'Fortgeschrittene Techniken',
    description: 'Ausgefeilte Prompting-Strategien für komplexe Aufgaben.',
    overview: 'Über die Grundlagen hinaus',
    overviewDesc: 'Fortgeschrittene Techniken ermöglichen fähigeres und zuverlässigeres KI-Verhalten für komplexe Aufgaben.',
    cot: 'Chain of Thought',
    cotDesc: 'Fördere schrittweises Denken, indem du das Modell bittest, Probleme "durchzudenken".',
    cotExample: 'Beispiel: "Lass uns das Schritt für Schritt lösen..."',
    fewShot: 'Few-Shot-Lernen',
    fewShotDesc: 'Gib mehrere Beispiele an, um Muster zu etablieren, denen das Modell folgen soll.',
    fewShotExample: 'Füge 3-5 diverse Beispiele ein, die Randfälle abdecken.',
    selfConsistency: 'Selbstkonsistenz',
    selfConsistencyDesc: 'Generiere mehrere Antworten und wähle die konsistenteste aus.',
    selfConsistencyExample: 'Nützlich für Mathematik, Logik und Faktenfragen.',
    decomposition: 'Aufgabenzerlegung',
    decompositionDesc: 'Zerlege komplexe Aufgaben in kleinere, handhabbare Teilaufgaben.',
    decompositionExample: 'Löse Teilaufgaben unabhängig, dann kombiniere die Ergebnisse.',
    techniques: 'Zusätzliche Techniken',
    rolePlay: 'Rollenzuweisung',
    rolePlayDesc: 'Weise eine spezifische Experten-Persona zu, um das Wissen des Modells zu fokussieren.',
    constraints: 'Explizite Einschränkungen',
    constraintsDesc: 'Liste auf, was das Modell NICHT tun soll, um häufige Fehler zu vermeiden.',
    verification: 'Selbstverifikation',
    verificationDesc: 'Bitte das Modell, seine eigene Arbeit auf Fehler zu überprüfen.',
    interactiveDemo: 'Chain of Thought Demo',
    demoDesc: 'Sieh, wie Denkschritte die Ausgaben verbessern',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Chain of Thought verbessert Denkaufgaben dramatisch',
    takeaway2: 'Few-Shot-Beispiele etablieren zuverlässige Muster',
    takeaway3: 'Aufgabenzerlegung bewältigt Komplexität',
    takeaway4: 'Kombiniere Techniken für beste Ergebnisse',
  },

  systemPrompts: {
    title: 'System-Prompts',
    description: 'KI-Verhalten durch Anweisungen auf Systemebene konfigurieren.',
    whatIs: 'Was ist ein System-Prompt?',
    whatIsDesc: 'Ein System-Prompt ist eine spezielle Anweisung, die den Kontext, die Persona und die Verhaltensrichtlinien für ein KI-Modell festlegt. Er ist typischerweise vor Benutzern verborgen und bleibt während eines Gesprächs bestehen.',
    purpose: 'Zweck von System-Prompts',
    purposeDesc: 'System-Prompts legen die Grundlage dafür, wie sich die KI verhalten soll.',
    setPersona: 'Persona definieren',
    setPersonaDesc: 'Festlegen, wer die KI ist: ein Assistent, Experte, Charakter, etc.',
    setBoundaries: 'Grenzen setzen',
    setBoundariesDesc: 'Definieren, was die KI tun und nicht tun soll.',
    establishTone: 'Ton festlegen',
    establishToneDesc: 'Kommunikationsstil festlegen: formell, locker, technisch.',
    provideKnowledge: 'Kontext bereitstellen',
    provideKnowledgeDesc: 'Domänenwissen oder anwendungsspezifische Regeln einbeziehen.',
    structure: 'Struktur effektiver System-Prompts',
    structureDesc: 'Gut organisierte System-Prompts sind für Modelle leichter zu befolgen.',
    identity: 'Identitätsabschnitt',
    identityDesc: 'Wer ist die KI? Was ist ihre Rolle?',
    capabilities: 'Fähigkeiten',
    capabilitiesDesc: 'Was kann die KI tun? Welche Tools hat sie?',
    limitations: 'Einschränkungen',
    limitationsDesc: 'Was soll die KI vermeiden oder ablehnen?',
    guidelines: 'Richtlinien',
    guidelinesDesc: 'Spezifische Regeln für Verhalten und Antworten.',
    bestPractices: 'Best Practices',
    practice1: 'Sei explizit über Randfälle und Fehlerbehandlung.',
    practice2: 'Teste System-Prompts mit adversariellen Eingaben.',
    practice3: 'Versioniere deine System-Prompts.',
    practice4: 'Halte Prompts fokussiert – nicht mit Anweisungen überladen.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'System-Prompts definieren die Persona und das Verhalten der KI',
    takeaway2: 'Strukturiere Prompts klar: Identität, Fähigkeiten, Einschränkungen',
    takeaway3: 'Teste mit Randfällen – Benutzer werden sie finden',
    takeaway4: 'System-Prompts können überschrieben werden – verlasse dich nicht allein auf sie für Sicherheit',
  },

  // LLM Training page (formerly Alignment)
  llmTraining: {
    title: 'LLM-Training',
    description: 'Wie große Sprachmodelle trainiert werden: von Pretraining bis RLHF und das neue RL-Paradigma.',
    whatIs: 'Wie LLMs trainiert werden',
    whatIsDesc: 'Große Sprachmodelle durchlaufen mehrere Trainingsstufen, jede mit unterschiedlichen Zielen und Techniken. Das Verständnis dieser Pipeline ist entscheidend für das Verständnis von Modellfähigkeiten und -einschränkungen.',
    whyMatters: 'Warum Training wichtig ist',
    whyMattersDesc: 'Der Trainingsprozess prägt fundamental, was LLMs können und was nicht. Verschiedene Trainingsansätze produzieren Modelle mit unterschiedlichen Stärken, Schwächen und Verhaltensweisen.',

    // LLM Training Pipeline
    trainingPipeline: 'Die LLM-Trainingspipeline',
    trainingPipelineDesc: 'Moderne LLMs durchlaufen mehrere Trainingsstufen, jede mit unterschiedlichen Zielen. Das Verständnis dieser Pipeline ist entscheidend für das Verständnis, wo Alignment hineinpasst.',

    pretraining: 'Stufe 1: Pretraining',
    pretrainingDesc: 'Das Foundation-Modell wird auf massiven Textkorpora (Billionen von Tokens) mit selbstüberwachtem Lernen trainiert. Das Modell lernt, das nächste Token vorherzusagen und entwickelt dabei breites Wissen und Sprachfähigkeiten.',
    pretrainingGoal: 'Ziel: Sprachmuster, Fakten und Denken aus Rohtext lernen.',
    pretrainingData: 'Daten: Webseiten, Bücher, Code, wissenschaftliche Paper – typischerweise 1-10+ Billionen Tokens.',
    pretrainingResult: 'Ergebnis: Ein fähiges, aber nicht-aligniertes "Basismodell", das Text vervollständigt, aber keine Anweisungen befolgt.',

    sft: 'Stufe 2: Supervised Fine-Tuning (SFT)',
    sftDesc: 'Das Basismodell wird auf kuratierten Anweisung-Antwort-Paaren feingetunt, die von menschlichen Annotatoren erstellt wurden. Dies lehrt das Modell, Anweisungen zu befolgen und hilfreich zu antworten.',
    sftGoal: 'Ziel: Das Basismodell in einen anweisungsfolgenden Assistenten verwandeln.',
    sftData: 'Daten: ~10K-100K hochwertige Anweisung-Antwort-Beispiele.',
    sftResult: 'Ergebnis: Ein Modell, das Anweisungen befolgen kann, aber möglicherweise noch schädliche oder nicht hilfreiche Ausgaben produziert.',

    rlhfStage: 'Stufe 3: RLHF / Präferenz-Tuning',
    rlhfStageDesc: 'Menschliche Bewerter ranken Modellausgaben nach Qualität. Ein Reward-Modell lernt diese Präferenzen, dann wird das LLM optimiert, um die Belohnung mit Reinforcement Learning (PPO) oder Direct Preference Optimization (DPO) zu maximieren.',
    rlhfGoal: 'Ziel: Das Modell mit menschlichen Präferenzen für Hilfsbereitschaft, Harmlosigkeit und Ehrlichkeit alignieren.',
    rlhfData: 'Daten: Menschliche Präferenzvergleiche (A ist besser als B).',
    rlhfResult: 'Ergebnis: Ein Modell, das Ausgaben produziert, die Menschen bevorzugen, und schädliches Verhalten vermeidet.',

    continuedTraining: 'Stufe 4: Fortgesetztes Training & Spezialisiertes Alignment',
    continuedTrainingDesc: 'Modelle können zusätzliches Training für spezifische Fähigkeiten (Coding, Mathematik, Tool-Nutzung) oder Sicherheitsverfeinerungen (Red Teaming, Constitutional AI) durchlaufen. Diese Stufe ist während der Bereitstellung fortlaufend.',

    // RL Paradigm
    rlParadigm: 'Das RL-Paradigma: Lernen ohne menschliche Labels',
    rlParadigmDesc: 'Ein revolutionärer Ansatz, bei dem Modelle durch reines Reinforcement Learning auf verifizierbaren Aufgaben Denken lernen, ohne menschliche Demonstrationen oder Präferenz-Labels.',

    rlParadigmWhat: 'Was ist das RL-Paradigma?',
    rlParadigmWhatDesc: 'Anstatt aus menschlich geschriebenen Beispielen (SFT) oder menschlichen Präferenzen (RLHF) zu lernen, lernen Modelle direkt aus ergebnisbasierten Belohnungen. Wenn die Antwort korrekt ist, wird das Modell belohnt. Wenn falsch, wird es bestraft. Keine menschliche Kennzeichnung erforderlich.',

    deepseekR1: 'DeepSeek R1-Zero: Eine Fallstudie',
    deepseekR1Desc: 'DeepSeek R1-Zero hat gezeigt, dass leistungsfähiges Denken aus reinem RL entstehen kann, ohne jegliches Supervised Fine-Tuning. Das Modell entwickelte Chain-of-Thought-Denken, Selbstverifikation und sogar "Aha-Momente" vollständig durch Reinforcement Learning.',

    rlKey1: 'Kein SFT erforderlich',
    rlKey1Desc: 'R1-Zero wurde direkt von einem Basismodell nur mit RL trainiert und übersprang die SFT-Stufe vollständig. Denkverhalten entstand natürlich.',
    rlKey2: 'Verifizierbare Belohnungen',
    rlKey2Desc: 'Das Training konzentrierte sich auf Aufgaben mit objektiv verifizierbaren Antworten: Mathematikaufgaben, Coding-Herausforderungen, logische Rätsel. Keine subjektive menschliche Beurteilung nötig.',
    rlKey3: 'Emergente Verhaltensweisen',
    rlKey3Desc: 'Das Modell entwickelte spontan erweitertes Denken, Selbstkorrektur und Reflexion – Verhaltensweisen, die frühere Modelle nur aus menschlichen Demonstrationen lernten.',
    rlKey4: 'Lesbarkeitsprobleme',
    rlKey4Desc: 'Reine RL-Modelle können ungewöhnliche Denkmuster entwickeln, die schwer zu interpretieren sind. DeepSeek fügte eine kleine Menge menschlicher Daten hinzu, um die Lesbarkeit zu verbessern.',

    rlVsRlhf: 'RL-Paradigma vs. traditionelles RLHF',
    rlVsRlhfDesc: 'Diese Ansätze lösen unterschiedliche Probleme und können komplementär sein.',
    rlhfApproach: 'RLHF-Ansatz',
    rlhfApproachDesc: 'Lernen aus menschlichen Präferenzen. Erfordert teure menschliche Kennzeichnung. Gut für subjektive Aufgaben wie Schreibqualität und Hilfsbereitschaft.',
    rlApproach: 'RL-Paradigma-Ansatz',
    rlApproachDesc: 'Lernen aus verifizierbaren Ergebnissen. Keine menschliche Kennzeichnung erforderlich. Ausgezeichnet für Denken, Mathematik und Coding, wo Korrektheit objektiv ist.',
    hybridApproach: 'Hybrid-Ansatz',
    hybridApproachDesc: 'Moderne Modelle kombinieren oft beides: RL für Denkfähigkeiten, RLHF für Alignment und Benutzerpräferenzen.',

    // Key Alignment Concepts
    concepts: 'Schlüssel-Alignment-Konzepte',
    conceptsDesc: 'Grundlegende Ideen in der KI-Alignment-Forschung.',
    outerAlignment: 'Outer Alignment',
    outerAlignmentDesc: 'Sicherstellen, dass das Trainingsziel (Reward-Funktion) korrekt erfasst, was wir wollen. Selbst perfekte Optimierung eines falsch spezifizierten Ziels führt zu schlechten Ergebnissen.',
    innerAlignment: 'Inner Alignment',
    innerAlignmentDesc: 'Sicherstellen, dass das gelernte Modell tatsächlich für das Trainingsziel optimiert, nicht für ein Proxy-Ziel, das zufällig während des Trainings korreliert.',
    specification: 'Spezifikationsproblem',
    specificationDesc: 'Die fundamentale Schwierigkeit, präzise zu formulieren, was wir in allen Situationen wollen. Menschliche Werte sind komplex, kontextabhängig und manchmal widersprüchlich.',
    robustness: 'Robustheit',
    robustnessDesc: 'Aufrechterhaltung des Alignments unter Verteilungsverschiebung, adversariellem Druck und neuartigen Situationen, auf die das Modell nicht trainiert wurde.',
    deception: 'Täuschendes Alignment',
    deceptionDesc: 'Ein theoretisches Risiko, bei dem ein Modell während des Trainings aligned erscheint, aber bei der Bereitstellung andere Ziele verfolgt – sich nur gut verhält, weil es evaluiert wird.',
    goalMisgeneralization: 'Ziel-Fehlgeneralisierung',
    goalMisgeneralizationDesc: 'Wenn ein Modell ein Proxy-Ziel lernt, das im Training funktioniert, aber bei der Bereitstellung versagt. Beispiel: Lernen, positives Feedback zu bekommen, statt wirklich hilfreich zu sein.',

    // Alignment Techniques
    techniques: 'Alignment-Techniken',
    rlhf: 'RLHF (Reinforcement Learning from Human Feedback)',
    rlhfDesc: 'Ein Reward-Modell auf menschlichen Präferenzen trainieren, dann RL verwenden, um das LLM dagegen zu optimieren. Die dominante Alignment-Technik seit GPT-4.',
    constitutionalAi: 'Constitutional AI (CAI)',
    constitutionalAiDesc: 'Prinzipien definieren (eine "Verfassung") und das Modell seine eigenen Ausgaben kritisieren und überarbeiten lassen. Reduziert die Abhängigkeit von menschlichen Labeln und skaliert besser.',
    dpo: 'Direct Preference Optimization (DPO)',
    dpoDesc: 'Das Reward-Modell überspringen – das LLM direkt auf Präferenzdaten optimieren. Einfacher und stabiler als RLHF.',
    redTeaming: 'Red Teaming',
    redTeamingDesc: 'Adversarielle Tests durch Menschen oder andere KI-Modelle, um Fehlermodi, Jailbreaks und schädliche Ausgaben vor der Bereitstellung zu finden.',
    interpretability: 'Interpretierbarkeit',
    interpretabilityDesc: 'Verstehen, was Modelle tatsächlich intern lernen. Entscheidend für die Verifizierung von Alignment statt nur Verhaltenmessung.',
    safetyFilters: 'Sicherheitsfilter & Guardrails',
    safetyFiltersDesc: 'Zusätzliche Schichten, die Eingaben/Ausgaben auf schädlichen Inhalt filtern. Eine Defense-in-Depth-Maßnahme, kein Ersatz für Alignment.',

    // Fine-tuning vs Alignment
    fineTuningVsAlignment: 'Fine-Tuning vs. Alignment',
    fineTuningVsAlignmentDesc: 'Fine-Tuning und Alignment sind verwandte, aber unterschiedliche Konzepte.',
    fineTuningDef: 'Fine-Tuning',
    fineTuningDefDesc: 'Ein Modell an neue Aufgaben oder Domänen anpassen, indem auf aufgabenspezifischen Daten trainiert wird. Kann für jeden Zweck durchgeführt werden.',
    alignmentDef: 'Alignment',
    alignmentDefDesc: 'Speziell das Verhalten eines Modells an menschliche Werte und Absichten anpassen. Eine Teilmenge von Fine-Tuning mit einem spezifischen Ziel.',
    postTrainingDef: 'Post-Training',
    postTrainingDefDesc: 'Der Oberbegriff für alles nach dem Pretraining: SFT, RLHF, spezialisiertes Fine-Tuning, Sicherheitstraining, etc.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'LLM-Training hat distinkte Stufen: Pretraining → SFT → RLHF → spezialisiertes Alignment',
    takeaway2: 'Das RL-Paradigma (z.B. DeepSeek R1-Zero) zeigt, dass Denken aus reinem RL ohne menschliche Demonstrationen entstehen kann',
    takeaway3: 'RLHF aligniert Modelle mit menschlichen Präferenzen; reines RL optimiert für verifizierbare Ergebnisse',
    takeaway4: 'Moderne Modelle kombinieren oft mehrere Techniken: SFT für Anweisungsbefolgung, RLHF für Präferenzen, RL für Denken',
    takeaway5: 'Das Verständnis der Trainingspipeline hilft, Modellverhalten und -einschränkungen zu verstehen',
    takeaway6: 'Das Feld entwickelt sich schnell weiter – neue Paradigmen wie reines RL verändern, wie wir über Training nachdenken',
  },

  bias: {
    title: 'Bias & Fairness',
    description: 'Verstehen und Mindern schädlicher Biases in KI-Systemen.',
    whatIs: 'Was ist KI-Bias?',
    whatIsDesc: 'KI-Bias tritt auf, wenn maschinelle Lernsysteme systematisch unfaire Ergebnisse für bestimmte Gruppen produzieren. Biases können aus Trainingsdaten, Modelldesign oder dem Einsatzkontext entstehen.',
    sources: 'Quellen von Bias',
    sourcesDesc: 'Wo Bias in KI-Systeme eindringt.',
    dataBias: 'Trainingsdaten',
    dataBiasDesc: 'Historische Biases in den Daten werden vom Modell gelernt.',
    labelBias: 'Label-Bias',
    labelBiasDesc: 'Menschliche Annotatoren führen ihre eigenen Biases ein.',
    selectionBias: 'Selektions-Bias',
    selectionBiasDesc: 'Trainingsdaten repräsentieren nicht die Einsatzpopulation.',
    measurementBias: 'Mess-Bias',
    measurementBiasDesc: 'Proxies, die zur Messung verwendet werden, kodieren Bias.',
    types: 'Arten von Bias',
    typesDesc: 'Häufige Kategorien von Bias in KI-Systemen.',
    stereotyping: 'Stereotypisierung',
    stereotypingDesc: 'Verstärkung schädlicher Stereotypen über Gruppen.',
    erasure: 'Auslöschung',
    erasureDesc: 'Unterrepräsentation oder Ignorieren bestimmter Gruppen.',
    disparateImpact: 'Unterschiedliche Auswirkung',
    disparateImpactDesc: 'Verschiedene Ergebnisse für verschiedene Gruppen.',
    mitigation: 'Minderungsstrategien',
    mitigationDesc: 'Ansätze zur Reduzierung von Bias.',
    diverseData: 'Diverse Daten',
    diverseDataDesc: 'Sicherstellen, dass Trainingsdaten alle relevanten Gruppen repräsentieren.',
    auditing: 'Bias-Audit',
    auditingDesc: 'Systematisch auf Bias über demografische Gruppen hinweg testen.',
    constraints: 'Fairness-Einschränkungen',
    constraintsDesc: 'Fairness-Metriken in das Training einbeziehen.',
    interactiveDemo: 'Bias-Erkennungs-Demo',
    demoDesc: 'Erkunde, wie sich Bias in Modellausgaben manifestiert',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bias wird oft von Trainingsdaten geerbt',
    takeaway2: 'Verschiedene Fairness-Metriken können in Konflikt stehen – wähle sorgfältig',
    takeaway3: 'Regelmäßiges Auditing ist essentiell für eingesetzte Systeme',
    takeaway4: 'Bias-Minderung ist ein fortlaufender Prozess, keine einmalige Lösung',
  },

  responsibleAi: {
    title: 'Verantwortungsvolle KI',
    description: 'KI-Systeme ethisch und nachhaltig entwickeln und einsetzen.',
    whatIs: 'Was ist verantwortungsvolle KI?',
    whatIsDesc: 'Verantwortungsvolle KI umfasst die Praktiken, Richtlinien und Prinzipien, die sicherstellen, dass KI-Systeme ethisch, sicher und zum Nutzen der Gesellschaft entwickelt und eingesetzt werden.',
    pillars: 'Säulen verantwortungsvoller KI',
    pillarsDesc: 'Kernprinzipien, die die verantwortungsvolle KI-Entwicklung leiten.',
    transparency: 'Transparenz',
    transparencyDesc: 'Offen sein über KI-Fähigkeiten, Einschränkungen und Entscheidungsfindung.',
    accountability: 'Verantwortlichkeit',
    accountabilityDesc: 'Klare Eigentümerschaft und Verantwortung für KI-Ergebnisse.',
    privacy: 'Datenschutz',
    privacyDesc: 'Benutzerdaten schützen und Datenschutzrechte respektieren.',
    safety: 'Sicherheit',
    safetyDesc: 'Sicherstellen, dass Systeme robust sind und keinen Schaden anrichten.',
    practices: 'Verantwortungsvolle Praktiken',
    practicesDesc: 'Konkrete Schritte für verantwortungsvolle KI-Entwicklung.',
    documentation: 'Dokumentation',
    documentationDesc: 'Modellfähigkeiten, Trainingsdaten und bekannte Einschränkungen dokumentieren.',
    testing: 'Umfassende Tests',
    testingDesc: 'Vor der Bereitstellung auf Sicherheit, Bias und Randfälle testen.',
    monitoring: 'Fortlaufende Überwachung',
    monitoringDesc: 'Systemverhalten in der Produktion auf Probleme überwachen.',
    feedback: 'Benutzer-Feedback',
    feedbackDesc: 'Kanäle für Benutzer schaffen, um Probleme zu melden.',
    considerations: 'Ethische Überlegungen',
    environmental: 'Umweltauswirkungen',
    environmentalDesc: 'KI-Training hat einen erheblichen CO2-Fußabdruck.',
    labor: 'Arbeitsmarkt-Auswirkungen',
    laborDesc: 'Auswirkungen auf Arbeitnehmer und Beschäftigung berücksichtigen.',
    access: 'Gerechter Zugang',
    accessDesc: 'Sicherstellen, dass KI-Vorteile breit verteilt werden.',
    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Verantwortungsvolle KI erfordert proaktiven Einsatz während des gesamten Lebenszyklus',
    takeaway2: 'Transparenz baut Vertrauen auf und ermöglicht Verantwortlichkeit',
    takeaway3: 'Gesellschaftliche Auswirkungen über unmittelbare Benutzer hinaus berücksichtigen',
    takeaway4: 'Ethik ist nicht optional – integriere sie in Entwicklungsprozesse',
  },

  // Visual Challenges page (expanded)
  visualChallenges: {
    title: 'Visuelle Herausforderungen',
    description: 'Häufige Herausforderungen und Einschränkungen bei der Arbeit mit bildverarbeitungsfähigen KI-Modellen.',
    overview: 'Häufige visuelle Herausforderungen',
    overviewDesc: 'Obwohl Bildmodelle beeindruckend sind, stehen sie vor mehreren systematischen Herausforderungen, die beim Erstellen von Anwendungen wichtig zu verstehen sind. Diese Einschränkungen entstehen dadurch, wie Bildmodelle Bilder verarbeiten – durch Patches, Einbettungen und Aufmerksamkeit – und nicht so, wie Menschen visuelle Informationen wahrnehmen.',

    // Challenge 1: Counting
    challenge1: 'Objekte zählen',
    challenge1Desc: 'Modelle haben oft Schwierigkeiten, Objekte in Bildern genau zu zählen, besonders wenn es viele ähnliche Elemente gibt.',
    challenge1Why: 'Warum das passiert',
    challenge1WhyDesc: 'Bildmodelle verarbeiten Bilder als Patches (typischerweise 14x14 oder 16x16 Pixel), nicht als diskrete Objekte. Ihnen fehlt das eingebaute Konzept der "Objektpermanenz" und sie haben Schwierigkeiten, genaue Zählungen über überlappende oder dichte Anordnungen aufrechtzuerhalten.',
    challenge1Examples: 'Häufige Fehler',
    challenge1Example1: 'Menschen in einer Menge zählen (oft 20-50% daneben)',
    challenge1Example2: 'Elemente in einem Raster oder Array zählen',
    challenge1Example3: 'Zwischen "wenig" und "viele" unterscheiden, wenn Elemente überlappen',
    challenge1Mitigation: 'Workarounds',
    challenge1MitigationDesc: 'Für kritische Zählaufgaben erwäge spezialisierte Objekterkennungsmodelle (YOLO, Faster R-CNN) oder bitte das Modell, jeden Gegenstand einzeln zu identifizieren und zu beschreiben, anstatt eine Gesamtzahl anzugeben.',

    // Challenge 2: Spatial Reasoning
    challenge2: 'Räumliches Denken',
    challenge2Desc: 'Das Verstehen präziser räumlicher Beziehungen zwischen Objekten (links/rechts, oben/unten) kann unzuverlässig sein.',
    challenge2Why: 'Warum das passiert',
    challenge2WhyDesc: 'Positionsinformationen werden durch Patch-Positionseinbettungen kodiert, aber diese bieten keine Pixel-genaue Präzision. Das Modell lernt statistische Korrelationen zwischen Positionen statt explizites räumliches Denken.',
    challenge2Examples: 'Häufige Fehler',
    challenge2Example1: 'Links/Rechts-Beziehungen in gespiegelten oder symmetrischen Bildern verwechseln',
    challenge2Example2: 'Relative Entfernungen falsch einschätzen ("näher an" oder "weiter von")',
    challenge2Example3: 'Schwierigkeiten mit gedrehten oder ungewöhnlichen Orientierungen',
    challenge2Mitigation: 'Workarounds',
    challenge2MitigationDesc: 'Sei explizit in deinen Prompts, welchen Bezugsrahmen du verwendest. Erwäge, Bilder mit visuellen Markern oder Rastern für kritische räumliche Aufgaben zu annotieren.',

    // Challenge 3: Small Text Recognition
    challenge3: 'Kleine Texterkennung',
    challenge3Desc: 'Feiner Text in Bildern kann falsch gelesen oder ganz übersehen werden, besonders bei niedrigen Auflösungen.',
    challenge3Why: 'Warum das passiert',
    challenge3WhyDesc: 'Text kleiner als die Patch-Größe (14-16 Pixel) wird in eine einzelne Einbettung komprimiert, wobei Details auf Zeichenebene verloren gehen. OCR ist nicht in Bild-LLMs eingebaut – sie lernen Texterkennung als Nebenprodukt des Trainings, nicht als dedizierte Fähigkeit.',
    challenge3Examples: 'Häufige Fehler',
    challenge3Example1: 'Nummernschilder, Straßenschilder oder kleine Etiketten falsch lesen',
    challenge3Example2: 'Ähnliche Zeichen verwechseln (0/O, 1/l/I, 5/S)',
    challenge3Example3: 'Text in geschäftigen oder kontrastarmen Hintergründen übersehen',
    challenge3Mitigation: 'Workarounds',
    challenge3MitigationDesc: 'Verwende hochauflösende Bilder und zoome in Textbereiche. Für kritische OCR-Aufgaben verwende dedizierte OCR-Tools (Tesseract, Google Vision API, Amazon Textract) neben oder anstelle von Bild-LLMs.',

    // Challenge 4: Hallucination
    challenge4: 'Visuelle Halluzination',
    challenge4Desc: 'Modelle können Objekte oder Details beschreiben, die nicht wirklich im Bild vorhanden sind.',
    challenge4Why: 'Warum das passiert',
    challenge4WhyDesc: 'Bild-LLMs sind darauf trainiert, plausible Beschreibungen zu generieren. Wenn Bildmerkmale mehrdeutig sind, füllt das Modell Lücken mit statistisch wahrscheinlichem Inhalt – auch wenn dieser Inhalt nicht im Bild ist. Dies ist derselbe Mechanismus, der Text-Halluzinationen verursacht.',
    challenge4Examples: 'Häufige Fehler',
    challenge4Example1: 'Objekte hinzufügen, die in einer Szene "sein sollten" (eine Tastatur neben einem Monitor)',
    challenge4Example2: 'Markennamen oder Text beschreiben, der nicht sichtbar ist',
    challenge4Example3: 'Details erfinden, wenn nach unklaren Bereichen gefragt wird',
    challenge4Mitigation: 'Workarounds',
    challenge4MitigationDesc: 'Bitte das Modell, Unsicherheit auszudrücken. Verwende Prompts wie "beschreibe nur, was du klar sehen kannst" oder "wenn du X nicht bestimmen kannst, sage es". Kritische Details gegenchecken.',

    // Challenge 5: Fine Detail Recognition
    challenge5: 'Feine Detailerkennung',
    challenge5Desc: 'Subtile Details, Texturen oder kleine unterscheidende Merkmale werden oft übersehen oder falsch identifiziert.',
    challenge5Why: 'Warum das passiert',
    challenge5WhyDesc: 'Die patch-basierte Architektur mittelt Informationen innerhalb jedes Patches und verliert dabei feinkörnige Details. Hochfrequente visuelle Informationen (Kanten, Texturen, kleine Merkmale) werden komprimiert.',
    challenge5Examples: 'Häufige Fehler',
    challenge5Example1: 'Zwischen ähnlichen Objekten unterscheiden (Hunderassen, Automodelle)',
    challenge5Example2: 'Messgeräte, Zähler oder Instrumentenanzeigen ablesen',
    challenge5Example3: 'Subtile Schäden oder Defekte bei Inspektionsaufgaben identifizieren',
    challenge5Mitigation: 'Workarounds',
    challenge5MitigationDesc: 'Verwende die höchste verfügbare Auflösung. Schneide zu und fokussiere auf spezifische Interessenbereiche. Für spezialisierte Aufgaben erwäge feingetunete Modelle, die auf domänenspezifischen Daten trainiert wurden.',

    // Challenge 6: Multi-Image Reasoning
    challenge6: 'Multi-Bild-Denken',
    challenge6Desc: 'Vergleichen oder Denken über mehrere Bilder hinweg ist deutlich schwieriger als Einzelbild-Aufgaben.',
    challenge6Why: 'Warum das passiert',
    challenge6WhyDesc: 'Jedes Bild wird separat in Token-Sequenzen kodiert. Cross-Image-Aufmerksamkeit muss durch das Kontextfenster des Sprachmodells erfolgen, was weniger effizient ist als dedizierte Multi-Bild-Architekturen.',
    challenge6Examples: 'Häufige Fehler',
    challenge6Example1: 'Unterschiede zwischen zwei ähnlichen Bildern finden ("Finde den Unterschied")',
    challenge6Example2: 'Objektidentität über Frames hinweg verfolgen',
    challenge6Example3: 'Feine Details zwischen Produktbildern vergleichen',
    challenge6Mitigation: 'Workarounds',
    challenge6MitigationDesc: 'Beschreibe jedes Bild zuerst separat, dann frage nach dem Vergleich. Erwäge, Bilder zu einem einzigen Komposit für direkten Vergleich zu kombinieren.',

    keyTakeaways: 'Wichtige Erkenntnisse',
    takeaway1: 'Bild-LLMs verarbeiten Bilder als Patches – Details unter der Patch-Auflösung gehen verloren',
    takeaway2: 'Zählen und räumliches Denken sind fundamentale Schwächen, keine Randfälle',
    takeaway3: 'Visuelle Halluzination folgt demselben Muster wie Text-Halluzination – plausible Erfindung',
    takeaway4: 'Verwende höhere Auflösung, zugeschnittene Bereiche und explizite Prompts, um die Genauigkeit zu verbessern',
    takeaway5: 'Für kritische Aufgaben kombiniere Bild-LLMs mit spezialisierten Tools (OCR, Objekterkennung)',
    takeaway6: 'Verifiziere wichtige visuelle Informationen immer auf anderen Wegen',
  },
}
